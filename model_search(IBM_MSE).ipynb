{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "C:\\ProgramData\\Anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\ProgramData\\Anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\ProgramData\\Anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\ProgramData\\Anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:529: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\ProgramData\\Anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:530: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\ProgramData\\Anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:535: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import preprocessing\n",
    "from joblib import dump\n",
    "\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, LSTM\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from tensorflow import set_random_seed\n",
    "set_random_seed(12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('./datasets/IBM_daily.csv').sort_values(by='date')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>1. open</th>\n",
       "      <th>2. high</th>\n",
       "      <th>3. low</th>\n",
       "      <th>4. close</th>\n",
       "      <th>5. volume</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5217</th>\n",
       "      <td>1999-11-01</td>\n",
       "      <td>98.50</td>\n",
       "      <td>98.8100</td>\n",
       "      <td>96.37</td>\n",
       "      <td>96.75</td>\n",
       "      <td>9551800.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5216</th>\n",
       "      <td>1999-11-02</td>\n",
       "      <td>96.75</td>\n",
       "      <td>96.8100</td>\n",
       "      <td>93.69</td>\n",
       "      <td>94.81</td>\n",
       "      <td>11105400.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5215</th>\n",
       "      <td>1999-11-03</td>\n",
       "      <td>95.87</td>\n",
       "      <td>95.9400</td>\n",
       "      <td>93.50</td>\n",
       "      <td>94.37</td>\n",
       "      <td>10369100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5214</th>\n",
       "      <td>1999-11-04</td>\n",
       "      <td>94.44</td>\n",
       "      <td>94.4400</td>\n",
       "      <td>90.00</td>\n",
       "      <td>91.56</td>\n",
       "      <td>16697600.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5213</th>\n",
       "      <td>1999-11-05</td>\n",
       "      <td>92.75</td>\n",
       "      <td>92.9400</td>\n",
       "      <td>90.19</td>\n",
       "      <td>90.25</td>\n",
       "      <td>13737600.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2020-07-22</td>\n",
       "      <td>125.90</td>\n",
       "      <td>129.4700</td>\n",
       "      <td>125.80</td>\n",
       "      <td>128.67</td>\n",
       "      <td>8195366.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2020-07-23</td>\n",
       "      <td>129.10</td>\n",
       "      <td>129.3700</td>\n",
       "      <td>127.15</td>\n",
       "      <td>127.33</td>\n",
       "      <td>4220136.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2020-07-24</td>\n",
       "      <td>126.48</td>\n",
       "      <td>127.6459</td>\n",
       "      <td>125.50</td>\n",
       "      <td>125.79</td>\n",
       "      <td>3531076.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2020-07-27</td>\n",
       "      <td>124.86</td>\n",
       "      <td>126.3200</td>\n",
       "      <td>124.71</td>\n",
       "      <td>126.21</td>\n",
       "      <td>3733547.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2020-07-28</td>\n",
       "      <td>125.82</td>\n",
       "      <td>126.3400</td>\n",
       "      <td>124.15</td>\n",
       "      <td>124.47</td>\n",
       "      <td>4157538.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5218 rows Ã— 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            date  1. open   2. high  3. low  4. close   5. volume\n",
       "5217  1999-11-01    98.50   98.8100   96.37     96.75   9551800.0\n",
       "5216  1999-11-02    96.75   96.8100   93.69     94.81  11105400.0\n",
       "5215  1999-11-03    95.87   95.9400   93.50     94.37  10369100.0\n",
       "5214  1999-11-04    94.44   94.4400   90.00     91.56  16697600.0\n",
       "5213  1999-11-05    92.75   92.9400   90.19     90.25  13737600.0\n",
       "...          ...      ...       ...     ...       ...         ...\n",
       "4     2020-07-22   125.90  129.4700  125.80    128.67   8195366.0\n",
       "3     2020-07-23   129.10  129.3700  127.15    127.33   4220136.0\n",
       "2     2020-07-24   126.48  127.6459  125.50    125.79   3531076.0\n",
       "1     2020-07-27   124.86  126.3200  124.71    126.21   3733547.0\n",
       "0     2020-07-28   125.82  126.3400  124.15    124.47   4157538.0\n",
       "\n",
       "[5218 rows x 6 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1. open</th>\n",
       "      <th>2. high</th>\n",
       "      <th>3. low</th>\n",
       "      <th>4. close</th>\n",
       "      <th>5. volume</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5217</th>\n",
       "      <td>98.50</td>\n",
       "      <td>98.8100</td>\n",
       "      <td>96.37</td>\n",
       "      <td>96.75</td>\n",
       "      <td>9551800.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5216</th>\n",
       "      <td>96.75</td>\n",
       "      <td>96.8100</td>\n",
       "      <td>93.69</td>\n",
       "      <td>94.81</td>\n",
       "      <td>11105400.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5215</th>\n",
       "      <td>95.87</td>\n",
       "      <td>95.9400</td>\n",
       "      <td>93.50</td>\n",
       "      <td>94.37</td>\n",
       "      <td>10369100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5214</th>\n",
       "      <td>94.44</td>\n",
       "      <td>94.4400</td>\n",
       "      <td>90.00</td>\n",
       "      <td>91.56</td>\n",
       "      <td>16697600.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5213</th>\n",
       "      <td>92.75</td>\n",
       "      <td>92.9400</td>\n",
       "      <td>90.19</td>\n",
       "      <td>90.25</td>\n",
       "      <td>13737600.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>125.90</td>\n",
       "      <td>129.4700</td>\n",
       "      <td>125.80</td>\n",
       "      <td>128.67</td>\n",
       "      <td>8195366.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>129.10</td>\n",
       "      <td>129.3700</td>\n",
       "      <td>127.15</td>\n",
       "      <td>127.33</td>\n",
       "      <td>4220136.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>126.48</td>\n",
       "      <td>127.6459</td>\n",
       "      <td>125.50</td>\n",
       "      <td>125.79</td>\n",
       "      <td>3531076.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>124.86</td>\n",
       "      <td>126.3200</td>\n",
       "      <td>124.71</td>\n",
       "      <td>126.21</td>\n",
       "      <td>3733547.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>125.82</td>\n",
       "      <td>126.3400</td>\n",
       "      <td>124.15</td>\n",
       "      <td>124.47</td>\n",
       "      <td>4157538.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5218 rows Ã— 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      1. open   2. high  3. low  4. close   5. volume\n",
       "5217    98.50   98.8100   96.37     96.75   9551800.0\n",
       "5216    96.75   96.8100   93.69     94.81  11105400.0\n",
       "5215    95.87   95.9400   93.50     94.37  10369100.0\n",
       "5214    94.44   94.4400   90.00     91.56  16697600.0\n",
       "5213    92.75   92.9400   90.19     90.25  13737600.0\n",
       "...       ...       ...     ...       ...         ...\n",
       "4      125.90  129.4700  125.80    128.67   8195366.0\n",
       "3      129.10  129.3700  127.15    127.33   4220136.0\n",
       "2      126.48  127.6459  125.50    125.79   3531076.0\n",
       "1      124.86  126.3200  124.71    126.21   3733547.0\n",
       "0      125.82  126.3400  124.15    124.47   4157538.0\n",
       "\n",
       "[5218 rows x 5 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = data.drop(columns='date')\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sequence for the LSTM network\n",
    "backlook = 92\n",
    "\n",
    "# Size of data split for testing\n",
    "train_size = 0.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train-Test split\n",
    "split = int(historical_sequences_norm.shape[0] * train_size)\n",
    "\n",
    "train = data.to_numpy()[:split]\n",
    "test = data.to_numpy()[split:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalise data\n",
    "normaliser = preprocessing.MinMaxScaler()\n",
    "train_norm = normaliser.fit_transform(train)\n",
    "test_norm = normaliser.transform(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalised train chunks\n",
    "X_train = np.array([train_norm[i : i + backlook].copy() for i in range(len(train) - backlook)])\n",
    "Y_train = np.array([train_norm[:,0][i + backlook].copy() for i in range(len(train) - backlook)])\n",
    "Y_train = np.expand_dims(Y_train, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalised test chunks\n",
    "X_test = np.array([test_norm[i : i + backlook].copy() for i in range(len(test) - backlook)])\n",
    "Y_test = np.array([test_norm[:,0][i + backlook].copy() for i in range(len(test) - backlook)])\n",
    "Y_test = np.expand_dims(Y_test, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.3303677 ],\n",
       "       [0.32881229],\n",
       "       [0.33814471],\n",
       "       ...,\n",
       "       [0.44689853],\n",
       "       [0.43681951],\n",
       "       [0.44279226]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Y raw data\n",
    "next_day_open_values = np.array([train[:,0][i + backlook].copy() for i in range(len(train) - backlook)])\n",
    "next_day_open_values = np.expand_dims(next_day_open_values, -1)\n",
    "\n",
    "# Y normaliser\n",
    "y_normaliser = preprocessing.MinMaxScaler()\n",
    "y_normaliser.fit_transform(next_day_open_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save scalers for future use\n",
    "dump(normaliser, './normalisers/x_normaliser.joblib')\n",
    "dump(y_normaliser, './normalisers/y_normaliser.joblib')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Model constructor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_lstm(x, y, batch_size=512, epochs=24, **params):\n",
    "    \n",
    "    # List of parameters\n",
    "    if 'lstmsize' not in params: params['lstmsize'] = x.shape[1]\n",
    "    if 'density' not in params: params['density'] = int((params['lstmsize']//1.5)*2)\n",
    "    if 'activation' not in params: params['activation'] = 'relu'\n",
    "    if 'twice' not in params: params['twice'] = False\n",
    "    if 'optimizer' not in params: params['optimizer'] = 'adam'\n",
    "    if 'shuffle' not in params: params['shuffle'] = False\n",
    "    \n",
    "    # Model definition\n",
    "    model = Sequential()\n",
    "    \n",
    "    model.add(LSTM(params['lstmsize'], input_shape=x.shape[1:], return_sequences=params['twice']))\n",
    "    \n",
    "    if 'dropout' in params:\n",
    "        model.add(Dropout(params['dropout']))\n",
    "    \n",
    "    if params['twice']:\n",
    "        model.add(LSTM(params['lstmsize']))\n",
    "        \n",
    "        if 'dropout' in params:\n",
    "            model.add(Dropout(params['dropout']))\n",
    "            \n",
    "    model.add(Dense(params['density'], activation=params['activation']))\n",
    "    \n",
    "    if 'full_density' in params and params['full_density']:\n",
    "        density = params['density']//2\n",
    "        while density >= 12:\n",
    "            model.add(Dense(density, activation=params['activation']))\n",
    "            density //= 2\n",
    "            \n",
    "    model.add(Dense(1, activation='linear'))\n",
    "    \n",
    "    model.compile(loss='mse', optimizer=params['optimizer'])\n",
    "    \n",
    "    if 'callbacks' in params:\n",
    "        callback = model.fit(x=x, y=y, validation_split=0.1, batch_size=batch_size, epochs=epochs, shuffle=params['shuffle'],\n",
    "                            callbacks=params['callbacks'])\n",
    "    else:\n",
    "        callback = model.fit(x=x, y=y, validation_split=0.1, batch_size=batch_size, epochs=epochs, shuffle=params['shuffle'])\n",
    "    \n",
    "    return [model, callback, params]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Evolutive algorith to search for the most optimal model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_genes(x, y, population_size, population=[]):\n",
    "    if population_size-len(population) < 0:\n",
    "        print('population size must be greater or equal than actual population')\n",
    "        return\n",
    "    \n",
    "    if len(population) > 0:\n",
    "        if len(population) < 3:\n",
    "            print('population should be empty or 3 at least')\n",
    "            return\n",
    "        \n",
    "        population[2] = breed_genes(x, y, population[0][2].copy())\n",
    "        population[1] = breed_genes(x, y, combine=(population[0][2], population[1][2]))\n",
    "        population = population[:3]\n",
    "    \n",
    "    for _ in range(population_size-len(population)):\n",
    "        subject = breed_genes(x, y)\n",
    "        population.append(subject)\n",
    "        \n",
    "    return population\n",
    "\n",
    "def breed_genes(x, y, genes={}, combine=None):\n",
    "    genes['x'] = x\n",
    "    genes['y'] = y\n",
    "    \n",
    "    if type(combine) is list or type(combine) is tuple:\n",
    "        genes['lstmsize'] = combine[np.random.randint(0,2)]['lstmsize']\n",
    "        genes['density'] = combine[np.random.randint(0,2)]['density']\n",
    "        genes['activation'] = combine[np.random.randint(0,2)]['activation']\n",
    "        genes['twice'] = combine[np.random.randint(0,2)]['twice']\n",
    "        genes['optimizer'] = combine[np.random.randint(0,2)]['optimizer']\n",
    "        genes['shuffle'] = combine[np.random.randint(0,2)]['shuffle']\n",
    "        \n",
    "        genes['dropout'] = combine[np.random.randint(0,2)].get('dropout')\n",
    "        if genes['dropout'] is None: del genes['dropout']\n",
    "        genes['full_density'] = combine[np.random.randint(0,2)].get('full_density')\n",
    "        if genes['full_density'] is None: del genes['full_density']\n",
    "            \n",
    "    else:\n",
    "        if np.random.randint(0,3) == 1:\n",
    "            genes['lstmsize'] = int((np.random.randint(x.shape[1],x.shape[1]*2)//2)*2)\n",
    "\n",
    "        if np.random.randint(0,3) == 1:\n",
    "            genes['density'] = int((np.random.randint(x.shape[1]//2,x.shape[1]*2.66)//2)*2)\n",
    "\n",
    "        if np.random.randint(0,3) == 1:\n",
    "            activation = [\n",
    "                'relu',\n",
    "                'sigmoid',\n",
    "                'softplus',\n",
    "                'softsign',\n",
    "                'tanh',\n",
    "                'selu',\n",
    "                'elu',\n",
    "                'exponential'\n",
    "            ]\n",
    "            genes['activation'] = activation[np.random.randint(0,8)]\n",
    "\n",
    "        if np.random.randint(0,3) == 1:\n",
    "            genes['twice'] = True\n",
    "\n",
    "        if np.random.randint(0,3) == 1:\n",
    "            optimizer = [\n",
    "                'sgd',\n",
    "                'rmsprop',\n",
    "                'adam',\n",
    "                'adadelta',\n",
    "                'adagrad',\n",
    "                'adamax',\n",
    "                'nadam'\n",
    "            ]\n",
    "            genes['optimizer'] = optimizer[np.random.randint(0,7)]\n",
    "\n",
    "        if np.random.randint(0,3) == 1:\n",
    "            genes['shuffle'] = True\n",
    "\n",
    "        if np.random.randint(0,3) == 1:\n",
    "            genes['dropout'] = np.random.randint(1,4)/10\n",
    "\n",
    "        if np.random.randint(0,3) == 1:\n",
    "            genes['full_density'] = True\n",
    "            \n",
    "    new_model = build_lstm(**genes)\n",
    "        \n",
    "    return new_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def breed_population(x, y, generations, population_size, population=[]):\n",
    "    \n",
    "    if generations > 1 and population_size < 3:\n",
    "        print('population size should be of a minimum of 3 for more than one generation')\n",
    "        return\n",
    "    \n",
    "    for g in range(generations):\n",
    "        print(f'\\nGENERATION {g}\\n')\n",
    "        population = set_genes(x, y, population_size, population)\n",
    "        population = sorted(population, key=lambda x: x[1].history['val_loss'][-1]+x[1].history['loss'][-1])\n",
    "        \n",
    "    return population"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "GENERATION 0\n",
      "\n",
      "WARNING:tensorflow:From C:\\ProgramData\\Anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\tensorflow\\python\\ops\\resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From C:\\ProgramData\\Anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Train on 3690 samples, validate on 410 samples\n",
      "Epoch 1/24\n",
      "3690/3690 [==============================] - 2s 408us/step - loss: 0.1251 - val_loss: 0.0017\n",
      "Epoch 2/24\n",
      "3690/3690 [==============================] - 1s 159us/step - loss: 0.0058 - val_loss: 0.0020\n",
      "Epoch 3/24\n",
      "3690/3690 [==============================] - 1s 159us/step - loss: 0.0032 - val_loss: 0.0031\n",
      "Epoch 4/24\n",
      "3690/3690 [==============================] - 1s 162us/step - loss: 0.0037 - val_loss: 6.2091e-04\n",
      "Epoch 5/24\n",
      "3690/3690 [==============================] - 1s 160us/step - loss: 0.0021 - val_loss: 0.0013\n",
      "Epoch 6/24\n",
      "3690/3690 [==============================] - 1s 159us/step - loss: 0.0035 - val_loss: 0.0018\n",
      "Epoch 7/24\n",
      "3690/3690 [==============================] - 1s 161us/step - loss: 0.0025 - val_loss: 0.0063\n",
      "Epoch 8/24\n",
      "3690/3690 [==============================] - 1s 159us/step - loss: 0.0045 - val_loss: 0.0138\n",
      "Epoch 9/24\n",
      "3690/3690 [==============================] - 1s 161us/step - loss: 0.0045 - val_loss: 0.0023\n",
      "Epoch 10/24\n",
      "3690/3690 [==============================] - 1s 159us/step - loss: 0.0025 - val_loss: 0.0042\n",
      "Epoch 11/24\n",
      "3690/3690 [==============================] - 1s 160us/step - loss: 0.0029 - val_loss: 0.0035\n",
      "Epoch 12/24\n",
      "3690/3690 [==============================] - 1s 162us/step - loss: 0.0037 - val_loss: 0.0036\n",
      "Epoch 13/24\n",
      "3690/3690 [==============================] - 1s 159us/step - loss: 0.0024 - val_loss: 0.0011\n",
      "Epoch 14/24\n",
      "3690/3690 [==============================] - 1s 158us/step - loss: 0.0016 - val_loss: 0.0035\n",
      "Epoch 15/24\n",
      "3690/3690 [==============================] - 1s 161us/step - loss: 0.0021 - val_loss: 0.0047\n",
      "Epoch 16/24\n",
      "3690/3690 [==============================] - 1s 167us/step - loss: 0.0033 - val_loss: 0.0049\n",
      "Epoch 17/24\n",
      "3690/3690 [==============================] - 1s 164us/step - loss: 0.0026 - val_loss: 0.0061\n",
      "Epoch 18/24\n",
      "3690/3690 [==============================] - 1s 160us/step - loss: 0.0031 - val_loss: 0.0015\n",
      "Epoch 19/24\n",
      "3690/3690 [==============================] - 1s 160us/step - loss: 0.0025 - val_loss: 0.0065\n",
      "Epoch 20/24\n",
      "3690/3690 [==============================] - 1s 162us/step - loss: 0.0027 - val_loss: 0.0040\n",
      "Epoch 21/24\n",
      "3690/3690 [==============================] - 1s 162us/step - loss: 0.0022 - val_loss: 0.0033\n",
      "Epoch 22/24\n",
      "3690/3690 [==============================] - 1s 159us/step - loss: 0.0025 - val_loss: 0.0011\n",
      "Epoch 23/24\n",
      "3690/3690 [==============================] - 1s 165us/step - loss: 0.0014 - val_loss: 0.0021\n",
      "Epoch 24/24\n",
      "3690/3690 [==============================] - 1s 161us/step - loss: 0.0020 - val_loss: 0.0044\n",
      "Train on 3690 samples, validate on 410 samples\n",
      "Epoch 1/24\n",
      "3690/3690 [==============================] - 2s 540us/step - loss: 0.1134 - val_loss: 0.0133\n",
      "Epoch 2/24\n",
      "3690/3690 [==============================] - 1s 331us/step - loss: 0.0169 - val_loss: 0.0181\n",
      "Epoch 3/24\n",
      "3690/3690 [==============================] - 1s 332us/step - loss: 0.0151 - val_loss: 0.0122\n",
      "Epoch 4/24\n",
      "3690/3690 [==============================] - 1s 332us/step - loss: 0.0098 - val_loss: 0.0127\n",
      "Epoch 5/24\n",
      "3690/3690 [==============================] - 1s 348us/step - loss: 0.0081 - val_loss: 0.0020\n",
      "Epoch 6/24\n",
      "3690/3690 [==============================] - 1s 351us/step - loss: 0.0053 - val_loss: 0.0080\n",
      "Epoch 7/24\n",
      "3690/3690 [==============================] - 1s 346us/step - loss: 0.0070 - val_loss: 0.0054\n",
      "Epoch 8/24\n",
      "3690/3690 [==============================] - 1s 339us/step - loss: 0.0066 - val_loss: 0.0065\n",
      "Epoch 9/24\n",
      "3690/3690 [==============================] - 1s 343us/step - loss: 0.0074 - val_loss: 0.0059\n",
      "Epoch 10/24\n",
      "3690/3690 [==============================] - 1s 339us/step - loss: 0.0043 - val_loss: 8.7176e-04\n",
      "Epoch 11/24\n",
      "3690/3690 [==============================] - 1s 348us/step - loss: 0.0028 - val_loss: 0.0039\n",
      "Epoch 12/24\n",
      "3690/3690 [==============================] - 1s 334us/step - loss: 0.0058 - val_loss: 0.0111\n",
      "Epoch 13/24\n",
      "3690/3690 [==============================] - 1s 336us/step - loss: 0.0050 - val_loss: 0.0047\n",
      "Epoch 14/24\n",
      "3690/3690 [==============================] - 1s 336us/step - loss: 0.0050 - val_loss: 0.0024\n",
      "Epoch 15/24\n",
      "3690/3690 [==============================] - 1s 343us/step - loss: 0.0046 - val_loss: 0.0077\n",
      "Epoch 16/24\n",
      "3690/3690 [==============================] - 1s 337us/step - loss: 0.0050 - val_loss: 0.0083\n",
      "Epoch 17/24\n",
      "3690/3690 [==============================] - 1s 331us/step - loss: 0.0065 - val_loss: 0.0012\n",
      "Epoch 18/24\n",
      "3690/3690 [==============================] - 1s 336us/step - loss: 0.0019 - val_loss: 0.0011\n",
      "Epoch 19/24\n",
      "3690/3690 [==============================] - 1s 344us/step - loss: 0.0018 - val_loss: 0.0017\n",
      "Epoch 20/24\n",
      "3690/3690 [==============================] - 1s 357us/step - loss: 0.0028 - val_loss: 0.0072\n",
      "Epoch 21/24\n",
      "3690/3690 [==============================] - 1s 351us/step - loss: 0.0048 - val_loss: 0.0033\n",
      "Epoch 22/24\n",
      "3690/3690 [==============================] - 1s 336us/step - loss: 0.0032 - val_loss: 0.0052\n",
      "Epoch 23/24\n",
      "3690/3690 [==============================] - 1s 335us/step - loss: 0.0055 - val_loss: 0.0036\n",
      "Epoch 24/24\n",
      "3690/3690 [==============================] - 1s 334us/step - loss: 0.0038 - val_loss: 0.0045\n",
      "Train on 3690 samples, validate on 410 samples\n",
      "Epoch 1/24\n",
      "3690/3690 [==============================] - 2s 520us/step - loss: 0.1817 - val_loss: 0.1757\n",
      "Epoch 2/24\n",
      "3690/3690 [==============================] - 1s 330us/step - loss: 0.1020 - val_loss: 0.0942\n",
      "Epoch 3/24\n",
      "3690/3690 [==============================] - 1s 333us/step - loss: 0.0703 - val_loss: 0.0541\n",
      "Epoch 4/24\n",
      "3690/3690 [==============================] - 1s 352us/step - loss: 0.0578 - val_loss: 0.0349\n",
      "Epoch 5/24\n",
      "3690/3690 [==============================] - 1s 348us/step - loss: 0.0525 - val_loss: 0.0272\n",
      "Epoch 6/24\n",
      "3690/3690 [==============================] - 1s 334us/step - loss: 0.0490 - val_loss: 0.0242\n",
      "Epoch 7/24\n",
      "3690/3690 [==============================] - 1s 348us/step - loss: 0.0466 - val_loss: 0.0217\n",
      "Epoch 8/24\n",
      "3690/3690 [==============================] - 1s 345us/step - loss: 0.0445 - val_loss: 0.0217\n",
      "Epoch 9/24\n",
      "3690/3690 [==============================] - 1s 344us/step - loss: 0.0427 - val_loss: 0.0204\n",
      "Epoch 10/24\n",
      "3690/3690 [==============================] - 1s 343us/step - loss: 0.0406 - val_loss: 0.0180\n",
      "Epoch 11/24\n",
      "3690/3690 [==============================] - 1s 343us/step - loss: 0.0383 - val_loss: 0.0170\n",
      "Epoch 12/24\n",
      "3690/3690 [==============================] - 1s 331us/step - loss: 0.0362 - val_loss: 0.0152\n",
      "Epoch 13/24\n",
      "3690/3690 [==============================] - 1s 343us/step - loss: 0.0343 - val_loss: 0.0141\n",
      "Epoch 14/24\n",
      "3690/3690 [==============================] - 1s 334us/step - loss: 0.0318 - val_loss: 0.0125\n",
      "Epoch 15/24\n",
      "3690/3690 [==============================] - 1s 332us/step - loss: 0.0299 - val_loss: 0.0117\n",
      "Epoch 16/24\n",
      "3690/3690 [==============================] - 1s 331us/step - loss: 0.0275 - val_loss: 0.0106\n",
      "Epoch 17/24\n",
      "3690/3690 [==============================] - 1s 330us/step - loss: 0.0249 - val_loss: 0.0085\n",
      "Epoch 18/24\n",
      "3690/3690 [==============================] - 1s 330us/step - loss: 0.0224 - val_loss: 0.0079\n",
      "Epoch 19/24\n",
      "3690/3690 [==============================] - 1s 331us/step - loss: 0.0201 - val_loss: 0.0070\n",
      "Epoch 20/24\n",
      "3690/3690 [==============================] - 1s 332us/step - loss: 0.0183 - val_loss: 0.0060\n",
      "Epoch 21/24\n",
      "3690/3690 [==============================] - 1s 349us/step - loss: 0.0163 - val_loss: 0.0044\n",
      "Epoch 22/24\n",
      "3690/3690 [==============================] - 1s 339us/step - loss: 0.0143 - val_loss: 0.0038\n",
      "Epoch 23/24\n",
      "3690/3690 [==============================] - 1s 336us/step - loss: 0.0128 - val_loss: 0.0030\n",
      "Epoch 24/24\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3690/3690 [==============================] - 1s 339us/step - loss: 0.0111 - val_loss: 0.0024\n",
      "Train on 3690 samples, validate on 410 samples\n",
      "Epoch 1/24\n",
      "3690/3690 [==============================] - 2s 540us/step - loss: 0.0387 - val_loss: 0.0065\n",
      "Epoch 2/24\n",
      "3690/3690 [==============================] - 1s 343us/step - loss: 0.0164 - val_loss: 0.0040\n",
      "Epoch 3/24\n",
      "3690/3690 [==============================] - 1s 356us/step - loss: 0.0122 - val_loss: 0.0015\n",
      "Epoch 4/24\n",
      "3690/3690 [==============================] - 1s 364us/step - loss: 0.0084 - val_loss: 0.0011\n",
      "Epoch 5/24\n",
      "3690/3690 [==============================] - 1s 331us/step - loss: 0.0068 - val_loss: 7.8017e-04\n",
      "Epoch 6/24\n",
      "3690/3690 [==============================] - 1s 329us/step - loss: 0.0054 - val_loss: 6.3327e-04\n",
      "Epoch 7/24\n",
      "3690/3690 [==============================] - 1s 328us/step - loss: 0.0051 - val_loss: 7.7232e-04\n",
      "Epoch 8/24\n",
      "3690/3690 [==============================] - 1s 325us/step - loss: 0.0045 - val_loss: 6.5197e-04\n",
      "Epoch 9/24\n",
      "3690/3690 [==============================] - 1s 327us/step - loss: 0.0044 - val_loss: 6.2224e-04\n",
      "Epoch 10/24\n",
      "3690/3690 [==============================] - 1s 326us/step - loss: 0.0041 - val_loss: 0.0011\n",
      "Epoch 11/24\n",
      "3690/3690 [==============================] - 1s 324us/step - loss: 0.0039 - val_loss: 9.3758e-04\n",
      "Epoch 12/24\n",
      "3690/3690 [==============================] - 1s 323us/step - loss: 0.0038 - val_loss: 0.0010\n",
      "Epoch 13/24\n",
      "3690/3690 [==============================] - 1s 323us/step - loss: 0.0036 - val_loss: 0.0013\n",
      "Epoch 14/24\n",
      "3690/3690 [==============================] - 1s 324us/step - loss: 0.0038 - val_loss: 9.5698e-04\n",
      "Epoch 15/24\n",
      "3690/3690 [==============================] - 1s 325us/step - loss: 0.0035 - val_loss: 8.3264e-04\n",
      "Epoch 16/24\n",
      "3690/3690 [==============================] - 1s 325us/step - loss: 0.0036 - val_loss: 9.2087e-04\n",
      "Epoch 17/24\n",
      "3690/3690 [==============================] - 1s 323us/step - loss: 0.0035 - val_loss: 8.8786e-04\n",
      "Epoch 18/24\n",
      "3690/3690 [==============================] - 1s 325us/step - loss: 0.0034 - val_loss: 8.5017e-04\n",
      "Epoch 19/24\n",
      "3690/3690 [==============================] - 1s 325us/step - loss: 0.0034 - val_loss: 0.0010\n",
      "Epoch 20/24\n",
      "3690/3690 [==============================] - 1s 328us/step - loss: 0.0037 - val_loss: 9.1543e-04\n",
      "Epoch 21/24\n",
      "3690/3690 [==============================] - 1s 324us/step - loss: 0.0032 - val_loss: 0.0012\n",
      "Epoch 22/24\n",
      "3690/3690 [==============================] - 1s 325us/step - loss: 0.0030 - val_loss: 7.8125e-04\n",
      "Epoch 23/24\n",
      "3690/3690 [==============================] - 1s 328us/step - loss: 0.0031 - val_loss: 9.3990e-04\n",
      "Epoch 24/24\n",
      "3690/3690 [==============================] - 1s 325us/step - loss: 0.0031 - val_loss: 9.6355e-04\n",
      "Train on 3690 samples, validate on 410 samples\n",
      "Epoch 1/24\n",
      "3690/3690 [==============================] - 3s 794us/step - loss: 1.2997 - val_loss: 0.0357\n",
      "Epoch 2/24\n",
      "3690/3690 [==============================] - 2s 569us/step - loss: 0.0142 - val_loss: 0.0116\n",
      "Epoch 3/24\n",
      "3690/3690 [==============================] - 2s 570us/step - loss: 0.0075 - val_loss: 0.0027\n",
      "Epoch 4/24\n",
      "3690/3690 [==============================] - 2s 570us/step - loss: 0.0036 - val_loss: 7.9812e-04\n",
      "Epoch 5/24\n",
      "3690/3690 [==============================] - 2s 569us/step - loss: 0.0016 - val_loss: 0.0023\n",
      "Epoch 6/24\n",
      "3690/3690 [==============================] - 2s 567us/step - loss: 0.0014 - val_loss: 0.0019\n",
      "Epoch 7/24\n",
      "3690/3690 [==============================] - 2s 571us/step - loss: 0.0013 - val_loss: 8.0308e-04\n",
      "Epoch 8/24\n",
      "3690/3690 [==============================] - 2s 568us/step - loss: 0.0015 - val_loss: 0.0012\n",
      "Epoch 9/24\n",
      "3690/3690 [==============================] - 2s 568us/step - loss: 0.0013 - val_loss: 8.5042e-04\n",
      "Epoch 10/24\n",
      "3690/3690 [==============================] - 2s 571us/step - loss: 0.0013 - val_loss: 9.1906e-04\n",
      "Epoch 11/24\n",
      "3690/3690 [==============================] - 2s 568us/step - loss: 0.0013 - val_loss: 0.0011\n",
      "Epoch 12/24\n",
      "3690/3690 [==============================] - 2s 572us/step - loss: 0.0013 - val_loss: 0.0022\n",
      "Epoch 13/24\n",
      "3690/3690 [==============================] - 2s 570us/step - loss: 0.0013 - val_loss: 0.0015\n",
      "Epoch 14/24\n",
      "3690/3690 [==============================] - 2s 569us/step - loss: 0.0012 - val_loss: 0.0012\n",
      "Epoch 15/24\n",
      "3690/3690 [==============================] - 2s 569us/step - loss: 0.0012 - val_loss: 7.5336e-04\n",
      "Epoch 16/24\n",
      "3690/3690 [==============================] - 2s 570us/step - loss: 0.0013 - val_loss: 8.1305e-04\n",
      "Epoch 17/24\n",
      "3690/3690 [==============================] - 2s 574us/step - loss: 0.0012 - val_loss: 8.4130e-04\n",
      "Epoch 18/24\n",
      "3690/3690 [==============================] - 2s 573us/step - loss: 0.0011 - val_loss: 0.0011\n",
      "Epoch 19/24\n",
      "3690/3690 [==============================] - 2s 571us/step - loss: 0.0011 - val_loss: 8.6864e-04\n",
      "Epoch 20/24\n",
      "3690/3690 [==============================] - 2s 570us/step - loss: 0.0011 - val_loss: 7.7595e-04\n",
      "Epoch 21/24\n",
      "3690/3690 [==============================] - 2s 569us/step - loss: 0.0011 - val_loss: 9.4156e-04\n",
      "Epoch 22/24\n",
      "3690/3690 [==============================] - 2s 567us/step - loss: 0.0010 - val_loss: 6.9540e-04\n",
      "Epoch 23/24\n",
      "3690/3690 [==============================] - 2s 571us/step - loss: 0.0011 - val_loss: 5.9496e-04\n",
      "Epoch 24/24\n",
      "3690/3690 [==============================] - 2s 571us/step - loss: 0.0011 - val_loss: 0.0010\n",
      "Train on 3690 samples, validate on 410 samples\n",
      "Epoch 1/24\n",
      "3690/3690 [==============================] - 3s 809us/step - loss: 0.9700 - val_loss: 0.0074\n",
      "Epoch 2/24\n",
      "3690/3690 [==============================] - 2s 570us/step - loss: 0.0162 - val_loss: 0.0026\n",
      "Epoch 3/24\n",
      "3690/3690 [==============================] - 2s 568us/step - loss: 0.0062 - val_loss: 0.0035\n",
      "Epoch 4/24\n",
      "3690/3690 [==============================] - 2s 572us/step - loss: 0.0039 - val_loss: 0.0019\n",
      "Epoch 5/24\n",
      "3690/3690 [==============================] - 2s 570us/step - loss: 0.0030 - val_loss: 0.0035\n",
      "Epoch 6/24\n",
      "3690/3690 [==============================] - 2s 569us/step - loss: 0.0028 - val_loss: 0.0015\n",
      "Epoch 7/24\n",
      "3690/3690 [==============================] - 2s 567us/step - loss: 0.0023 - val_loss: 0.0029\n",
      "Epoch 8/24\n",
      "3690/3690 [==============================] - 2s 569us/step - loss: 0.0023 - val_loss: 0.0017\n",
      "Epoch 9/24\n",
      "3690/3690 [==============================] - 2s 570us/step - loss: 0.0021 - val_loss: 0.0016\n",
      "Epoch 10/24\n",
      "3690/3690 [==============================] - 2s 572us/step - loss: 0.0021 - val_loss: 0.0011\n",
      "Epoch 11/24\n",
      "3690/3690 [==============================] - 2s 568us/step - loss: 0.0019 - val_loss: 0.0012\n",
      "Epoch 12/24\n",
      "3690/3690 [==============================] - 2s 567us/step - loss: 0.0019 - val_loss: 0.0018\n",
      "Epoch 13/24\n",
      "3690/3690 [==============================] - 2s 569us/step - loss: 0.0019 - val_loss: 8.3583e-04\n",
      "Epoch 14/24\n",
      "3690/3690 [==============================] - 2s 569us/step - loss: 0.0018 - val_loss: 0.0024\n",
      "Epoch 15/24\n",
      "3690/3690 [==============================] - 2s 569us/step - loss: 0.0017 - val_loss: 0.0020\n",
      "Epoch 16/24\n",
      "3690/3690 [==============================] - 2s 569us/step - loss: 0.0018 - val_loss: 0.0012\n",
      "Epoch 17/24\n",
      "3690/3690 [==============================] - 2s 569us/step - loss: 0.0017 - val_loss: 0.0017\n",
      "Epoch 18/24\n",
      "3690/3690 [==============================] - 2s 569us/step - loss: 0.0017 - val_loss: 7.7224e-04\n",
      "Epoch 19/24\n",
      "3690/3690 [==============================] - 2s 571us/step - loss: 0.0018 - val_loss: 8.1999e-04\n",
      "Epoch 20/24\n",
      "3690/3690 [==============================] - 2s 567us/step - loss: 0.0023 - val_loss: 7.7897e-04\n",
      "Epoch 21/24\n",
      "3690/3690 [==============================] - 2s 571us/step - loss: 0.0016 - val_loss: 0.0014\n",
      "Epoch 22/24\n",
      "3690/3690 [==============================] - 2s 571us/step - loss: 0.0015 - val_loss: 9.5953e-04\n",
      "Epoch 23/24\n",
      "3690/3690 [==============================] - 2s 571us/step - loss: 0.0016 - val_loss: 9.0297e-04\n",
      "Epoch 24/24\n",
      "3690/3690 [==============================] - 2s 568us/step - loss: 0.0015 - val_loss: 0.0015\n",
      "Train on 3690 samples, validate on 410 samples\n",
      "Epoch 1/24\n",
      "3690/3690 [==============================] - 3s 835us/step - loss: 0.6673 - val_loss: 0.0603\n",
      "Epoch 2/24\n",
      "3690/3690 [==============================] - 2s 569us/step - loss: 0.0665 - val_loss: 0.0503\n",
      "Epoch 3/24\n",
      "3690/3690 [==============================] - 2s 568us/step - loss: 0.0559 - val_loss: 0.0278\n",
      "Epoch 4/24\n",
      "3690/3690 [==============================] - 2s 570us/step - loss: 0.0597 - val_loss: 0.0061\n",
      "Epoch 5/24\n",
      "3690/3690 [==============================] - 2s 571us/step - loss: 0.0409 - val_loss: 0.0016\n",
      "Epoch 6/24\n",
      "3690/3690 [==============================] - 2s 570us/step - loss: 0.0162 - val_loss: 0.0446\n",
      "Epoch 7/24\n",
      "3690/3690 [==============================] - 2s 570us/step - loss: 0.0116 - val_loss: 0.0057\n",
      "Epoch 8/24\n",
      "3690/3690 [==============================] - 2s 571us/step - loss: 0.0040 - val_loss: 0.0115\n",
      "Epoch 9/24\n",
      "3690/3690 [==============================] - 2s 569us/step - loss: 0.0040 - val_loss: 0.0175\n",
      "Epoch 10/24\n",
      "3690/3690 [==============================] - 2s 573us/step - loss: 0.0087 - val_loss: 0.0177\n",
      "Epoch 11/24\n",
      "3690/3690 [==============================] - 2s 569us/step - loss: 0.0046 - val_loss: 0.0065\n",
      "Epoch 12/24\n",
      "3690/3690 [==============================] - 2s 569us/step - loss: 0.0029 - val_loss: 0.0027\n",
      "Epoch 13/24\n",
      "3690/3690 [==============================] - 2s 571us/step - loss: 0.0035 - val_loss: 0.0020\n",
      "Epoch 14/24\n",
      "3690/3690 [==============================] - 2s 570us/step - loss: 0.0026 - val_loss: 0.0021\n",
      "Epoch 15/24\n",
      "3690/3690 [==============================] - 2s 579us/step - loss: 0.0023 - val_loss: 0.0014\n",
      "Epoch 16/24\n",
      "3690/3690 [==============================] - 2s 569us/step - loss: 0.0035 - val_loss: 0.0042\n",
      "Epoch 17/24\n",
      "3690/3690 [==============================] - 2s 569us/step - loss: 0.0062 - val_loss: 0.0055\n",
      "Epoch 18/24\n",
      "3690/3690 [==============================] - 2s 570us/step - loss: 0.0038 - val_loss: 0.0016\n",
      "Epoch 19/24\n",
      "3690/3690 [==============================] - 2s 570us/step - loss: 0.0023 - val_loss: 0.0013\n",
      "Epoch 20/24\n",
      "3690/3690 [==============================] - 2s 570us/step - loss: 0.0024 - val_loss: 0.0025\n",
      "Epoch 21/24\n",
      "3690/3690 [==============================] - 2s 568us/step - loss: 0.0034 - val_loss: 0.0019\n",
      "Epoch 22/24\n",
      "3690/3690 [==============================] - 2s 569us/step - loss: 0.0026 - val_loss: 0.0013\n",
      "Epoch 23/24\n",
      "3690/3690 [==============================] - 2s 570us/step - loss: 0.0019 - val_loss: 0.0035\n",
      "Epoch 24/24\n",
      "3690/3690 [==============================] - 2s 569us/step - loss: 0.0020 - val_loss: 0.0012\n",
      "Train on 3690 samples, validate on 410 samples\n",
      "Epoch 1/24\n",
      "3690/3690 [==============================] - 2s 604us/step - loss: 0.8325 - val_loss: 0.0012\n",
      "Epoch 2/24\n",
      "3690/3690 [==============================] - 1s 330us/step - loss: 0.0047 - val_loss: 0.0019\n",
      "Epoch 3/24\n",
      "3690/3690 [==============================] - 1s 329us/step - loss: 0.0027 - val_loss: 0.0030\n",
      "Epoch 4/24\n",
      "3690/3690 [==============================] - 1s 330us/step - loss: 0.0024 - val_loss: 0.0013\n",
      "Epoch 5/24\n",
      "3690/3690 [==============================] - 1s 326us/step - loss: 0.0023 - val_loss: 0.0011\n",
      "Epoch 6/24\n",
      "3690/3690 [==============================] - 1s 328us/step - loss: 0.0022 - val_loss: 0.0022\n",
      "Epoch 7/24\n",
      "3690/3690 [==============================] - 1s 335us/step - loss: 0.0020 - val_loss: 0.0015\n",
      "Epoch 8/24\n",
      "3690/3690 [==============================] - 1s 328us/step - loss: 0.0020 - val_loss: 0.0013\n",
      "Epoch 9/24\n",
      "3690/3690 [==============================] - 1s 329us/step - loss: 0.0018 - val_loss: 0.0029\n",
      "Epoch 10/24\n",
      "3690/3690 [==============================] - 1s 328us/step - loss: 0.0018 - val_loss: 0.0016\n",
      "Epoch 11/24\n",
      "3690/3690 [==============================] - 1s 326us/step - loss: 0.0017 - val_loss: 0.0015\n",
      "Epoch 12/24\n",
      "3690/3690 [==============================] - 1s 329us/step - loss: 0.0016 - val_loss: 9.4562e-04\n",
      "Epoch 13/24\n",
      "3690/3690 [==============================] - 1s 328us/step - loss: 0.0017 - val_loss: 0.0019\n",
      "Epoch 14/24\n",
      "3690/3690 [==============================] - 1s 328us/step - loss: 0.0016 - val_loss: 0.0011\n",
      "Epoch 15/24\n",
      "3690/3690 [==============================] - 1s 330us/step - loss: 0.0016 - val_loss: 0.0012\n",
      "Epoch 16/24\n",
      "3690/3690 [==============================] - 1s 326us/step - loss: 0.0014 - val_loss: 0.0021\n",
      "Epoch 17/24\n",
      "3690/3690 [==============================] - 1s 326us/step - loss: 0.0014 - val_loss: 0.0018\n",
      "Epoch 18/24\n",
      "3690/3690 [==============================] - 1s 330us/step - loss: 0.0015 - val_loss: 0.0012\n",
      "Epoch 19/24\n",
      "3690/3690 [==============================] - 1s 327us/step - loss: 0.0014 - val_loss: 8.4143e-04\n",
      "Epoch 20/24\n",
      "3690/3690 [==============================] - 1s 330us/step - loss: 0.0015 - val_loss: 0.0022\n",
      "Epoch 21/24\n",
      "3690/3690 [==============================] - 1s 331us/step - loss: 0.0014 - val_loss: 5.4368e-04\n",
      "Epoch 22/24\n",
      "3690/3690 [==============================] - 1s 328us/step - loss: 0.0014 - val_loss: 0.0011\n",
      "Epoch 23/24\n",
      "3690/3690 [==============================] - 1s 328us/step - loss: 0.0012 - val_loss: 6.5853e-04\n",
      "Epoch 24/24\n",
      "3690/3690 [==============================] - 1s 330us/step - loss: 0.0013 - val_loss: 0.0021\n",
      "Train on 3690 samples, validate on 410 samples\n",
      "Epoch 1/24\n",
      "3690/3690 [==============================] - 2s 611us/step - loss: 1.2972 - val_loss: 0.0135\n",
      "Epoch 2/24\n",
      "3690/3690 [==============================] - 1s 325us/step - loss: 0.0091 - val_loss: 0.0059\n",
      "Epoch 3/24\n",
      "3690/3690 [==============================] - 1s 325us/step - loss: 0.0041 - val_loss: 0.0033\n",
      "Epoch 4/24\n",
      "3690/3690 [==============================] - 1s 326us/step - loss: 0.0023 - val_loss: 0.0029\n",
      "Epoch 5/24\n",
      "3690/3690 [==============================] - 1s 344us/step - loss: 0.0024 - val_loss: 0.0018\n",
      "Epoch 6/24\n",
      "3690/3690 [==============================] - 1s 330us/step - loss: 0.0020 - val_loss: 0.0032\n",
      "Epoch 7/24\n",
      "3690/3690 [==============================] - 1s 330us/step - loss: 0.0023 - val_loss: 0.0026\n",
      "Epoch 8/24\n",
      "3690/3690 [==============================] - 1s 327us/step - loss: 0.0020 - val_loss: 0.0018\n",
      "Epoch 9/24\n",
      "3690/3690 [==============================] - 1s 325us/step - loss: 0.0019 - val_loss: 7.0364e-04\n",
      "Epoch 10/24\n",
      "3690/3690 [==============================] - 1s 326us/step - loss: 0.0019 - val_loss: 9.1493e-04\n",
      "Epoch 11/24\n",
      "3690/3690 [==============================] - 1s 327us/step - loss: 0.0018 - val_loss: 6.7967e-04\n",
      "Epoch 12/24\n",
      "3690/3690 [==============================] - 1s 324us/step - loss: 0.0017 - val_loss: 8.2170e-04\n",
      "Epoch 13/24\n",
      "3690/3690 [==============================] - 1s 324us/step - loss: 0.0017 - val_loss: 0.0014\n",
      "Epoch 14/24\n",
      "3690/3690 [==============================] - 1s 325us/step - loss: 0.0018 - val_loss: 6.5203e-04\n",
      "Epoch 15/24\n",
      "3690/3690 [==============================] - 1s 328us/step - loss: 0.0016 - val_loss: 0.0013\n",
      "Epoch 16/24\n",
      "3690/3690 [==============================] - 1s 324us/step - loss: 0.0017 - val_loss: 6.8500e-04\n",
      "Epoch 17/24\n",
      "3690/3690 [==============================] - 1s 326us/step - loss: 0.0017 - val_loss: 6.4214e-04\n",
      "Epoch 18/24\n",
      "3690/3690 [==============================] - 1s 326us/step - loss: 0.0016 - val_loss: 0.0011\n",
      "Epoch 19/24\n",
      "3690/3690 [==============================] - 1s 323us/step - loss: 0.0016 - val_loss: 0.0015\n",
      "Epoch 20/24\n",
      "3690/3690 [==============================] - 1s 330us/step - loss: 0.0016 - val_loss: 7.7099e-04\n",
      "Epoch 21/24\n",
      "3690/3690 [==============================] - 1s 328us/step - loss: 0.0015 - val_loss: 6.5214e-04\n",
      "Epoch 22/24\n",
      "3690/3690 [==============================] - 1s 326us/step - loss: 0.0014 - val_loss: 7.4751e-04\n",
      "Epoch 23/24\n",
      "3690/3690 [==============================] - 1s 328us/step - loss: 0.0015 - val_loss: 9.6585e-04\n",
      "Epoch 24/24\n",
      "3690/3690 [==============================] - 1s 325us/step - loss: 0.0015 - val_loss: 0.0010\n",
      "Train on 3690 samples, validate on 410 samples\n",
      "Epoch 1/24\n",
      "3690/3690 [==============================] - 3s 846us/step - loss: 0.7793 - val_loss: 0.0089\n",
      "Epoch 2/24\n",
      "3690/3690 [==============================] - 2s 538us/step - loss: 0.0147 - val_loss: 8.9418e-04\n",
      "Epoch 3/24\n",
      "3690/3690 [==============================] - 2s 537us/step - loss: 0.0029 - val_loss: 0.0023\n",
      "Epoch 4/24\n",
      "3690/3690 [==============================] - 2s 540us/step - loss: 0.0015 - val_loss: 0.0018\n",
      "Epoch 5/24\n",
      "3690/3690 [==============================] - 2s 538us/step - loss: 0.0014 - val_loss: 0.0024\n",
      "Epoch 6/24\n",
      "3690/3690 [==============================] - 2s 539us/step - loss: 0.0012 - val_loss: 0.0015\n",
      "Epoch 7/24\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3690/3690 [==============================] - 2s 539us/step - loss: 0.0012 - val_loss: 0.0019\n",
      "Epoch 8/24\n",
      "3690/3690 [==============================] - 2s 537us/step - loss: 0.0011 - val_loss: 0.0015\n",
      "Epoch 9/24\n",
      "3690/3690 [==============================] - 2s 537us/step - loss: 0.0011 - val_loss: 0.0028\n",
      "Epoch 10/24\n",
      "3690/3690 [==============================] - 2s 579us/step - loss: 0.0010 - val_loss: 0.0014\n",
      "Epoch 11/24\n",
      "3690/3690 [==============================] - 2s 548us/step - loss: 9.5952e-04 - val_loss: 0.0011\n",
      "Epoch 12/24\n",
      "3690/3690 [==============================] - 2s 570us/step - loss: 9.8709e-04 - val_loss: 0.0010\n",
      "Epoch 13/24\n",
      "3690/3690 [==============================] - 2s 572us/step - loss: 9.0943e-04 - val_loss: 8.9709e-04\n",
      "Epoch 14/24\n",
      "3690/3690 [==============================] - 2s 551us/step - loss: 8.8784e-04 - val_loss: 0.0016\n",
      "Epoch 15/24\n",
      "3690/3690 [==============================] - 2s 555us/step - loss: 9.4083e-04 - val_loss: 5.2854e-04\n",
      "Epoch 16/24\n",
      "3690/3690 [==============================] - 2s 548us/step - loss: 9.4802e-04 - val_loss: 7.0632e-04\n",
      "Epoch 17/24\n",
      "3690/3690 [==============================] - 2s 548us/step - loss: 8.1122e-04 - val_loss: 8.4873e-04\n",
      "Epoch 18/24\n",
      "3690/3690 [==============================] - 2s 548us/step - loss: 8.2105e-04 - val_loss: 7.5083e-04\n",
      "Epoch 19/24\n",
      "3690/3690 [==============================] - 2s 556us/step - loss: 8.1309e-04 - val_loss: 9.0034e-04\n",
      "Epoch 20/24\n",
      "3690/3690 [==============================] - 2s 554us/step - loss: 7.8620e-04 - val_loss: 0.0014\n",
      "Epoch 21/24\n",
      "3690/3690 [==============================] - 2s 553us/step - loss: 8.1914e-04 - val_loss: 0.0011\n",
      "Epoch 22/24\n",
      "3690/3690 [==============================] - 2s 553us/step - loss: 7.6320e-04 - val_loss: 8.6927e-04\n",
      "Epoch 23/24\n",
      "3690/3690 [==============================] - 2s 547us/step - loss: 7.3671e-04 - val_loss: 0.0010\n",
      "Epoch 24/24\n",
      "3690/3690 [==============================] - 2s 548us/step - loss: 7.3061e-04 - val_loss: 9.8052e-04\n",
      "\n",
      "GENERATION 1\n",
      "\n",
      "Train on 3690 samples, validate on 410 samples\n",
      "Epoch 1/24\n",
      "3690/3690 [==============================] - 3s 872us/step - loss: 1.2783 - val_loss: 0.0043\n",
      "Epoch 2/24\n",
      "3690/3690 [==============================] - 2s 548us/step - loss: 0.0084 - val_loss: 0.0012\n",
      "Epoch 3/24\n",
      "3690/3690 [==============================] - 2s 545us/step - loss: 0.0027 - val_loss: 0.0015\n",
      "Epoch 4/24\n",
      "3690/3690 [==============================] - 2s 546us/step - loss: 0.0022 - val_loss: 0.0018\n",
      "Epoch 5/24\n",
      "3690/3690 [==============================] - 2s 547us/step - loss: 0.0020 - val_loss: 0.0019\n",
      "Epoch 6/24\n",
      "3690/3690 [==============================] - 2s 546us/step - loss: 0.0019 - val_loss: 0.0014\n",
      "Epoch 7/24\n",
      "3690/3690 [==============================] - 2s 549us/step - loss: 0.0018 - val_loss: 0.0014\n",
      "Epoch 8/24\n",
      "3690/3690 [==============================] - 2s 547us/step - loss: 0.0017 - val_loss: 0.0017\n",
      "Epoch 9/24\n",
      "3690/3690 [==============================] - 2s 550us/step - loss: 0.0016 - val_loss: 0.0018\n",
      "Epoch 10/24\n",
      "3690/3690 [==============================] - 2s 547us/step - loss: 0.0015 - val_loss: 0.0011\n",
      "Epoch 11/24\n",
      "3690/3690 [==============================] - 2s 547us/step - loss: 0.0016 - val_loss: 0.0017\n",
      "Epoch 12/24\n",
      "3690/3690 [==============================] - 2s 549us/step - loss: 0.0015 - val_loss: 0.0012\n",
      "Epoch 13/24\n",
      "3690/3690 [==============================] - 2s 548us/step - loss: 0.0014 - val_loss: 0.0018\n",
      "Epoch 14/24\n",
      "3690/3690 [==============================] - 2s 548us/step - loss: 0.0014 - val_loss: 0.0016\n",
      "Epoch 15/24\n",
      "3690/3690 [==============================] - 2s 548us/step - loss: 0.0014 - val_loss: 0.0020\n",
      "Epoch 16/24\n",
      "3690/3690 [==============================] - 2s 549us/step - loss: 0.0013 - val_loss: 0.0015\n",
      "Epoch 17/24\n",
      "3690/3690 [==============================] - 2s 548us/step - loss: 0.0013 - val_loss: 0.0012\n",
      "Epoch 18/24\n",
      "3690/3690 [==============================] - 2s 551us/step - loss: 0.0013 - val_loss: 8.5965e-04\n",
      "Epoch 19/24\n",
      "3690/3690 [==============================] - 2s 547us/step - loss: 0.0012 - val_loss: 7.8396e-04\n",
      "Epoch 20/24\n",
      "3690/3690 [==============================] - 2s 550us/step - loss: 0.0012 - val_loss: 0.0013\n",
      "Epoch 21/24\n",
      "3690/3690 [==============================] - 2s 548us/step - loss: 0.0012 - val_loss: 0.0017\n",
      "Epoch 22/24\n",
      "3690/3690 [==============================] - 2s 550us/step - loss: 0.0012 - val_loss: 0.0011\n",
      "Epoch 23/24\n",
      "3690/3690 [==============================] - 2s 548us/step - loss: 0.0011 - val_loss: 0.0013\n",
      "Epoch 24/24\n",
      "3690/3690 [==============================] - 2s 548us/step - loss: 0.0011 - val_loss: 0.0013\n",
      "Train on 3690 samples, validate on 410 samples\n",
      "Epoch 1/24\n",
      "3690/3690 [==============================] - 3s 938us/step - loss: 0.7625 - val_loss: 0.0057\n",
      "Epoch 2/24\n",
      "3690/3690 [==============================] - 2s 588us/step - loss: 0.0033 - val_loss: 0.0056\n",
      "Epoch 3/24\n",
      "3690/3690 [==============================] - 2s 587us/step - loss: 0.0028 - val_loss: 0.0136\n",
      "Epoch 4/24\n",
      "3690/3690 [==============================] - 2s 585us/step - loss: 0.0031 - val_loss: 0.0054\n",
      "Epoch 5/24\n",
      "3690/3690 [==============================] - 2s 598us/step - loss: 0.0018 - val_loss: 0.0044\n",
      "Epoch 6/24\n",
      "3690/3690 [==============================] - 2s 598us/step - loss: 0.0015 - val_loss: 0.0029\n",
      "Epoch 7/24\n",
      "3690/3690 [==============================] - 2s 587us/step - loss: 0.0014 - val_loss: 9.0605e-04\n",
      "Epoch 8/24\n",
      "3690/3690 [==============================] - 2s 586us/step - loss: 0.0017 - val_loss: 9.5244e-04\n",
      "Epoch 9/24\n",
      "3690/3690 [==============================] - 2s 593us/step - loss: 0.0021 - val_loss: 8.7344e-04\n",
      "Epoch 10/24\n",
      "3690/3690 [==============================] - 2s 606us/step - loss: 0.0016 - val_loss: 8.9031e-04\n",
      "Epoch 11/24\n",
      "3690/3690 [==============================] - 2s 586us/step - loss: 0.0014 - val_loss: 0.0027\n",
      "Epoch 12/24\n",
      "3690/3690 [==============================] - 2s 598us/step - loss: 0.0013 - val_loss: 0.0023\n",
      "Epoch 13/24\n",
      "3690/3690 [==============================] - 2s 617us/step - loss: 0.0012 - val_loss: 0.0012\n",
      "Epoch 14/24\n",
      "3690/3690 [==============================] - 2s 582us/step - loss: 0.0013 - val_loss: 0.0012\n",
      "Epoch 15/24\n",
      "3690/3690 [==============================] - 2s 584us/step - loss: 0.0012 - val_loss: 0.0012\n",
      "Epoch 16/24\n",
      "3690/3690 [==============================] - 2s 585us/step - loss: 0.0012 - val_loss: 0.0017\n",
      "Epoch 17/24\n",
      "3690/3690 [==============================] - 2s 597us/step - loss: 0.0011 - val_loss: 0.0011\n",
      "Epoch 18/24\n",
      "3690/3690 [==============================] - 2s 595us/step - loss: 0.0011 - val_loss: 0.0020\n",
      "Epoch 19/24\n",
      "3690/3690 [==============================] - 2s 587us/step - loss: 0.0011 - val_loss: 0.0039\n",
      "Epoch 20/24\n",
      "3690/3690 [==============================] - 2s 582us/step - loss: 0.0016 - val_loss: 0.0050\n",
      "Epoch 21/24\n",
      "3690/3690 [==============================] - 2s 583us/step - loss: 0.0015 - val_loss: 0.0056\n",
      "Epoch 22/24\n",
      "3690/3690 [==============================] - 2s 585us/step - loss: 0.0014 - val_loss: 0.0018\n",
      "Epoch 23/24\n",
      "3690/3690 [==============================] - 2s 588us/step - loss: 0.0011 - val_loss: 9.5259e-04\n",
      "Epoch 24/24\n",
      "3690/3690 [==============================] - 2s 589us/step - loss: 0.0011 - val_loss: 9.0540e-04\n",
      "Train on 3690 samples, validate on 410 samples\n",
      "Epoch 1/24\n",
      "3690/3690 [==============================] - 3s 934us/step - loss: 1.8302 - val_loss: 0.0233\n",
      "Epoch 2/24\n",
      "3690/3690 [==============================] - 2s 580us/step - loss: 0.0283 - val_loss: 0.0025\n",
      "Epoch 3/24\n",
      "3690/3690 [==============================] - 2s 578us/step - loss: 0.0064 - val_loss: 0.0414\n",
      "Epoch 4/24\n",
      "3690/3690 [==============================] - 2s 610us/step - loss: 0.0061 - val_loss: 0.0040\n",
      "Epoch 5/24\n",
      "3690/3690 [==============================] - 2s 599us/step - loss: 0.0019 - val_loss: 0.0033\n",
      "Epoch 6/24\n",
      "3690/3690 [==============================] - 2s 585us/step - loss: 0.0018 - val_loss: 0.0018\n",
      "Epoch 7/24\n",
      "3690/3690 [==============================] - 2s 594us/step - loss: 0.0017 - val_loss: 0.0012\n",
      "Epoch 8/24\n",
      "3690/3690 [==============================] - 2s 589us/step - loss: 0.0016 - val_loss: 0.0018\n",
      "Epoch 9/24\n",
      "3690/3690 [==============================] - 2s 581us/step - loss: 0.0017 - val_loss: 0.0018\n",
      "Epoch 10/24\n",
      "3690/3690 [==============================] - 2s 580us/step - loss: 0.0015 - val_loss: 0.0023\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11/24\n",
      "3690/3690 [==============================] - 2s 577us/step - loss: 0.0014 - val_loss: 0.0013\n",
      "Epoch 12/24\n",
      "3690/3690 [==============================] - 2s 576us/step - loss: 0.0015 - val_loss: 6.8414e-04\n",
      "Epoch 13/24\n",
      "3690/3690 [==============================] - 2s 581us/step - loss: 0.0016 - val_loss: 0.0013\n",
      "Epoch 14/24\n",
      "3690/3690 [==============================] - 2s 579us/step - loss: 0.0014 - val_loss: 0.0012\n",
      "Epoch 15/24\n",
      "3690/3690 [==============================] - 2s 579us/step - loss: 0.0013 - val_loss: 0.0012\n",
      "Epoch 16/24\n",
      "3690/3690 [==============================] - 2s 578us/step - loss: 0.0013 - val_loss: 7.5550e-04\n",
      "Epoch 17/24\n",
      "3690/3690 [==============================] - 2s 577us/step - loss: 0.0014 - val_loss: 0.0014\n",
      "Epoch 18/24\n",
      "3690/3690 [==============================] - 2s 579us/step - loss: 0.0013 - val_loss: 6.5538e-04\n",
      "Epoch 19/24\n",
      "3690/3690 [==============================] - 2s 579us/step - loss: 0.0013 - val_loss: 7.4739e-04\n",
      "Epoch 20/24\n",
      "3690/3690 [==============================] - 2s 577us/step - loss: 0.0014 - val_loss: 9.3278e-04\n",
      "Epoch 21/24\n",
      "3690/3690 [==============================] - 2s 576us/step - loss: 0.0012 - val_loss: 0.0010\n",
      "Epoch 22/24\n",
      "3690/3690 [==============================] - 2s 576us/step - loss: 0.0012 - val_loss: 6.8215e-04\n",
      "Epoch 23/24\n",
      "3690/3690 [==============================] - 2s 578us/step - loss: 0.0012 - val_loss: 9.1042e-04\n",
      "Epoch 24/24\n",
      "3690/3690 [==============================] - 2s 578us/step - loss: 0.0012 - val_loss: 0.0014\n",
      "Train on 3690 samples, validate on 410 samples\n",
      "Epoch 1/24\n",
      "3690/3690 [==============================] - 3s 914us/step - loss: 1.1900 - val_loss: 0.0040\n",
      "Epoch 2/24\n",
      "3690/3690 [==============================] - 2s 539us/step - loss: 0.0081 - val_loss: 0.0015\n",
      "Epoch 3/24\n",
      "3690/3690 [==============================] - 2s 536us/step - loss: 0.0023 - val_loss: 0.0021\n",
      "Epoch 4/24\n",
      "3690/3690 [==============================] - 2s 536us/step - loss: 0.0019 - val_loss: 0.0027\n",
      "Epoch 5/24\n",
      "3690/3690 [==============================] - 2s 537us/step - loss: 0.0018 - val_loss: 0.0020\n",
      "Epoch 6/24\n",
      "3690/3690 [==============================] - 2s 541us/step - loss: 0.0018 - val_loss: 0.0011\n",
      "Epoch 7/24\n",
      "3690/3690 [==============================] - 2s 542us/step - loss: 0.0017 - val_loss: 0.0012\n",
      "Epoch 8/24\n",
      "3690/3690 [==============================] - 2s 537us/step - loss: 0.0016 - val_loss: 0.0011\n",
      "Epoch 9/24\n",
      "3690/3690 [==============================] - 2s 540us/step - loss: 0.0016 - val_loss: 0.0013\n",
      "Epoch 10/24\n",
      "3690/3690 [==============================] - 2s 535us/step - loss: 0.0016 - val_loss: 8.6527e-04\n",
      "Epoch 11/24\n",
      "3690/3690 [==============================] - 2s 534us/step - loss: 0.0016 - val_loss: 0.0010\n",
      "Epoch 12/24\n",
      "3690/3690 [==============================] - 2s 536us/step - loss: 0.0015 - val_loss: 8.9216e-04\n",
      "Epoch 13/24\n",
      "3690/3690 [==============================] - 2s 536us/step - loss: 0.0015 - val_loss: 7.3543e-04\n",
      "Epoch 14/24\n",
      "3690/3690 [==============================] - 2s 538us/step - loss: 0.0014 - val_loss: 6.7078e-04\n",
      "Epoch 15/24\n",
      "3690/3690 [==============================] - 2s 539us/step - loss: 0.0014 - val_loss: 9.1765e-04\n",
      "Epoch 16/24\n",
      "3690/3690 [==============================] - 2s 542us/step - loss: 0.0014 - val_loss: 0.0010\n",
      "Epoch 17/24\n",
      "3690/3690 [==============================] - 2s 540us/step - loss: 0.0014 - val_loss: 0.0014\n",
      "Epoch 18/24\n",
      "3690/3690 [==============================] - 2s 538us/step - loss: 0.0013 - val_loss: 0.0011\n",
      "Epoch 19/24\n",
      "3690/3690 [==============================] - 2s 538us/step - loss: 0.0013 - val_loss: 0.0011\n",
      "Epoch 20/24\n",
      "3690/3690 [==============================] - 2s 537us/step - loss: 0.0013 - val_loss: 8.3910e-04\n",
      "Epoch 21/24\n",
      "3690/3690 [==============================] - 2s 539us/step - loss: 0.0014 - val_loss: 0.0016\n",
      "Epoch 22/24\n",
      "3690/3690 [==============================] - 2s 537us/step - loss: 0.0013 - val_loss: 0.0010\n",
      "Epoch 23/24\n",
      "3690/3690 [==============================] - 2s 536us/step - loss: 0.0013 - val_loss: 6.8162e-04\n",
      "Epoch 24/24\n",
      "3690/3690 [==============================] - 2s 538us/step - loss: 0.0012 - val_loss: 0.0014\n",
      "Train on 3690 samples, validate on 410 samples\n",
      "Epoch 1/24\n",
      "3690/3690 [==============================] - 3s 932us/step - loss: 0.6795 - val_loss: 0.0593\n",
      "Epoch 2/24\n",
      "3690/3690 [==============================] - 2s 534us/step - loss: 0.0677 - val_loss: 0.0203\n",
      "Epoch 3/24\n",
      "3690/3690 [==============================] - 2s 532us/step - loss: 0.0687 - val_loss: 0.0285\n",
      "Epoch 4/24\n",
      "3690/3690 [==============================] - 2s 533us/step - loss: 0.0673 - val_loss: 0.0191\n",
      "Epoch 5/24\n",
      "3690/3690 [==============================] - 2s 532us/step - loss: 0.0628 - val_loss: 0.0143\n",
      "Epoch 6/24\n",
      "3690/3690 [==============================] - 2s 534us/step - loss: 0.0688 - val_loss: 0.0182\n",
      "Epoch 7/24\n",
      "3690/3690 [==============================] - 2s 534us/step - loss: 0.0682 - val_loss: 0.0403\n",
      "Epoch 8/24\n",
      "3690/3690 [==============================] - 2s 535us/step - loss: 0.0439 - val_loss: 0.1778\n",
      "Epoch 9/24\n",
      "3690/3690 [==============================] - 2s 533us/step - loss: 0.0728 - val_loss: 0.0263\n",
      "Epoch 10/24\n",
      "3690/3690 [==============================] - 2s 534us/step - loss: 0.0359 - val_loss: 0.0142\n",
      "Epoch 11/24\n",
      "3690/3690 [==============================] - 2s 534us/step - loss: 0.0588 - val_loss: 0.0379\n",
      "Epoch 12/24\n",
      "3690/3690 [==============================] - 2s 531us/step - loss: 0.0526 - val_loss: 0.0034\n",
      "Epoch 13/24\n",
      "3690/3690 [==============================] - 2s 534us/step - loss: 0.0106 - val_loss: 0.0177\n",
      "Epoch 14/24\n",
      "3690/3690 [==============================] - 2s 533us/step - loss: 0.0074 - val_loss: 0.0184\n",
      "Epoch 15/24\n",
      "3690/3690 [==============================] - 2s 535us/step - loss: 0.0039 - val_loss: 0.0126\n",
      "Epoch 16/24\n",
      "3690/3690 [==============================] - 2s 531us/step - loss: 0.0060 - val_loss: 0.0106\n",
      "Epoch 17/24\n",
      "3690/3690 [==============================] - 2s 531us/step - loss: 0.0033 - val_loss: 0.0333\n",
      "Epoch 18/24\n",
      "3690/3690 [==============================] - 2s 535us/step - loss: 0.0063 - val_loss: 0.0066\n",
      "Epoch 19/24\n",
      "3690/3690 [==============================] - 2s 533us/step - loss: 0.0023 - val_loss: 0.0043\n",
      "Epoch 20/24\n",
      "3690/3690 [==============================] - 2s 534us/step - loss: 0.0024 - val_loss: 0.0027\n",
      "Epoch 21/24\n",
      "3690/3690 [==============================] - 2s 533us/step - loss: 0.0031 - val_loss: 0.0067\n",
      "Epoch 22/24\n",
      "3690/3690 [==============================] - 2s 533us/step - loss: 0.0030 - val_loss: 0.0029\n",
      "Epoch 23/24\n",
      "3690/3690 [==============================] - 2s 535us/step - loss: 0.0027 - val_loss: 0.0031\n",
      "Epoch 24/24\n",
      "3690/3690 [==============================] - 2s 533us/step - loss: 0.0035 - val_loss: 0.0028\n",
      "Train on 3690 samples, validate on 410 samples\n",
      "Epoch 1/24\n",
      "3690/3690 [==============================] - 4s 979us/step - loss: 0.4611 - val_loss: 0.0287\n",
      "Epoch 2/24\n",
      "3690/3690 [==============================] - 2s 561us/step - loss: 0.0684 - val_loss: 0.0344\n",
      "Epoch 3/24\n",
      "3690/3690 [==============================] - 2s 561us/step - loss: 0.0682 - val_loss: 0.0358\n",
      "Epoch 4/24\n",
      "3690/3690 [==============================] - 2s 566us/step - loss: 0.0683 - val_loss: 0.0286\n",
      "Epoch 5/24\n",
      "3690/3690 [==============================] - 2s 563us/step - loss: 0.0683 - val_loss: 0.0470\n",
      "Epoch 6/24\n",
      "3690/3690 [==============================] - 2s 563us/step - loss: 0.0681 - val_loss: 0.0339\n",
      "Epoch 7/24\n",
      "3690/3690 [==============================] - 2s 565us/step - loss: 0.0682 - val_loss: 0.0351\n",
      "Epoch 8/24\n",
      "3690/3690 [==============================] - 2s 562us/step - loss: 0.0681 - val_loss: 0.0431\n",
      "Epoch 9/24\n",
      "3690/3690 [==============================] - 2s 565us/step - loss: 0.0682 - val_loss: 0.0489\n",
      "Epoch 10/24\n",
      "3690/3690 [==============================] - 2s 570us/step - loss: 0.0677 - val_loss: 0.0440\n",
      "Epoch 11/24\n",
      "3690/3690 [==============================] - 2s 563us/step - loss: 0.0683 - val_loss: 0.0409\n",
      "Epoch 12/24\n",
      "3690/3690 [==============================] - 2s 564us/step - loss: 0.0679 - val_loss: 0.0452\n",
      "Epoch 13/24\n",
      "3690/3690 [==============================] - 2s 565us/step - loss: 0.0677 - val_loss: 0.0358\n",
      "Epoch 14/24\n",
      "3690/3690 [==============================] - 2s 568us/step - loss: 0.0682 - val_loss: 0.0406\n",
      "Epoch 15/24\n",
      "3690/3690 [==============================] - 2s 563us/step - loss: 0.0679 - val_loss: 0.0378\n",
      "Epoch 16/24\n",
      "3690/3690 [==============================] - 2s 565us/step - loss: 0.0676 - val_loss: 0.0469\n",
      "Epoch 17/24\n",
      "3690/3690 [==============================] - 2s 563us/step - loss: 0.0668 - val_loss: 0.0421\n",
      "Epoch 18/24\n",
      "3690/3690 [==============================] - 2s 562us/step - loss: 0.0669 - val_loss: 0.0326\n",
      "Epoch 19/24\n",
      "3690/3690 [==============================] - 2s 564us/step - loss: 0.0672 - val_loss: 0.0437\n",
      "Epoch 20/24\n",
      "3690/3690 [==============================] - 2s 564us/step - loss: 0.0659 - val_loss: 0.0463\n",
      "Epoch 21/24\n",
      "3690/3690 [==============================] - 2s 564us/step - loss: 0.0664 - val_loss: 0.0487\n",
      "Epoch 22/24\n",
      "3690/3690 [==============================] - 2s 565us/step - loss: 0.0658 - val_loss: 0.0589\n",
      "Epoch 23/24\n",
      "3690/3690 [==============================] - 2s 563us/step - loss: 0.0594 - val_loss: 0.0094\n",
      "Epoch 24/24\n",
      "3690/3690 [==============================] - 2s 565us/step - loss: 0.0667 - val_loss: 0.0346\n",
      "Train on 3690 samples, validate on 410 samples\n",
      "Epoch 1/24\n",
      "3690/3690 [==============================] - 4s 1ms/step - loss: 0.8053 - val_loss: 0.0111\n",
      "Epoch 2/24\n",
      "3690/3690 [==============================] - 2s 560us/step - loss: 0.0149 - val_loss: 0.0069\n",
      "Epoch 3/24\n",
      "3690/3690 [==============================] - 2s 563us/step - loss: 0.0056 - val_loss: 0.0019\n",
      "Epoch 4/24\n",
      "3690/3690 [==============================] - 2s 562us/step - loss: 0.0024 - val_loss: 0.0044\n",
      "Epoch 5/24\n",
      "3690/3690 [==============================] - 2s 563us/step - loss: 0.0020 - val_loss: 0.0037\n",
      "Epoch 6/24\n",
      "3690/3690 [==============================] - 2s 567us/step - loss: 0.0020 - val_loss: 0.0083\n",
      "Epoch 7/24\n",
      "3690/3690 [==============================] - 2s 562us/step - loss: 0.0023 - val_loss: 0.0058\n",
      "Epoch 8/24\n",
      "3690/3690 [==============================] - 2s 563us/step - loss: 0.0019 - val_loss: 0.0098\n",
      "Epoch 9/24\n",
      "3690/3690 [==============================] - 2s 562us/step - loss: 0.0020 - val_loss: 0.0035\n",
      "Epoch 10/24\n",
      "3690/3690 [==============================] - 2s 563us/step - loss: 0.0018 - val_loss: 0.0034\n",
      "Epoch 11/24\n",
      "3690/3690 [==============================] - 2s 563us/step - loss: 0.0016 - val_loss: 0.0051\n",
      "Epoch 12/24\n",
      "3690/3690 [==============================] - 2s 567us/step - loss: 0.0018 - val_loss: 0.0064\n",
      "Epoch 13/24\n",
      "3690/3690 [==============================] - 2s 560us/step - loss: 0.0022 - val_loss: 0.0050\n",
      "Epoch 14/24\n",
      "3690/3690 [==============================] - 2s 563us/step - loss: 0.0017 - val_loss: 0.0024\n",
      "Epoch 15/24\n",
      "3690/3690 [==============================] - 2s 561us/step - loss: 0.0015 - val_loss: 0.0010\n",
      "Epoch 16/24\n",
      "3690/3690 [==============================] - 2s 562us/step - loss: 0.0017 - val_loss: 0.0010\n",
      "Epoch 17/24\n",
      "3690/3690 [==============================] - 2s 566us/step - loss: 0.0017 - val_loss: 0.0022\n",
      "Epoch 18/24\n",
      "3690/3690 [==============================] - 2s 563us/step - loss: 0.0014 - val_loss: 0.0016\n",
      "Epoch 19/24\n",
      "3690/3690 [==============================] - 2s 564us/step - loss: 0.0015 - val_loss: 8.1901e-04\n",
      "Epoch 20/24\n",
      "3690/3690 [==============================] - 2s 559us/step - loss: 0.0017 - val_loss: 0.0022\n",
      "Epoch 21/24\n",
      "3690/3690 [==============================] - 2s 561us/step - loss: 0.0014 - val_loss: 0.0018\n",
      "Epoch 22/24\n",
      "3690/3690 [==============================] - 2s 561us/step - loss: 0.0013 - val_loss: 0.0012\n",
      "Epoch 23/24\n",
      "3690/3690 [==============================] - 2s 567us/step - loss: 0.0013 - val_loss: 0.0027\n",
      "Epoch 24/24\n",
      "3690/3690 [==============================] - 2s 566us/step - loss: 0.0013 - val_loss: 0.0015\n",
      "Train on 3690 samples, validate on 410 samples\n",
      "Epoch 1/24\n",
      "3690/3690 [==============================] - 4s 1ms/step - loss: 0.0828 - val_loss: 7.1995e-04\n",
      "Epoch 2/24\n",
      "3690/3690 [==============================] - 2s 568us/step - loss: 0.0029 - val_loss: 8.1196e-04\n",
      "Epoch 3/24\n",
      "3690/3690 [==============================] - 2s 562us/step - loss: 0.0021 - val_loss: 5.5077e-04\n",
      "Epoch 4/24\n",
      "3690/3690 [==============================] - 2s 564us/step - loss: 0.0021 - val_loss: 5.5763e-04\n",
      "Epoch 5/24\n",
      "3690/3690 [==============================] - 2s 565us/step - loss: 0.0036 - val_loss: 0.0138\n",
      "Epoch 6/24\n",
      "3690/3690 [==============================] - 2s 565us/step - loss: 0.0131 - val_loss: 0.0181\n",
      "Epoch 7/24\n",
      "3690/3690 [==============================] - 2s 566us/step - loss: 0.0080 - val_loss: 0.0013\n",
      "Epoch 8/24\n",
      "3690/3690 [==============================] - 2s 564us/step - loss: 0.0033 - val_loss: 0.0135\n",
      "Epoch 9/24\n",
      "3690/3690 [==============================] - 2s 564us/step - loss: 0.0109 - val_loss: 0.0071\n",
      "Epoch 10/24\n",
      "3690/3690 [==============================] - 2s 567us/step - loss: 0.0073 - val_loss: 0.0119\n",
      "Epoch 11/24\n",
      "3690/3690 [==============================] - 2s 565us/step - loss: 0.0073 - val_loss: 0.0094\n",
      "Epoch 12/24\n",
      "3690/3690 [==============================] - 2s 566us/step - loss: 0.0076 - val_loss: 0.0114\n",
      "Epoch 13/24\n",
      "3690/3690 [==============================] - 2s 568us/step - loss: 0.0067 - val_loss: 0.0142\n",
      "Epoch 14/24\n",
      "3690/3690 [==============================] - 2s 563us/step - loss: 0.0075 - val_loss: 0.0063\n",
      "Epoch 15/24\n",
      "3690/3690 [==============================] - 2s 564us/step - loss: 0.0075 - val_loss: 0.0075\n",
      "Epoch 16/24\n",
      "3690/3690 [==============================] - 2s 562us/step - loss: 0.0054 - val_loss: 0.0048\n",
      "Epoch 17/24\n",
      "3690/3690 [==============================] - 2s 566us/step - loss: 0.0052 - val_loss: 0.0081\n",
      "Epoch 18/24\n",
      "3690/3690 [==============================] - 2s 568us/step - loss: 0.0067 - val_loss: 0.0129\n",
      "Epoch 19/24\n",
      "3690/3690 [==============================] - 2s 569us/step - loss: 0.0073 - val_loss: 0.0078\n",
      "Epoch 20/24\n",
      "3690/3690 [==============================] - 2s 564us/step - loss: 0.0059 - val_loss: 0.0043\n",
      "Epoch 21/24\n",
      "3690/3690 [==============================] - 2s 565us/step - loss: 0.0054 - val_loss: 0.0014\n",
      "Epoch 22/24\n",
      "3690/3690 [==============================] - 2s 560us/step - loss: 0.0030 - val_loss: 0.0044\n",
      "Epoch 23/24\n",
      "3690/3690 [==============================] - 2s 563us/step - loss: 0.0041 - val_loss: 0.0083\n",
      "Epoch 24/24\n",
      "3690/3690 [==============================] - 2s 563us/step - loss: 0.0066 - val_loss: 0.0102\n",
      "Train on 3690 samples, validate on 410 samples\n",
      "Epoch 1/24\n",
      "3690/3690 [==============================] - 4s 1ms/step - loss: 0.0504 - val_loss: 0.0341\n",
      "Epoch 2/24\n",
      "3690/3690 [==============================] - 2s 566us/step - loss: 0.0130 - val_loss: 0.0140\n",
      "Epoch 3/24\n",
      "3690/3690 [==============================] - 2s 562us/step - loss: 0.0081 - val_loss: 0.0033\n",
      "Epoch 4/24\n",
      "3690/3690 [==============================] - 2s 565us/step - loss: 0.0064 - val_loss: 0.0026\n",
      "Epoch 5/24\n",
      "3690/3690 [==============================] - 2s 561us/step - loss: 0.0046 - val_loss: 0.0039\n",
      "Epoch 6/24\n",
      "3690/3690 [==============================] - 2s 564us/step - loss: 0.0054 - val_loss: 0.0030\n",
      "Epoch 7/24\n",
      "3690/3690 [==============================] - 2s 565us/step - loss: 0.0051 - val_loss: 0.0030\n",
      "Epoch 8/24\n",
      "3690/3690 [==============================] - 2s 562us/step - loss: 0.0058 - val_loss: 0.0042\n",
      "Epoch 9/24\n",
      "3690/3690 [==============================] - 2s 563us/step - loss: 0.0069 - val_loss: 0.0036\n",
      "Epoch 10/24\n",
      "3690/3690 [==============================] - 2s 565us/step - loss: 0.0054 - val_loss: 0.0106\n",
      "Epoch 11/24\n",
      "3690/3690 [==============================] - 2s 562us/step - loss: 0.0065 - val_loss: 9.1590e-04\n",
      "Epoch 12/24\n",
      "3690/3690 [==============================] - 2s 564us/step - loss: 0.0037 - val_loss: 0.0033\n",
      "Epoch 13/24\n",
      "3690/3690 [==============================] - 2s 565us/step - loss: 0.0051 - val_loss: 0.0026\n",
      "Epoch 14/24\n",
      "3690/3690 [==============================] - 2s 565us/step - loss: 0.0053 - val_loss: 0.0032\n",
      "Epoch 15/24\n",
      "3690/3690 [==============================] - 2s 565us/step - loss: 0.0056 - val_loss: 0.0028\n",
      "Epoch 16/24\n",
      "3690/3690 [==============================] - 2s 565us/step - loss: 0.0052 - val_loss: 0.0040\n",
      "Epoch 17/24\n",
      "3690/3690 [==============================] - 2s 564us/step - loss: 0.0053 - val_loss: 0.0029\n",
      "Epoch 18/24\n",
      "3690/3690 [==============================] - 2s 565us/step - loss: 0.0033 - val_loss: 0.0014\n",
      "Epoch 19/24\n",
      "3690/3690 [==============================] - 2s 566us/step - loss: 0.0041 - val_loss: 0.0040\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20/24\n",
      "3690/3690 [==============================] - 2s 561us/step - loss: 0.0046 - val_loss: 0.0023\n",
      "Epoch 21/24\n",
      "3690/3690 [==============================] - 2s 563us/step - loss: 0.0035 - val_loss: 0.0041\n",
      "Epoch 22/24\n",
      "3690/3690 [==============================] - 2s 563us/step - loss: 0.0056 - val_loss: 0.0053\n",
      "Epoch 23/24\n",
      "3690/3690 [==============================] - 2s 562us/step - loss: 0.0047 - val_loss: 0.0031\n",
      "Epoch 24/24\n",
      "3690/3690 [==============================] - 2s 563us/step - loss: 0.0034 - val_loss: 0.0036\n",
      "\n",
      "GENERATION 2\n",
      "\n",
      "Train on 3690 samples, validate on 410 samples\n",
      "Epoch 1/24\n",
      "3690/3690 [==============================] - 4s 1ms/step - loss: 1.3517 - val_loss: 0.0780\n",
      "Epoch 2/24\n",
      "3690/3690 [==============================] - 2s 550us/step - loss: 0.0680 - val_loss: 0.1311\n",
      "Epoch 3/24\n",
      "3690/3690 [==============================] - 2s 549us/step - loss: 0.0665 - val_loss: 0.0381\n",
      "Epoch 4/24\n",
      "3690/3690 [==============================] - 2s 548us/step - loss: 0.0456 - val_loss: 0.0039\n",
      "Epoch 5/24\n",
      "3690/3690 [==============================] - 2s 553us/step - loss: 0.0414 - val_loss: 0.0110\n",
      "Epoch 6/24\n",
      "3690/3690 [==============================] - 2s 550us/step - loss: 0.0065 - val_loss: 0.0036\n",
      "Epoch 7/24\n",
      "3690/3690 [==============================] - 2s 549us/step - loss: 0.0037 - val_loss: 0.0028\n",
      "Epoch 8/24\n",
      "3690/3690 [==============================] - 2s 553us/step - loss: 0.0074 - val_loss: 0.0053\n",
      "Epoch 9/24\n",
      "3690/3690 [==============================] - 2s 549us/step - loss: 0.0045 - val_loss: 0.0015\n",
      "Epoch 10/24\n",
      "3690/3690 [==============================] - 2s 551us/step - loss: 0.0019 - val_loss: 0.0010\n",
      "Epoch 11/24\n",
      "3690/3690 [==============================] - 2s 549us/step - loss: 0.0043 - val_loss: 0.0136\n",
      "Epoch 12/24\n",
      "3690/3690 [==============================] - 2s 551us/step - loss: 0.0150 - val_loss: 0.0017\n",
      "Epoch 13/24\n",
      "3690/3690 [==============================] - 2s 551us/step - loss: 0.0023 - val_loss: 0.0028\n",
      "Epoch 14/24\n",
      "3690/3690 [==============================] - 2s 553us/step - loss: 0.0016 - val_loss: 0.0032\n",
      "Epoch 15/24\n",
      "3690/3690 [==============================] - 2s 549us/step - loss: 0.0022 - val_loss: 0.0173\n",
      "Epoch 16/24\n",
      "3690/3690 [==============================] - 2s 553us/step - loss: 0.0167 - val_loss: 0.0148\n",
      "Epoch 17/24\n",
      "3690/3690 [==============================] - 2s 549us/step - loss: 0.0043 - val_loss: 0.0059\n",
      "Epoch 18/24\n",
      "3690/3690 [==============================] - 2s 551us/step - loss: 0.0016 - val_loss: 0.0022\n",
      "Epoch 19/24\n",
      "3690/3690 [==============================] - 2s 551us/step - loss: 0.0013 - val_loss: 7.6092e-04\n",
      "Epoch 20/24\n",
      "3690/3690 [==============================] - 2s 552us/step - loss: 0.0023 - val_loss: 0.0045\n",
      "Epoch 21/24\n",
      "3690/3690 [==============================] - 2s 551us/step - loss: 0.0143 - val_loss: 0.0132\n",
      "Epoch 22/24\n",
      "3690/3690 [==============================] - 2s 556us/step - loss: 0.0052 - val_loss: 0.0034\n",
      "Epoch 23/24\n",
      "3690/3690 [==============================] - 2s 554us/step - loss: 0.0015 - val_loss: 0.0021\n",
      "Epoch 24/24\n",
      "3690/3690 [==============================] - 2s 553us/step - loss: 0.0012 - val_loss: 7.0610e-04\n",
      "Train on 3690 samples, validate on 410 samples\n",
      "Epoch 1/24\n",
      "3690/3690 [==============================] - 4s 1ms/step - loss: 1.0281 - val_loss: 0.0013\n",
      "Epoch 2/24\n",
      "3690/3690 [==============================] - 2s 549us/step - loss: 0.0039 - val_loss: 0.0033\n",
      "Epoch 3/24\n",
      "3690/3690 [==============================] - 2s 555us/step - loss: 0.0043 - val_loss: 0.0115\n",
      "Epoch 4/24\n",
      "3690/3690 [==============================] - 2s 551us/step - loss: 0.0047 - val_loss: 0.0053\n",
      "Epoch 5/24\n",
      "3690/3690 [==============================] - 2s 555us/step - loss: 0.0023 - val_loss: 0.0040\n",
      "Epoch 6/24\n",
      "3690/3690 [==============================] - 2s 551us/step - loss: 0.0018 - val_loss: 0.0010\n",
      "Epoch 7/24\n",
      "3690/3690 [==============================] - 2s 554us/step - loss: 0.0015 - val_loss: 0.0017\n",
      "Epoch 8/24\n",
      "3690/3690 [==============================] - 2s 550us/step - loss: 0.0014 - val_loss: 0.0016\n",
      "Epoch 9/24\n",
      "3690/3690 [==============================] - 2s 549us/step - loss: 0.0014 - val_loss: 0.0012\n",
      "Epoch 10/24\n",
      "3690/3690 [==============================] - 2s 550us/step - loss: 0.0014 - val_loss: 0.0010\n",
      "Epoch 11/24\n",
      "3690/3690 [==============================] - 2s 552us/step - loss: 0.0013 - val_loss: 0.0011\n",
      "Epoch 12/24\n",
      "3690/3690 [==============================] - 2s 550us/step - loss: 0.0014 - val_loss: 8.9069e-04\n",
      "Epoch 13/24\n",
      "3690/3690 [==============================] - 2s 552us/step - loss: 0.0016 - val_loss: 7.4351e-04\n",
      "Epoch 14/24\n",
      "3690/3690 [==============================] - 2s 551us/step - loss: 0.0013 - val_loss: 9.5592e-04\n",
      "Epoch 15/24\n",
      "3690/3690 [==============================] - 2s 551us/step - loss: 0.0013 - val_loss: 8.3652e-04\n",
      "Epoch 16/24\n",
      "3690/3690 [==============================] - 2s 550us/step - loss: 0.0013 - val_loss: 0.0012\n",
      "Epoch 17/24\n",
      "3690/3690 [==============================] - 2s 548us/step - loss: 0.0013 - val_loss: 0.0012\n",
      "Epoch 18/24\n",
      "3690/3690 [==============================] - 2s 551us/step - loss: 0.0012 - val_loss: 0.0010\n",
      "Epoch 19/24\n",
      "3690/3690 [==============================] - 2s 553us/step - loss: 0.0013 - val_loss: 0.0010\n",
      "Epoch 20/24\n",
      "3690/3690 [==============================] - 2s 549us/step - loss: 0.0012 - val_loss: 8.7845e-04\n",
      "Epoch 21/24\n",
      "3690/3690 [==============================] - 2s 549us/step - loss: 0.0012 - val_loss: 7.6698e-04\n",
      "Epoch 22/24\n",
      "3690/3690 [==============================] - 2s 551us/step - loss: 0.0012 - val_loss: 0.0013\n",
      "Epoch 23/24\n",
      "3690/3690 [==============================] - 2s 549us/step - loss: 0.0012 - val_loss: 0.0012\n",
      "Epoch 24/24\n",
      "3690/3690 [==============================] - 2s 551us/step - loss: 0.0012 - val_loss: 8.3077e-04\n",
      "Train on 3690 samples, validate on 410 samples\n",
      "Epoch 1/24\n",
      "3690/3690 [==============================] - 4s 1ms/step - loss: 1.1078 - val_loss: 0.0015\n",
      "Epoch 2/24\n",
      "3690/3690 [==============================] - 2s 549us/step - loss: 0.0064 - val_loss: 7.6323e-04\n",
      "Epoch 3/24\n",
      "3690/3690 [==============================] - 2s 551us/step - loss: 0.0021 - val_loss: 0.0022\n",
      "Epoch 4/24\n",
      "3690/3690 [==============================] - 2s 548us/step - loss: 0.0044 - val_loss: 0.0013\n",
      "Epoch 5/24\n",
      "3690/3690 [==============================] - 2s 547us/step - loss: 0.0020 - val_loss: 0.0019\n",
      "Epoch 6/24\n",
      "3690/3690 [==============================] - 2s 552us/step - loss: 0.0015 - val_loss: 0.0014\n",
      "Epoch 7/24\n",
      "3690/3690 [==============================] - 2s 548us/step - loss: 0.0014 - val_loss: 0.0022\n",
      "Epoch 8/24\n",
      "3690/3690 [==============================] - 2s 553us/step - loss: 0.0014 - val_loss: 0.0018\n",
      "Epoch 9/24\n",
      "3690/3690 [==============================] - 2s 551us/step - loss: 0.0013 - val_loss: 9.7595e-04\n",
      "Epoch 10/24\n",
      "3690/3690 [==============================] - 2s 551us/step - loss: 0.0013 - val_loss: 0.0053\n",
      "Epoch 11/24\n",
      "3690/3690 [==============================] - 2s 552us/step - loss: 0.0018 - val_loss: 0.0017\n",
      "Epoch 12/24\n",
      "3690/3690 [==============================] - 2s 548us/step - loss: 0.0013 - val_loss: 0.0026\n",
      "Epoch 13/24\n",
      "3690/3690 [==============================] - 2s 552us/step - loss: 0.0013 - val_loss: 0.0016\n",
      "Epoch 14/24\n",
      "3690/3690 [==============================] - 2s 550us/step - loss: 0.0013 - val_loss: 0.0028\n",
      "Epoch 15/24\n",
      "3690/3690 [==============================] - 2s 548us/step - loss: 0.0012 - val_loss: 0.0018\n",
      "Epoch 16/24\n",
      "3690/3690 [==============================] - 2s 551us/step - loss: 0.0012 - val_loss: 8.3637e-04\n",
      "Epoch 17/24\n",
      "3690/3690 [==============================] - 2s 551us/step - loss: 0.0011 - val_loss: 0.0020\n",
      "Epoch 18/24\n",
      "3690/3690 [==============================] - 2s 551us/step - loss: 0.0012 - val_loss: 0.0013\n",
      "Epoch 19/24\n",
      "3690/3690 [==============================] - 2s 550us/step - loss: 0.0011 - val_loss: 0.0014\n",
      "Epoch 20/24\n",
      "3690/3690 [==============================] - 2s 548us/step - loss: 0.0011 - val_loss: 0.0013\n",
      "Epoch 21/24\n",
      "3690/3690 [==============================] - 2s 549us/step - loss: 0.0011 - val_loss: 6.4871e-04\n",
      "Epoch 22/24\n",
      "3690/3690 [==============================] - 2s 551us/step - loss: 0.0011 - val_loss: 0.0010\n",
      "Epoch 23/24\n",
      "3690/3690 [==============================] - 2s 551us/step - loss: 0.0011 - val_loss: 0.0011\n",
      "Epoch 24/24\n",
      "3690/3690 [==============================] - 2s 554us/step - loss: 0.0011 - val_loss: 5.9742e-04\n",
      "Train on 3690 samples, validate on 410 samples\n",
      "Epoch 1/24\n",
      "3690/3690 [==============================] - 4s 1ms/step - loss: 1.1145 - val_loss: 0.0563\n",
      "Epoch 2/24\n",
      "3690/3690 [==============================] - 2s 557us/step - loss: 0.0646 - val_loss: 0.0230\n",
      "Epoch 3/24\n",
      "3690/3690 [==============================] - 2s 556us/step - loss: 0.0310 - val_loss: 0.0039\n",
      "Epoch 4/24\n",
      "3690/3690 [==============================] - 2s 548us/step - loss: 0.0085 - val_loss: 0.0026\n",
      "Epoch 5/24\n",
      "3690/3690 [==============================] - 2s 550us/step - loss: 0.0177 - val_loss: 8.1583e-04\n",
      "Epoch 6/24\n",
      "3690/3690 [==============================] - 2s 552us/step - loss: 0.0038 - val_loss: 0.0036\n",
      "Epoch 7/24\n",
      "3690/3690 [==============================] - 2s 552us/step - loss: 0.0025 - val_loss: 0.0019\n",
      "Epoch 8/24\n",
      "3690/3690 [==============================] - 2s 552us/step - loss: 0.0020 - val_loss: 0.0012\n",
      "Epoch 9/24\n",
      "3690/3690 [==============================] - 2s 550us/step - loss: 0.0018 - val_loss: 0.0012\n",
      "Epoch 10/24\n",
      "3690/3690 [==============================] - 2s 549us/step - loss: 0.0017 - val_loss: 0.0011\n",
      "Epoch 11/24\n",
      "3690/3690 [==============================] - 2s 549us/step - loss: 0.0016 - val_loss: 0.0020\n",
      "Epoch 12/24\n",
      "3690/3690 [==============================] - 2s 548us/step - loss: 0.0015 - val_loss: 0.0011\n",
      "Epoch 13/24\n",
      "3690/3690 [==============================] - 2s 552us/step - loss: 0.0015 - val_loss: 0.0012\n",
      "Epoch 14/24\n",
      "3690/3690 [==============================] - 2s 552us/step - loss: 0.0014 - val_loss: 0.0011\n",
      "Epoch 15/24\n",
      "3690/3690 [==============================] - 2s 552us/step - loss: 0.0014 - val_loss: 0.0011\n",
      "Epoch 16/24\n",
      "3690/3690 [==============================] - 2s 551us/step - loss: 0.0014 - val_loss: 9.0338e-04\n",
      "Epoch 17/24\n",
      "3690/3690 [==============================] - 2s 551us/step - loss: 0.0013 - val_loss: 0.0015\n",
      "Epoch 18/24\n",
      "3690/3690 [==============================] - 2s 548us/step - loss: 0.0014 - val_loss: 0.0015\n",
      "Epoch 19/24\n",
      "3690/3690 [==============================] - 2s 550us/step - loss: 0.0015 - val_loss: 0.0015\n",
      "Epoch 20/24\n",
      "3690/3690 [==============================] - 2s 552us/step - loss: 0.0012 - val_loss: 0.0012\n",
      "Epoch 21/24\n",
      "3690/3690 [==============================] - 2s 556us/step - loss: 0.0013 - val_loss: 0.0012\n",
      "Epoch 22/24\n",
      "3690/3690 [==============================] - 2s 549us/step - loss: 0.0012 - val_loss: 0.0010\n",
      "Epoch 23/24\n",
      "3690/3690 [==============================] - 2s 549us/step - loss: 0.0012 - val_loss: 0.0012\n",
      "Epoch 24/24\n",
      "3690/3690 [==============================] - 2s 549us/step - loss: 0.0012 - val_loss: 0.0020\n",
      "Train on 3690 samples, validate on 410 samples\n",
      "Epoch 1/24\n",
      "3690/3690 [==============================] - 4s 1ms/step - loss: 0.6583 - val_loss: 0.1024\n",
      "Epoch 2/24\n",
      "3690/3690 [==============================] - 2s 554us/step - loss: 0.0787 - val_loss: 0.0063\n",
      "Epoch 3/24\n",
      "3690/3690 [==============================] - 2s 550us/step - loss: 0.1060 - val_loss: 0.0065\n",
      "Epoch 4/24\n",
      "3690/3690 [==============================] - 2s 552us/step - loss: 0.0833 - val_loss: 0.0235\n",
      "Epoch 5/24\n",
      "3690/3690 [==============================] - 2s 552us/step - loss: 0.0685 - val_loss: 0.0493\n",
      "Epoch 6/24\n",
      "3690/3690 [==============================] - 2s 550us/step - loss: 0.0697 - val_loss: 0.0561\n",
      "Epoch 7/24\n",
      "3690/3690 [==============================] - 2s 549us/step - loss: 0.0696 - val_loss: 0.0482\n",
      "Epoch 8/24\n",
      "3690/3690 [==============================] - 2s 552us/step - loss: 0.0685 - val_loss: 0.0398\n",
      "Epoch 9/24\n",
      "3690/3690 [==============================] - 2s 552us/step - loss: 0.0682 - val_loss: 0.0362\n",
      "Epoch 10/24\n",
      "3690/3690 [==============================] - 2s 552us/step - loss: 0.0682 - val_loss: 0.0357\n",
      "Epoch 11/24\n",
      "3690/3690 [==============================] - 2s 552us/step - loss: 0.0682 - val_loss: 0.0374\n",
      "Epoch 12/24\n",
      "3690/3690 [==============================] - 2s 551us/step - loss: 0.0681 - val_loss: 0.0381\n",
      "Epoch 13/24\n",
      "3690/3690 [==============================] - 2s 554us/step - loss: 0.0681 - val_loss: 0.0386\n",
      "Epoch 14/24\n",
      "3690/3690 [==============================] - 2s 551us/step - loss: 0.0681 - val_loss: 0.0381\n",
      "Epoch 15/24\n",
      "3690/3690 [==============================] - 2s 554us/step - loss: 0.0681 - val_loss: 0.0378\n",
      "Epoch 16/24\n",
      "3690/3690 [==============================] - 2s 551us/step - loss: 0.0681 - val_loss: 0.0382\n",
      "Epoch 17/24\n",
      "3690/3690 [==============================] - 2s 558us/step - loss: 0.0681 - val_loss: 0.0381\n",
      "Epoch 18/24\n",
      "3690/3690 [==============================] - 2s 553us/step - loss: 0.0681 - val_loss: 0.0380\n",
      "Epoch 19/24\n",
      "3690/3690 [==============================] - 2s 552us/step - loss: 0.0681 - val_loss: 0.0378\n",
      "Epoch 20/24\n",
      "3690/3690 [==============================] - 2s 551us/step - loss: 0.0681 - val_loss: 0.0388\n",
      "Epoch 21/24\n",
      "3690/3690 [==============================] - 2s 550us/step - loss: 0.0681 - val_loss: 0.0391\n",
      "Epoch 22/24\n",
      "3690/3690 [==============================] - 2s 548us/step - loss: 0.0681 - val_loss: 0.0377\n",
      "Epoch 23/24\n",
      "3690/3690 [==============================] - 2s 550us/step - loss: 0.0682 - val_loss: 0.0366\n",
      "Epoch 24/24\n",
      "3690/3690 [==============================] - 2s 553us/step - loss: 0.0682 - val_loss: 0.0371\n",
      "Train on 3690 samples, validate on 410 samples\n",
      "Epoch 1/24\n",
      "3690/3690 [==============================] - 4s 996us/step - loss: 0.6749 - val_loss: 0.2282\n",
      "Epoch 2/24\n",
      "3690/3690 [==============================] - 1s 403us/step - loss: 0.1048 - val_loss: 0.0710\n",
      "Epoch 3/24\n",
      "3690/3690 [==============================] - 1s 406us/step - loss: 0.0705 - val_loss: 0.0443\n",
      "Epoch 4/24\n",
      "3690/3690 [==============================] - 1s 404us/step - loss: 0.0682 - val_loss: 0.0372\n",
      "Epoch 5/24\n",
      "3690/3690 [==============================] - 2s 407us/step - loss: 0.0681 - val_loss: 0.0372\n",
      "Epoch 6/24\n",
      "3690/3690 [==============================] - 1s 405us/step - loss: 0.0681 - val_loss: 0.0408\n",
      "Epoch 7/24\n",
      "3690/3690 [==============================] - 2s 407us/step - loss: 0.0681 - val_loss: 0.0395\n",
      "Epoch 8/24\n",
      "3690/3690 [==============================] - 2s 412us/step - loss: 0.0681 - val_loss: 0.0401\n",
      "Epoch 9/24\n",
      "3690/3690 [==============================] - 1s 402us/step - loss: 0.0681 - val_loss: 0.0399\n",
      "Epoch 10/24\n",
      "3690/3690 [==============================] - 2s 408us/step - loss: 0.0681 - val_loss: 0.0403\n",
      "Epoch 11/24\n",
      "3690/3690 [==============================] - 2s 409us/step - loss: 0.0681 - val_loss: 0.0384\n",
      "Epoch 12/24\n",
      "3690/3690 [==============================] - 1s 404us/step - loss: 0.0681 - val_loss: 0.0366\n",
      "Epoch 13/24\n",
      "3690/3690 [==============================] - 1s 403us/step - loss: 0.0681 - val_loss: 0.0373\n",
      "Epoch 14/24\n",
      "3690/3690 [==============================] - 1s 405us/step - loss: 0.0681 - val_loss: 0.0395\n",
      "Epoch 15/24\n",
      "3690/3690 [==============================] - 2s 408us/step - loss: 0.0681 - val_loss: 0.0408\n",
      "Epoch 16/24\n",
      "3690/3690 [==============================] - 1s 404us/step - loss: 0.0681 - val_loss: 0.0399\n",
      "Epoch 17/24\n",
      "3690/3690 [==============================] - 2s 408us/step - loss: 0.0681 - val_loss: 0.0381\n",
      "Epoch 18/24\n",
      "3690/3690 [==============================] - 1s 404us/step - loss: 0.0681 - val_loss: 0.0378\n",
      "Epoch 19/24\n",
      "3690/3690 [==============================] - 1s 407us/step - loss: 0.0681 - val_loss: 0.0396\n",
      "Epoch 20/24\n",
      "3690/3690 [==============================] - 1s 404us/step - loss: 0.0681 - val_loss: 0.0386\n",
      "Epoch 21/24\n",
      "3690/3690 [==============================] - 2s 407us/step - loss: 0.0681 - val_loss: 0.0404\n",
      "Epoch 22/24\n",
      "3690/3690 [==============================] - 1s 405us/step - loss: 0.0681 - val_loss: 0.0396\n",
      "Epoch 23/24\n",
      "3690/3690 [==============================] - 1s 405us/step - loss: 0.0681 - val_loss: 0.0352\n",
      "Epoch 24/24\n",
      "3690/3690 [==============================] - 1s 403us/step - loss: 0.0681 - val_loss: 0.0390\n",
      "Train on 3690 samples, validate on 410 samples\n",
      "Epoch 1/24\n",
      "3690/3690 [==============================] - 4s 1ms/step - loss: 0.0694 - val_loss: 0.0403\n",
      "Epoch 2/24\n",
      "3690/3690 [==============================] - 2s 588us/step - loss: 0.0683 - val_loss: 0.0398\n",
      "Epoch 3/24\n",
      "3690/3690 [==============================] - 2s 588us/step - loss: 0.0683 - val_loss: 0.0406\n",
      "Epoch 4/24\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3690/3690 [==============================] - 2s 586us/step - loss: 0.0683 - val_loss: 0.0391\n",
      "Epoch 5/24\n",
      "3690/3690 [==============================] - 2s 589us/step - loss: 0.0682 - val_loss: 0.0414\n",
      "Epoch 6/24\n",
      "3690/3690 [==============================] - 2s 585us/step - loss: 0.0683 - val_loss: 0.0366\n",
      "Epoch 7/24\n",
      "3690/3690 [==============================] - 2s 585us/step - loss: 0.0683 - val_loss: 0.0350\n",
      "Epoch 8/24\n",
      "3690/3690 [==============================] - 2s 589us/step - loss: 0.0683 - val_loss: 0.0383\n",
      "Epoch 9/24\n",
      "3690/3690 [==============================] - 2s 590us/step - loss: 0.0682 - val_loss: 0.0370\n",
      "Epoch 10/24\n",
      "3690/3690 [==============================] - 2s 587us/step - loss: 0.0683 - val_loss: 0.0361\n",
      "Epoch 11/24\n",
      "3690/3690 [==============================] - 2s 585us/step - loss: 0.0683 - val_loss: 0.0404\n",
      "Epoch 12/24\n",
      "3690/3690 [==============================] - 2s 588us/step - loss: 0.0683 - val_loss: 0.0390\n",
      "Epoch 13/24\n",
      "3690/3690 [==============================] - 2s 587us/step - loss: 0.0683 - val_loss: 0.0396\n",
      "Epoch 14/24\n",
      "3690/3690 [==============================] - 2s 585us/step - loss: 0.0683 - val_loss: 0.0378\n",
      "Epoch 15/24\n",
      "3690/3690 [==============================] - 2s 586us/step - loss: 0.0683 - val_loss: 0.0397\n",
      "Epoch 16/24\n",
      "3690/3690 [==============================] - 2s 587us/step - loss: 0.0683 - val_loss: 0.0398\n",
      "Epoch 17/24\n",
      "3690/3690 [==============================] - 2s 585us/step - loss: 0.0682 - val_loss: 0.0412\n",
      "Epoch 18/24\n",
      "3690/3690 [==============================] - 2s 584us/step - loss: 0.0683 - val_loss: 0.0397\n",
      "Epoch 19/24\n",
      "3690/3690 [==============================] - 2s 587us/step - loss: 0.0682 - val_loss: 0.0399\n",
      "Epoch 20/24\n",
      "3690/3690 [==============================] - 2s 587us/step - loss: 0.0682 - val_loss: 0.0374\n",
      "Epoch 21/24\n",
      "3690/3690 [==============================] - 2s 586us/step - loss: 0.0682 - val_loss: 0.0372\n",
      "Epoch 22/24\n",
      "3690/3690 [==============================] - 2s 585us/step - loss: 0.0682 - val_loss: 0.0389\n",
      "Epoch 23/24\n",
      "3690/3690 [==============================] - 2s 585us/step - loss: 0.0683 - val_loss: 0.0385\n",
      "Epoch 24/24\n",
      "3690/3690 [==============================] - 2s 585us/step - loss: 0.0682 - val_loss: 0.0396\n",
      "Train on 3690 samples, validate on 410 samples\n",
      "Epoch 1/24\n",
      "3690/3690 [==============================] - 5s 1ms/step - loss: 0.0838 - val_loss: 0.0016\n",
      "Epoch 2/24\n",
      "3690/3690 [==============================] - 2s 589us/step - loss: 0.0108 - val_loss: 0.0016\n",
      "Epoch 3/24\n",
      "3690/3690 [==============================] - 2s 587us/step - loss: 0.0065 - val_loss: 0.0032\n",
      "Epoch 4/24\n",
      "3690/3690 [==============================] - 2s 592us/step - loss: 0.0038 - val_loss: 0.0070\n",
      "Epoch 5/24\n",
      "3690/3690 [==============================] - 2s 589us/step - loss: 0.0030 - val_loss: 0.0021\n",
      "Epoch 6/24\n",
      "3690/3690 [==============================] - 2s 589us/step - loss: 0.0021 - val_loss: 0.0014\n",
      "Epoch 7/24\n",
      "3690/3690 [==============================] - 2s 589us/step - loss: 0.0018 - val_loss: 0.0014\n",
      "Epoch 8/24\n",
      "3690/3690 [==============================] - 2s 587us/step - loss: 0.0015 - val_loss: 0.0019\n",
      "Epoch 9/24\n",
      "3690/3690 [==============================] - 2s 589us/step - loss: 0.0013 - val_loss: 0.0017\n",
      "Epoch 10/24\n",
      "3690/3690 [==============================] - 2s 587us/step - loss: 0.0012 - val_loss: 0.0014\n",
      "Epoch 11/24\n",
      "3690/3690 [==============================] - 2s 592us/step - loss: 0.0012 - val_loss: 0.0011\n",
      "Epoch 12/24\n",
      "3690/3690 [==============================] - 2s 588us/step - loss: 0.0011 - val_loss: 0.0012\n",
      "Epoch 13/24\n",
      "3690/3690 [==============================] - 2s 588us/step - loss: 0.0011 - val_loss: 0.0011\n",
      "Epoch 14/24\n",
      "3690/3690 [==============================] - 2s 594us/step - loss: 0.0011 - val_loss: 0.0010\n",
      "Epoch 15/24\n",
      "3690/3690 [==============================] - 2s 591us/step - loss: 0.0010 - val_loss: 8.8185e-04\n",
      "Epoch 16/24\n",
      "3690/3690 [==============================] - 2s 592us/step - loss: 9.4083e-04 - val_loss: 8.1301e-04\n",
      "Epoch 17/24\n",
      "3690/3690 [==============================] - 2s 592us/step - loss: 9.0835e-04 - val_loss: 0.0010\n",
      "Epoch 18/24\n",
      "3690/3690 [==============================] - 2s 590us/step - loss: 9.3158e-04 - val_loss: 8.8841e-04\n",
      "Epoch 19/24\n",
      "3690/3690 [==============================] - 2s 589us/step - loss: 8.5997e-04 - val_loss: 9.6537e-04\n",
      "Epoch 20/24\n",
      "3690/3690 [==============================] - 2s 591us/step - loss: 8.9600e-04 - val_loss: 0.0012\n",
      "Epoch 21/24\n",
      "3690/3690 [==============================] - 2s 588us/step - loss: 9.1957e-04 - val_loss: 8.2924e-04\n",
      "Epoch 22/24\n",
      "3690/3690 [==============================] - 2s 589us/step - loss: 8.7064e-04 - val_loss: 0.0011\n",
      "Epoch 23/24\n",
      "3690/3690 [==============================] - 2s 587us/step - loss: 8.6923e-04 - val_loss: 6.4508e-04\n",
      "Epoch 24/24\n",
      "3690/3690 [==============================] - 2s 590us/step - loss: 8.1229e-04 - val_loss: 7.4959e-04\n",
      "Train on 3690 samples, validate on 410 samples\n",
      "Epoch 1/24\n",
      "3690/3690 [==============================] - 5s 1ms/step - loss: 0.0856 - val_loss: 0.0338\n",
      "Epoch 2/24\n",
      "3690/3690 [==============================] - 2s 588us/step - loss: 0.0155 - val_loss: 0.0131\n",
      "Epoch 3/24\n",
      "3690/3690 [==============================] - 2s 588us/step - loss: 0.0118 - val_loss: 0.0139\n",
      "Epoch 4/24\n",
      "3690/3690 [==============================] - 2s 589us/step - loss: 0.0104 - val_loss: 0.0224\n",
      "Epoch 5/24\n",
      "3690/3690 [==============================] - 2s 588us/step - loss: 0.0119 - val_loss: 0.0207\n",
      "Epoch 6/24\n",
      "3690/3690 [==============================] - 2s 592us/step - loss: 0.0055 - val_loss: 0.0179\n",
      "Epoch 7/24\n",
      "3690/3690 [==============================] - 2s 588us/step - loss: 0.0122 - val_loss: 0.0097\n",
      "Epoch 8/24\n",
      "3690/3690 [==============================] - 2s 589us/step - loss: 0.0051 - val_loss: 0.0115\n",
      "Epoch 9/24\n",
      "3690/3690 [==============================] - 2s 589us/step - loss: 0.0076 - val_loss: 0.0291\n",
      "Epoch 10/24\n",
      "3690/3690 [==============================] - 2s 589us/step - loss: 0.0085 - val_loss: 0.0104\n",
      "Epoch 11/24\n",
      "3690/3690 [==============================] - 2s 589us/step - loss: 0.0035 - val_loss: 0.0072\n",
      "Epoch 12/24\n",
      "3690/3690 [==============================] - 2s 592us/step - loss: 0.0087 - val_loss: 0.0316\n",
      "Epoch 13/24\n",
      "3690/3690 [==============================] - 2s 589us/step - loss: 0.0049 - val_loss: 0.0085\n",
      "Epoch 14/24\n",
      "3690/3690 [==============================] - 2s 590us/step - loss: 0.0052 - val_loss: 0.0178\n",
      "Epoch 15/24\n",
      "3690/3690 [==============================] - 2s 591us/step - loss: 0.0056 - val_loss: 0.0033\n",
      "Epoch 16/24\n",
      "3690/3690 [==============================] - 2s 589us/step - loss: 0.0040 - val_loss: 0.0182\n",
      "Epoch 17/24\n",
      "3690/3690 [==============================] - 2s 590us/step - loss: 0.0063 - val_loss: 0.0085\n",
      "Epoch 18/24\n",
      "3690/3690 [==============================] - 2s 587us/step - loss: 0.0026 - val_loss: 0.0266\n",
      "Epoch 19/24\n",
      "3690/3690 [==============================] - 2s 589us/step - loss: 0.0047 - val_loss: 0.0089\n",
      "Epoch 20/24\n",
      "3690/3690 [==============================] - 2s 591us/step - loss: 0.0032 - val_loss: 0.0212\n",
      "Epoch 21/24\n",
      "3690/3690 [==============================] - 2s 588us/step - loss: 0.0058 - val_loss: 0.0051\n",
      "Epoch 22/24\n",
      "3690/3690 [==============================] - 2s 587us/step - loss: 0.0025 - val_loss: 0.0352\n",
      "Epoch 23/24\n",
      "3690/3690 [==============================] - 2s 591us/step - loss: 0.0059 - val_loss: 0.0078\n",
      "Epoch 24/24\n",
      "3690/3690 [==============================] - 2s 589us/step - loss: 0.0021 - val_loss: 0.0234\n",
      "\n",
      "GENERATION 3\n",
      "\n",
      "Train on 3690 samples, validate on 410 samples\n",
      "Epoch 1/24\n",
      "3690/3690 [==============================] - 5s 1ms/step - loss: 0.0517 - val_loss: 0.0125\n",
      "Epoch 2/24\n",
      "3690/3690 [==============================] - 2s 588us/step - loss: 0.0094 - val_loss: 0.0011\n",
      "Epoch 3/24\n",
      "3690/3690 [==============================] - 2s 591us/step - loss: 0.0056 - val_loss: 0.0060\n",
      "Epoch 4/24\n",
      "3690/3690 [==============================] - 2s 587us/step - loss: 0.0029 - val_loss: 0.0065\n",
      "Epoch 5/24\n",
      "3690/3690 [==============================] - 2s 587us/step - loss: 0.0020 - val_loss: 0.0015\n",
      "Epoch 6/24\n",
      "3690/3690 [==============================] - 2s 587us/step - loss: 0.0013 - val_loss: 5.6304e-04\n",
      "Epoch 7/24\n",
      "3690/3690 [==============================] - 2s 588us/step - loss: 0.0011 - val_loss: 5.9580e-04\n",
      "Epoch 8/24\n",
      "3690/3690 [==============================] - 2s 588us/step - loss: 0.0011 - val_loss: 6.2930e-04\n",
      "Epoch 9/24\n",
      "3690/3690 [==============================] - 2s 587us/step - loss: 0.0011 - val_loss: 9.6004e-04\n",
      "Epoch 10/24\n",
      "3690/3690 [==============================] - 2s 588us/step - loss: 9.8708e-04 - val_loss: 0.0011\n",
      "Epoch 11/24\n",
      "3690/3690 [==============================] - 2s 588us/step - loss: 0.0010 - val_loss: 0.0012\n",
      "Epoch 12/24\n",
      "3690/3690 [==============================] - 2s 591us/step - loss: 9.7349e-04 - val_loss: 0.0013\n",
      "Epoch 13/24\n",
      "3690/3690 [==============================] - 2s 590us/step - loss: 9.2553e-04 - val_loss: 0.0012\n",
      "Epoch 14/24\n",
      "3690/3690 [==============================] - 2s 591us/step - loss: 8.8039e-04 - val_loss: 0.0010\n",
      "Epoch 15/24\n",
      "3690/3690 [==============================] - 2s 593us/step - loss: 8.4967e-04 - val_loss: 8.9510e-04\n",
      "Epoch 16/24\n",
      "3690/3690 [==============================] - 2s 594us/step - loss: 9.2146e-04 - val_loss: 7.1408e-04\n",
      "Epoch 17/24\n",
      "3690/3690 [==============================] - 2s 593us/step - loss: 8.4521e-04 - val_loss: 6.0055e-04\n",
      "Epoch 18/24\n",
      "3690/3690 [==============================] - 2s 588us/step - loss: 8.5069e-04 - val_loss: 7.8793e-04\n",
      "Epoch 19/24\n",
      "3690/3690 [==============================] - 2s 588us/step - loss: 8.1500e-04 - val_loss: 0.0011\n",
      "Epoch 20/24\n",
      "3690/3690 [==============================] - 2s 599us/step - loss: 8.0842e-04 - val_loss: 0.0019\n",
      "Epoch 21/24\n",
      "3690/3690 [==============================] - 2s 582us/step - loss: 8.7603e-04 - val_loss: 7.4318e-04\n",
      "Epoch 22/24\n",
      "3690/3690 [==============================] - 2s 581us/step - loss: 8.5733e-04 - val_loss: 8.5064e-04\n",
      "Epoch 23/24\n",
      "3690/3690 [==============================] - 2s 581us/step - loss: 8.2402e-04 - val_loss: 9.3778e-04\n",
      "Epoch 24/24\n",
      "3690/3690 [==============================] - 2s 576us/step - loss: 8.6350e-04 - val_loss: 5.8188e-04\n",
      "Train on 3690 samples, validate on 410 samples\n",
      "Epoch 1/24\n",
      "3690/3690 [==============================] - 5s 1ms/step - loss: 0.3290 - val_loss: 0.0048\n",
      "Epoch 2/24\n",
      "3690/3690 [==============================] - 2s 545us/step - loss: 0.0295 - val_loss: 0.0036\n",
      "Epoch 3/24\n",
      "3690/3690 [==============================] - 2s 549us/step - loss: 0.0063 - val_loss: 0.0045\n",
      "Epoch 4/24\n",
      "3690/3690 [==============================] - 2s 545us/step - loss: 0.0021 - val_loss: 0.0050\n",
      "Epoch 5/24\n",
      "3690/3690 [==============================] - 2s 547us/step - loss: 0.0015 - val_loss: 0.0034\n",
      "Epoch 6/24\n",
      "3690/3690 [==============================] - 2s 547us/step - loss: 0.0012 - val_loss: 0.0037\n",
      "Epoch 7/24\n",
      "3690/3690 [==============================] - 2s 543us/step - loss: 0.0013 - val_loss: 0.0045\n",
      "Epoch 8/24\n",
      "3690/3690 [==============================] - 2s 548us/step - loss: 0.0014 - val_loss: 0.0051\n",
      "Epoch 9/24\n",
      "3690/3690 [==============================] - 2s 546us/step - loss: 0.0019 - val_loss: 0.0087\n",
      "Epoch 10/24\n",
      "3690/3690 [==============================] - 2s 546us/step - loss: 0.0016 - val_loss: 0.0051\n",
      "Epoch 11/24\n",
      "3690/3690 [==============================] - 2s 547us/step - loss: 0.0012 - val_loss: 0.0017\n",
      "Epoch 12/24\n",
      "3690/3690 [==============================] - 2s 545us/step - loss: 0.0010 - val_loss: 0.0012\n",
      "Epoch 13/24\n",
      "3690/3690 [==============================] - 2s 548us/step - loss: 0.0011 - val_loss: 8.3557e-04\n",
      "Epoch 14/24\n",
      "3690/3690 [==============================] - 2s 547us/step - loss: 0.0011 - val_loss: 7.1137e-04\n",
      "Epoch 15/24\n",
      "3690/3690 [==============================] - 2s 547us/step - loss: 0.0013 - val_loss: 7.0365e-04\n",
      "Epoch 16/24\n",
      "3690/3690 [==============================] - 2s 547us/step - loss: 0.0012 - val_loss: 7.0935e-04\n",
      "Epoch 17/24\n",
      "3690/3690 [==============================] - 2s 546us/step - loss: 0.0011 - val_loss: 6.8051e-04\n",
      "Epoch 18/24\n",
      "3690/3690 [==============================] - 2s 547us/step - loss: 0.0011 - val_loss: 0.0011\n",
      "Epoch 19/24\n",
      "3690/3690 [==============================] - 2s 551us/step - loss: 8.4812e-04 - val_loss: 0.0010\n",
      "Epoch 20/24\n",
      "3690/3690 [==============================] - 2s 547us/step - loss: 8.5048e-04 - val_loss: 7.7718e-04\n",
      "Epoch 21/24\n",
      "3690/3690 [==============================] - 2s 548us/step - loss: 0.0011 - val_loss: 8.3224e-04\n",
      "Epoch 22/24\n",
      "3690/3690 [==============================] - 2s 547us/step - loss: 0.0013 - val_loss: 7.5797e-04\n",
      "Epoch 23/24\n",
      "3690/3690 [==============================] - 2s 545us/step - loss: 0.0012 - val_loss: 6.5007e-04\n",
      "Epoch 24/24\n",
      "3690/3690 [==============================] - 2s 545us/step - loss: 8.9043e-04 - val_loss: 0.0011\n",
      "Train on 3690 samples, validate on 410 samples\n",
      "Epoch 1/24\n",
      "3690/3690 [==============================] - 5s 1ms/step - loss: 0.0439 - val_loss: 0.0019\n",
      "Epoch 2/24\n",
      "3690/3690 [==============================] - 2s 545us/step - loss: 0.0072 - val_loss: 0.0114\n",
      "Epoch 3/24\n",
      "3690/3690 [==============================] - 2s 544us/step - loss: 0.0095 - val_loss: 0.0131\n",
      "Epoch 4/24\n",
      "3690/3690 [==============================] - 2s 546us/step - loss: 0.0078 - val_loss: 0.0154\n",
      "Epoch 5/24\n",
      "3690/3690 [==============================] - 2s 546us/step - loss: 0.0080 - val_loss: 0.0115\n",
      "Epoch 6/24\n",
      "3690/3690 [==============================] - 2s 547us/step - loss: 0.0060 - val_loss: 0.0065\n",
      "Epoch 7/24\n",
      "3690/3690 [==============================] - 2s 544us/step - loss: 0.0046 - val_loss: 0.0119\n",
      "Epoch 8/24\n",
      "3690/3690 [==============================] - 2s 542us/step - loss: 0.0107 - val_loss: 0.0124\n",
      "Epoch 9/24\n",
      "3690/3690 [==============================] - 2s 544us/step - loss: 0.0055 - val_loss: 0.0073\n",
      "Epoch 10/24\n",
      "3690/3690 [==============================] - 2s 541us/step - loss: 0.0047 - val_loss: 0.0150\n",
      "Epoch 11/24\n",
      "3690/3690 [==============================] - 2s 545us/step - loss: 0.0071 - val_loss: 0.0068\n",
      "Epoch 12/24\n",
      "3690/3690 [==============================] - 2s 543us/step - loss: 0.0042 - val_loss: 0.0084\n",
      "Epoch 13/24\n",
      "3690/3690 [==============================] - 2s 547us/step - loss: 0.0051 - val_loss: 0.0149\n",
      "Epoch 14/24\n",
      "3690/3690 [==============================] - 2s 544us/step - loss: 0.0068 - val_loss: 0.0113\n",
      "Epoch 15/24\n",
      "3690/3690 [==============================] - 2s 542us/step - loss: 0.0072 - val_loss: 0.0147\n",
      "Epoch 16/24\n",
      "3690/3690 [==============================] - 2s 542us/step - loss: 0.0057 - val_loss: 0.0084\n",
      "Epoch 17/24\n",
      "3690/3690 [==============================] - 2s 540us/step - loss: 0.0035 - val_loss: 0.0088\n",
      "Epoch 18/24\n",
      "3690/3690 [==============================] - 2s 541us/step - loss: 0.0038 - val_loss: 0.0083\n",
      "Epoch 19/24\n",
      "3690/3690 [==============================] - 2s 544us/step - loss: 0.0037 - val_loss: 0.0060\n",
      "Epoch 20/24\n",
      "3690/3690 [==============================] - 2s 544us/step - loss: 0.0042 - val_loss: 0.0139\n",
      "Epoch 21/24\n",
      "3690/3690 [==============================] - 2s 545us/step - loss: 0.0050 - val_loss: 0.0068\n",
      "Epoch 22/24\n",
      "3690/3690 [==============================] - 2s 545us/step - loss: 0.0029 - val_loss: 0.0075\n",
      "Epoch 23/24\n",
      "3690/3690 [==============================] - 2s 540us/step - loss: 0.0042 - val_loss: 0.0116\n",
      "Epoch 24/24\n",
      "3690/3690 [==============================] - 2s 542us/step - loss: 0.0040 - val_loss: 0.0114\n",
      "Train on 3690 samples, validate on 410 samples\n",
      "Epoch 1/24\n",
      "3690/3690 [==============================] - 4s 1ms/step - loss: nan - val_loss: nan\n",
      "Epoch 2/24\n",
      "3690/3690 [==============================] - 1s 382us/step - loss: nan - val_loss: nan\n",
      "Epoch 3/24\n",
      "3690/3690 [==============================] - 1s 390us/step - loss: nan - val_loss: nan\n",
      "Epoch 4/24\n",
      "3690/3690 [==============================] - 1s 384us/step - loss: nan - val_loss: nan\n",
      "Epoch 5/24\n",
      "3690/3690 [==============================] - 1s 384us/step - loss: nan - val_loss: nan\n",
      "Epoch 6/24\n",
      "3690/3690 [==============================] - 1s 386us/step - loss: nan - val_loss: nan\n",
      "Epoch 7/24\n",
      "3690/3690 [==============================] - 1s 383us/step - loss: nan - val_loss: nan\n",
      "Epoch 8/24\n",
      "3690/3690 [==============================] - 1s 385us/step - loss: nan - val_loss: nan\n",
      "Epoch 9/24\n",
      "3690/3690 [==============================] - 1s 383us/step - loss: nan - val_loss: nan\n",
      "Epoch 10/24\n",
      "3690/3690 [==============================] - 1s 385us/step - loss: nan - val_loss: nan\n",
      "Epoch 11/24\n",
      "3690/3690 [==============================] - 1s 385us/step - loss: nan - val_loss: nan\n",
      "Epoch 12/24\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3690/3690 [==============================] - 1s 382us/step - loss: nan - val_loss: nan\n",
      "Epoch 13/24\n",
      "3690/3690 [==============================] - 1s 385us/step - loss: nan - val_loss: nan\n",
      "Epoch 14/24\n",
      "3690/3690 [==============================] - 1s 388us/step - loss: nan - val_loss: nan\n",
      "Epoch 15/24\n",
      "3690/3690 [==============================] - 1s 385us/step - loss: nan - val_loss: nan\n",
      "Epoch 16/24\n",
      "3690/3690 [==============================] - 1s 382us/step - loss: nan - val_loss: nan\n",
      "Epoch 17/24\n",
      "3690/3690 [==============================] - 1s 383us/step - loss: nan - val_loss: nan\n",
      "Epoch 18/24\n",
      "3690/3690 [==============================] - 1s 385us/step - loss: nan - val_loss: nan\n",
      "Epoch 19/24\n",
      "3690/3690 [==============================] - 1s 384us/step - loss: nan - val_loss: nan\n",
      "Epoch 20/24\n",
      "3690/3690 [==============================] - 1s 383us/step - loss: nan - val_loss: nan\n",
      "Epoch 21/24\n",
      "3690/3690 [==============================] - 1s 382us/step - loss: nan - val_loss: nan\n",
      "Epoch 22/24\n",
      "3690/3690 [==============================] - 1s 384us/step - loss: nan - val_loss: nan\n",
      "Epoch 23/24\n",
      "3690/3690 [==============================] - 1s 384us/step - loss: nan - val_loss: nan\n",
      "Epoch 24/24\n",
      "3690/3690 [==============================] - 1s 382us/step - loss: nan - val_loss: nan\n",
      "Train on 3690 samples, validate on 410 samples\n",
      "Epoch 1/24\n",
      "3690/3690 [==============================] - 5s 1ms/step - loss: 4.3954 - val_loss: 0.1061\n",
      "Epoch 2/24\n",
      "3690/3690 [==============================] - 1s 384us/step - loss: 0.0788 - val_loss: 0.0459\n",
      "Epoch 3/24\n",
      "3690/3690 [==============================] - 1s 384us/step - loss: 0.0680 - val_loss: 0.0350\n",
      "Epoch 4/24\n",
      "3690/3690 [==============================] - 1s 385us/step - loss: 0.0678 - val_loss: 0.0255\n",
      "Epoch 5/24\n",
      "3690/3690 [==============================] - 1s 381us/step - loss: 0.0670 - val_loss: 0.0224\n",
      "Epoch 6/24\n",
      "3690/3690 [==============================] - 1s 384us/step - loss: 0.0626 - val_loss: 0.0055\n",
      "Epoch 7/24\n",
      "3690/3690 [==============================] - 1s 388us/step - loss: 0.0562 - val_loss: 0.0039\n",
      "Epoch 8/24\n",
      "3690/3690 [==============================] - 1s 383us/step - loss: 0.0574 - val_loss: 0.0218\n",
      "Epoch 9/24\n",
      "3690/3690 [==============================] - 1s 385us/step - loss: 0.0653 - val_loss: 0.0039\n",
      "Epoch 10/24\n",
      "3690/3690 [==============================] - 1s 384us/step - loss: 0.0472 - val_loss: 0.0042\n",
      "Epoch 11/24\n",
      "3690/3690 [==============================] - 1s 383us/step - loss: 0.0092 - val_loss: 0.1015\n",
      "Epoch 12/24\n",
      "3690/3690 [==============================] - 1s 384us/step - loss: 0.0357 - val_loss: 0.0062\n",
      "Epoch 13/24\n",
      "3690/3690 [==============================] - 1s 383us/step - loss: 0.0062 - val_loss: 0.0157\n",
      "Epoch 14/24\n",
      "3690/3690 [==============================] - 1s 384us/step - loss: 0.0063 - val_loss: 0.0318\n",
      "Epoch 15/24\n",
      "3690/3690 [==============================] - 1s 384us/step - loss: 0.0147 - val_loss: 0.0094\n",
      "Epoch 16/24\n",
      "3690/3690 [==============================] - ETA: 0s - loss: 0.004 - 1s 380us/step - loss: 0.0045 - val_loss: 0.0125\n",
      "Epoch 17/24\n",
      "3690/3690 [==============================] - 1s 387us/step - loss: 0.0056 - val_loss: 0.0189\n",
      "Epoch 18/24\n",
      "3690/3690 [==============================] - 1s 387us/step - loss: 0.0150 - val_loss: 0.0062\n",
      "Epoch 19/24\n",
      "3690/3690 [==============================] - 1s 387us/step - loss: 0.0045 - val_loss: 0.0060\n",
      "Epoch 20/24\n",
      "3690/3690 [==============================] - 1s 382us/step - loss: 0.0037 - val_loss: 0.0050\n",
      "Epoch 21/24\n",
      "3690/3690 [==============================] - 1s 384us/step - loss: 0.0096 - val_loss: 0.0328\n",
      "Epoch 22/24\n",
      "3690/3690 [==============================] - 1s 384us/step - loss: 0.0094 - val_loss: 0.0059\n",
      "Epoch 23/24\n",
      "3690/3690 [==============================] - 1s 385us/step - loss: 0.0035 - val_loss: 0.0022\n",
      "Epoch 24/24\n",
      "3690/3690 [==============================] - 1s 383us/step - loss: 0.0035 - val_loss: 0.0024\n",
      "Train on 3690 samples, validate on 410 samples\n",
      "Epoch 1/24\n",
      "3690/3690 [==============================] - 5s 1ms/step - loss: 0.0506 - val_loss: 7.0101e-04\n",
      "Epoch 2/24\n",
      "3690/3690 [==============================] - 1s 384us/step - loss: 0.0021 - val_loss: 0.0224\n",
      "Epoch 3/24\n",
      "3690/3690 [==============================] - 1s 385us/step - loss: 0.0112 - val_loss: 7.1475e-04\n",
      "Epoch 4/24\n",
      "3690/3690 [==============================] - 1s 384us/step - loss: 0.0014 - val_loss: 6.0502e-04\n",
      "Epoch 5/24\n",
      "3690/3690 [==============================] - 1s 384us/step - loss: 0.0015 - val_loss: 9.9534e-04\n",
      "Epoch 6/24\n",
      "3690/3690 [==============================] - 1s 383us/step - loss: 0.0030 - val_loss: 0.0068\n",
      "Epoch 7/24\n",
      "3690/3690 [==============================] - 1s 385us/step - loss: 0.0044 - val_loss: 0.0030\n",
      "Epoch 8/24\n",
      "3690/3690 [==============================] - 1s 384us/step - loss: 0.0035 - val_loss: 0.0020\n",
      "Epoch 9/24\n",
      "3690/3690 [==============================] - 1s 383us/step - loss: 0.0025 - val_loss: 8.3232e-04\n",
      "Epoch 10/24\n",
      "3690/3690 [==============================] - 1s 384us/step - loss: 0.0022 - val_loss: 0.0014\n",
      "Epoch 11/24\n",
      "3690/3690 [==============================] - 1s 387us/step - loss: 0.0030 - val_loss: 0.0013\n",
      "Epoch 12/24\n",
      "3690/3690 [==============================] - 1s 385us/step - loss: 0.0015 - val_loss: 5.6022e-04\n",
      "Epoch 13/24\n",
      "3690/3690 [==============================] - 1s 387us/step - loss: 0.0010 - val_loss: 0.0011\n",
      "Epoch 14/24\n",
      "3690/3690 [==============================] - 1s 386us/step - loss: 0.0046 - val_loss: 0.0053\n",
      "Epoch 15/24\n",
      "3690/3690 [==============================] - 1s 386us/step - loss: 0.0035 - val_loss: 0.0013\n",
      "Epoch 16/24\n",
      "3690/3690 [==============================] - 1s 385us/step - loss: 0.0018 - val_loss: 0.0012\n",
      "Epoch 17/24\n",
      "3690/3690 [==============================] - 1s 384us/step - loss: 0.0016 - val_loss: 9.7220e-04\n",
      "Epoch 18/24\n",
      "3690/3690 [==============================] - 1s 384us/step - loss: 0.0020 - val_loss: 0.0010\n",
      "Epoch 19/24\n",
      "3690/3690 [==============================] - 1s 386us/step - loss: 0.0015 - val_loss: 0.0012\n",
      "Epoch 20/24\n",
      "3690/3690 [==============================] - 1s 385us/step - loss: 0.0023 - val_loss: 5.4390e-04\n",
      "Epoch 21/24\n",
      "3690/3690 [==============================] - 1s 383us/step - loss: 0.0010 - val_loss: 4.2097e-04\n",
      "Epoch 22/24\n",
      "3690/3690 [==============================] - 1s 389us/step - loss: 9.6919e-04 - val_loss: 8.6853e-04\n",
      "Epoch 23/24\n",
      "3690/3690 [==============================] - 1s 386us/step - loss: 0.0028 - val_loss: 0.0026\n",
      "Epoch 24/24\n",
      "3690/3690 [==============================] - 1s 385us/step - loss: 0.0027 - val_loss: 0.0030\n",
      "Train on 3690 samples, validate on 410 samples\n",
      "Epoch 1/24\n",
      "3690/3690 [==============================] - 5s 1ms/step - loss: 0.0437 - val_loss: 0.0023\n",
      "Epoch 2/24\n",
      "3690/3690 [==============================] - 1s 387us/step - loss: 0.0127 - val_loss: 7.3576e-04\n",
      "Epoch 3/24\n",
      "3690/3690 [==============================] - 1s 385us/step - loss: 0.0015 - val_loss: 0.0026\n",
      "Epoch 4/24\n",
      "3690/3690 [==============================] - 1s 386us/step - loss: 0.0015 - val_loss: 0.0043\n",
      "Epoch 5/24\n",
      "3690/3690 [==============================] - 1s 383us/step - loss: 0.0043 - val_loss: 0.0131\n",
      "Epoch 6/24\n",
      "3690/3690 [==============================] - 1s 386us/step - loss: 0.0049 - val_loss: 0.0027\n",
      "Epoch 7/24\n",
      "3690/3690 [==============================] - 1s 387us/step - loss: 0.0013 - val_loss: 0.0032\n",
      "Epoch 8/24\n",
      "3690/3690 [==============================] - 1s 384us/step - loss: 0.0025 - val_loss: 0.0110\n",
      "Epoch 9/24\n",
      "3690/3690 [==============================] - 1s 384us/step - loss: 0.0039 - val_loss: 0.0045\n",
      "Epoch 10/24\n",
      "3690/3690 [==============================] - 1s 385us/step - loss: 0.0020 - val_loss: 0.0043\n",
      "Epoch 11/24\n",
      "3690/3690 [==============================] - 1s 385us/step - loss: 0.0021 - val_loss: 0.0061\n",
      "Epoch 12/24\n",
      "3690/3690 [==============================] - 1s 384us/step - loss: 0.0027 - val_loss: 0.0087\n",
      "Epoch 13/24\n",
      "3690/3690 [==============================] - 1s 387us/step - loss: 0.0037 - val_loss: 0.0054\n",
      "Epoch 14/24\n",
      "3690/3690 [==============================] - 1s 388us/step - loss: 0.0016 - val_loss: 0.0024\n",
      "Epoch 15/24\n",
      "3690/3690 [==============================] - 1s 388us/step - loss: 0.0011 - val_loss: 0.0015\n",
      "Epoch 16/24\n",
      "3690/3690 [==============================] - 1s 387us/step - loss: 0.0011 - val_loss: 0.0035\n",
      "Epoch 17/24\n",
      "3690/3690 [==============================] - 1s 384us/step - loss: 0.0022 - val_loss: 0.0066\n",
      "Epoch 18/24\n",
      "3690/3690 [==============================] - 1s 384us/step - loss: 0.0031 - val_loss: 0.0064\n",
      "Epoch 19/24\n",
      "3690/3690 [==============================] - 1s 385us/step - loss: 0.0021 - val_loss: 0.0020\n",
      "Epoch 20/24\n",
      "3690/3690 [==============================] - 1s 387us/step - loss: 8.7492e-04 - val_loss: 0.0012\n",
      "Epoch 21/24\n",
      "3690/3690 [==============================] - 1s 385us/step - loss: 7.6245e-04 - val_loss: 4.0761e-04\n",
      "Epoch 22/24\n",
      "3690/3690 [==============================] - 1s 387us/step - loss: 0.0010 - val_loss: 0.0019\n",
      "Epoch 23/24\n",
      "3690/3690 [==============================] - 1s 385us/step - loss: 0.0041 - val_loss: 4.4992e-04\n",
      "Epoch 24/24\n",
      "3690/3690 [==============================] - 1s 386us/step - loss: 0.0013 - val_loss: 5.0590e-04\n",
      "Train on 3690 samples, validate on 410 samples\n",
      "Epoch 1/24\n",
      "3690/3690 [==============================] - 5s 1ms/step - loss: 0.0779 - val_loss: 0.0101\n",
      "Epoch 2/24\n",
      "3690/3690 [==============================] - 1s 381us/step - loss: 0.0234 - val_loss: 0.0045\n",
      "Epoch 3/24\n",
      "3690/3690 [==============================] - 1s 381us/step - loss: 0.0179 - val_loss: 0.0035\n",
      "Epoch 4/24\n",
      "3690/3690 [==============================] - 1s 382us/step - loss: 0.0133 - val_loss: 0.0022\n",
      "Epoch 5/24\n",
      "3690/3690 [==============================] - 1s 382us/step - loss: 0.0100 - val_loss: 0.0016\n",
      "Epoch 6/24\n",
      "3690/3690 [==============================] - 1s 381us/step - loss: 0.0082 - val_loss: 0.0011\n",
      "Epoch 7/24\n",
      "3690/3690 [==============================] - 1s 386us/step - loss: 0.0065 - val_loss: 8.8738e-04\n",
      "Epoch 8/24\n",
      "3690/3690 [==============================] - 1s 383us/step - loss: 0.0055 - val_loss: 9.2775e-04\n",
      "Epoch 9/24\n",
      "3690/3690 [==============================] - 1s 382us/step - loss: 0.0048 - val_loss: 8.6441e-04\n",
      "Epoch 10/24\n",
      "3690/3690 [==============================] - 1s 384us/step - loss: 0.0041 - val_loss: 0.0011\n",
      "Epoch 11/24\n",
      "3690/3690 [==============================] - 1s 381us/step - loss: 0.0038 - val_loss: 0.0014\n",
      "Epoch 12/24\n",
      "3690/3690 [==============================] - 1s 382us/step - loss: 0.0035 - val_loss: 0.0013\n",
      "Epoch 13/24\n",
      "3690/3690 [==============================] - 1s 383us/step - loss: 0.0033 - val_loss: 0.0016\n",
      "Epoch 14/24\n",
      "3690/3690 [==============================] - 1s 383us/step - loss: 0.0033 - val_loss: 0.0016\n",
      "Epoch 15/24\n",
      "3690/3690 [==============================] - 1s 381us/step - loss: 0.0032 - val_loss: 0.0018\n",
      "Epoch 16/24\n",
      "3690/3690 [==============================] - 1s 381us/step - loss: 0.0031 - val_loss: 0.0016\n",
      "Epoch 17/24\n",
      "3690/3690 [==============================] - 1s 379us/step - loss: 0.0033 - val_loss: 0.0015\n",
      "Epoch 18/24\n",
      "3690/3690 [==============================] - 1s 384us/step - loss: 0.0031 - val_loss: 0.0017\n",
      "Epoch 19/24\n",
      "3690/3690 [==============================] - 1s 384us/step - loss: 0.0029 - val_loss: 0.0017\n",
      "Epoch 20/24\n",
      "3690/3690 [==============================] - 1s 380us/step - loss: 0.0030 - val_loss: 0.0019\n",
      "Epoch 21/24\n",
      "3690/3690 [==============================] - 1s 385us/step - loss: 0.0030 - val_loss: 0.0019\n",
      "Epoch 22/24\n",
      "3690/3690 [==============================] - 1s 384us/step - loss: 0.0029 - val_loss: 0.0021\n",
      "Epoch 23/24\n",
      "3690/3690 [==============================] - 1s 384us/step - loss: 0.0029 - val_loss: 0.0021\n",
      "Epoch 24/24\n",
      "3690/3690 [==============================] - 1s 381us/step - loss: 0.0029 - val_loss: 0.0019\n",
      "Train on 3690 samples, validate on 410 samples\n",
      "Epoch 1/24\n",
      "3690/3690 [==============================] - 5s 1ms/step - loss: 0.0353 - val_loss: 0.0022\n",
      "Epoch 2/24\n",
      "3690/3690 [==============================] - 1s 382us/step - loss: 0.0148 - val_loss: 0.0020\n",
      "Epoch 3/24\n",
      "3690/3690 [==============================] - 1s 382us/step - loss: 0.0111 - val_loss: 0.0011\n",
      "Epoch 4/24\n",
      "3690/3690 [==============================] - 1s 385us/step - loss: 0.0088 - val_loss: 8.3526e-04\n",
      "Epoch 5/24\n",
      "3690/3690 [==============================] - 1s 381us/step - loss: 0.0075 - val_loss: 6.8743e-04\n",
      "Epoch 6/24\n",
      "3690/3690 [==============================] - 1s 385us/step - loss: 0.0069 - val_loss: 6.8528e-04\n",
      "Epoch 7/24\n",
      "3690/3690 [==============================] - 1s 387us/step - loss: 0.0062 - val_loss: 7.8810e-04\n",
      "Epoch 8/24\n",
      "3690/3690 [==============================] - 1s 380us/step - loss: 0.0058 - val_loss: 6.3159e-04\n",
      "Epoch 9/24\n",
      "3690/3690 [==============================] - 1s 385us/step - loss: 0.0054 - val_loss: 7.9880e-04\n",
      "Epoch 10/24\n",
      "3690/3690 [==============================] - 1s 382us/step - loss: 0.0054 - val_loss: 9.4172e-04\n",
      "Epoch 11/24\n",
      "3690/3690 [==============================] - 1s 390us/step - loss: 0.0050 - val_loss: 9.9119e-04\n",
      "Epoch 12/24\n",
      "3690/3690 [==============================] - 1s 382us/step - loss: 0.0047 - val_loss: 0.0011\n",
      "Epoch 13/24\n",
      "3690/3690 [==============================] - 1s 384us/step - loss: 0.0048 - val_loss: 0.0011\n",
      "Epoch 14/24\n",
      "3690/3690 [==============================] - 1s 383us/step - loss: 0.0050 - val_loss: 0.0013\n",
      "Epoch 15/24\n",
      "3690/3690 [==============================] - 1s 384us/step - loss: 0.0046 - val_loss: 8.9449e-04\n",
      "Epoch 16/24\n",
      "3690/3690 [==============================] - 1s 381us/step - loss: 0.0045 - val_loss: 0.0010\n",
      "Epoch 17/24\n",
      "3690/3690 [==============================] - 1s 381us/step - loss: 0.0045 - val_loss: 8.6860e-04\n",
      "Epoch 18/24\n",
      "3690/3690 [==============================] - 1s 383us/step - loss: 0.0041 - val_loss: 8.8310e-04\n",
      "Epoch 19/24\n",
      "3690/3690 [==============================] - 1s 380us/step - loss: 0.0043 - val_loss: 0.0013\n",
      "Epoch 20/24\n",
      "3690/3690 [==============================] - 1s 382us/step - loss: 0.0043 - val_loss: 9.9938e-04\n",
      "Epoch 21/24\n",
      "3690/3690 [==============================] - ETA: 0s - loss: 0.004 - 1s 381us/step - loss: 0.0041 - val_loss: 0.0013\n",
      "Epoch 22/24\n",
      "3690/3690 [==============================] - 1s 387us/step - loss: 0.0041 - val_loss: 8.1241e-04\n",
      "Epoch 23/24\n",
      "3690/3690 [==============================] - 1s 380us/step - loss: 0.0039 - val_loss: 9.6506e-04\n",
      "Epoch 24/24\n",
      "3690/3690 [==============================] - 1s 380us/step - loss: 0.0041 - val_loss: 7.2487e-04\n",
      "\n",
      "GENERATION 4\n",
      "\n",
      "Train on 3690 samples, validate on 410 samples\n",
      "Epoch 1/24\n",
      "3690/3690 [==============================] - 6s 2ms/step - loss: 0.0614 - val_loss: 0.0130\n",
      "Epoch 2/24\n",
      "3690/3690 [==============================] - 2s 581us/step - loss: 0.0093 - val_loss: 0.0029\n",
      "Epoch 3/24\n",
      "3690/3690 [==============================] - 2s 579us/step - loss: 0.0046 - val_loss: 0.0128\n",
      "Epoch 4/24\n",
      "3690/3690 [==============================] - 2s 578us/step - loss: 0.0035 - val_loss: 0.0015\n",
      "Epoch 5/24\n",
      "3690/3690 [==============================] - 2s 580us/step - loss: 0.0024 - val_loss: 7.4337e-04\n",
      "Epoch 6/24\n",
      "3690/3690 [==============================] - 2s 577us/step - loss: 0.0019 - val_loss: 0.0030\n",
      "Epoch 7/24\n",
      "3690/3690 [==============================] - 2s 581us/step - loss: 0.0014 - val_loss: 0.0024\n",
      "Epoch 8/24\n",
      "3690/3690 [==============================] - 2s 581us/step - loss: 0.0012 - val_loss: 0.0014\n",
      "Epoch 9/24\n",
      "3690/3690 [==============================] - 2s 581us/step - loss: 0.0011 - val_loss: 6.3126e-04\n",
      "Epoch 10/24\n",
      "3690/3690 [==============================] - 2s 580us/step - loss: 9.9650e-04 - val_loss: 5.1513e-04\n",
      "Epoch 11/24\n",
      "3690/3690 [==============================] - 2s 582us/step - loss: 0.0010 - val_loss: 6.4582e-04\n",
      "Epoch 12/24\n",
      "3690/3690 [==============================] - 2s 578us/step - loss: 9.7045e-04 - val_loss: 7.3027e-04\n",
      "Epoch 13/24\n",
      "3690/3690 [==============================] - 2s 581us/step - loss: 9.6640e-04 - val_loss: 6.9853e-04\n",
      "Epoch 14/24\n",
      "3690/3690 [==============================] - 2s 581us/step - loss: 9.2160e-04 - val_loss: 7.1597e-04\n",
      "Epoch 15/24\n",
      "3690/3690 [==============================] - 2s 577us/step - loss: 9.2403e-04 - val_loss: 6.3374e-04\n",
      "Epoch 16/24\n",
      "3690/3690 [==============================] - 2s 581us/step - loss: 8.8949e-04 - val_loss: 5.5265e-04\n",
      "Epoch 17/24\n",
      "3690/3690 [==============================] - 2s 579us/step - loss: 8.8713e-04 - val_loss: 6.3886e-04\n",
      "Epoch 18/24\n",
      "3690/3690 [==============================] - 2s 578us/step - loss: 8.9730e-04 - val_loss: 7.9607e-04\n",
      "Epoch 19/24\n",
      "3690/3690 [==============================] - 2s 584us/step - loss: 9.0130e-04 - val_loss: 9.9027e-04\n",
      "Epoch 20/24\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3690/3690 [==============================] - 2s 582us/step - loss: 8.5818e-04 - val_loss: 7.0591e-04\n",
      "Epoch 21/24\n",
      "3690/3690 [==============================] - 2s 578us/step - loss: 7.7319e-04 - val_loss: 4.5916e-04\n",
      "Epoch 22/24\n",
      "3690/3690 [==============================] - 2s 577us/step - loss: 8.5472e-04 - val_loss: 5.3973e-04\n",
      "Epoch 23/24\n",
      "3690/3690 [==============================] - 2s 581us/step - loss: 8.7640e-04 - val_loss: 0.0010\n",
      "Epoch 24/24\n",
      "3690/3690 [==============================] - 2s 586us/step - loss: 8.3102e-04 - val_loss: 6.4350e-04\n",
      "Train on 3690 samples, validate on 410 samples\n",
      "Epoch 1/24\n",
      "3690/3690 [==============================] - 6s 2ms/step - loss: 0.0726 - val_loss: 0.0138\n",
      "Epoch 2/24\n",
      "3690/3690 [==============================] - 2s 583us/step - loss: 0.0123 - val_loss: 0.0105\n",
      "Epoch 3/24\n",
      "3690/3690 [==============================] - 2s 581us/step - loss: 0.0048 - val_loss: 0.0113\n",
      "Epoch 4/24\n",
      "3690/3690 [==============================] - 2s 582us/step - loss: 0.0028 - val_loss: 0.0023\n",
      "Epoch 5/24\n",
      "3690/3690 [==============================] - 2s 579us/step - loss: 0.0016 - val_loss: 6.0659e-04\n",
      "Epoch 6/24\n",
      "3690/3690 [==============================] - 2s 578us/step - loss: 0.0014 - val_loss: 5.3371e-04\n",
      "Epoch 7/24\n",
      "3690/3690 [==============================] - 2s 578us/step - loss: 0.0013 - val_loss: 8.6205e-04\n",
      "Epoch 8/24\n",
      "3690/3690 [==============================] - 2s 578us/step - loss: 0.0011 - val_loss: 0.0013\n",
      "Epoch 9/24\n",
      "3690/3690 [==============================] - 2s 578us/step - loss: 0.0010 - val_loss: 0.0016\n",
      "Epoch 10/24\n",
      "3690/3690 [==============================] - 2s 580us/step - loss: 0.0010 - val_loss: 0.0014\n",
      "Epoch 11/24\n",
      "3690/3690 [==============================] - 2s 579us/step - loss: 9.8973e-04 - val_loss: 0.0013\n",
      "Epoch 12/24\n",
      "3690/3690 [==============================] - 2s 579us/step - loss: 9.4018e-04 - val_loss: 0.0012\n",
      "Epoch 13/24\n",
      "3690/3690 [==============================] - 2s 578us/step - loss: 9.2108e-04 - val_loss: 0.0015\n",
      "Epoch 14/24\n",
      "3690/3690 [==============================] - 2s 582us/step - loss: 9.4763e-04 - val_loss: 0.0012\n",
      "Epoch 15/24\n",
      "3690/3690 [==============================] - 2s 582us/step - loss: 8.7807e-04 - val_loss: 9.7799e-04\n",
      "Epoch 16/24\n",
      "3690/3690 [==============================] - 2s 582us/step - loss: 8.6175e-04 - val_loss: 0.0010\n",
      "Epoch 17/24\n",
      "3690/3690 [==============================] - 2s 582us/step - loss: 8.8020e-04 - val_loss: 6.6744e-04\n",
      "Epoch 18/24\n",
      "3690/3690 [==============================] - 2s 584us/step - loss: 8.8015e-04 - val_loss: 8.4310e-04\n",
      "Epoch 19/24\n",
      "3690/3690 [==============================] - 2s 578us/step - loss: 8.7959e-04 - val_loss: 0.0013\n",
      "Epoch 20/24\n",
      "3690/3690 [==============================] - 2s 582us/step - loss: 8.4173e-04 - val_loss: 8.9538e-04\n",
      "Epoch 21/24\n",
      "3690/3690 [==============================] - 2s 580us/step - loss: 8.6210e-04 - val_loss: 5.8443e-04\n",
      "Epoch 22/24\n",
      "3690/3690 [==============================] - 2s 579us/step - loss: 8.9562e-04 - val_loss: 0.0013\n",
      "Epoch 23/24\n",
      "3690/3690 [==============================] - 2s 579us/step - loss: 8.4797e-04 - val_loss: 0.0015\n",
      "Epoch 24/24\n",
      "3690/3690 [==============================] - 2s 578us/step - loss: 7.9444e-04 - val_loss: 7.5879e-04\n",
      "Train on 3690 samples, validate on 410 samples\n",
      "Epoch 1/24\n",
      "3690/3690 [==============================] - 6s 2ms/step - loss: 0.0675 - val_loss: 0.0151\n",
      "Epoch 2/24\n",
      "3690/3690 [==============================] - 2s 509us/step - loss: 0.0147 - val_loss: 0.0156\n",
      "Epoch 3/24\n",
      "3690/3690 [==============================] - 2s 508us/step - loss: 0.0061 - val_loss: 7.9990e-04\n",
      "Epoch 4/24\n",
      "3690/3690 [==============================] - 2s 507us/step - loss: 0.0040 - val_loss: 0.0054\n",
      "Epoch 5/24\n",
      "3690/3690 [==============================] - 2s 511us/step - loss: 0.0029 - val_loss: 0.0030\n",
      "Epoch 6/24\n",
      "3690/3690 [==============================] - 2s 505us/step - loss: 0.0021 - val_loss: 0.0014\n",
      "Epoch 7/24\n",
      "3690/3690 [==============================] - 2s 509us/step - loss: 0.0017 - val_loss: 0.0030\n",
      "Epoch 8/24\n",
      "3690/3690 [==============================] - 2s 505us/step - loss: 0.0015 - val_loss: 0.0019\n",
      "Epoch 9/24\n",
      "3690/3690 [==============================] - 2s 506us/step - loss: 0.0013 - val_loss: 7.4008e-04\n",
      "Epoch 10/24\n",
      "3690/3690 [==============================] - 2s 508us/step - loss: 0.0013 - val_loss: 6.0744e-04\n",
      "Epoch 11/24\n",
      "3690/3690 [==============================] - 2s 508us/step - loss: 0.0012 - val_loss: 6.2810e-04\n",
      "Epoch 12/24\n",
      "3690/3690 [==============================] - 2s 506us/step - loss: 0.0013 - val_loss: 0.0010\n",
      "Epoch 13/24\n",
      "3690/3690 [==============================] - 2s 510us/step - loss: 0.0011 - val_loss: 9.5152e-04\n",
      "Epoch 14/24\n",
      "3690/3690 [==============================] - 2s 508us/step - loss: 0.0011 - val_loss: 0.0014\n",
      "Epoch 15/24\n",
      "3690/3690 [==============================] - 2s 506us/step - loss: 0.0011 - val_loss: 9.1784e-04\n",
      "Epoch 16/24\n",
      "3690/3690 [==============================] - 2s 508us/step - loss: 0.0011 - val_loss: 0.0011\n",
      "Epoch 17/24\n",
      "3690/3690 [==============================] - 2s 509us/step - loss: 0.0010 - val_loss: 7.5229e-04\n",
      "Epoch 18/24\n",
      "3690/3690 [==============================] - 2s 505us/step - loss: 0.0010 - val_loss: 7.5043e-04\n",
      "Epoch 19/24\n",
      "3690/3690 [==============================] - 2s 509us/step - loss: 0.0010 - val_loss: 7.4471e-04\n",
      "Epoch 20/24\n",
      "3690/3690 [==============================] - 2s 507us/step - loss: 9.8884e-04 - val_loss: 7.7110e-04\n",
      "Epoch 21/24\n",
      "3690/3690 [==============================] - 2s 507us/step - loss: 0.0010 - val_loss: 0.0011\n",
      "Epoch 22/24\n",
      "3690/3690 [==============================] - 2s 507us/step - loss: 9.8264e-04 - val_loss: 8.9678e-04\n",
      "Epoch 23/24\n",
      "3690/3690 [==============================] - 2s 508us/step - loss: 9.7406e-04 - val_loss: 8.6207e-04\n",
      "Epoch 24/24\n",
      "3690/3690 [==============================] - 2s 508us/step - loss: 9.9011e-04 - val_loss: 9.3609e-04\n",
      "Train on 3690 samples, validate on 410 samples\n",
      "Epoch 1/24\n",
      "3690/3690 [==============================] - 6s 2ms/step - loss: 1.3176 - val_loss: 0.0011\n",
      "Epoch 2/24\n",
      "3690/3690 [==============================] - 2s 484us/step - loss: 0.0094 - val_loss: 0.0025\n",
      "Epoch 3/24\n",
      "3690/3690 [==============================] - 2s 486us/step - loss: 0.0050 - val_loss: 0.0034\n",
      "Epoch 4/24\n",
      "3690/3690 [==============================] - 2s 485us/step - loss: 0.0039 - val_loss: 7.8446e-04\n",
      "Epoch 5/24\n",
      "3690/3690 [==============================] - 2s 486us/step - loss: 0.0034 - val_loss: 7.8620e-04\n",
      "Epoch 6/24\n",
      "3690/3690 [==============================] - 2s 486us/step - loss: 0.0033 - val_loss: 0.0014\n",
      "Epoch 7/24\n",
      "3690/3690 [==============================] - 2s 484us/step - loss: 0.0029 - val_loss: 0.0033\n",
      "Epoch 8/24\n",
      "3690/3690 [==============================] - 2s 486us/step - loss: 0.0027 - val_loss: 0.0022\n",
      "Epoch 9/24\n",
      "3690/3690 [==============================] - 2s 485us/step - loss: 0.0027 - val_loss: 0.0037\n",
      "Epoch 10/24\n",
      "3690/3690 [==============================] - 2s 488us/step - loss: 0.0026 - val_loss: 5.0976e-04\n",
      "Epoch 11/24\n",
      "3690/3690 [==============================] - 2s 488us/step - loss: 0.0025 - val_loss: 6.2005e-04\n",
      "Epoch 12/24\n",
      "3690/3690 [==============================] - 2s 485us/step - loss: 0.0024 - val_loss: 0.0024\n",
      "Epoch 13/24\n",
      "3690/3690 [==============================] - 2s 486us/step - loss: 0.0024 - val_loss: 0.0011\n",
      "Epoch 14/24\n",
      "3690/3690 [==============================] - 2s 488us/step - loss: 0.0022 - val_loss: 6.1586e-04\n",
      "Epoch 15/24\n",
      "3690/3690 [==============================] - 2s 485us/step - loss: 0.0023 - val_loss: 0.0017\n",
      "Epoch 16/24\n",
      "3690/3690 [==============================] - 2s 487us/step - loss: 0.0021 - val_loss: 7.2694e-04\n",
      "Epoch 17/24\n",
      "3690/3690 [==============================] - 2s 486us/step - loss: 0.0021 - val_loss: 0.0016\n",
      "Epoch 18/24\n",
      "3690/3690 [==============================] - 2s 484us/step - loss: 0.0019 - val_loss: 9.5416e-04\n",
      "Epoch 19/24\n",
      "3690/3690 [==============================] - 2s 488us/step - loss: 0.0020 - val_loss: 0.0021\n",
      "Epoch 20/24\n",
      "3690/3690 [==============================] - 2s 484us/step - loss: 0.0019 - val_loss: 0.0020\n",
      "Epoch 21/24\n",
      "3690/3690 [==============================] - 2s 485us/step - loss: 0.0021 - val_loss: 0.0022\n",
      "Epoch 22/24\n",
      "3690/3690 [==============================] - 2s 482us/step - loss: 0.0020 - val_loss: 0.0012\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23/24\n",
      "3690/3690 [==============================] - 2s 487us/step - loss: 0.0019 - val_loss: 6.9852e-04\n",
      "Epoch 24/24\n",
      "3690/3690 [==============================] - 2s 485us/step - loss: 0.0019 - val_loss: 7.2648e-04\n",
      "Train on 3690 samples, validate on 410 samples\n",
      "Epoch 1/24\n",
      "3690/3690 [==============================] - 6s 2ms/step - loss: 1.7621 - val_loss: 0.0031\n",
      "Epoch 2/24\n",
      "3690/3690 [==============================] - 2s 485us/step - loss: 0.0103 - val_loss: 0.0014\n",
      "Epoch 3/24\n",
      "3690/3690 [==============================] - 2s 489us/step - loss: 0.0056 - val_loss: 0.0017\n",
      "Epoch 4/24\n",
      "3690/3690 [==============================] - 2s 487us/step - loss: 0.0044 - val_loss: 0.0016\n",
      "Epoch 5/24\n",
      "3690/3690 [==============================] - 2s 487us/step - loss: 0.0042 - val_loss: 0.0016\n",
      "Epoch 6/24\n",
      "3690/3690 [==============================] - 2s 486us/step - loss: 0.0039 - val_loss: 9.5527e-04\n",
      "Epoch 7/24\n",
      "3690/3690 [==============================] - 2s 486us/step - loss: 0.0037 - val_loss: 0.0010\n",
      "Epoch 8/24\n",
      "3690/3690 [==============================] - 2s 484us/step - loss: 0.0034 - val_loss: 9.6080e-04\n",
      "Epoch 9/24\n",
      "3690/3690 [==============================] - 2s 487us/step - loss: 0.0034 - val_loss: 0.0015\n",
      "Epoch 10/24\n",
      "3690/3690 [==============================] - 2s 485us/step - loss: 0.0035 - val_loss: 8.9468e-04\n",
      "Epoch 11/24\n",
      "3690/3690 [==============================] - 2s 487us/step - loss: 0.0031 - val_loss: 9.9269e-04\n",
      "Epoch 12/24\n",
      "3690/3690 [==============================] - 2s 486us/step - loss: 0.0030 - val_loss: 8.4941e-04\n",
      "Epoch 13/24\n",
      "3690/3690 [==============================] - 2s 486us/step - loss: 0.0030 - val_loss: 8.8674e-04\n",
      "Epoch 14/24\n",
      "3690/3690 [==============================] - 2s 486us/step - loss: 0.0030 - val_loss: 8.3306e-04\n",
      "Epoch 15/24\n",
      "3690/3690 [==============================] - 2s 484us/step - loss: 0.0030 - val_loss: 0.0014\n",
      "Epoch 16/24\n",
      "3690/3690 [==============================] - 2s 487us/step - loss: 0.0029 - val_loss: 8.3607e-04\n",
      "Epoch 17/24\n",
      "3690/3690 [==============================] - 2s 486us/step - loss: 0.0027 - val_loss: 9.7518e-04\n",
      "Epoch 18/24\n",
      "3690/3690 [==============================] - 2s 486us/step - loss: 0.0027 - val_loss: 8.5179e-04\n",
      "Epoch 19/24\n",
      "3690/3690 [==============================] - 2s 484us/step - loss: 0.0028 - val_loss: 0.0010\n",
      "Epoch 20/24\n",
      "3690/3690 [==============================] - 2s 486us/step - loss: 0.0026 - val_loss: 0.0012\n",
      "Epoch 21/24\n",
      "3690/3690 [==============================] - 2s 486us/step - loss: 0.0026 - val_loss: 8.1746e-04\n",
      "Epoch 22/24\n",
      "3690/3690 [==============================] - 2s 484us/step - loss: 0.0026 - val_loss: 7.8327e-04\n",
      "Epoch 23/24\n",
      "3690/3690 [==============================] - 2s 485us/step - loss: 0.0027 - val_loss: 8.0939e-04\n",
      "Epoch 24/24\n",
      "3690/3690 [==============================] - 2s 482us/step - loss: 0.0025 - val_loss: 7.7206e-04\n",
      "Train on 3690 samples, validate on 410 samples\n",
      "Epoch 1/24\n",
      "3690/3690 [==============================] - 6s 2ms/step - loss: 40.6041 - val_loss: 0.2712\n",
      "Epoch 2/24\n",
      "3690/3690 [==============================] - 2s 488us/step - loss: 0.1400 - val_loss: 0.0355\n",
      "Epoch 3/24\n",
      "3690/3690 [==============================] - 2s 490us/step - loss: 0.0729 - val_loss: 0.0548\n",
      "Epoch 4/24\n",
      "3690/3690 [==============================] - 2s 491us/step - loss: 0.0736 - val_loss: 0.0285\n",
      "Epoch 5/24\n",
      "3690/3690 [==============================] - 2s 487us/step - loss: 0.0724 - val_loss: 0.0367\n",
      "Epoch 6/24\n",
      "3690/3690 [==============================] - 2s 488us/step - loss: 0.0719 - val_loss: 0.0392\n",
      "Epoch 7/24\n",
      "3690/3690 [==============================] - 2s 488us/step - loss: 0.0728 - val_loss: 0.0335\n",
      "Epoch 8/24\n",
      "3690/3690 [==============================] - 2s 486us/step - loss: 0.0701 - val_loss: 0.0319\n",
      "Epoch 9/24\n",
      "3690/3690 [==============================] - 2s 488us/step - loss: 0.0714 - val_loss: 0.0302\n",
      "Epoch 10/24\n",
      "3690/3690 [==============================] - 2s 488us/step - loss: 0.0709 - val_loss: 0.0355\n",
      "Epoch 11/24\n",
      "3690/3690 [==============================] - 2s 488us/step - loss: 0.0699 - val_loss: 0.0305\n",
      "Epoch 12/24\n",
      "3690/3690 [==============================] - 2s 489us/step - loss: 0.0697 - val_loss: 0.0271\n",
      "Epoch 13/24\n",
      "3690/3690 [==============================] - 2s 488us/step - loss: 0.0690 - val_loss: 0.0371\n",
      "Epoch 14/24\n",
      "3690/3690 [==============================] - 2s 489us/step - loss: 0.0685 - val_loss: 0.0367\n",
      "Epoch 15/24\n",
      "3690/3690 [==============================] - 2s 488us/step - loss: 0.0677 - val_loss: 0.0302\n",
      "Epoch 16/24\n",
      "3690/3690 [==============================] - 2s 487us/step - loss: 0.0655 - val_loss: 0.0363\n",
      "Epoch 17/24\n",
      "3690/3690 [==============================] - 2s 488us/step - loss: 0.0649 - val_loss: 0.0345\n",
      "Epoch 18/24\n",
      "3690/3690 [==============================] - 2s 488us/step - loss: 0.0621 - val_loss: 0.0336\n",
      "Epoch 19/24\n",
      "3690/3690 [==============================] - 2s 488us/step - loss: 0.0573 - val_loss: 0.0283\n",
      "Epoch 20/24\n",
      "3690/3690 [==============================] - 2s 488us/step - loss: 0.0528 - val_loss: 0.0180\n",
      "Epoch 21/24\n",
      "3690/3690 [==============================] - 2s 491us/step - loss: 0.0455 - val_loss: 0.0107\n",
      "Epoch 22/24\n",
      "3690/3690 [==============================] - 2s 491us/step - loss: 0.0375 - val_loss: 0.0060\n",
      "Epoch 23/24\n",
      "3690/3690 [==============================] - 2s 490us/step - loss: 0.0274 - val_loss: 0.0041\n",
      "Epoch 24/24\n",
      "3690/3690 [==============================] - 2s 491us/step - loss: 0.0294 - val_loss: 0.0251\n",
      "Train on 3690 samples, validate on 410 samples\n",
      "Epoch 1/24\n",
      "3690/3690 [==============================] - 6s 2ms/step - loss: 2.2043 - val_loss: 0.0290\n",
      "Epoch 2/24\n",
      "3690/3690 [==============================] - 2s 488us/step - loss: 0.0393 - val_loss: 0.0087\n",
      "Epoch 3/24\n",
      "3690/3690 [==============================] - 2s 486us/step - loss: 0.0235 - val_loss: 0.0051\n",
      "Epoch 4/24\n",
      "3690/3690 [==============================] - 2s 486us/step - loss: 0.0127 - val_loss: 0.0019\n",
      "Epoch 5/24\n",
      "3690/3690 [==============================] - 2s 488us/step - loss: 0.0065 - val_loss: 8.1310e-04\n",
      "Epoch 6/24\n",
      "3690/3690 [==============================] - 2s 488us/step - loss: 0.0040 - val_loss: 0.0015\n",
      "Epoch 7/24\n",
      "3690/3690 [==============================] - 2s 492us/step - loss: 0.0031 - val_loss: 9.8884e-04\n",
      "Epoch 8/24\n",
      "3690/3690 [==============================] - 2s 487us/step - loss: 0.0026 - val_loss: 0.0012\n",
      "Epoch 9/24\n",
      "3690/3690 [==============================] - 2s 490us/step - loss: 0.0024 - val_loss: 0.0016\n",
      "Epoch 10/24\n",
      "3690/3690 [==============================] - 2s 488us/step - loss: 0.0025 - val_loss: 0.0024\n",
      "Epoch 11/24\n",
      "3690/3690 [==============================] - 2s 487us/step - loss: 0.0023 - val_loss: 0.0012\n",
      "Epoch 12/24\n",
      "3690/3690 [==============================] - 2s 489us/step - loss: 0.0021 - val_loss: 0.0012\n",
      "Epoch 13/24\n",
      "3690/3690 [==============================] - 2s 486us/step - loss: 0.0020 - val_loss: 0.0014\n",
      "Epoch 14/24\n",
      "3690/3690 [==============================] - 2s 491us/step - loss: 0.0021 - val_loss: 0.0012\n",
      "Epoch 15/24\n",
      "3690/3690 [==============================] - 2s 490us/step - loss: 0.0019 - val_loss: 0.0016\n",
      "Epoch 16/24\n",
      "3690/3690 [==============================] - 2s 487us/step - loss: 0.0020 - val_loss: 0.0017\n",
      "Epoch 17/24\n",
      "3690/3690 [==============================] - 2s 485us/step - loss: 0.0019 - val_loss: 0.0012\n",
      "Epoch 18/24\n",
      "3690/3690 [==============================] - 2s 488us/step - loss: 0.0018 - val_loss: 9.3290e-04\n",
      "Epoch 19/24\n",
      "3690/3690 [==============================] - 2s 488us/step - loss: 0.0019 - val_loss: 0.0017\n",
      "Epoch 20/24\n",
      "3690/3690 [==============================] - 2s 489us/step - loss: 0.0018 - val_loss: 0.0013\n",
      "Epoch 21/24\n",
      "3690/3690 [==============================] - 2s 490us/step - loss: 0.0017 - val_loss: 8.7100e-04\n",
      "Epoch 22/24\n",
      "3690/3690 [==============================] - 2s 486us/step - loss: 0.0019 - val_loss: 8.1162e-04\n",
      "Epoch 23/24\n",
      "3690/3690 [==============================] - 2s 487us/step - loss: 0.0018 - val_loss: 9.0121e-04\n",
      "Epoch 24/24\n",
      "3690/3690 [==============================] - 2s 487us/step - loss: 0.0017 - val_loss: 0.0011\n",
      "Train on 3690 samples, validate on 410 samples\n",
      "Epoch 1/24\n",
      "3690/3690 [==============================] - 6s 2ms/step - loss: 0.9291 - val_loss: 0.0079\n",
      "Epoch 2/24\n",
      "3690/3690 [==============================] - 2s 490us/step - loss: 0.0194 - val_loss: 0.0015\n",
      "Epoch 3/24\n",
      "3690/3690 [==============================] - 2s 489us/step - loss: 0.0046 - val_loss: 0.0014\n",
      "Epoch 4/24\n",
      "3690/3690 [==============================] - 2s 487us/step - loss: 0.0028 - val_loss: 0.0030\n",
      "Epoch 5/24\n",
      "3690/3690 [==============================] - 2s 491us/step - loss: 0.0022 - val_loss: 0.0040\n",
      "Epoch 6/24\n",
      "3690/3690 [==============================] - 2s 491us/step - loss: 0.0024 - val_loss: 0.0066\n",
      "Epoch 7/24\n",
      "3690/3690 [==============================] - 2s 489us/step - loss: 0.0028 - val_loss: 0.0088\n",
      "Epoch 8/24\n",
      "3690/3690 [==============================] - 2s 487us/step - loss: 0.0028 - val_loss: 0.0041\n",
      "Epoch 9/24\n",
      "3690/3690 [==============================] - 2s 489us/step - loss: 0.0019 - val_loss: 0.0037\n",
      "Epoch 10/24\n",
      "3690/3690 [==============================] - 2s 488us/step - loss: 0.0018 - val_loss: 0.0039\n",
      "Epoch 11/24\n",
      "3690/3690 [==============================] - 2s 488us/step - loss: 0.0016 - val_loss: 0.0025\n",
      "Epoch 12/24\n",
      "3690/3690 [==============================] - 2s 489us/step - loss: 0.0015 - val_loss: 0.0018\n",
      "Epoch 13/24\n",
      "3690/3690 [==============================] - 2s 491us/step - loss: 0.0015 - val_loss: 0.0012\n",
      "Epoch 14/24\n",
      "3690/3690 [==============================] - 2s 489us/step - loss: 0.0015 - val_loss: 0.0013\n",
      "Epoch 15/24\n",
      "3690/3690 [==============================] - 2s 489us/step - loss: 0.0015 - val_loss: 0.0022\n",
      "Epoch 16/24\n",
      "3690/3690 [==============================] - 2s 490us/step - loss: 0.0014 - val_loss: 0.0014\n",
      "Epoch 17/24\n",
      "3690/3690 [==============================] - 2s 488us/step - loss: 0.0014 - val_loss: 0.0016\n",
      "Epoch 18/24\n",
      "3690/3690 [==============================] - 2s 488us/step - loss: 0.0013 - val_loss: 0.0011\n",
      "Epoch 19/24\n",
      "3690/3690 [==============================] - 2s 489us/step - loss: 0.0013 - val_loss: 0.0016\n",
      "Epoch 20/24\n",
      "3690/3690 [==============================] - 2s 488us/step - loss: 0.0013 - val_loss: 0.0012\n",
      "Epoch 21/24\n",
      "3690/3690 [==============================] - 2s 493us/step - loss: 0.0013 - val_loss: 9.2466e-04\n",
      "Epoch 22/24\n",
      "3690/3690 [==============================] - 2s 489us/step - loss: 0.0012 - val_loss: 0.0012\n",
      "Epoch 23/24\n",
      "3690/3690 [==============================] - 2s 493us/step - loss: 0.0012 - val_loss: 7.3907e-04\n",
      "Epoch 24/24\n",
      "3690/3690 [==============================] - 2s 487us/step - loss: 0.0013 - val_loss: 0.0024\n",
      "Train on 3690 samples, validate on 410 samples\n",
      "Epoch 1/24\n",
      "3690/3690 [==============================] - 6s 2ms/step - loss: 1.5213 - val_loss: 0.0050\n",
      "Epoch 2/24\n",
      "3690/3690 [==============================] - 2s 494us/step - loss: 0.0142 - val_loss: 0.0029\n",
      "Epoch 3/24\n",
      "3690/3690 [==============================] - 2s 489us/step - loss: 0.0072 - val_loss: 0.0055\n",
      "Epoch 4/24\n",
      "3690/3690 [==============================] - 2s 490us/step - loss: 0.0083 - val_loss: 0.0019\n",
      "Epoch 5/24\n",
      "3690/3690 [==============================] - 2s 491us/step - loss: 0.0044 - val_loss: 0.0022\n",
      "Epoch 6/24\n",
      "3690/3690 [==============================] - 2s 488us/step - loss: 0.0034 - val_loss: 0.0046\n",
      "Epoch 7/24\n",
      "3690/3690 [==============================] - 2s 489us/step - loss: 0.0031 - val_loss: 0.0020\n",
      "Epoch 8/24\n",
      "3690/3690 [==============================] - 2s 488us/step - loss: 0.0028 - val_loss: 0.0021\n",
      "Epoch 9/24\n",
      "3690/3690 [==============================] - 2s 490us/step - loss: 0.0025 - val_loss: 0.0027\n",
      "Epoch 10/24\n",
      "3690/3690 [==============================] - 2s 490us/step - loss: 0.0024 - val_loss: 0.0016\n",
      "Epoch 11/24\n",
      "3690/3690 [==============================] - 2s 492us/step - loss: 0.0024 - val_loss: 0.0034\n",
      "Epoch 12/24\n",
      "3690/3690 [==============================] - 2s 490us/step - loss: 0.0022 - val_loss: 0.0022\n",
      "Epoch 13/24\n",
      "3690/3690 [==============================] - 2s 488us/step - loss: 0.0022 - val_loss: 0.0015\n",
      "Epoch 14/24\n",
      "3690/3690 [==============================] - 2s 490us/step - loss: 0.0024 - val_loss: 0.0030\n",
      "Epoch 15/24\n",
      "3690/3690 [==============================] - 2s 488us/step - loss: 0.0021 - val_loss: 0.0020\n",
      "Epoch 16/24\n",
      "3690/3690 [==============================] - 2s 491us/step - loss: 0.0021 - val_loss: 0.0038\n",
      "Epoch 17/24\n",
      "3690/3690 [==============================] - 2s 489us/step - loss: 0.0021 - val_loss: 0.0047\n",
      "Epoch 18/24\n",
      "3690/3690 [==============================] - 2s 490us/step - loss: 0.0021 - val_loss: 0.0044\n",
      "Epoch 19/24\n",
      "3690/3690 [==============================] - 2s 489us/step - loss: 0.0018 - val_loss: 0.0033\n",
      "Epoch 20/24\n",
      "3690/3690 [==============================] - 2s 489us/step - loss: 0.0019 - val_loss: 0.0015\n",
      "Epoch 21/24\n",
      "3690/3690 [==============================] - 2s 486us/step - loss: 0.0021 - val_loss: 0.0024\n",
      "Epoch 22/24\n",
      "3690/3690 [==============================] - 2s 488us/step - loss: 0.0018 - val_loss: 0.0035\n",
      "Epoch 23/24\n",
      "3690/3690 [==============================] - 2s 489us/step - loss: 0.0018 - val_loss: 0.0019\n",
      "Epoch 24/24\n",
      "3690/3690 [==============================] - 2s 489us/step - loss: 0.0018 - val_loss: 0.0032\n",
      "\n",
      "GENERATION 5\n",
      "\n",
      "Train on 3690 samples, validate on 410 samples\n",
      "Epoch 1/24\n",
      "3690/3690 [==============================] - 7s 2ms/step - loss: 0.0563 - val_loss: 0.0152\n",
      "Epoch 2/24\n",
      "3690/3690 [==============================] - 2s 582us/step - loss: 0.0095 - val_loss: 0.0129\n",
      "Epoch 3/24\n",
      "3690/3690 [==============================] - 2s 584us/step - loss: 0.0055 - val_loss: 0.0076\n",
      "Epoch 4/24\n",
      "3690/3690 [==============================] - 2s 586us/step - loss: 0.0032 - val_loss: 0.0048\n",
      "Epoch 5/24\n",
      "3690/3690 [==============================] - 2s 583us/step - loss: 0.0019 - val_loss: 0.0020\n",
      "Epoch 6/24\n",
      "3690/3690 [==============================] - 2s 580us/step - loss: 0.0014 - val_loss: 0.0011\n",
      "Epoch 7/24\n",
      "3690/3690 [==============================] - 2s 590us/step - loss: 0.0013 - val_loss: 9.3931e-04\n",
      "Epoch 8/24\n",
      "3690/3690 [==============================] - 2s 583us/step - loss: 0.0012 - val_loss: 0.0015\n",
      "Epoch 9/24\n",
      "3690/3690 [==============================] - 2s 583us/step - loss: 0.0010 - val_loss: 0.0016\n",
      "Epoch 10/24\n",
      "3690/3690 [==============================] - 2s 580us/step - loss: 0.0011 - val_loss: 0.0017\n",
      "Epoch 11/24\n",
      "3690/3690 [==============================] - 2s 582us/step - loss: 9.8889e-04 - val_loss: 0.0014\n",
      "Epoch 12/24\n",
      "3690/3690 [==============================] - 2s 585us/step - loss: 9.3955e-04 - val_loss: 0.0015\n",
      "Epoch 13/24\n",
      "3690/3690 [==============================] - 2s 585us/step - loss: 9.4876e-04 - val_loss: 0.0013\n",
      "Epoch 14/24\n",
      "3690/3690 [==============================] - 2s 580us/step - loss: 9.0255e-04 - val_loss: 6.2240e-04\n",
      "Epoch 15/24\n",
      "3690/3690 [==============================] - 2s 584us/step - loss: 8.9962e-04 - val_loss: 0.0019\n",
      "Epoch 16/24\n",
      "3690/3690 [==============================] - 2s 583us/step - loss: 9.0758e-04 - val_loss: 6.0306e-04\n",
      "Epoch 17/24\n",
      "3690/3690 [==============================] - 2s 584us/step - loss: 8.8843e-04 - val_loss: 0.0014\n",
      "Epoch 18/24\n",
      "3690/3690 [==============================] - 2s 583us/step - loss: 8.3121e-04 - val_loss: 5.9779e-04\n",
      "Epoch 19/24\n",
      "3690/3690 [==============================] - 2s 583us/step - loss: 8.5904e-04 - val_loss: 0.0018\n",
      "Epoch 20/24\n",
      "3690/3690 [==============================] - 2s 584us/step - loss: 8.0318e-04 - val_loss: 7.4313e-04\n",
      "Epoch 21/24\n",
      "3690/3690 [==============================] - 2s 586us/step - loss: 7.8033e-04 - val_loss: 0.0010\n",
      "Epoch 22/24\n",
      "3690/3690 [==============================] - 2s 585us/step - loss: 7.6789e-04 - val_loss: 0.0013\n",
      "Epoch 23/24\n",
      "3690/3690 [==============================] - 2s 583us/step - loss: 7.4996e-04 - val_loss: 7.6050e-04\n",
      "Epoch 24/24\n",
      "3690/3690 [==============================] - 2s 587us/step - loss: 7.2647e-04 - val_loss: 0.0016\n",
      "Train on 3690 samples, validate on 410 samples\n",
      "Epoch 1/24\n",
      "3690/3690 [==============================] - 7s 2ms/step - loss: 0.0576 - val_loss: 0.0146\n",
      "Epoch 2/24\n",
      "3690/3690 [==============================] - 2s 581us/step - loss: 0.0097 - val_loss: 0.0024\n",
      "Epoch 3/24\n",
      "3690/3690 [==============================] - 2s 582us/step - loss: 0.0043 - val_loss: 0.0135\n",
      "Epoch 4/24\n",
      "3690/3690 [==============================] - 2s 584us/step - loss: 0.0031 - val_loss: 0.0031\n",
      "Epoch 5/24\n",
      "3690/3690 [==============================] - 2s 584us/step - loss: 0.0023 - val_loss: 9.0032e-04\n",
      "Epoch 6/24\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3690/3690 [==============================] - 2s 583us/step - loss: 0.0017 - val_loss: 0.0014\n",
      "Epoch 7/24\n",
      "3690/3690 [==============================] - 2s 581us/step - loss: 0.0014 - val_loss: 0.0020\n",
      "Epoch 8/24\n",
      "3690/3690 [==============================] - 2s 581us/step - loss: 0.0013 - val_loss: 0.0022\n",
      "Epoch 9/24\n",
      "3690/3690 [==============================] - 2s 578us/step - loss: 0.0013 - val_loss: 0.0019\n",
      "Epoch 10/24\n",
      "3690/3690 [==============================] - 2s 581us/step - loss: 0.0012 - val_loss: 0.0015\n",
      "Epoch 11/24\n",
      "3690/3690 [==============================] - 2s 584us/step - loss: 0.0011 - val_loss: 0.0014\n",
      "Epoch 12/24\n",
      "3690/3690 [==============================] - 2s 585us/step - loss: 0.0011 - val_loss: 0.0014\n",
      "Epoch 13/24\n",
      "3690/3690 [==============================] - 2s 579us/step - loss: 0.0011 - val_loss: 6.3381e-04\n",
      "Epoch 14/24\n",
      "3690/3690 [==============================] - 2s 582us/step - loss: 0.0010 - val_loss: 5.9803e-04\n",
      "Epoch 15/24\n",
      "3690/3690 [==============================] - 2s 584us/step - loss: 0.0011 - val_loss: 8.4089e-04\n",
      "Epoch 16/24\n",
      "3690/3690 [==============================] - 2s 583us/step - loss: 9.9121e-04 - val_loss: 0.0017\n",
      "Epoch 17/24\n",
      "3690/3690 [==============================] - 2s 580us/step - loss: 9.7765e-04 - val_loss: 0.0014\n",
      "Epoch 18/24\n",
      "3690/3690 [==============================] - 2s 581us/step - loss: 9.6556e-04 - val_loss: 9.7397e-04\n",
      "Epoch 19/24\n",
      "3690/3690 [==============================] - 2s 584us/step - loss: 9.0297e-04 - val_loss: 7.5303e-04\n",
      "Epoch 20/24\n",
      "3690/3690 [==============================] - 2s 583us/step - loss: 9.1454e-04 - val_loss: 0.0010\n",
      "Epoch 21/24\n",
      "3690/3690 [==============================] - 2s 580us/step - loss: 8.6874e-04 - val_loss: 0.0017\n",
      "Epoch 22/24\n",
      "3690/3690 [==============================] - 2s 584us/step - loss: 8.9397e-04 - val_loss: 0.0013\n",
      "Epoch 23/24\n",
      "3690/3690 [==============================] - 2s 587us/step - loss: 8.7993e-04 - val_loss: 7.3294e-04\n",
      "Epoch 24/24\n",
      "3690/3690 [==============================] - 2s 587us/step - loss: 8.6429e-04 - val_loss: 0.0013\n",
      "Train on 3690 samples, validate on 410 samples\n",
      "Epoch 1/24\n",
      "3690/3690 [==============================] - 7s 2ms/step - loss: 0.0663 - val_loss: 0.0186\n",
      "Epoch 2/24\n",
      "3690/3690 [==============================] - 2s 581us/step - loss: 0.0419 - val_loss: 0.0108\n",
      "Epoch 3/24\n",
      "3690/3690 [==============================] - 2s 584us/step - loss: 0.0267 - val_loss: 0.0102\n",
      "Epoch 4/24\n",
      "3690/3690 [==============================] - 2s 582us/step - loss: 0.0146 - val_loss: 0.0147\n",
      "Epoch 5/24\n",
      "3690/3690 [==============================] - 2s 580us/step - loss: 0.0085 - val_loss: 0.0317\n",
      "Epoch 6/24\n",
      "3690/3690 [==============================] - 2s 581us/step - loss: 0.0042 - val_loss: 0.0177\n",
      "Epoch 7/24\n",
      "3690/3690 [==============================] - 2s 583us/step - loss: 0.0033 - val_loss: 0.0012\n",
      "Epoch 8/24\n",
      "3690/3690 [==============================] - 2s 586us/step - loss: 0.0027 - val_loss: 0.0037\n",
      "Epoch 9/24\n",
      "3690/3690 [==============================] - 2s 580us/step - loss: 0.0023 - val_loss: 0.0044\n",
      "Epoch 10/24\n",
      "3690/3690 [==============================] - 2s 581us/step - loss: 0.0021 - val_loss: 0.0049\n",
      "Epoch 11/24\n",
      "3690/3690 [==============================] - 2s 581us/step - loss: 0.0019 - val_loss: 0.0058\n",
      "Epoch 12/24\n",
      "3690/3690 [==============================] - 2s 582us/step - loss: 0.0017 - val_loss: 0.0034\n",
      "Epoch 13/24\n",
      "3690/3690 [==============================] - 2s 579us/step - loss: 0.0015 - val_loss: 0.0025\n",
      "Epoch 14/24\n",
      "3690/3690 [==============================] - 2s 584us/step - loss: 0.0013 - val_loss: 0.0016\n",
      "Epoch 15/24\n",
      "3690/3690 [==============================] - 2s 583us/step - loss: 0.0012 - val_loss: 7.6225e-04\n",
      "Epoch 16/24\n",
      "3690/3690 [==============================] - 2s 581us/step - loss: 0.0011 - val_loss: 0.0015\n",
      "Epoch 17/24\n",
      "3690/3690 [==============================] - 2s 590us/step - loss: 9.8103e-04 - val_loss: 8.6127e-04\n",
      "Epoch 18/24\n",
      "3690/3690 [==============================] - 2s 581us/step - loss: 9.6129e-04 - val_loss: 5.8314e-04\n",
      "Epoch 19/24\n",
      "3690/3690 [==============================] - 2s 584us/step - loss: 9.2690e-04 - val_loss: 0.0014\n",
      "Epoch 20/24\n",
      "3690/3690 [==============================] - 2s 583us/step - loss: 8.7299e-04 - val_loss: 6.5990e-04\n",
      "Epoch 21/24\n",
      "3690/3690 [==============================] - 2s 581us/step - loss: 8.5684e-04 - val_loss: 6.8910e-04\n",
      "Epoch 22/24\n",
      "3690/3690 [==============================] - 2s 583us/step - loss: 8.0248e-04 - val_loss: 6.8278e-04\n",
      "Epoch 23/24\n",
      "3690/3690 [==============================] - 2s 583us/step - loss: 8.0434e-04 - val_loss: 8.5759e-04\n",
      "Epoch 24/24\n",
      "3690/3690 [==============================] - 2s 585us/step - loss: 7.6696e-04 - val_loss: 8.9439e-04\n",
      "Train on 3690 samples, validate on 410 samples\n",
      "Epoch 1/24\n",
      "3690/3690 [==============================] - 7s 2ms/step - loss: 0.0734 - val_loss: 9.8605e-04\n",
      "Epoch 2/24\n",
      "3690/3690 [==============================] - 2s 583us/step - loss: 0.0042 - val_loss: 0.0027\n",
      "Epoch 3/24\n",
      "3690/3690 [==============================] - 2s 585us/step - loss: 0.0037 - val_loss: 0.0012\n",
      "Epoch 4/24\n",
      "3690/3690 [==============================] - 2s 587us/step - loss: 0.0033 - val_loss: 0.0011\n",
      "Epoch 5/24\n",
      "3690/3690 [==============================] - 2s 581us/step - loss: 0.0037 - val_loss: 0.0114\n",
      "Epoch 6/24\n",
      "3690/3690 [==============================] - 2s 584us/step - loss: 0.0150 - val_loss: 0.0118\n",
      "Epoch 7/24\n",
      "3690/3690 [==============================] - 2s 584us/step - loss: 0.0072 - val_loss: 0.0015\n",
      "Epoch 8/24\n",
      "3690/3690 [==============================] - 2s 586us/step - loss: 0.0031 - val_loss: 0.0023\n",
      "Epoch 9/24\n",
      "3690/3690 [==============================] - 2s 582us/step - loss: 0.0055 - val_loss: 0.0087\n",
      "Epoch 10/24\n",
      "3690/3690 [==============================] - 2s 586us/step - loss: 0.0075 - val_loss: 0.0011\n",
      "Epoch 11/24\n",
      "3690/3690 [==============================] - 2s 582us/step - loss: 0.0032 - val_loss: 4.9062e-04\n",
      "Epoch 12/24\n",
      "3690/3690 [==============================] - 2s 586us/step - loss: 0.0039 - val_loss: 0.0087\n",
      "Epoch 13/24\n",
      "3690/3690 [==============================] - 2s 583us/step - loss: 0.0093 - val_loss: 0.0068\n",
      "Epoch 14/24\n",
      "3690/3690 [==============================] - 2s 583us/step - loss: 0.0082 - val_loss: 0.0033\n",
      "Epoch 15/24\n",
      "3690/3690 [==============================] - 2s 586us/step - loss: 0.0028 - val_loss: 7.6643e-04\n",
      "Epoch 16/24\n",
      "3690/3690 [==============================] - 2s 583us/step - loss: 0.0043 - val_loss: 0.0070\n",
      "Epoch 17/24\n",
      "3690/3690 [==============================] - 2s 584us/step - loss: 0.0047 - val_loss: 0.0036\n",
      "Epoch 18/24\n",
      "3690/3690 [==============================] - 2s 586us/step - loss: 0.0049 - val_loss: 0.0067\n",
      "Epoch 19/24\n",
      "3690/3690 [==============================] - 2s 587us/step - loss: 0.0050 - val_loss: 0.0061\n",
      "Epoch 20/24\n",
      "3690/3690 [==============================] - 2s 584us/step - loss: 0.0050 - val_loss: 0.0045\n",
      "Epoch 21/24\n",
      "3690/3690 [==============================] - 2s 585us/step - loss: 0.0044 - val_loss: 0.0035\n",
      "Epoch 22/24\n",
      "3690/3690 [==============================] - 2s 582us/step - loss: 0.0053 - val_loss: 0.0040\n",
      "Epoch 23/24\n",
      "3690/3690 [==============================] - 2s 582us/step - loss: 0.0036 - val_loss: 0.0031\n",
      "Epoch 24/24\n",
      "3690/3690 [==============================] - 2s 586us/step - loss: 0.0037 - val_loss: 0.0013\n",
      "Train on 3690 samples, validate on 410 samples\n",
      "Epoch 1/24\n",
      "3690/3690 [==============================] - 7s 2ms/step - loss: 0.0709 - val_loss: 8.0414e-04\n",
      "Epoch 2/24\n",
      "3690/3690 [==============================] - 2s 492us/step - loss: 0.0038 - val_loss: 0.0011\n",
      "Epoch 3/24\n",
      "3690/3690 [==============================] - 2s 490us/step - loss: 0.0063 - val_loss: 0.0032\n",
      "Epoch 4/24\n",
      "3690/3690 [==============================] - 2s 493us/step - loss: 0.0082 - val_loss: 0.0114\n",
      "Epoch 5/24\n",
      "3690/3690 [==============================] - 2s 491us/step - loss: 0.0099 - val_loss: 0.0130\n",
      "Epoch 6/24\n",
      "3690/3690 [==============================] - 2s 490us/step - loss: 0.0071 - val_loss: 0.0137\n",
      "Epoch 7/24\n",
      "3690/3690 [==============================] - 2s 491us/step - loss: 0.0094 - val_loss: 0.0082\n",
      "Epoch 8/24\n",
      "3690/3690 [==============================] - 2s 492us/step - loss: 0.0062 - val_loss: 0.0046\n",
      "Epoch 9/24\n",
      "3690/3690 [==============================] - 2s 491us/step - loss: 0.0080 - val_loss: 0.0185\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/24\n",
      "3690/3690 [==============================] - 2s 492us/step - loss: 0.0104 - val_loss: 0.0142\n",
      "Epoch 11/24\n",
      "3690/3690 [==============================] - 2s 490us/step - loss: 0.0083 - val_loss: 0.0138\n",
      "Epoch 12/24\n",
      "3690/3690 [==============================] - 2s 489us/step - loss: 0.0080 - val_loss: 0.0034\n",
      "Epoch 13/24\n",
      "3690/3690 [==============================] - 2s 490us/step - loss: 0.0060 - val_loss: 0.0083\n",
      "Epoch 14/24\n",
      "3690/3690 [==============================] - 2s 491us/step - loss: 0.0064 - val_loss: 0.0047\n",
      "Epoch 15/24\n",
      "3690/3690 [==============================] - 2s 492us/step - loss: 0.0054 - val_loss: 0.0013\n",
      "Epoch 16/24\n",
      "3690/3690 [==============================] - 2s 493us/step - loss: 0.0075 - val_loss: 0.0036\n",
      "Epoch 17/24\n",
      "3690/3690 [==============================] - 2s 495us/step - loss: 0.0072 - val_loss: 0.0091\n",
      "Epoch 18/24\n",
      "3690/3690 [==============================] - 2s 490us/step - loss: 0.0074 - val_loss: 0.0056\n",
      "Epoch 19/24\n",
      "3690/3690 [==============================] - 2s 492us/step - loss: 0.0049 - val_loss: 0.0021\n",
      "Epoch 20/24\n",
      "3690/3690 [==============================] - 2s 491us/step - loss: 0.0041 - val_loss: 0.0061\n",
      "Epoch 21/24\n",
      "3690/3690 [==============================] - 2s 489us/step - loss: 0.0054 - val_loss: 0.0155\n",
      "Epoch 22/24\n",
      "3690/3690 [==============================] - 2s 494us/step - loss: 0.0082 - val_loss: 0.0030\n",
      "Epoch 23/24\n",
      "3690/3690 [==============================] - 2s 494us/step - loss: 0.0046 - val_loss: 0.0039\n",
      "Epoch 24/24\n",
      "3690/3690 [==============================] - 2s 494us/step - loss: 0.0044 - val_loss: 0.0097\n",
      "Train on 3690 samples, validate on 410 samples\n",
      "Epoch 1/24\n",
      "3690/3690 [==============================] - 7s 2ms/step - loss: 0.0728 - val_loss: 0.0055\n",
      "Epoch 2/24\n",
      "3690/3690 [==============================] - 2s 490us/step - loss: 0.0106 - val_loss: 0.0312\n",
      "Epoch 3/24\n",
      "3690/3690 [==============================] - 2s 491us/step - loss: 0.0120 - val_loss: 0.0199\n",
      "Epoch 4/24\n",
      "3690/3690 [==============================] - 2s 491us/step - loss: 0.0085 - val_loss: 0.0129\n",
      "Epoch 5/24\n",
      "3690/3690 [==============================] - 2s 489us/step - loss: 0.0093 - val_loss: 0.0097\n",
      "Epoch 6/24\n",
      "3690/3690 [==============================] - 2s 493us/step - loss: 0.0070 - val_loss: 0.0084\n",
      "Epoch 7/24\n",
      "3690/3690 [==============================] - 2s 492us/step - loss: 0.0085 - val_loss: 0.0089\n",
      "Epoch 8/24\n",
      "3690/3690 [==============================] - 2s 497us/step - loss: 0.0080 - val_loss: 0.0093\n",
      "Epoch 9/24\n",
      "3690/3690 [==============================] - 2s 491us/step - loss: 0.0067 - val_loss: 0.0101\n",
      "Epoch 10/24\n",
      "3690/3690 [==============================] - 2s 492us/step - loss: 0.0075 - val_loss: 0.0133\n",
      "Epoch 11/24\n",
      "3690/3690 [==============================] - 2s 490us/step - loss: 0.0112 - val_loss: 0.0029\n",
      "Epoch 12/24\n",
      "3690/3690 [==============================] - 2s 490us/step - loss: 0.0037 - val_loss: 0.0100\n",
      "Epoch 13/24\n",
      "3690/3690 [==============================] - 2s 492us/step - loss: 0.0076 - val_loss: 0.0119\n",
      "Epoch 14/24\n",
      "3690/3690 [==============================] - 2s 494us/step - loss: 0.0081 - val_loss: 0.0035\n",
      "Epoch 15/24\n",
      "3690/3690 [==============================] - 2s 491us/step - loss: 0.0064 - val_loss: 0.0038\n",
      "Epoch 16/24\n",
      "3690/3690 [==============================] - 2s 491us/step - loss: 0.0075 - val_loss: 0.0042\n",
      "Epoch 17/24\n",
      "3690/3690 [==============================] - 2s 494us/step - loss: 0.0047 - val_loss: 0.0179\n",
      "Epoch 18/24\n",
      "3690/3690 [==============================] - 2s 494us/step - loss: 0.0085 - val_loss: 0.0094\n",
      "Epoch 19/24\n",
      "3690/3690 [==============================] - 2s 491us/step - loss: 0.0065 - val_loss: 0.0070\n",
      "Epoch 20/24\n",
      "3690/3690 [==============================] - 2s 493us/step - loss: 0.0072 - val_loss: 0.0054\n",
      "Epoch 21/24\n",
      "3690/3690 [==============================] - 2s 493us/step - loss: 0.0073 - val_loss: 0.0041\n",
      "Epoch 22/24\n",
      "3690/3690 [==============================] - 2s 489us/step - loss: 0.0049 - val_loss: 0.0042\n",
      "Epoch 23/24\n",
      "3690/3690 [==============================] - 2s 494us/step - loss: 0.0066 - val_loss: 0.0025\n",
      "Epoch 24/24\n",
      "3690/3690 [==============================] - 2s 494us/step - loss: 0.0037 - val_loss: 0.0022\n",
      "Train on 3690 samples, validate on 410 samples\n",
      "Epoch 1/24\n",
      "3690/3690 [==============================] - 7s 2ms/step - loss: 0.1305 - val_loss: 0.0015\n",
      "Epoch 2/24\n",
      "3690/3690 [==============================] - 2s 494us/step - loss: 0.0046 - val_loss: 7.2602e-04\n",
      "Epoch 3/24\n",
      "3690/3690 [==============================] - 2s 491us/step - loss: 0.0036 - val_loss: 7.7379e-04\n",
      "Epoch 4/24\n",
      "3690/3690 [==============================] - 2s 491us/step - loss: 0.0032 - val_loss: 7.7285e-04\n",
      "Epoch 5/24\n",
      "3690/3690 [==============================] - 2s 490us/step - loss: 0.0030 - val_loss: 0.0026\n",
      "Epoch 6/24\n",
      "3690/3690 [==============================] - 2s 490us/step - loss: 0.0037 - val_loss: 0.0047\n",
      "Epoch 7/24\n",
      "3690/3690 [==============================] - 2s 492us/step - loss: 0.0077 - val_loss: 0.0194\n",
      "Epoch 8/24\n",
      "3690/3690 [==============================] - 2s 491us/step - loss: 0.0085 - val_loss: 0.0059\n",
      "Epoch 9/24\n",
      "3690/3690 [==============================] - 2s 493us/step - loss: 0.0060 - val_loss: 0.0091\n",
      "Epoch 10/24\n",
      "3690/3690 [==============================] - 2s 490us/step - loss: 0.0069 - val_loss: 0.0216\n",
      "Epoch 11/24\n",
      "3690/3690 [==============================] - 2s 493us/step - loss: 0.0119 - val_loss: 0.0117\n",
      "Epoch 12/24\n",
      "3690/3690 [==============================] - 2s 490us/step - loss: 0.0094 - val_loss: 0.0055\n",
      "Epoch 13/24\n",
      "3690/3690 [==============================] - 2s 491us/step - loss: 0.0057 - val_loss: 0.0028\n",
      "Epoch 14/24\n",
      "3690/3690 [==============================] - 2s 491us/step - loss: 0.0058 - val_loss: 0.0082\n",
      "Epoch 15/24\n",
      "3690/3690 [==============================] - 2s 489us/step - loss: 0.0108 - val_loss: 0.0091\n",
      "Epoch 16/24\n",
      "3690/3690 [==============================] - 2s 492us/step - loss: 0.0093 - val_loss: 0.0140\n",
      "Epoch 17/24\n",
      "3690/3690 [==============================] - 2s 491us/step - loss: 0.0095 - val_loss: 0.0043\n",
      "Epoch 18/24\n",
      "3690/3690 [==============================] - 2s 489us/step - loss: 0.0050 - val_loss: 0.0066\n",
      "Epoch 19/24\n",
      "3690/3690 [==============================] - 2s 489us/step - loss: 0.0065 - val_loss: 0.0103\n",
      "Epoch 20/24\n",
      "3690/3690 [==============================] - 2s 495us/step - loss: 0.0074 - val_loss: 0.0121\n",
      "Epoch 21/24\n",
      "3690/3690 [==============================] - 2s 490us/step - loss: 0.0064 - val_loss: 0.0088\n",
      "Epoch 22/24\n",
      "3690/3690 [==============================] - 2s 494us/step - loss: 0.0066 - val_loss: 0.0034\n",
      "Epoch 23/24\n",
      "3690/3690 [==============================] - 2s 492us/step - loss: 0.0040 - val_loss: 0.0202\n",
      "Epoch 24/24\n",
      "3690/3690 [==============================] - 2s 494us/step - loss: 0.0089 - val_loss: 0.0049\n",
      "Train on 3690 samples, validate on 410 samples\n",
      "Epoch 1/24\n",
      "3690/3690 [==============================] - 8s 2ms/step - loss: 0.0779 - val_loss: 7.3208e-04\n",
      "Epoch 2/24\n",
      "3690/3690 [==============================] - 2s 504us/step - loss: 0.0040 - val_loss: 0.0010\n",
      "Epoch 3/24\n",
      "3690/3690 [==============================] - 2s 504us/step - loss: 0.0032 - val_loss: 7.6330e-04\n",
      "Epoch 4/24\n",
      "3690/3690 [==============================] - 2s 504us/step - loss: 0.0030 - val_loss: 6.4281e-04\n",
      "Epoch 5/24\n",
      "3690/3690 [==============================] - 2s 507us/step - loss: 0.0031 - val_loss: 9.2925e-04\n",
      "Epoch 6/24\n",
      "3690/3690 [==============================] - 2s 502us/step - loss: 0.0108 - val_loss: 0.0270\n",
      "Epoch 7/24\n",
      "3690/3690 [==============================] - 2s 505us/step - loss: 0.0109 - val_loss: 0.0107\n",
      "Epoch 8/24\n",
      "3690/3690 [==============================] - 2s 507us/step - loss: 0.0073 - val_loss: 0.0035\n",
      "Epoch 9/24\n",
      "3690/3690 [==============================] - 2s 506us/step - loss: 0.0051 - val_loss: 0.0025\n",
      "Epoch 10/24\n",
      "3690/3690 [==============================] - 2s 504us/step - loss: 0.0082 - val_loss: 0.0139\n",
      "Epoch 11/24\n",
      "3690/3690 [==============================] - 2s 503us/step - loss: 0.0088 - val_loss: 0.0026\n",
      "Epoch 12/24\n",
      "3690/3690 [==============================] - 2s 504us/step - loss: 0.0067 - val_loss: 0.0093\n",
      "Epoch 13/24\n",
      "3690/3690 [==============================] - 2s 504us/step - loss: 0.0081 - val_loss: 0.0197\n",
      "Epoch 14/24\n",
      "3690/3690 [==============================] - 2s 504us/step - loss: 0.0096 - val_loss: 0.0027\n",
      "Epoch 15/24\n",
      "3690/3690 [==============================] - 2s 505us/step - loss: 0.0046 - val_loss: 0.0033\n",
      "Epoch 16/24\n",
      "3690/3690 [==============================] - 2s 504us/step - loss: 0.0057 - val_loss: 0.0104\n",
      "Epoch 17/24\n",
      "3690/3690 [==============================] - 2s 503us/step - loss: 0.0095 - val_loss: 0.0031\n",
      "Epoch 18/24\n",
      "3690/3690 [==============================] - 2s 502us/step - loss: 0.0063 - val_loss: 0.0063\n",
      "Epoch 19/24\n",
      "3690/3690 [==============================] - 2s 503us/step - loss: 0.0062 - val_loss: 0.0032\n",
      "Epoch 20/24\n",
      "3690/3690 [==============================] - 2s 503us/step - loss: 0.0049 - val_loss: 0.0020\n",
      "Epoch 21/24\n",
      "3690/3690 [==============================] - 2s 502us/step - loss: 0.0034 - val_loss: 9.6459e-04\n",
      "Epoch 22/24\n",
      "3690/3690 [==============================] - 2s 506us/step - loss: 0.0031 - val_loss: 0.0024\n",
      "Epoch 23/24\n",
      "3690/3690 [==============================] - 2s 504us/step - loss: 0.0059 - val_loss: 0.0115\n",
      "Epoch 24/24\n",
      "3690/3690 [==============================] - 2s 502us/step - loss: 0.0083 - val_loss: 0.0101\n",
      "Train on 3690 samples, validate on 410 samples\n",
      "Epoch 1/24\n",
      "3690/3690 [==============================] - 8s 2ms/step - loss: 1.3662 - val_loss: 0.0013\n",
      "Epoch 2/24\n",
      "3690/3690 [==============================] - 2s 506us/step - loss: 0.0065 - val_loss: 0.0036\n",
      "Epoch 3/24\n",
      "3690/3690 [==============================] - 2s 507us/step - loss: 0.0093 - val_loss: 0.0032\n",
      "Epoch 4/24\n",
      "3690/3690 [==============================] - 2s 510us/step - loss: 0.0035 - val_loss: 0.0019\n",
      "Epoch 5/24\n",
      "3690/3690 [==============================] - 2s 503us/step - loss: 0.0019 - val_loss: 0.0013\n",
      "Epoch 6/24\n",
      "3690/3690 [==============================] - 2s 507us/step - loss: 0.0018 - val_loss: 0.0013\n",
      "Epoch 7/24\n",
      "3690/3690 [==============================] - 2s 507us/step - loss: 0.0019 - val_loss: 9.2012e-04\n",
      "Epoch 8/24\n",
      "3690/3690 [==============================] - 2s 509us/step - loss: 0.0019 - val_loss: 0.0031\n",
      "Epoch 9/24\n",
      "3690/3690 [==============================] - 2s 509us/step - loss: 0.0020 - val_loss: 0.0013\n",
      "Epoch 10/24\n",
      "3690/3690 [==============================] - 2s 505us/step - loss: 0.0016 - val_loss: 0.0017\n",
      "Epoch 11/24\n",
      "3690/3690 [==============================] - 2s 506us/step - loss: 0.0017 - val_loss: 0.0035\n",
      "Epoch 12/24\n",
      "3690/3690 [==============================] - 2s 511us/step - loss: 0.0018 - val_loss: 0.0012\n",
      "Epoch 13/24\n",
      "3690/3690 [==============================] - 2s 507us/step - loss: 0.0017 - val_loss: 0.0012\n",
      "Epoch 14/24\n",
      "3690/3690 [==============================] - 2s 507us/step - loss: 0.0015 - val_loss: 8.6844e-04\n",
      "Epoch 15/24\n",
      "3690/3690 [==============================] - 2s 506us/step - loss: 0.0015 - val_loss: 0.0013\n",
      "Epoch 16/24\n",
      "3690/3690 [==============================] - 2s 510us/step - loss: 0.0015 - val_loss: 7.2160e-04\n",
      "Epoch 17/24\n",
      "3690/3690 [==============================] - 2s 505us/step - loss: 0.0016 - val_loss: 0.0018\n",
      "Epoch 18/24\n",
      "3690/3690 [==============================] - 2s 507us/step - loss: 0.0015 - val_loss: 9.8675e-04\n",
      "Epoch 19/24\n",
      "3690/3690 [==============================] - 2s 505us/step - loss: 0.0016 - val_loss: 0.0015\n",
      "Epoch 20/24\n",
      "3690/3690 [==============================] - 2s 509us/step - loss: 0.0015 - val_loss: 0.0014\n",
      "Epoch 21/24\n",
      "3690/3690 [==============================] - 2s 506us/step - loss: 0.0015 - val_loss: 7.2775e-04\n",
      "Epoch 22/24\n",
      "3690/3690 [==============================] - 2s 506us/step - loss: 0.0014 - val_loss: 0.0011\n",
      "Epoch 23/24\n",
      "3690/3690 [==============================] - 2s 505us/step - loss: 0.0014 - val_loss: 0.0019\n",
      "Epoch 24/24\n",
      "3690/3690 [==============================] - 2s 508us/step - loss: 0.0014 - val_loss: 8.1069e-04\n",
      "\n",
      "GENERATION 6\n",
      "\n",
      "Train on 3690 samples, validate on 410 samples\n",
      "Epoch 1/24\n",
      "3690/3690 [==============================] - 8s 2ms/step - loss: 2.1131 - val_loss: 1.7797\n",
      "Epoch 2/24\n",
      "3690/3690 [==============================] - 2s 584us/step - loss: 1.1780 - val_loss: 1.2030\n",
      "Epoch 3/24\n",
      "3690/3690 [==============================] - 2s 583us/step - loss: 0.7902 - val_loss: 0.8739\n",
      "Epoch 4/24\n",
      "3690/3690 [==============================] - 2s 584us/step - loss: 0.5575 - val_loss: 0.6455\n",
      "Epoch 5/24\n",
      "3690/3690 [==============================] - 2s 588us/step - loss: 0.4002 - val_loss: 0.4819\n",
      "Epoch 6/24\n",
      "3690/3690 [==============================] - 2s 583us/step - loss: 0.2930 - val_loss: 0.3639\n",
      "Epoch 7/24\n",
      "3690/3690 [==============================] - 2s 585us/step - loss: 0.2189 - val_loss: 0.2787\n",
      "Epoch 8/24\n",
      "3690/3690 [==============================] - 2s 587us/step - loss: 0.1688 - val_loss: 0.2163\n",
      "Epoch 9/24\n",
      "3690/3690 [==============================] - 2s 588us/step - loss: 0.1346 - val_loss: 0.1703\n",
      "Epoch 10/24\n",
      "3690/3690 [==============================] - 2s 583us/step - loss: 0.1114 - val_loss: 0.1365\n",
      "Epoch 11/24\n",
      "3690/3690 [==============================] - 2s 585us/step - loss: 0.0959 - val_loss: 0.1114\n",
      "Epoch 12/24\n",
      "3690/3690 [==============================] - 2s 583us/step - loss: 0.0856 - val_loss: 0.0928\n",
      "Epoch 13/24\n",
      "3690/3690 [==============================] - 2s 585us/step - loss: 0.0790 - val_loss: 0.0787\n",
      "Epoch 14/24\n",
      "3690/3690 [==============================] - 2s 584us/step - loss: 0.0747 - val_loss: 0.0681\n",
      "Epoch 15/24\n",
      "3690/3690 [==============================] - 2s 584us/step - loss: 0.0719 - val_loss: 0.0601\n",
      "Epoch 16/24\n",
      "3690/3690 [==============================] - 2s 590us/step - loss: 0.0703 - val_loss: 0.0538\n",
      "Epoch 17/24\n",
      "3690/3690 [==============================] - 2s 589us/step - loss: 0.0693 - val_loss: 0.0494\n",
      "Epoch 18/24\n",
      "3690/3690 [==============================] - 2s 587us/step - loss: 0.0687 - val_loss: 0.0461\n",
      "Epoch 19/24\n",
      "3690/3690 [==============================] - 2s 582us/step - loss: 0.0684 - val_loss: 0.0437\n",
      "Epoch 20/24\n",
      "3690/3690 [==============================] - 2s 586us/step - loss: 0.0683 - val_loss: 0.0420\n",
      "Epoch 21/24\n",
      "3690/3690 [==============================] - 2s 584us/step - loss: 0.0682 - val_loss: 0.0408\n",
      "Epoch 22/24\n",
      "3690/3690 [==============================] - 2s 587us/step - loss: 0.0682 - val_loss: 0.0400\n",
      "Epoch 23/24\n",
      "3690/3690 [==============================] - 2s 585us/step - loss: 0.0681 - val_loss: 0.0395\n",
      "Epoch 24/24\n",
      "3690/3690 [==============================] - 2s 584us/step - loss: 0.0681 - val_loss: 0.0391\n",
      "Train on 3690 samples, validate on 410 samples\n",
      "Epoch 1/24\n",
      "3690/3690 [==============================] - 9s 2ms/step - loss: 0.1488 - val_loss: 0.0366\n",
      "Epoch 2/24\n",
      "3690/3690 [==============================] - 2s 588us/step - loss: 0.0743 - val_loss: 0.0115\n",
      "Epoch 3/24\n",
      "3690/3690 [==============================] - 2s 587us/step - loss: 0.0741 - val_loss: 0.0347\n",
      "Epoch 4/24\n",
      "3690/3690 [==============================] - 2s 585us/step - loss: 0.0683 - val_loss: 0.0499\n",
      "Epoch 5/24\n",
      "3690/3690 [==============================] - 2s 582us/step - loss: 0.0684 - val_loss: 0.0396\n",
      "Epoch 6/24\n",
      "3690/3690 [==============================] - 2s 587us/step - loss: 0.0667 - val_loss: 0.0327\n",
      "Epoch 7/24\n",
      "3690/3690 [==============================] - 2s 587us/step - loss: 0.0626 - val_loss: 0.0290\n",
      "Epoch 8/24\n",
      "3690/3690 [==============================] - 2s 584us/step - loss: 0.0642 - val_loss: 0.0553\n",
      "Epoch 9/24\n",
      "3690/3690 [==============================] - 2s 586us/step - loss: 0.0653 - val_loss: 0.0418\n",
      "Epoch 10/24\n",
      "3690/3690 [==============================] - 2s 586us/step - loss: 0.0393 - val_loss: 0.0063\n",
      "Epoch 11/24\n",
      "3690/3690 [==============================] - 2s 587us/step - loss: 0.0304 - val_loss: 0.0289\n",
      "Epoch 12/24\n",
      "3690/3690 [==============================] - 2s 587us/step - loss: 0.0769 - val_loss: 0.0104\n",
      "Epoch 13/24\n",
      "3690/3690 [==============================] - 2s 587us/step - loss: 0.0933 - val_loss: 0.0407\n",
      "Epoch 14/24\n",
      "3690/3690 [==============================] - 2s 585us/step - loss: 0.0737 - val_loss: 0.0862\n",
      "Epoch 15/24\n",
      "3690/3690 [==============================] - 2s 588us/step - loss: 0.0743 - val_loss: 0.0471\n",
      "Epoch 16/24\n",
      "3690/3690 [==============================] - 2s 588us/step - loss: 0.0676 - val_loss: 0.0269\n",
      "Epoch 17/24\n",
      "3690/3690 [==============================] - 2s 588us/step - loss: 0.0667 - val_loss: 0.0297\n",
      "Epoch 18/24\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3690/3690 [==============================] - 2s 586us/step - loss: 0.0623 - val_loss: 0.0378\n",
      "Epoch 19/24\n",
      "3690/3690 [==============================] - 2s 587us/step - loss: 0.0541 - val_loss: 0.0245\n",
      "Epoch 20/24\n",
      "3690/3690 [==============================] - 2s 591us/step - loss: 0.0386 - val_loss: 0.0146\n",
      "Epoch 21/24\n",
      "3690/3690 [==============================] - 2s 586us/step - loss: 0.0196 - val_loss: 0.0081\n",
      "Epoch 22/24\n",
      "3690/3690 [==============================] - 2s 586us/step - loss: 0.0110 - val_loss: 0.0190\n",
      "Epoch 23/24\n",
      "3690/3690 [==============================] - 2s 585us/step - loss: 0.0063 - val_loss: 0.0163\n",
      "Epoch 24/24\n",
      "3690/3690 [==============================] - 2s 585us/step - loss: 0.0045 - val_loss: 0.0055\n",
      "Train on 3690 samples, validate on 410 samples\n",
      "Epoch 1/24\n",
      "3690/3690 [==============================] - 8s 2ms/step - loss: 0.0789 - val_loss: 0.0260\n",
      "Epoch 2/24\n",
      "3690/3690 [==============================] - 2s 586us/step - loss: 0.0131 - val_loss: 0.0064\n",
      "Epoch 3/24\n",
      "3690/3690 [==============================] - 2s 585us/step - loss: 0.0048 - val_loss: 0.0142\n",
      "Epoch 4/24\n",
      "3690/3690 [==============================] - 2s 585us/step - loss: 0.0040 - val_loss: 9.1423e-04\n",
      "Epoch 5/24\n",
      "3690/3690 [==============================] - 2s 589us/step - loss: 0.0025 - val_loss: 0.0015\n",
      "Epoch 6/24\n",
      "3690/3690 [==============================] - 2s 583us/step - loss: 0.0022 - val_loss: 0.0043\n",
      "Epoch 7/24\n",
      "3690/3690 [==============================] - 2s 582us/step - loss: 0.0017 - val_loss: 0.0020\n",
      "Epoch 8/24\n",
      "3690/3690 [==============================] - 2s 582us/step - loss: 0.0014 - val_loss: 0.0012\n",
      "Epoch 9/24\n",
      "3690/3690 [==============================] - 2s 582us/step - loss: 0.0013 - val_loss: 9.8518e-04\n",
      "Epoch 10/24\n",
      "3690/3690 [==============================] - 2s 585us/step - loss: 0.0011 - val_loss: 7.5730e-04\n",
      "Epoch 11/24\n",
      "3690/3690 [==============================] - 2s 584us/step - loss: 0.0011 - val_loss: 7.4096e-04\n",
      "Epoch 12/24\n",
      "3690/3690 [==============================] - 2s 586us/step - loss: 0.0011 - val_loss: 7.8483e-04\n",
      "Epoch 13/24\n",
      "3690/3690 [==============================] - 2s 585us/step - loss: 0.0011 - val_loss: 8.1208e-04\n",
      "Epoch 14/24\n",
      "3690/3690 [==============================] - 2s 582us/step - loss: 0.0011 - val_loss: 7.4700e-04\n",
      "Epoch 15/24\n",
      "3690/3690 [==============================] - 2s 582us/step - loss: 0.0010 - val_loss: 0.0011\n",
      "Epoch 16/24\n",
      "3690/3690 [==============================] - 2s 581us/step - loss: 0.0010 - val_loss: 9.2726e-04\n",
      "Epoch 17/24\n",
      "3690/3690 [==============================] - 2s 583us/step - loss: 0.0010 - val_loss: 7.1432e-04\n",
      "Epoch 18/24\n",
      "3690/3690 [==============================] - 2s 583us/step - loss: 9.6634e-04 - val_loss: 0.0011\n",
      "Epoch 19/24\n",
      "3690/3690 [==============================] - 2s 586us/step - loss: 9.2497e-04 - val_loss: 7.6156e-04\n",
      "Epoch 20/24\n",
      "3690/3690 [==============================] - 2s 584us/step - loss: 8.7433e-04 - val_loss: 5.4031e-04\n",
      "Epoch 21/24\n",
      "3690/3690 [==============================] - 2s 582us/step - loss: 9.1262e-04 - val_loss: 8.5490e-04\n",
      "Epoch 22/24\n",
      "3690/3690 [==============================] - 2s 581us/step - loss: 9.3815e-04 - val_loss: 6.6471e-04\n",
      "Epoch 23/24\n",
      "3690/3690 [==============================] - 2s 581us/step - loss: 9.1170e-04 - val_loss: 6.9081e-04\n",
      "Epoch 24/24\n",
      "3690/3690 [==============================] - 2s 582us/step - loss: 8.5094e-04 - val_loss: 8.2466e-04\n",
      "Train on 3690 samples, validate on 410 samples\n",
      "Epoch 1/24\n",
      "3690/3690 [==============================] - 9s 2ms/step - loss: 0.6179 - val_loss: 0.0018\n",
      "Epoch 2/24\n",
      "3690/3690 [==============================] - 2s 591us/step - loss: 0.0065 - val_loss: 0.0388\n",
      "Epoch 3/24\n",
      "3690/3690 [==============================] - 2s 583us/step - loss: 0.0111 - val_loss: 0.0028\n",
      "Epoch 4/24\n",
      "3690/3690 [==============================] - 2s 584us/step - loss: 0.0032 - val_loss: 0.0049\n",
      "Epoch 5/24\n",
      "3690/3690 [==============================] - 2s 586us/step - loss: 0.0033 - val_loss: 0.0046\n",
      "Epoch 6/24\n",
      "3690/3690 [==============================] - 2s 586us/step - loss: 0.0028 - val_loss: 9.7708e-04\n",
      "Epoch 7/24\n",
      "3690/3690 [==============================] - 2s 586us/step - loss: 0.0023 - val_loss: 5.3209e-04\n",
      "Epoch 8/24\n",
      "3690/3690 [==============================] - 2s 585us/step - loss: 0.0035 - val_loss: 0.0041\n",
      "Epoch 9/24\n",
      "3690/3690 [==============================] - 2s 581us/step - loss: 0.0036 - val_loss: 4.5480e-04\n",
      "Epoch 10/24\n",
      "3690/3690 [==============================] - 2s 587us/step - loss: 0.0019 - val_loss: 9.3622e-04\n",
      "Epoch 11/24\n",
      "3690/3690 [==============================] - 2s 587us/step - loss: 0.0017 - val_loss: 6.0595e-04\n",
      "Epoch 12/24\n",
      "3690/3690 [==============================] - 2s 587us/step - loss: 0.0026 - val_loss: 0.0043\n",
      "Epoch 13/24\n",
      "3690/3690 [==============================] - 2s 589us/step - loss: 0.0042 - val_loss: 0.0012\n",
      "Epoch 14/24\n",
      "3690/3690 [==============================] - 2s 585us/step - loss: 0.0020 - val_loss: 5.1146e-04\n",
      "Epoch 15/24\n",
      "3690/3690 [==============================] - 2s 587us/step - loss: 0.0018 - val_loss: 6.0907e-04\n",
      "Epoch 16/24\n",
      "3690/3690 [==============================] - 2s 585us/step - loss: 0.0017 - val_loss: 4.6540e-04\n",
      "Epoch 17/24\n",
      "3690/3690 [==============================] - 2s 587us/step - loss: 0.0016 - val_loss: 6.7538e-04\n",
      "Epoch 18/24\n",
      "3690/3690 [==============================] - 2s 589us/step - loss: 0.0014 - val_loss: 0.0012\n",
      "Epoch 19/24\n",
      "3690/3690 [==============================] - 2s 585us/step - loss: 0.0016 - val_loss: 0.0032\n",
      "Epoch 20/24\n",
      "3690/3690 [==============================] - 2s 587us/step - loss: 0.0037 - val_loss: 0.0051\n",
      "Epoch 21/24\n",
      "3690/3690 [==============================] - 2s 585us/step - loss: 0.0020 - val_loss: 0.0013\n",
      "Epoch 22/24\n",
      "3690/3690 [==============================] - 2s 584us/step - loss: 0.0014 - val_loss: 7.3165e-04\n",
      "Epoch 23/24\n",
      "3690/3690 [==============================] - 2s 585us/step - loss: 0.0013 - val_loss: 0.0014\n",
      "Epoch 24/24\n",
      "3690/3690 [==============================] - 2s 587us/step - loss: 0.0013 - val_loss: 7.0871e-04\n",
      "Train on 3690 samples, validate on 410 samples\n",
      "Epoch 1/24\n",
      "3690/3690 [==============================] - 9s 2ms/step - loss: 2.8894 - val_loss: 0.0070\n",
      "Epoch 2/24\n",
      "3690/3690 [==============================] - 2s 589us/step - loss: 0.0142 - val_loss: 0.0023\n",
      "Epoch 3/24\n",
      "3690/3690 [==============================] - 2s 588us/step - loss: 0.0075 - val_loss: 0.0023\n",
      "Epoch 4/24\n",
      "3690/3690 [==============================] - 2s 587us/step - loss: 0.0059 - val_loss: 0.0035\n",
      "Epoch 5/24\n",
      "3690/3690 [==============================] - 2s 587us/step - loss: 0.0054 - val_loss: 0.0013\n",
      "Epoch 6/24\n",
      "3690/3690 [==============================] - 2s 585us/step - loss: 0.0052 - val_loss: 0.0019\n",
      "Epoch 7/24\n",
      "3690/3690 [==============================] - 2s 587us/step - loss: 0.0049 - val_loss: 0.0020\n",
      "Epoch 8/24\n",
      "3690/3690 [==============================] - 2s 587us/step - loss: 0.0046 - val_loss: 0.0015\n",
      "Epoch 9/24\n",
      "3690/3690 [==============================] - 2s 584us/step - loss: 0.0045 - val_loss: 0.0022\n",
      "Epoch 10/24\n",
      "3690/3690 [==============================] - 2s 584us/step - loss: 0.0042 - val_loss: 0.0012\n",
      "Epoch 11/24\n",
      "3690/3690 [==============================] - 2s 585us/step - loss: 0.0042 - val_loss: 0.0032\n",
      "Epoch 12/24\n",
      "3690/3690 [==============================] - 2s 587us/step - loss: 0.0041 - val_loss: 0.0022\n",
      "Epoch 13/24\n",
      "3690/3690 [==============================] - 2s 587us/step - loss: 0.0038 - val_loss: 0.0014\n",
      "Epoch 14/24\n",
      "3690/3690 [==============================] - 2s 590us/step - loss: 0.0038 - val_loss: 0.0019\n",
      "Epoch 15/24\n",
      "3690/3690 [==============================] - 2s 587us/step - loss: 0.0035 - val_loss: 0.0014\n",
      "Epoch 16/24\n",
      "3690/3690 [==============================] - 2s 586us/step - loss: 0.0035 - val_loss: 0.0011\n",
      "Epoch 17/24\n",
      "3690/3690 [==============================] - 2s 588us/step - loss: 0.0034 - val_loss: 0.0021\n",
      "Epoch 18/24\n",
      "3690/3690 [==============================] - 2s 587us/step - loss: 0.0033 - val_loss: 0.0012\n",
      "Epoch 19/24\n",
      "3690/3690 [==============================] - 2s 588us/step - loss: 0.0032 - val_loss: 0.0011\n",
      "Epoch 20/24\n",
      "3690/3690 [==============================] - 2s 585us/step - loss: 0.0032 - val_loss: 9.0843e-04\n",
      "Epoch 21/24\n",
      "3690/3690 [==============================] - 2s 586us/step - loss: 0.0030 - val_loss: 0.0018\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 22/24\n",
      "3690/3690 [==============================] - 2s 590us/step - loss: 0.0030 - val_loss: 0.0014\n",
      "Epoch 23/24\n",
      "3690/3690 [==============================] - 2s 586us/step - loss: 0.0029 - val_loss: 0.0014\n",
      "Epoch 24/24\n",
      "3690/3690 [==============================] - 2s 586us/step - loss: 0.0028 - val_loss: 0.0012\n",
      "Train on 3690 samples, validate on 410 samples\n",
      "Epoch 1/24\n",
      "3690/3690 [==============================] - 9s 2ms/step - loss: 12.0756 - val_loss: 0.1646\n",
      "Epoch 2/24\n",
      "3690/3690 [==============================] - 2s 534us/step - loss: 0.0705 - val_loss: 0.0064\n",
      "Epoch 3/24\n",
      "3690/3690 [==============================] - 2s 535us/step - loss: 0.0230 - val_loss: 0.0036\n",
      "Epoch 4/24\n",
      "3690/3690 [==============================] - 2s 537us/step - loss: 0.0168 - val_loss: 0.0054\n",
      "Epoch 5/24\n",
      "3690/3690 [==============================] - 2s 534us/step - loss: 0.0137 - val_loss: 0.0031\n",
      "Epoch 6/24\n",
      "3690/3690 [==============================] - 2s 533us/step - loss: 0.0119 - val_loss: 0.0068\n",
      "Epoch 7/24\n",
      "3690/3690 [==============================] - 2s 534us/step - loss: 0.0101 - val_loss: 0.0128\n",
      "Epoch 8/24\n",
      "3690/3690 [==============================] - 2s 536us/step - loss: 0.0095 - val_loss: 0.0034\n",
      "Epoch 9/24\n",
      "3690/3690 [==============================] - 2s 531us/step - loss: 0.0086 - val_loss: 0.0078\n",
      "Epoch 10/24\n",
      "3690/3690 [==============================] - 2s 533us/step - loss: 0.0078 - val_loss: 0.0040\n",
      "Epoch 11/24\n",
      "3690/3690 [==============================] - 2s 533us/step - loss: 0.0074 - val_loss: 0.0041\n",
      "Epoch 12/24\n",
      "3690/3690 [==============================] - 2s 534us/step - loss: 0.0067 - val_loss: 0.0110\n",
      "Epoch 13/24\n",
      "3690/3690 [==============================] - 2s 534us/step - loss: 0.0067 - val_loss: 0.0050\n",
      "Epoch 14/24\n",
      "3690/3690 [==============================] - 2s 535us/step - loss: 0.0062 - val_loss: 0.0018\n",
      "Epoch 15/24\n",
      "3690/3690 [==============================] - 2s 534us/step - loss: 0.0059 - val_loss: 0.0027\n",
      "Epoch 16/24\n",
      "3690/3690 [==============================] - 2s 531us/step - loss: 0.0057 - val_loss: 0.0048\n",
      "Epoch 17/24\n",
      "3690/3690 [==============================] - 2s 532us/step - loss: 0.0055 - val_loss: 0.0038\n",
      "Epoch 18/24\n",
      "3690/3690 [==============================] - 2s 532us/step - loss: 0.0053 - val_loss: 0.0034\n",
      "Epoch 19/24\n",
      "3690/3690 [==============================] - 2s 531us/step - loss: 0.0051 - val_loss: 0.0041\n",
      "Epoch 20/24\n",
      "3690/3690 [==============================] - 2s 530us/step - loss: 0.0050 - val_loss: 0.0056\n",
      "Epoch 21/24\n",
      "3690/3690 [==============================] - 2s 534us/step - loss: 0.0047 - val_loss: 0.0050\n",
      "Epoch 22/24\n",
      "3690/3690 [==============================] - 2s 536us/step - loss: 0.0048 - val_loss: 0.0030\n",
      "Epoch 23/24\n",
      "3690/3690 [==============================] - 2s 535us/step - loss: 0.0046 - val_loss: 0.0028\n",
      "Epoch 24/24\n",
      "3690/3690 [==============================] - 2s 532us/step - loss: 0.0045 - val_loss: 0.0048\n",
      "Train on 3690 samples, validate on 410 samples\n",
      "Epoch 1/24\n",
      "3690/3690 [==============================] - 9s 2ms/step - loss: 7.2417 - val_loss: 0.0020\n",
      "Epoch 2/24\n",
      "3690/3690 [==============================] - 2s 532us/step - loss: 0.0060 - val_loss: 0.0020\n",
      "Epoch 3/24\n",
      "3690/3690 [==============================] - 2s 531us/step - loss: 0.0029 - val_loss: 0.0027\n",
      "Epoch 4/24\n",
      "3690/3690 [==============================] - 2s 532us/step - loss: 0.0026 - val_loss: 0.0028\n",
      "Epoch 5/24\n",
      "3690/3690 [==============================] - 2s 535us/step - loss: 0.0025 - val_loss: 0.0018\n",
      "Epoch 6/24\n",
      "3690/3690 [==============================] - 2s 533us/step - loss: 0.0024 - val_loss: 0.0022\n",
      "Epoch 7/24\n",
      "3690/3690 [==============================] - 2s 532us/step - loss: 0.0024 - val_loss: 0.0015\n",
      "Epoch 8/24\n",
      "3690/3690 [==============================] - 2s 534us/step - loss: 0.0023 - val_loss: 0.0029\n",
      "Epoch 9/24\n",
      "3690/3690 [==============================] - 2s 535us/step - loss: 0.0022 - val_loss: 0.0014\n",
      "Epoch 10/24\n",
      "3690/3690 [==============================] - 2s 533us/step - loss: 0.0021 - val_loss: 0.0035\n",
      "Epoch 11/24\n",
      "3690/3690 [==============================] - 2s 535us/step - loss: 0.0022 - val_loss: 0.0020\n",
      "Epoch 12/24\n",
      "3690/3690 [==============================] - 2s 537us/step - loss: 0.0019 - val_loss: 0.0019\n",
      "Epoch 13/24\n",
      "3690/3690 [==============================] - 2s 535us/step - loss: 0.0020 - val_loss: 0.0019\n",
      "Epoch 14/24\n",
      "3690/3690 [==============================] - 2s 533us/step - loss: 0.0020 - val_loss: 0.0024\n",
      "Epoch 15/24\n",
      "3690/3690 [==============================] - 2s 532us/step - loss: 0.0020 - val_loss: 0.0026\n",
      "Epoch 16/24\n",
      "3690/3690 [==============================] - 2s 531us/step - loss: 0.0019 - val_loss: 0.0018\n",
      "Epoch 17/24\n",
      "3690/3690 [==============================] - 2s 531us/step - loss: 0.0019 - val_loss: 0.0023\n",
      "Epoch 18/24\n",
      "3690/3690 [==============================] - 2s 534us/step - loss: 0.0017 - val_loss: 0.0027\n",
      "Epoch 19/24\n",
      "3690/3690 [==============================] - 2s 531us/step - loss: 0.0017 - val_loss: 0.0022\n",
      "Epoch 20/24\n",
      "3690/3690 [==============================] - 2s 535us/step - loss: 0.0017 - val_loss: 0.0021\n",
      "Epoch 21/24\n",
      "3690/3690 [==============================] - 2s 534us/step - loss: 0.0017 - val_loss: 0.0018\n",
      "Epoch 22/24\n",
      "3690/3690 [==============================] - 2s 534us/step - loss: 0.0017 - val_loss: 0.0017\n",
      "Epoch 23/24\n",
      "3690/3690 [==============================] - 2s 531us/step - loss: 0.0017 - val_loss: 0.0030\n",
      "Epoch 24/24\n",
      "3690/3690 [==============================] - 2s 536us/step - loss: 0.0017 - val_loss: 0.0024\n",
      "Train on 3690 samples, validate on 410 samples\n",
      "Epoch 1/24\n",
      "3690/3690 [==============================] - 9s 2ms/step - loss: 5.3241 - val_loss: 0.0015\n",
      "Epoch 2/24\n",
      "3690/3690 [==============================] - 2s 529us/step - loss: 0.0052 - val_loss: 0.0014\n",
      "Epoch 3/24\n",
      "3690/3690 [==============================] - 2s 533us/step - loss: 0.0047 - val_loss: 0.0023\n",
      "Epoch 4/24\n",
      "3690/3690 [==============================] - 2s 529us/step - loss: 0.0045 - val_loss: 0.0022\n",
      "Epoch 5/24\n",
      "3690/3690 [==============================] - 2s 535us/step - loss: 0.0041 - val_loss: 0.0017\n",
      "Epoch 6/24\n",
      "3690/3690 [==============================] - 2s 533us/step - loss: 0.0040 - val_loss: 0.0014\n",
      "Epoch 7/24\n",
      "3690/3690 [==============================] - 2s 532us/step - loss: 0.0039 - val_loss: 0.0017\n",
      "Epoch 8/24\n",
      "3690/3690 [==============================] - 2s 532us/step - loss: 0.0037 - val_loss: 0.0013\n",
      "Epoch 9/24\n",
      "3690/3690 [==============================] - 2s 531us/step - loss: 0.0038 - val_loss: 0.0017\n",
      "Epoch 10/24\n",
      "3690/3690 [==============================] - 2s 532us/step - loss: 0.0037 - val_loss: 0.0013\n",
      "Epoch 11/24\n",
      "3690/3690 [==============================] - 2s 530us/step - loss: 0.0036 - val_loss: 0.0020\n",
      "Epoch 12/24\n",
      "3690/3690 [==============================] - 2s 531us/step - loss: 0.0033 - val_loss: 0.0019\n",
      "Epoch 13/24\n",
      "3690/3690 [==============================] - 2s 533us/step - loss: 0.0034 - val_loss: 0.0012\n",
      "Epoch 14/24\n",
      "3690/3690 [==============================] - 2s 533us/step - loss: 0.0033 - val_loss: 0.0017\n",
      "Epoch 15/24\n",
      "3690/3690 [==============================] - 2s 533us/step - loss: 0.0032 - val_loss: 0.0014\n",
      "Epoch 16/24\n",
      "3690/3690 [==============================] - 2s 534us/step - loss: 0.0031 - val_loss: 0.0018\n",
      "Epoch 17/24\n",
      "3690/3690 [==============================] - 2s 532us/step - loss: 0.0031 - val_loss: 0.0014\n",
      "Epoch 18/24\n",
      "3690/3690 [==============================] - 2s 535us/step - loss: 0.0031 - val_loss: 0.0012\n",
      "Epoch 19/24\n",
      "3690/3690 [==============================] - 2s 531us/step - loss: 0.0030 - val_loss: 0.0016\n",
      "Epoch 20/24\n",
      "3690/3690 [==============================] - 2s 530us/step - loss: 0.0030 - val_loss: 0.0012\n",
      "Epoch 21/24\n",
      "3690/3690 [==============================] - 2s 536us/step - loss: 0.0028 - val_loss: 0.0013\n",
      "Epoch 22/24\n",
      "3690/3690 [==============================] - 2s 533us/step - loss: 0.0028 - val_loss: 0.0019\n",
      "Epoch 23/24\n",
      "3690/3690 [==============================] - 2s 531us/step - loss: 0.0027 - val_loss: 0.0018\n",
      "Epoch 24/24\n",
      "3690/3690 [==============================] - 2s 533us/step - loss: 0.0026 - val_loss: 0.0015\n",
      "Train on 3690 samples, validate on 410 samples\n",
      "Epoch 1/24\n",
      "3690/3690 [==============================] - 9s 3ms/step - loss: 7.3518 - val_loss: 0.0555\n",
      "Epoch 2/24\n",
      "3690/3690 [==============================] - 2s 543us/step - loss: 0.0190 - val_loss: 0.0030\n",
      "Epoch 3/24\n",
      "3690/3690 [==============================] - 2s 544us/step - loss: 0.0055 - val_loss: 0.0016\n",
      "Epoch 4/24\n",
      "3690/3690 [==============================] - 2s 544us/step - loss: 0.0047 - val_loss: 0.0010\n",
      "Epoch 5/24\n",
      "3690/3690 [==============================] - 2s 542us/step - loss: 0.0044 - val_loss: 0.0029\n",
      "Epoch 6/24\n",
      "3690/3690 [==============================] - 2s 544us/step - loss: 0.0040 - val_loss: 0.0036\n",
      "Epoch 7/24\n",
      "3690/3690 [==============================] - 2s 541us/step - loss: 0.0039 - val_loss: 0.0030\n",
      "Epoch 8/24\n",
      "3690/3690 [==============================] - 2s 541us/step - loss: 0.0036 - val_loss: 0.0014\n",
      "Epoch 9/24\n",
      "3690/3690 [==============================] - 2s 544us/step - loss: 0.0033 - val_loss: 0.0015\n",
      "Epoch 10/24\n",
      "3690/3690 [==============================] - 2s 548us/step - loss: 0.0031 - val_loss: 0.0018\n",
      "Epoch 11/24\n",
      "3690/3690 [==============================] - 2s 545us/step - loss: 0.0030 - val_loss: 0.0013\n",
      "Epoch 12/24\n",
      "3690/3690 [==============================] - 2s 543us/step - loss: 0.0030 - val_loss: 0.0012\n",
      "Epoch 13/24\n",
      "3690/3690 [==============================] - 2s 543us/step - loss: 0.0027 - val_loss: 9.9036e-04\n",
      "Epoch 14/24\n",
      "3690/3690 [==============================] - 2s 543us/step - loss: 0.0026 - val_loss: 0.0014\n",
      "Epoch 15/24\n",
      "3690/3690 [==============================] - 2s 540us/step - loss: 0.0027 - val_loss: 0.0014\n",
      "Epoch 16/24\n",
      "3690/3690 [==============================] - 2s 542us/step - loss: 0.0026 - val_loss: 0.0011\n",
      "Epoch 17/24\n",
      "3690/3690 [==============================] - 2s 544us/step - loss: 0.0024 - val_loss: 0.0012\n",
      "Epoch 18/24\n",
      "3690/3690 [==============================] - 2s 542us/step - loss: 0.0025 - val_loss: 0.0018\n",
      "Epoch 19/24\n",
      "3690/3690 [==============================] - 2s 546us/step - loss: 0.0023 - val_loss: 8.9351e-04\n",
      "Epoch 20/24\n",
      "3690/3690 [==============================] - 2s 549us/step - loss: 0.0022 - val_loss: 0.0013\n",
      "Epoch 21/24\n",
      "3690/3690 [==============================] - 2s 546us/step - loss: 0.0023 - val_loss: 8.4869e-04\n",
      "Epoch 22/24\n",
      "3690/3690 [==============================] - 2s 544us/step - loss: 0.0022 - val_loss: 7.7649e-04\n",
      "Epoch 23/24\n",
      "3690/3690 [==============================] - 2s 543us/step - loss: 0.0022 - val_loss: 9.5178e-04\n",
      "Epoch 24/24\n",
      "3690/3690 [==============================] - 2s 544us/step - loss: 0.0022 - val_loss: 0.0011\n",
      "\n",
      "GENERATION 7\n",
      "\n",
      "Train on 3690 samples, validate on 410 samples\n",
      "Epoch 1/24\n",
      "3690/3690 [==============================] - 9s 3ms/step - loss: 0.0708 - val_loss: 0.0230\n",
      "Epoch 2/24\n",
      "3690/3690 [==============================] - 2s 586us/step - loss: 0.0111 - val_loss: 0.0076\n",
      "Epoch 3/24\n",
      "3690/3690 [==============================] - 2s 587us/step - loss: 0.0047 - val_loss: 0.0090\n",
      "Epoch 4/24\n",
      "3690/3690 [==============================] - 2s 584us/step - loss: 0.0037 - val_loss: 6.6655e-04\n",
      "Epoch 5/24\n",
      "3690/3690 [==============================] - 2s 584us/step - loss: 0.0028 - val_loss: 0.0043\n",
      "Epoch 6/24\n",
      "3690/3690 [==============================] - 2s 585us/step - loss: 0.0022 - val_loss: 0.0014\n",
      "Epoch 7/24\n",
      "3690/3690 [==============================] - 2s 584us/step - loss: 0.0018 - val_loss: 0.0019\n",
      "Epoch 8/24\n",
      "3690/3690 [==============================] - 2s 590us/step - loss: 0.0015 - val_loss: 0.0011\n",
      "Epoch 9/24\n",
      "3690/3690 [==============================] - 2s 588us/step - loss: 0.0013 - val_loss: 8.0686e-04\n",
      "Epoch 10/24\n",
      "3690/3690 [==============================] - 2s 592us/step - loss: 0.0012 - val_loss: 6.7397e-04\n",
      "Epoch 11/24\n",
      "3690/3690 [==============================] - 2s 585us/step - loss: 0.0012 - val_loss: 5.4950e-04\n",
      "Epoch 12/24\n",
      "3690/3690 [==============================] - 2s 587us/step - loss: 0.0012 - val_loss: 9.4577e-04\n",
      "Epoch 13/24\n",
      "3690/3690 [==============================] - 2s 588us/step - loss: 0.0011 - val_loss: 0.0011\n",
      "Epoch 14/24\n",
      "3690/3690 [==============================] - 2s 588us/step - loss: 0.0010 - val_loss: 0.0015\n",
      "Epoch 15/24\n",
      "3690/3690 [==============================] - 2s 588us/step - loss: 0.0010 - val_loss: 0.0012\n",
      "Epoch 16/24\n",
      "3690/3690 [==============================] - 2s 587us/step - loss: 0.0010 - val_loss: 0.0012\n",
      "Epoch 17/24\n",
      "3690/3690 [==============================] - 2s 588us/step - loss: 0.0010 - val_loss: 9.0989e-04\n",
      "Epoch 18/24\n",
      "3690/3690 [==============================] - 2s 584us/step - loss: 0.0010 - val_loss: 7.0082e-04\n",
      "Epoch 19/24\n",
      "3690/3690 [==============================] - 2s 584us/step - loss: 9.8407e-04 - val_loss: 7.8953e-04\n",
      "Epoch 20/24\n",
      "3690/3690 [==============================] - 2s 585us/step - loss: 9.1076e-04 - val_loss: 7.4281e-04\n",
      "Epoch 21/24\n",
      "3690/3690 [==============================] - 2s 584us/step - loss: 9.2971e-04 - val_loss: 0.0011\n",
      "Epoch 22/24\n",
      "3690/3690 [==============================] - 2s 586us/step - loss: 8.7449e-04 - val_loss: 9.8909e-04\n",
      "Epoch 23/24\n",
      "3690/3690 [==============================] - 2s 584us/step - loss: 9.0411e-04 - val_loss: 9.8543e-04\n",
      "Epoch 24/24\n",
      "3690/3690 [==============================] - 2s 588us/step - loss: 9.0326e-04 - val_loss: 7.7320e-04\n",
      "Train on 3690 samples, validate on 410 samples\n",
      "Epoch 1/24\n",
      "3690/3690 [==============================] - 10s 3ms/step - loss: 0.0658 - val_loss: 0.0098\n",
      "Epoch 2/24\n",
      "3690/3690 [==============================] - 2s 586us/step - loss: 0.0120 - val_loss: 0.0144\n",
      "Epoch 3/24\n",
      "3690/3690 [==============================] - 2s 586us/step - loss: 0.0059 - val_loss: 0.0117\n",
      "Epoch 4/24\n",
      "3690/3690 [==============================] - 2s 583us/step - loss: 0.0035 - val_loss: 0.0012\n",
      "Epoch 5/24\n",
      "3690/3690 [==============================] - 2s 588us/step - loss: 0.0019 - val_loss: 6.2089e-04\n",
      "Epoch 6/24\n",
      "3690/3690 [==============================] - 2s 586us/step - loss: 0.0015 - val_loss: 5.9797e-04\n",
      "Epoch 7/24\n",
      "3690/3690 [==============================] - 2s 588us/step - loss: 0.0013 - val_loss: 0.0011\n",
      "Epoch 8/24\n",
      "3690/3690 [==============================] - 2s 589us/step - loss: 0.0012 - val_loss: 0.0019\n",
      "Epoch 9/24\n",
      "3690/3690 [==============================] - 2s 587us/step - loss: 0.0010 - val_loss: 0.0013\n",
      "Epoch 10/24\n",
      "3690/3690 [==============================] - 2s 587us/step - loss: 9.7908e-04 - val_loss: 0.0015\n",
      "Epoch 11/24\n",
      "3690/3690 [==============================] - 2s 585us/step - loss: 0.0010 - val_loss: 0.0013\n",
      "Epoch 12/24\n",
      "3690/3690 [==============================] - 2s 587us/step - loss: 9.7071e-04 - val_loss: 0.0010\n",
      "Epoch 13/24\n",
      "3690/3690 [==============================] - 2s 587us/step - loss: 9.4244e-04 - val_loss: 0.0012\n",
      "Epoch 14/24\n",
      "3690/3690 [==============================] - 2s 588us/step - loss: 9.0575e-04 - val_loss: 0.0011\n",
      "Epoch 15/24\n",
      "3690/3690 [==============================] - 2s 587us/step - loss: 8.7959e-04 - val_loss: 9.3098e-04\n",
      "Epoch 16/24\n",
      "3690/3690 [==============================] - 2s 588us/step - loss: 8.8684e-04 - val_loss: 0.0011\n",
      "Epoch 17/24\n",
      "3690/3690 [==============================] - 2s 587us/step - loss: 8.6019e-04 - val_loss: 7.6335e-04\n",
      "Epoch 18/24\n",
      "3690/3690 [==============================] - 2s 586us/step - loss: 8.4895e-04 - val_loss: 8.6346e-04\n",
      "Epoch 19/24\n",
      "3690/3690 [==============================] - 2s 587us/step - loss: 8.4447e-04 - val_loss: 8.0590e-04\n",
      "Epoch 20/24\n",
      "3690/3690 [==============================] - 2s 589us/step - loss: 7.9115e-04 - val_loss: 0.0010\n",
      "Epoch 21/24\n",
      "3690/3690 [==============================] - 2s 589us/step - loss: 8.0338e-04 - val_loss: 0.0011\n",
      "Epoch 22/24\n",
      "3690/3690 [==============================] - 2s 585us/step - loss: 7.9835e-04 - val_loss: 7.4101e-04\n",
      "Epoch 23/24\n",
      "3690/3690 [==============================] - 2s 584us/step - loss: 7.7110e-04 - val_loss: 6.6366e-04\n",
      "Epoch 24/24\n",
      "3690/3690 [==============================] - 2s 585us/step - loss: 7.6707e-04 - val_loss: 5.6728e-04\n",
      "Train on 3690 samples, validate on 410 samples\n",
      "Epoch 1/24\n",
      "3690/3690 [==============================] - 10s 3ms/step - loss: 0.0602 - val_loss: 0.0178\n",
      "Epoch 2/24\n",
      "3690/3690 [==============================] - 2s 566us/step - loss: 0.0110 - val_loss: 0.0077\n",
      "Epoch 3/24\n",
      "3690/3690 [==============================] - 2s 562us/step - loss: 0.0049 - val_loss: 0.0103\n",
      "Epoch 4/24\n",
      "3690/3690 [==============================] - 2s 564us/step - loss: 0.0037 - val_loss: 5.4975e-04\n",
      "Epoch 5/24\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3690/3690 [==============================] - 2s 563us/step - loss: 0.0027 - val_loss: 0.0033\n",
      "Epoch 6/24\n",
      "3690/3690 [==============================] - 2s 559us/step - loss: 0.0021 - val_loss: 0.0040\n",
      "Epoch 7/24\n",
      "3690/3690 [==============================] - 2s 564us/step - loss: 0.0018 - val_loss: 8.4854e-04\n",
      "Epoch 8/24\n",
      "3690/3690 [==============================] - 2s 561us/step - loss: 0.0015 - val_loss: 0.0011\n",
      "Epoch 9/24\n",
      "3690/3690 [==============================] - 2s 561us/step - loss: 0.0014 - val_loss: 0.0016\n",
      "Epoch 10/24\n",
      "3690/3690 [==============================] - 2s 565us/step - loss: 0.0012 - val_loss: 0.0018\n",
      "Epoch 11/24\n",
      "3690/3690 [==============================] - 2s 566us/step - loss: 0.0012 - val_loss: 0.0010\n",
      "Epoch 12/24\n",
      "3690/3690 [==============================] - 2s 561us/step - loss: 0.0011 - val_loss: 8.4257e-04\n",
      "Epoch 13/24\n",
      "3690/3690 [==============================] - 2s 563us/step - loss: 0.0011 - val_loss: 0.0012\n",
      "Epoch 14/24\n",
      "3690/3690 [==============================] - 2s 559us/step - loss: 0.0012 - val_loss: 0.0015\n",
      "Epoch 15/24\n",
      "3690/3690 [==============================] - 2s 563us/step - loss: 0.0010 - val_loss: 0.0015\n",
      "Epoch 16/24\n",
      "3690/3690 [==============================] - 2s 564us/step - loss: 0.0011 - val_loss: 0.0016\n",
      "Epoch 17/24\n",
      "3690/3690 [==============================] - 2s 565us/step - loss: 0.0010 - val_loss: 0.0012\n",
      "Epoch 18/24\n",
      "3690/3690 [==============================] - 2s 564us/step - loss: 0.0010 - val_loss: 0.0013\n",
      "Epoch 19/24\n",
      "3690/3690 [==============================] - 2s 562us/step - loss: 9.6203e-04 - val_loss: 0.0011\n",
      "Epoch 20/24\n",
      "3690/3690 [==============================] - 2s 562us/step - loss: 9.5972e-04 - val_loss: 0.0012\n",
      "Epoch 21/24\n",
      "3690/3690 [==============================] - 2s 565us/step - loss: 9.0483e-04 - val_loss: 0.0013\n",
      "Epoch 22/24\n",
      "3690/3690 [==============================] - 2s 559us/step - loss: 9.1097e-04 - val_loss: 0.0020\n",
      "Epoch 23/24\n",
      "3690/3690 [==============================] - 2s 564us/step - loss: 9.2883e-04 - val_loss: 0.0011\n",
      "Epoch 24/24\n",
      "3690/3690 [==============================] - 2s 561us/step - loss: 8.7146e-04 - val_loss: 0.0013\n",
      "Train on 3690 samples, validate on 410 samples\n",
      "Epoch 1/24\n",
      "3690/3690 [==============================] - 10s 3ms/step - loss: nan - val_loss: nan\n",
      "Epoch 2/24\n",
      "3690/3690 [==============================] - 2s 561us/step - loss: nan - val_loss: nan\n",
      "Epoch 3/24\n",
      "3690/3690 [==============================] - 2s 561us/step - loss: nan - val_loss: nan\n",
      "Epoch 4/24\n",
      "3690/3690 [==============================] - 2s 561us/step - loss: nan - val_loss: nan\n",
      "Epoch 5/24\n",
      "3690/3690 [==============================] - 2s 561us/step - loss: nan - val_loss: nan\n",
      "Epoch 6/24\n",
      "3690/3690 [==============================] - 2s 561us/step - loss: nan - val_loss: nan\n",
      "Epoch 7/24\n",
      "3690/3690 [==============================] - 2s 559us/step - loss: nan - val_loss: nan\n",
      "Epoch 8/24\n",
      "3690/3690 [==============================] - 2s 563us/step - loss: nan - val_loss: nan\n",
      "Epoch 9/24\n",
      "3690/3690 [==============================] - 2s 565us/step - loss: nan - val_loss: nan\n",
      "Epoch 10/24\n",
      "3690/3690 [==============================] - 2s 562us/step - loss: nan - val_loss: nan\n",
      "Epoch 11/24\n",
      "3690/3690 [==============================] - 2s 562us/step - loss: nan - val_loss: nan\n",
      "Epoch 12/24\n",
      "3690/3690 [==============================] - 2s 558us/step - loss: nan - val_loss: nan\n",
      "Epoch 13/24\n",
      "3690/3690 [==============================] - 2s 561us/step - loss: nan - val_loss: nan\n",
      "Epoch 14/24\n",
      "3690/3690 [==============================] - 2s 564us/step - loss: nan - val_loss: nan\n",
      "Epoch 15/24\n",
      "3690/3690 [==============================] - 2s 559us/step - loss: nan - val_loss: nan\n",
      "Epoch 16/24\n",
      "3690/3690 [==============================] - 2s 561us/step - loss: nan - val_loss: nan\n",
      "Epoch 17/24\n",
      "3690/3690 [==============================] - 2s 564us/step - loss: nan - val_loss: nan\n",
      "Epoch 18/24\n",
      "3690/3690 [==============================] - 2s 560us/step - loss: nan - val_loss: nan\n",
      "Epoch 19/24\n",
      "3690/3690 [==============================] - 2s 560us/step - loss: nan - val_loss: nan\n",
      "Epoch 20/24\n",
      "3690/3690 [==============================] - 2s 560us/step - loss: nan - val_loss: nan\n",
      "Epoch 21/24\n",
      "3690/3690 [==============================] - 2s 561us/step - loss: nan - val_loss: nan\n",
      "Epoch 22/24\n",
      "3690/3690 [==============================] - 2s 560us/step - loss: nan - val_loss: nan\n",
      "Epoch 23/24\n",
      "3690/3690 [==============================] - 2s 561us/step - loss: nan - val_loss: nan\n",
      "Epoch 24/24\n",
      "3690/3690 [==============================] - 2s 562us/step - loss: nan - val_loss: nan\n",
      "Train on 3690 samples, validate on 410 samples\n",
      "Epoch 1/24\n",
      "3690/3690 [==============================] - 10s 3ms/step - loss: 0.0686 - val_loss: 0.0021\n",
      "Epoch 2/24\n",
      "3690/3690 [==============================] - 2s 565us/step - loss: 0.0074 - val_loss: 6.9249e-04\n",
      "Epoch 3/24\n",
      "3690/3690 [==============================] - 2s 559us/step - loss: 0.0043 - val_loss: 0.0023\n",
      "Epoch 4/24\n",
      "3690/3690 [==============================] - 2s 564us/step - loss: 0.0022 - val_loss: 0.0038\n",
      "Epoch 5/24\n",
      "3690/3690 [==============================] - 2s 569us/step - loss: 0.0015 - val_loss: 0.0023\n",
      "Epoch 6/24\n",
      "3690/3690 [==============================] - 2s 561us/step - loss: 0.0014 - val_loss: 0.0020\n",
      "Epoch 7/24\n",
      "3690/3690 [==============================] - 2s 562us/step - loss: 0.0013 - val_loss: 0.0010\n",
      "Epoch 8/24\n",
      "3690/3690 [==============================] - 2s 564us/step - loss: 0.0012 - val_loss: 9.7796e-04\n",
      "Epoch 9/24\n",
      "3690/3690 [==============================] - 2s 562us/step - loss: 0.0012 - val_loss: 0.0016\n",
      "Epoch 10/24\n",
      "3690/3690 [==============================] - 2s 564us/step - loss: 0.0011 - val_loss: 0.0017\n",
      "Epoch 11/24\n",
      "3690/3690 [==============================] - 2s 562us/step - loss: 0.0011 - val_loss: 0.0013\n",
      "Epoch 12/24\n",
      "3690/3690 [==============================] - 2s 563us/step - loss: 0.0011 - val_loss: 0.0014\n",
      "Epoch 13/24\n",
      "3690/3690 [==============================] - 2s 566us/step - loss: 0.0011 - val_loss: 0.0014\n",
      "Epoch 14/24\n",
      "3690/3690 [==============================] - 2s 564us/step - loss: 0.0010 - val_loss: 0.0012\n",
      "Epoch 15/24\n",
      "3690/3690 [==============================] - 2s 567us/step - loss: 0.0010 - val_loss: 0.0010\n",
      "Epoch 16/24\n",
      "3690/3690 [==============================] - 2s 562us/step - loss: 0.0010 - val_loss: 0.0013\n",
      "Epoch 17/24\n",
      "3690/3690 [==============================] - 2s 561us/step - loss: 0.0010 - val_loss: 0.0014\n",
      "Epoch 18/24\n",
      "3690/3690 [==============================] - 2s 565us/step - loss: 9.3382e-04 - val_loss: 0.0014\n",
      "Epoch 19/24\n",
      "3690/3690 [==============================] - 2s 561us/step - loss: 9.5062e-04 - val_loss: 0.0012\n",
      "Epoch 20/24\n",
      "3690/3690 [==============================] - 2s 572us/step - loss: 9.2824e-04 - val_loss: 0.0013\n",
      "Epoch 21/24\n",
      "3690/3690 [==============================] - 2s 569us/step - loss: 9.6115e-04 - val_loss: 8.7787e-04\n",
      "Epoch 22/24\n",
      "3690/3690 [==============================] - 2s 564us/step - loss: 9.4609e-04 - val_loss: 0.0011\n",
      "Epoch 23/24\n",
      "3690/3690 [==============================] - 2s 566us/step - loss: 9.1189e-04 - val_loss: 0.0012\n",
      "Epoch 24/24\n",
      "3690/3690 [==============================] - 2s 565us/step - loss: 9.2901e-04 - val_loss: 0.0012\n",
      "Train on 3690 samples, validate on 410 samples\n",
      "Epoch 1/24\n",
      "3690/3690 [==============================] - 10s 3ms/step - loss: 0.0600 - val_loss: 7.7536e-04\n",
      "Epoch 2/24\n",
      "3690/3690 [==============================] - 2s 568us/step - loss: 0.0064 - val_loss: 0.0033\n",
      "Epoch 3/24\n",
      "3690/3690 [==============================] - 2s 564us/step - loss: 0.0027 - val_loss: 0.0044\n",
      "Epoch 4/24\n",
      "3690/3690 [==============================] - 2s 561us/step - loss: 0.0016 - val_loss: 0.0019\n",
      "Epoch 5/24\n",
      "3690/3690 [==============================] - 2s 564us/step - loss: 0.0012 - val_loss: 0.0021\n",
      "Epoch 6/24\n",
      "3690/3690 [==============================] - 2s 564us/step - loss: 0.0011 - val_loss: 0.0011\n",
      "Epoch 7/24\n",
      "3690/3690 [==============================] - 2s 567us/step - loss: 9.7731e-04 - val_loss: 0.0012\n",
      "Epoch 8/24\n",
      "3690/3690 [==============================] - 2s 563us/step - loss: 8.8539e-04 - val_loss: 0.0011\n",
      "Epoch 9/24\n",
      "3690/3690 [==============================] - 2s 568us/step - loss: 8.6411e-04 - val_loss: 0.0010\n",
      "Epoch 10/24\n",
      "3690/3690 [==============================] - 2s 564us/step - loss: 8.6943e-04 - val_loss: 7.4147e-04\n",
      "Epoch 11/24\n",
      "3690/3690 [==============================] - 2s 564us/step - loss: 8.4709e-04 - val_loss: 7.6401e-04\n",
      "Epoch 12/24\n",
      "3690/3690 [==============================] - 2s 564us/step - loss: 8.1009e-04 - val_loss: 8.0433e-04\n",
      "Epoch 13/24\n",
      "3690/3690 [==============================] - 2s 562us/step - loss: 7.9261e-04 - val_loss: 8.4480e-04\n",
      "Epoch 14/24\n",
      "3690/3690 [==============================] - 2s 561us/step - loss: 7.9040e-04 - val_loss: 8.2205e-04\n",
      "Epoch 15/24\n",
      "3690/3690 [==============================] - 2s 564us/step - loss: 7.7273e-04 - val_loss: 8.0828e-04\n",
      "Epoch 16/24\n",
      "3690/3690 [==============================] - 2s 568us/step - loss: 7.5293e-04 - val_loss: 7.5366e-04\n",
      "Epoch 17/24\n",
      "3690/3690 [==============================] - 2s 564us/step - loss: 7.4957e-04 - val_loss: 9.0613e-04\n",
      "Epoch 18/24\n",
      "3690/3690 [==============================] - 2s 564us/step - loss: 7.6768e-04 - val_loss: 8.7318e-04\n",
      "Epoch 19/24\n",
      "3690/3690 [==============================] - 2s 561us/step - loss: 7.4743e-04 - val_loss: 7.9280e-04\n",
      "Epoch 20/24\n",
      "3690/3690 [==============================] - 2s 561us/step - loss: 7.2510e-04 - val_loss: 6.1872e-04\n",
      "Epoch 21/24\n",
      "3690/3690 [==============================] - 2s 565us/step - loss: 7.2408e-04 - val_loss: 5.7359e-04\n",
      "Epoch 22/24\n",
      "3690/3690 [==============================] - 2s 564us/step - loss: 7.0465e-04 - val_loss: 6.3277e-04\n",
      "Epoch 23/24\n",
      "3690/3690 [==============================] - 2s 565us/step - loss: 7.1777e-04 - val_loss: 5.9135e-04\n",
      "Epoch 24/24\n",
      "3690/3690 [==============================] - 2s 568us/step - loss: 6.9667e-04 - val_loss: 7.2867e-04\n",
      "Train on 3690 samples, validate on 410 samples\n",
      "Epoch 1/24\n",
      "3690/3690 [==============================] - 11s 3ms/step - loss: 0.1048 - val_loss: 0.0093\n",
      "Epoch 2/24\n",
      "3690/3690 [==============================] - 2s 564us/step - loss: 0.0172 - val_loss: 0.0034\n",
      "Epoch 3/24\n",
      "3690/3690 [==============================] - 2s 563us/step - loss: 0.0080 - val_loss: 0.0112\n",
      "Epoch 4/24\n",
      "3690/3690 [==============================] - 2s 564us/step - loss: 0.0065 - val_loss: 0.0030\n",
      "Epoch 5/24\n",
      "3690/3690 [==============================] - 2s 563us/step - loss: 0.0050 - val_loss: 0.0028\n",
      "Epoch 6/24\n",
      "3690/3690 [==============================] - 2s 565us/step - loss: 0.0044 - val_loss: 0.0022\n",
      "Epoch 7/24\n",
      "3690/3690 [==============================] - 2s 565us/step - loss: 0.0036 - val_loss: 0.0036\n",
      "Epoch 8/24\n",
      "3690/3690 [==============================] - 2s 561us/step - loss: 0.0030 - val_loss: 0.0031\n",
      "Epoch 9/24\n",
      "3690/3690 [==============================] - 2s 562us/step - loss: 0.0026 - val_loss: 0.0025\n",
      "Epoch 10/24\n",
      "3690/3690 [==============================] - 2s 564us/step - loss: 0.0022 - val_loss: 0.0031\n",
      "Epoch 11/24\n",
      "3690/3690 [==============================] - 2s 560us/step - loss: 0.0019 - val_loss: 0.0025\n",
      "Epoch 12/24\n",
      "3690/3690 [==============================] - 2s 565us/step - loss: 0.0016 - val_loss: 0.0015\n",
      "Epoch 13/24\n",
      "3690/3690 [==============================] - 2s 564us/step - loss: 0.0015 - val_loss: 0.0023\n",
      "Epoch 14/24\n",
      "3690/3690 [==============================] - 2s 562us/step - loss: 0.0014 - val_loss: 0.0014\n",
      "Epoch 15/24\n",
      "3690/3690 [==============================] - 2s 563us/step - loss: 0.0012 - val_loss: 8.3825e-04\n",
      "Epoch 16/24\n",
      "3690/3690 [==============================] - 2s 563us/step - loss: 0.0013 - val_loss: 0.0013\n",
      "Epoch 17/24\n",
      "3690/3690 [==============================] - 2s 656us/step - loss: 0.0012 - val_loss: 0.0014\n",
      "Epoch 18/24\n",
      "3690/3690 [==============================] - 2s 575us/step - loss: 0.0012 - val_loss: 0.0011\n",
      "Epoch 19/24\n",
      "3690/3690 [==============================] - 2s 576us/step - loss: 0.0012 - val_loss: 0.0011\n",
      "Epoch 20/24\n",
      "3690/3690 [==============================] - 2s 578us/step - loss: 0.0011 - val_loss: 0.0011\n",
      "Epoch 21/24\n",
      "3690/3690 [==============================] - 2s 572us/step - loss: 0.0011 - val_loss: 0.0013\n",
      "Epoch 22/24\n",
      "3690/3690 [==============================] - 2s 574us/step - loss: 0.0010 - val_loss: 0.0012\n",
      "Epoch 23/24\n",
      "3690/3690 [==============================] - 2s 572us/step - loss: 0.0011 - val_loss: 0.0012\n",
      "Epoch 24/24\n",
      "3690/3690 [==============================] - 2s 574us/step - loss: 0.0011 - val_loss: 0.0010\n",
      "Train on 3690 samples, validate on 410 samples\n",
      "Epoch 1/24\n",
      "3690/3690 [==============================] - 11s 3ms/step - loss: 0.0680 - val_loss: 0.0093\n",
      "Epoch 2/24\n",
      "3690/3690 [==============================] - 2s 563us/step - loss: 0.0090 - val_loss: 0.0154\n",
      "Epoch 3/24\n",
      "3690/3690 [==============================] - 2s 563us/step - loss: 0.0058 - val_loss: 9.1378e-04\n",
      "Epoch 4/24\n",
      "3690/3690 [==============================] - 2s 565us/step - loss: 0.0041 - val_loss: 0.0028\n",
      "Epoch 5/24\n",
      "3690/3690 [==============================] - 2s 564us/step - loss: 0.0031 - val_loss: 0.0051\n",
      "Epoch 6/24\n",
      "3690/3690 [==============================] - 2s 561us/step - loss: 0.0024 - val_loss: 0.0019\n",
      "Epoch 7/24\n",
      "3690/3690 [==============================] - 2s 565us/step - loss: 0.0020 - val_loss: 0.0026\n",
      "Epoch 8/24\n",
      "3690/3690 [==============================] - 2s 565us/step - loss: 0.0016 - val_loss: 0.0023\n",
      "Epoch 9/24\n",
      "3690/3690 [==============================] - 2s 565us/step - loss: 0.0015 - val_loss: 0.0015\n",
      "Epoch 10/24\n",
      "3690/3690 [==============================] - 2s 561us/step - loss: 0.0014 - val_loss: 0.0010\n",
      "Epoch 11/24\n",
      "3690/3690 [==============================] - 2s 563us/step - loss: 0.0013 - val_loss: 9.8597e-04\n",
      "Epoch 12/24\n",
      "3690/3690 [==============================] - 2s 564us/step - loss: 0.0012 - val_loss: 0.0017\n",
      "Epoch 13/24\n",
      "3690/3690 [==============================] - 2s 563us/step - loss: 0.0012 - val_loss: 8.7455e-04\n",
      "Epoch 14/24\n",
      "3690/3690 [==============================] - 2s 563us/step - loss: 0.0012 - val_loss: 0.0011\n",
      "Epoch 15/24\n",
      "3690/3690 [==============================] - 2s 563us/step - loss: 0.0012 - val_loss: 0.0010\n",
      "Epoch 16/24\n",
      "3690/3690 [==============================] - 2s 563us/step - loss: 0.0011 - val_loss: 9.4067e-04\n",
      "Epoch 17/24\n",
      "3690/3690 [==============================] - 2s 562us/step - loss: 0.0011 - val_loss: 9.3538e-04\n",
      "Epoch 18/24\n",
      "3690/3690 [==============================] - 2s 560us/step - loss: 0.0011 - val_loss: 0.0017\n",
      "Epoch 19/24\n",
      "3690/3690 [==============================] - 2s 561us/step - loss: 0.0011 - val_loss: 7.7991e-04\n",
      "Epoch 20/24\n",
      "3690/3690 [==============================] - 2s 561us/step - loss: 0.0011 - val_loss: 9.7064e-04\n",
      "Epoch 21/24\n",
      "3690/3690 [==============================] - 2s 561us/step - loss: 0.0010 - val_loss: 0.0012\n",
      "Epoch 22/24\n",
      "3690/3690 [==============================] - 2s 562us/step - loss: 9.9446e-04 - val_loss: 0.0012\n",
      "Epoch 23/24\n",
      "3690/3690 [==============================] - 2s 562us/step - loss: 9.7633e-04 - val_loss: 7.2782e-04\n",
      "Epoch 24/24\n",
      "3690/3690 [==============================] - 2s 566us/step - loss: 0.0010 - val_loss: 0.0011\n",
      "Train on 3690 samples, validate on 410 samples\n",
      "Epoch 1/24\n",
      "3690/3690 [==============================] - 11s 3ms/step - loss: 0.1613 - val_loss: 0.0017\n",
      "Epoch 2/24\n",
      "3690/3690 [==============================] - 2s 563us/step - loss: 0.0138 - val_loss: 0.0016\n",
      "Epoch 3/24\n",
      "3690/3690 [==============================] - 2s 561us/step - loss: 0.0083 - val_loss: 8.2008e-04\n",
      "Epoch 4/24\n",
      "3690/3690 [==============================] - 2s 562us/step - loss: 0.0043 - val_loss: 0.0027\n",
      "Epoch 5/24\n",
      "3690/3690 [==============================] - 2s 566us/step - loss: 0.0027 - val_loss: 5.7875e-04\n",
      "Epoch 6/24\n",
      "3690/3690 [==============================] - 2s 566us/step - loss: 0.0024 - val_loss: 0.0038\n",
      "Epoch 7/24\n",
      "3690/3690 [==============================] - 2s 567us/step - loss: 0.0024 - val_loss: 0.0011\n",
      "Epoch 8/24\n",
      "3690/3690 [==============================] - 2s 564us/step - loss: 0.0023 - val_loss: 0.0017\n",
      "Epoch 9/24\n",
      "3690/3690 [==============================] - 2s 567us/step - loss: 0.0020 - val_loss: 0.0015\n",
      "Epoch 10/24\n",
      "3690/3690 [==============================] - 2s 564us/step - loss: 0.0020 - val_loss: 0.0012\n",
      "Epoch 11/24\n",
      "3690/3690 [==============================] - 2s 562us/step - loss: 0.0020 - val_loss: 0.0016\n",
      "Epoch 12/24\n",
      "3690/3690 [==============================] - 2s 568us/step - loss: 0.0020 - val_loss: 0.0014\n",
      "Epoch 13/24\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3690/3690 [==============================] - 2s 565us/step - loss: 0.0019 - val_loss: 0.0013\n",
      "Epoch 14/24\n",
      "3690/3690 [==============================] - 2s 563us/step - loss: 0.0018 - val_loss: 0.0011\n",
      "Epoch 15/24\n",
      "3690/3690 [==============================] - 2s 561us/step - loss: 0.0017 - val_loss: 0.0014\n",
      "Epoch 16/24\n",
      "3690/3690 [==============================] - 2s 564us/step - loss: 0.0018 - val_loss: 0.0013\n",
      "Epoch 17/24\n",
      "3690/3690 [==============================] - 2s 563us/step - loss: 0.0017 - val_loss: 0.0011\n",
      "Epoch 18/24\n",
      "3690/3690 [==============================] - 2s 564us/step - loss: 0.0018 - val_loss: 0.0013\n",
      "Epoch 19/24\n",
      "3690/3690 [==============================] - 2s 561us/step - loss: 0.0017 - val_loss: 0.0011\n",
      "Epoch 20/24\n",
      "3690/3690 [==============================] - 2s 563us/step - loss: 0.0017 - val_loss: 0.0011\n",
      "Epoch 21/24\n",
      "3690/3690 [==============================] - 2s 564us/step - loss: 0.0016 - val_loss: 0.0011\n",
      "Epoch 22/24\n",
      "3690/3690 [==============================] - 2s 562us/step - loss: 0.0017 - val_loss: 9.6829e-04\n",
      "Epoch 23/24\n",
      "3690/3690 [==============================] - 2s 560us/step - loss: 0.0015 - val_loss: 0.0012\n",
      "Epoch 24/24\n",
      "3690/3690 [==============================] - 2s 564us/step - loss: 0.0016 - val_loss: 0.0010\n",
      "\n",
      "GENERATION 8\n",
      "\n",
      "Train on 3690 samples, validate on 410 samples\n",
      "Epoch 1/24\n",
      "3690/3690 [==============================] - 11s 3ms/step - loss: 0.0578 - val_loss: 0.0155\n",
      "Epoch 2/24\n",
      "3690/3690 [==============================] - 2s 586us/step - loss: 0.0102 - val_loss: 0.0034\n",
      "Epoch 3/24\n",
      "3690/3690 [==============================] - 2s 586us/step - loss: 0.0050 - val_loss: 0.0113\n",
      "Epoch 4/24\n",
      "3690/3690 [==============================] - 2s 588us/step - loss: 0.0032 - val_loss: 0.0045\n",
      "Epoch 5/24\n",
      "3690/3690 [==============================] - 2s 586us/step - loss: 0.0023 - val_loss: 0.0012\n",
      "Epoch 6/24\n",
      "3690/3690 [==============================] - 2s 585us/step - loss: 0.0016 - val_loss: 0.0015\n",
      "Epoch 7/24\n",
      "3690/3690 [==============================] - 2s 591us/step - loss: 0.0013 - val_loss: 0.0012\n",
      "Epoch 8/24\n",
      "3690/3690 [==============================] - 2s 593us/step - loss: 0.0012 - val_loss: 5.8589e-04\n",
      "Epoch 9/24\n",
      "3690/3690 [==============================] - 2s 589us/step - loss: 0.0012 - val_loss: 7.7240e-04\n",
      "Epoch 10/24\n",
      "3690/3690 [==============================] - 2s 587us/step - loss: 0.0011 - val_loss: 7.2761e-04\n",
      "Epoch 11/24\n",
      "3690/3690 [==============================] - 2s 589us/step - loss: 0.0011 - val_loss: 9.9578e-04\n",
      "Epoch 12/24\n",
      "3690/3690 [==============================] - 2s 589us/step - loss: 0.0010 - val_loss: 0.0012\n",
      "Epoch 13/24\n",
      "3690/3690 [==============================] - 2s 585us/step - loss: 0.0010 - val_loss: 0.0013\n",
      "Epoch 14/24\n",
      "3690/3690 [==============================] - 2s 589us/step - loss: 0.0010 - val_loss: 0.0015\n",
      "Epoch 15/24\n",
      "3690/3690 [==============================] - 2s 591us/step - loss: 9.7428e-04 - val_loss: 0.0014\n",
      "Epoch 16/24\n",
      "3690/3690 [==============================] - 2s 588us/step - loss: 0.0010 - val_loss: 7.0971e-04\n",
      "Epoch 17/24\n",
      "3690/3690 [==============================] - 2s 589us/step - loss: 9.6587e-04 - val_loss: 8.3569e-04\n",
      "Epoch 18/24\n",
      "3690/3690 [==============================] - 2s 592us/step - loss: 8.6503e-04 - val_loss: 9.4091e-04\n",
      "Epoch 19/24\n",
      "3690/3690 [==============================] - 2s 587us/step - loss: 8.4744e-04 - val_loss: 0.0013\n",
      "Epoch 20/24\n",
      "3690/3690 [==============================] - 2s 588us/step - loss: 8.8720e-04 - val_loss: 0.0011\n",
      "Epoch 21/24\n",
      "3690/3690 [==============================] - 2s 588us/step - loss: 8.2088e-04 - val_loss: 0.0013\n",
      "Epoch 22/24\n",
      "3690/3690 [==============================] - 2s 591us/step - loss: 8.2750e-04 - val_loss: 0.0012\n",
      "Epoch 23/24\n",
      "3690/3690 [==============================] - 2s 591us/step - loss: 8.0798e-04 - val_loss: 0.0013\n",
      "Epoch 24/24\n",
      "3690/3690 [==============================] - 2s 589us/step - loss: 8.1134e-04 - val_loss: 0.0011\n",
      "Train on 3690 samples, validate on 410 samples\n",
      "Epoch 1/24\n",
      "3690/3690 [==============================] - 11s 3ms/step - loss: 0.1039 - val_loss: 0.0110\n",
      "Epoch 2/24\n",
      "3690/3690 [==============================] - 2s 589us/step - loss: 0.0164 - val_loss: 0.0017\n",
      "Epoch 3/24\n",
      "3690/3690 [==============================] - 2s 590us/step - loss: 0.0081 - val_loss: 0.0107\n",
      "Epoch 4/24\n",
      "3690/3690 [==============================] - 2s 587us/step - loss: 0.0057 - val_loss: 0.0021\n",
      "Epoch 5/24\n",
      "3690/3690 [==============================] - 2s 590us/step - loss: 0.0040 - val_loss: 0.0036\n",
      "Epoch 6/24\n",
      "3690/3690 [==============================] - 2s 588us/step - loss: 0.0028 - val_loss: 0.0043\n",
      "Epoch 7/24\n",
      "3690/3690 [==============================] - 2s 597us/step - loss: 0.0020 - val_loss: 0.0013\n",
      "Epoch 8/24\n",
      "3690/3690 [==============================] - 2s 588us/step - loss: 0.0017 - val_loss: 0.0020\n",
      "Epoch 9/24\n",
      "3690/3690 [==============================] - 2s 587us/step - loss: 0.0015 - val_loss: 0.0018\n",
      "Epoch 10/24\n",
      "3690/3690 [==============================] - 2s 589us/step - loss: 0.0014 - val_loss: 0.0016\n",
      "Epoch 11/24\n",
      "3690/3690 [==============================] - 2s 588us/step - loss: 0.0012 - val_loss: 0.0010\n",
      "Epoch 12/24\n",
      "3690/3690 [==============================] - 2s 586us/step - loss: 0.0013 - val_loss: 0.0023\n",
      "Epoch 13/24\n",
      "3690/3690 [==============================] - 2s 588us/step - loss: 0.0011 - val_loss: 0.0017\n",
      "Epoch 14/24\n",
      "3690/3690 [==============================] - 2s 588us/step - loss: 0.0012 - val_loss: 0.0011\n",
      "Epoch 15/24\n",
      "3690/3690 [==============================] - 2s 589us/step - loss: 0.0011 - val_loss: 0.0020\n",
      "Epoch 16/24\n",
      "3690/3690 [==============================] - 2s 586us/step - loss: 0.0011 - val_loss: 0.0020\n",
      "Epoch 17/24\n",
      "3690/3690 [==============================] - 2s 591us/step - loss: 0.0011 - val_loss: 0.0014\n",
      "Epoch 18/24\n",
      "3690/3690 [==============================] - 2s 587us/step - loss: 0.0011 - val_loss: 0.0014\n",
      "Epoch 19/24\n",
      "3690/3690 [==============================] - 2s 586us/step - loss: 0.0010 - val_loss: 0.0016\n",
      "Epoch 20/24\n",
      "3690/3690 [==============================] - 2s 587us/step - loss: 0.0010 - val_loss: 0.0016\n",
      "Epoch 21/24\n",
      "3690/3690 [==============================] - 2s 589us/step - loss: 9.8109e-04 - val_loss: 0.0015\n",
      "Epoch 22/24\n",
      "3690/3690 [==============================] - 2s 587us/step - loss: 0.0010 - val_loss: 0.0014\n",
      "Epoch 23/24\n",
      "3690/3690 [==============================] - 2s 588us/step - loss: 9.7416e-04 - val_loss: 0.0018\n",
      "Epoch 24/24\n",
      "3690/3690 [==============================] - 2s 590us/step - loss: 9.6815e-04 - val_loss: 0.0014\n",
      "Train on 3690 samples, validate on 410 samples\n",
      "Epoch 1/24\n",
      "3690/3690 [==============================] - 11s 3ms/step - loss: 0.1476 - val_loss: 0.1125\n",
      "Epoch 2/24\n",
      "3690/3690 [==============================] - 1s 341us/step - loss: 0.0740 - val_loss: 0.0093\n",
      "Epoch 3/24\n",
      "3690/3690 [==============================] - 1s 340us/step - loss: 0.0496 - val_loss: 0.0111\n",
      "Epoch 4/24\n",
      "3690/3690 [==============================] - 1s 340us/step - loss: 0.0153 - val_loss: 0.0546\n",
      "Epoch 5/24\n",
      "3690/3690 [==============================] - 1s 344us/step - loss: 0.0087 - val_loss: 0.0288\n",
      "Epoch 6/24\n",
      "3690/3690 [==============================] - 1s 342us/step - loss: 0.0044 - val_loss: 0.0046\n",
      "Epoch 7/24\n",
      "3690/3690 [==============================] - 1s 349us/step - loss: 0.0034 - val_loss: 0.0031\n",
      "Epoch 8/24\n",
      "3690/3690 [==============================] - 1s 341us/step - loss: 0.0029 - val_loss: 0.0023\n",
      "Epoch 9/24\n",
      "3690/3690 [==============================] - 1s 342us/step - loss: 0.0024 - val_loss: 0.0014\n",
      "Epoch 10/24\n",
      "3690/3690 [==============================] - 1s 341us/step - loss: 0.0023 - val_loss: 0.0018\n",
      "Epoch 11/24\n",
      "3690/3690 [==============================] - 1s 342us/step - loss: 0.0022 - val_loss: 0.0015\n",
      "Epoch 12/24\n",
      "3690/3690 [==============================] - 1s 340us/step - loss: 0.0020 - val_loss: 0.0017\n",
      "Epoch 13/24\n",
      "3690/3690 [==============================] - 1s 341us/step - loss: 0.0020 - val_loss: 0.0014\n",
      "Epoch 14/24\n",
      "3690/3690 [==============================] - 1s 346us/step - loss: 0.0020 - val_loss: 0.0016\n",
      "Epoch 15/24\n",
      "3690/3690 [==============================] - 1s 347us/step - loss: 0.0019 - val_loss: 0.0010\n",
      "Epoch 16/24\n",
      "3690/3690 [==============================] - 1s 342us/step - loss: 0.0019 - val_loss: 9.2220e-04\n",
      "Epoch 17/24\n",
      "3690/3690 [==============================] - 1s 340us/step - loss: 0.0019 - val_loss: 0.0011\n",
      "Epoch 18/24\n",
      "3690/3690 [==============================] - 1s 343us/step - loss: 0.0017 - val_loss: 0.0012\n",
      "Epoch 19/24\n",
      "3690/3690 [==============================] - 1s 339us/step - loss: 0.0018 - val_loss: 9.7079e-04\n",
      "Epoch 20/24\n",
      "3690/3690 [==============================] - 1s 345us/step - loss: 0.0017 - val_loss: 9.2646e-04\n",
      "Epoch 21/24\n",
      "3690/3690 [==============================] - 1s 341us/step - loss: 0.0017 - val_loss: 0.0015\n",
      "Epoch 22/24\n",
      "3690/3690 [==============================] - 1s 342us/step - loss: 0.0017 - val_loss: 0.0011\n",
      "Epoch 23/24\n",
      "3690/3690 [==============================] - 1s 341us/step - loss: 0.0016 - val_loss: 9.4204e-04\n",
      "Epoch 24/24\n",
      "3690/3690 [==============================] - 1s 340us/step - loss: 0.0016 - val_loss: 9.4408e-04\n",
      "Train on 3690 samples, validate on 410 samples\n",
      "Epoch 1/24\n",
      "3690/3690 [==============================] - 11s 3ms/step - loss: 0.6744 - val_loss: 0.5214\n",
      "Epoch 2/24\n",
      "3690/3690 [==============================] - 1s 345us/step - loss: 0.2581 - val_loss: 0.2294\n",
      "Epoch 3/24\n",
      "3690/3690 [==============================] - 1s 339us/step - loss: 0.1233 - val_loss: 0.1179\n",
      "Epoch 4/24\n",
      "3690/3690 [==============================] - 1s 338us/step - loss: 0.0833 - val_loss: 0.0746\n",
      "Epoch 5/24\n",
      "3690/3690 [==============================] - 1s 339us/step - loss: 0.0723 - val_loss: 0.0550\n",
      "Epoch 6/24\n",
      "3690/3690 [==============================] - 1s 337us/step - loss: 0.0692 - val_loss: 0.0463\n",
      "Epoch 7/24\n",
      "3690/3690 [==============================] - 1s 339us/step - loss: 0.0684 - val_loss: 0.0408\n",
      "Epoch 8/24\n",
      "3690/3690 [==============================] - 1s 338us/step - loss: 0.0682 - val_loss: 0.0387\n",
      "Epoch 9/24\n",
      "3690/3690 [==============================] - 1s 340us/step - loss: 0.0681 - val_loss: 0.0395\n",
      "Epoch 10/24\n",
      "3690/3690 [==============================] - 1s 342us/step - loss: 0.0681 - val_loss: 0.0396\n",
      "Epoch 11/24\n",
      "3690/3690 [==============================] - 1s 343us/step - loss: 0.0681 - val_loss: 0.0396\n",
      "Epoch 12/24\n",
      "3690/3690 [==============================] - 1s 340us/step - loss: 0.0681 - val_loss: 0.0382\n",
      "Epoch 13/24\n",
      "3690/3690 [==============================] - 1s 340us/step - loss: 0.0681 - val_loss: 0.0392\n",
      "Epoch 14/24\n",
      "3690/3690 [==============================] - 1s 344us/step - loss: 0.0682 - val_loss: 0.0377\n",
      "Epoch 15/24\n",
      "3690/3690 [==============================] - 1s 342us/step - loss: 0.0681 - val_loss: 0.0402\n",
      "Epoch 16/24\n",
      "3690/3690 [==============================] - 1s 337us/step - loss: 0.0681 - val_loss: 0.0388\n",
      "Epoch 17/24\n",
      "3690/3690 [==============================] - 1s 341us/step - loss: 0.0681 - val_loss: 0.0386\n",
      "Epoch 18/24\n",
      "3690/3690 [==============================] - 1s 339us/step - loss: 0.0682 - val_loss: 0.0394\n",
      "Epoch 19/24\n",
      "3690/3690 [==============================] - 1s 342us/step - loss: 0.0681 - val_loss: 0.0329\n",
      "Epoch 20/24\n",
      "3690/3690 [==============================] - 1s 336us/step - loss: 0.0681 - val_loss: 0.0448\n",
      "Epoch 21/24\n",
      "3690/3690 [==============================] - 1s 340us/step - loss: 0.0682 - val_loss: 0.0420\n",
      "Epoch 22/24\n",
      "3690/3690 [==============================] - 1s 341us/step - loss: 0.0682 - val_loss: 0.0511\n",
      "Epoch 23/24\n",
      "3690/3690 [==============================] - 1s 341us/step - loss: 0.0684 - val_loss: 0.0331\n",
      "Epoch 24/24\n",
      "3690/3690 [==============================] - 1s 338us/step - loss: 0.0682 - val_loss: 0.0377\n",
      "Train on 3690 samples, validate on 410 samples\n",
      "Epoch 1/24\n",
      "3690/3690 [==============================] - 12s 3ms/step - loss: 0.5845 - val_loss: 0.0055\n",
      "Epoch 2/24\n",
      "3690/3690 [==============================] - 2s 590us/step - loss: 0.0542 - val_loss: 0.0509\n",
      "Epoch 3/24\n",
      "3690/3690 [==============================] - 2s 588us/step - loss: 0.0667 - val_loss: 0.0265\n",
      "Epoch 4/24\n",
      "3690/3690 [==============================] - 2s 588us/step - loss: 0.0613 - val_loss: 0.0136\n",
      "Epoch 5/24\n",
      "3690/3690 [==============================] - 2s 592us/step - loss: 0.0450 - val_loss: 0.0401\n",
      "Epoch 6/24\n",
      "3690/3690 [==============================] - 2s 586us/step - loss: 0.0472 - val_loss: 0.0286\n",
      "Epoch 7/24\n",
      "3690/3690 [==============================] - 2s 594us/step - loss: 0.0384 - val_loss: 0.0357\n",
      "Epoch 8/24\n",
      "3690/3690 [==============================] - 2s 589us/step - loss: 0.0370 - val_loss: 0.0390\n",
      "Epoch 9/24\n",
      "3690/3690 [==============================] - 2s 591us/step - loss: 0.0330 - val_loss: 0.0560\n",
      "Epoch 10/24\n",
      "3690/3690 [==============================] - 2s 588us/step - loss: 0.0400 - val_loss: 0.0598\n",
      "Epoch 11/24\n",
      "3690/3690 [==============================] - 2s 590us/step - loss: 0.0281 - val_loss: 0.0478\n",
      "Epoch 12/24\n",
      "3690/3690 [==============================] - 2s 589us/step - loss: 0.0206 - val_loss: 0.0490\n",
      "Epoch 13/24\n",
      "3690/3690 [==============================] - 2s 593us/step - loss: 0.0232 - val_loss: 0.0394\n",
      "Epoch 14/24\n",
      "3690/3690 [==============================] - 2s 591us/step - loss: 0.0173 - val_loss: 0.0471\n",
      "Epoch 15/24\n",
      "3690/3690 [==============================] - 2s 588us/step - loss: 0.0266 - val_loss: 0.0591\n",
      "Epoch 16/24\n",
      "3690/3690 [==============================] - 2s 589us/step - loss: 0.0193 - val_loss: 0.0324\n",
      "Epoch 17/24\n",
      "3690/3690 [==============================] - 2s 587us/step - loss: 0.0142 - val_loss: 0.0389\n",
      "Epoch 18/24\n",
      "3690/3690 [==============================] - 2s 590us/step - loss: 0.0171 - val_loss: 0.0492\n",
      "Epoch 19/24\n",
      "3690/3690 [==============================] - 2s 587us/step - loss: 0.0184 - val_loss: 0.0333\n",
      "Epoch 20/24\n",
      "3690/3690 [==============================] - 2s 591us/step - loss: 0.0135 - val_loss: 0.0270\n",
      "Epoch 21/24\n",
      "3690/3690 [==============================] - 2s 588us/step - loss: 0.0167 - val_loss: 0.0533\n",
      "Epoch 22/24\n",
      "3690/3690 [==============================] - 2s 588us/step - loss: 0.0193 - val_loss: 0.0300\n",
      "Epoch 23/24\n",
      "3690/3690 [==============================] - 2s 587us/step - loss: 0.0116 - val_loss: 0.0393\n",
      "Epoch 24/24\n",
      "3690/3690 [==============================] - 2s 586us/step - loss: 0.0159 - val_loss: 0.0422\n",
      "Train on 3690 samples, validate on 410 samples\n",
      "Epoch 1/24\n",
      "3690/3690 [==============================] - 12s 3ms/step - loss: 0.9232 - val_loss: 0.1299\n",
      "Epoch 2/24\n",
      "3690/3690 [==============================] - 2s 489us/step - loss: 0.0667 - val_loss: 0.0033\n",
      "Epoch 3/24\n",
      "3690/3690 [==============================] - 2s 493us/step - loss: 0.0473 - val_loss: 0.1267\n",
      "Epoch 4/24\n",
      "3690/3690 [==============================] - 2s 492us/step - loss: 0.1017 - val_loss: 0.0105\n",
      "Epoch 5/24\n",
      "3690/3690 [==============================] - 2s 495us/step - loss: 0.0872 - val_loss: 0.0142\n",
      "Epoch 6/24\n",
      "3690/3690 [==============================] - 2s 491us/step - loss: 0.0167 - val_loss: 0.0425\n",
      "Epoch 7/24\n",
      "3690/3690 [==============================] - 2s 491us/step - loss: 0.0075 - val_loss: 0.0875\n",
      "Epoch 8/24\n",
      "3690/3690 [==============================] - 2s 496us/step - loss: 0.0105 - val_loss: 0.0505\n",
      "Epoch 9/24\n",
      "3690/3690 [==============================] - 2s 496us/step - loss: 0.0068 - val_loss: 0.0261\n",
      "Epoch 10/24\n",
      "3690/3690 [==============================] - 2s 493us/step - loss: 0.0049 - val_loss: 0.0141\n",
      "Epoch 11/24\n",
      "3690/3690 [==============================] - 2s 490us/step - loss: 0.0039 - val_loss: 0.0129\n",
      "Epoch 12/24\n",
      "3690/3690 [==============================] - 2s 493us/step - loss: 0.0046 - val_loss: 0.0143\n",
      "Epoch 13/24\n",
      "3690/3690 [==============================] - 2s 493us/step - loss: 0.0042 - val_loss: 0.0118\n",
      "Epoch 14/24\n",
      "3690/3690 [==============================] - 2s 495us/step - loss: 0.0030 - val_loss: 0.0103\n",
      "Epoch 15/24\n",
      "3690/3690 [==============================] - 2s 491us/step - loss: 0.0032 - val_loss: 0.0087\n",
      "Epoch 16/24\n",
      "3690/3690 [==============================] - 2s 495us/step - loss: 0.0027 - val_loss: 0.0050\n",
      "Epoch 17/24\n",
      "3690/3690 [==============================] - 2s 491us/step - loss: 0.0024 - val_loss: 0.0023\n",
      "Epoch 18/24\n",
      "3690/3690 [==============================] - 2s 489us/step - loss: 0.0040 - val_loss: 0.0074\n",
      "Epoch 19/24\n",
      "3690/3690 [==============================] - 2s 493us/step - loss: 0.0034 - val_loss: 0.0049\n",
      "Epoch 20/24\n",
      "3690/3690 [==============================] - 2s 492us/step - loss: 0.0021 - val_loss: 0.0033\n",
      "Epoch 21/24\n",
      "3690/3690 [==============================] - 2s 491us/step - loss: 0.0020 - val_loss: 0.0015\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 22/24\n",
      "3690/3690 [==============================] - 2s 490us/step - loss: 0.0018 - val_loss: 9.0329e-04\n",
      "Epoch 23/24\n",
      "3690/3690 [==============================] - 2s 492us/step - loss: 0.0018 - val_loss: 0.0037\n",
      "Epoch 24/24\n",
      "3690/3690 [==============================] - 2s 489us/step - loss: 0.0023 - val_loss: 0.0079\n",
      "Train on 3690 samples, validate on 410 samples\n",
      "Epoch 1/24\n",
      "3690/3690 [==============================] - 12s 3ms/step - loss: 1.0436 - val_loss: 0.0675\n",
      "Epoch 2/24\n",
      "3690/3690 [==============================] - 2s 491us/step - loss: 0.0559 - val_loss: 0.0287\n",
      "Epoch 3/24\n",
      "3690/3690 [==============================] - 2s 490us/step - loss: 0.0316 - val_loss: 0.0245\n",
      "Epoch 4/24\n",
      "3690/3690 [==============================] - 2s 490us/step - loss: 0.0234 - val_loss: 0.0142\n",
      "Epoch 5/24\n",
      "3690/3690 [==============================] - 2s 494us/step - loss: 0.0151 - val_loss: 0.0055\n",
      "Epoch 6/24\n",
      "3690/3690 [==============================] - 2s 492us/step - loss: 0.0125 - val_loss: 0.0028\n",
      "Epoch 7/24\n",
      "3690/3690 [==============================] - 2s 494us/step - loss: 0.0110 - val_loss: 0.0065\n",
      "Epoch 8/24\n",
      "3690/3690 [==============================] - 2s 492us/step - loss: 0.0091 - val_loss: 0.0080\n",
      "Epoch 9/24\n",
      "3690/3690 [==============================] - 2s 492us/step - loss: 0.0147 - val_loss: 0.0166\n",
      "Epoch 10/24\n",
      "3690/3690 [==============================] - 2s 491us/step - loss: 0.0131 - val_loss: 0.0047\n",
      "Epoch 11/24\n",
      "3690/3690 [==============================] - 2s 491us/step - loss: 0.0090 - val_loss: 0.0041\n",
      "Epoch 12/24\n",
      "3690/3690 [==============================] - 2s 493us/step - loss: 0.0081 - val_loss: 0.0069\n",
      "Epoch 13/24\n",
      "3690/3690 [==============================] - 2s 493us/step - loss: 0.0103 - val_loss: 0.0149\n",
      "Epoch 14/24\n",
      "3690/3690 [==============================] - 2s 494us/step - loss: 0.0143 - val_loss: 0.0096\n",
      "Epoch 15/24\n",
      "3690/3690 [==============================] - 2s 496us/step - loss: 0.0134 - val_loss: 0.0178\n",
      "Epoch 16/24\n",
      "3690/3690 [==============================] - 2s 493us/step - loss: 0.0158 - val_loss: 0.0207\n",
      "Epoch 17/24\n",
      "3690/3690 [==============================] - 2s 491us/step - loss: 0.0161 - val_loss: 0.0155\n",
      "Epoch 18/24\n",
      "3690/3690 [==============================] - 2s 491us/step - loss: 0.0140 - val_loss: 0.0164\n",
      "Epoch 19/24\n",
      "3690/3690 [==============================] - 2s 491us/step - loss: 0.0142 - val_loss: 0.0223\n",
      "Epoch 20/24\n",
      "3690/3690 [==============================] - 2s 493us/step - loss: 0.0123 - val_loss: 0.0209\n",
      "Epoch 21/24\n",
      "3690/3690 [==============================] - 2s 492us/step - loss: 0.0181 - val_loss: 0.0107\n",
      "Epoch 22/24\n",
      "3690/3690 [==============================] - 2s 493us/step - loss: 0.0104 - val_loss: 0.0130\n",
      "Epoch 23/24\n",
      "3690/3690 [==============================] - 2s 494us/step - loss: 0.0155 - val_loss: 0.0180\n",
      "Epoch 24/24\n",
      "3690/3690 [==============================] - 2s 491us/step - loss: 0.0148 - val_loss: 0.0127\n",
      "Train on 3690 samples, validate on 410 samples\n",
      "Epoch 1/24\n",
      "3690/3690 [==============================] - 12s 3ms/step - loss: 0.0689 - val_loss: 0.0036\n",
      "Epoch 2/24\n",
      "3690/3690 [==============================] - 2s 408us/step - loss: 0.0121 - val_loss: 0.0100\n",
      "Epoch 3/24\n",
      "3690/3690 [==============================] - 1s 401us/step - loss: 0.0097 - val_loss: 0.0101\n",
      "Epoch 4/24\n",
      "3690/3690 [==============================] - 1s 400us/step - loss: 0.0089 - val_loss: 0.0030\n",
      "Epoch 5/24\n",
      "3690/3690 [==============================] - 1s 400us/step - loss: 0.0039 - val_loss: 0.0019\n",
      "Epoch 6/24\n",
      "3690/3690 [==============================] - 1s 402us/step - loss: 0.0050 - val_loss: 0.0067\n",
      "Epoch 7/24\n",
      "3690/3690 [==============================] - 1s 402us/step - loss: 0.0078 - val_loss: 0.0103\n",
      "Epoch 8/24\n",
      "3690/3690 [==============================] - 1s 400us/step - loss: 0.0063 - val_loss: 0.0023\n",
      "Epoch 9/24\n",
      "3690/3690 [==============================] - 1s 402us/step - loss: 0.0040 - val_loss: 0.0063\n",
      "Epoch 10/24\n",
      "3690/3690 [==============================] - 1s 401us/step - loss: 0.0069 - val_loss: 0.0117\n",
      "Epoch 11/24\n",
      "3690/3690 [==============================] - 1s 400us/step - loss: 0.0060 - val_loss: 0.0021\n",
      "Epoch 12/24\n",
      "3690/3690 [==============================] - 1s 399us/step - loss: 0.0058 - val_loss: 0.0046\n",
      "Epoch 13/24\n",
      "3690/3690 [==============================] - 2s 407us/step - loss: 0.0045 - val_loss: 0.0075\n",
      "Epoch 14/24\n",
      "3690/3690 [==============================] - 1s 404us/step - loss: 0.0048 - val_loss: 0.0117\n",
      "Epoch 15/24\n",
      "3690/3690 [==============================] - 1s 401us/step - loss: 0.0052 - val_loss: 0.0065\n",
      "Epoch 16/24\n",
      "3690/3690 [==============================] - 1s 402us/step - loss: 0.0047 - val_loss: 0.0053\n",
      "Epoch 17/24\n",
      "3690/3690 [==============================] - 1s 401us/step - loss: 0.0045 - val_loss: 0.0067\n",
      "Epoch 18/24\n",
      "3690/3690 [==============================] - 1s 402us/step - loss: 0.0040 - val_loss: 0.0053\n",
      "Epoch 19/24\n",
      "3690/3690 [==============================] - 1s 401us/step - loss: 0.0035 - val_loss: 0.0027\n",
      "Epoch 20/24\n",
      "3690/3690 [==============================] - 1s 406us/step - loss: 0.0032 - val_loss: 0.0073\n",
      "Epoch 21/24\n",
      "3690/3690 [==============================] - 1s 401us/step - loss: 0.0044 - val_loss: 0.0035\n",
      "Epoch 22/24\n",
      "3690/3690 [==============================] - 1s 402us/step - loss: 0.0027 - val_loss: 0.0061\n",
      "Epoch 23/24\n",
      "3690/3690 [==============================] - 1s 399us/step - loss: 0.0057 - val_loss: 0.0078\n",
      "Epoch 24/24\n",
      "3690/3690 [==============================] - 1s 405us/step - loss: 0.0033 - val_loss: 0.0105\n",
      "Train on 3690 samples, validate on 410 samples\n",
      "Epoch 1/24\n",
      "3690/3690 [==============================] - 12s 3ms/step - loss: 0.0462 - val_loss: 0.0044\n",
      "Epoch 2/24\n",
      "3690/3690 [==============================] - 1s 402us/step - loss: 0.0067 - val_loss: 0.0218\n",
      "Epoch 3/24\n",
      "3690/3690 [==============================] - 1s 399us/step - loss: 0.0095 - val_loss: 0.0083\n",
      "Epoch 4/24\n",
      "3690/3690 [==============================] - 1s 400us/step - loss: 0.0047 - val_loss: 0.0050\n",
      "Epoch 5/24\n",
      "3690/3690 [==============================] - 1s 401us/step - loss: 0.0045 - val_loss: 0.0105\n",
      "Epoch 6/24\n",
      "3690/3690 [==============================] - 1s 401us/step - loss: 0.0064 - val_loss: 0.0125\n",
      "Epoch 7/24\n",
      "3690/3690 [==============================] - 1s 402us/step - loss: 0.0054 - val_loss: 0.0118\n",
      "Epoch 8/24\n",
      "3690/3690 [==============================] - 1s 398us/step - loss: 0.0059 - val_loss: 0.0095\n",
      "Epoch 9/24\n",
      "3690/3690 [==============================] - 1s 399us/step - loss: 0.0049 - val_loss: 0.0074\n",
      "Epoch 10/24\n",
      "3690/3690 [==============================] - 1s 398us/step - loss: 0.0057 - val_loss: 0.0089\n",
      "Epoch 11/24\n",
      "3690/3690 [==============================] - 1s 402us/step - loss: 0.0040 - val_loss: 0.0106\n",
      "Epoch 12/24\n",
      "3690/3690 [==============================] - 1s 399us/step - loss: 0.0050 - val_loss: 0.0075\n",
      "Epoch 13/24\n",
      "3690/3690 [==============================] - 1s 395us/step - loss: 0.0034 - val_loss: 0.0056\n",
      "Epoch 14/24\n",
      "3690/3690 [==============================] - 1s 401us/step - loss: 0.0035 - val_loss: 0.0089\n",
      "Epoch 15/24\n",
      "3690/3690 [==============================] - 1s 399us/step - loss: 0.0055 - val_loss: 0.0133\n",
      "Epoch 16/24\n",
      "3690/3690 [==============================] - 1s 398us/step - loss: 0.0047 - val_loss: 0.0117\n",
      "Epoch 17/24\n",
      "3690/3690 [==============================] - 1s 399us/step - loss: 0.0042 - val_loss: 0.0096\n",
      "Epoch 18/24\n",
      "3690/3690 [==============================] - 1s 399us/step - loss: 0.0045 - val_loss: 0.0077\n",
      "Epoch 19/24\n",
      "3690/3690 [==============================] - 1s 400us/step - loss: 0.0030 - val_loss: 0.0088\n",
      "Epoch 20/24\n",
      "3690/3690 [==============================] - 1s 398us/step - loss: 0.0033 - val_loss: 0.0084\n",
      "Epoch 21/24\n",
      "3690/3690 [==============================] - 1s 401us/step - loss: 0.0033 - val_loss: 0.0109\n",
      "Epoch 22/24\n",
      "3690/3690 [==============================] - 1s 403us/step - loss: 0.0043 - val_loss: 0.0091\n",
      "Epoch 23/24\n",
      "3690/3690 [==============================] - 1s 399us/step - loss: 0.0042 - val_loss: 0.0088\n",
      "Epoch 24/24\n",
      "3690/3690 [==============================] - 1s 400us/step - loss: 0.0039 - val_loss: 0.0072\n",
      "\n",
      "GENERATION 9\n",
      "\n",
      "Train on 3690 samples, validate on 410 samples\n",
      "Epoch 1/24\n",
      "3690/3690 [==============================] - 13s 4ms/step - loss: 0.0548 - val_loss: 0.0170\n",
      "Epoch 2/24\n",
      "3690/3690 [==============================] - 2s 588us/step - loss: 0.0107 - val_loss: 0.0071\n",
      "Epoch 3/24\n",
      "3690/3690 [==============================] - 2s 589us/step - loss: 0.0049 - val_loss: 0.0073\n",
      "Epoch 4/24\n",
      "3690/3690 [==============================] - 2s 586us/step - loss: 0.0038 - val_loss: 7.5542e-04\n",
      "Epoch 5/24\n",
      "3690/3690 [==============================] - 2s 591us/step - loss: 0.0026 - val_loss: 0.0041\n",
      "Epoch 6/24\n",
      "3690/3690 [==============================] - 2s 588us/step - loss: 0.0019 - val_loss: 0.0025\n",
      "Epoch 7/24\n",
      "3690/3690 [==============================] - 2s 585us/step - loss: 0.0015 - val_loss: 9.4827e-04\n",
      "Epoch 8/24\n",
      "3690/3690 [==============================] - 2s 586us/step - loss: 0.0012 - val_loss: 6.4377e-04\n",
      "Epoch 9/24\n",
      "3690/3690 [==============================] - 2s 587us/step - loss: 0.0011 - val_loss: 5.2220e-04\n",
      "Epoch 10/24\n",
      "3690/3690 [==============================] - 2s 585us/step - loss: 0.0011 - val_loss: 6.8270e-04\n",
      "Epoch 11/24\n",
      "3690/3690 [==============================] - 2s 590us/step - loss: 0.0010 - val_loss: 6.7246e-04\n",
      "Epoch 12/24\n",
      "3690/3690 [==============================] - 2s 585us/step - loss: 0.0010 - val_loss: 0.0012\n",
      "Epoch 13/24\n",
      "3690/3690 [==============================] - 2s 584us/step - loss: 0.0010 - val_loss: 8.7652e-04\n",
      "Epoch 14/24\n",
      "3690/3690 [==============================] - 2s 587us/step - loss: 9.3981e-04 - val_loss: 0.0011\n",
      "Epoch 15/24\n",
      "3690/3690 [==============================] - 2s 587us/step - loss: 9.5522e-04 - val_loss: 9.9479e-04\n",
      "Epoch 16/24\n",
      "3690/3690 [==============================] - 2s 587us/step - loss: 8.9544e-04 - val_loss: 6.8422e-04\n",
      "Epoch 17/24\n",
      "3690/3690 [==============================] - 2s 586us/step - loss: 9.3317e-04 - val_loss: 6.8475e-04\n",
      "Epoch 18/24\n",
      "3690/3690 [==============================] - 2s 587us/step - loss: 9.1254e-04 - val_loss: 6.3453e-04\n",
      "Epoch 19/24\n",
      "3690/3690 [==============================] - 2s 589us/step - loss: 8.9115e-04 - val_loss: 8.5178e-04\n",
      "Epoch 20/24\n",
      "3690/3690 [==============================] - 2s 587us/step - loss: 8.5828e-04 - val_loss: 8.5277e-04\n",
      "Epoch 21/24\n",
      "3690/3690 [==============================] - 2s 586us/step - loss: 8.4938e-04 - val_loss: 0.0010\n",
      "Epoch 22/24\n",
      "3690/3690 [==============================] - 2s 586us/step - loss: 8.7200e-04 - val_loss: 8.5078e-04\n",
      "Epoch 23/24\n",
      "3690/3690 [==============================] - 2s 586us/step - loss: 8.3206e-04 - val_loss: 6.6766e-04\n",
      "Epoch 24/24\n",
      "3690/3690 [==============================] - 2s 588us/step - loss: 8.3237e-04 - val_loss: 6.7304e-04\n",
      "Train on 3690 samples, validate on 410 samples\n",
      "Epoch 1/24\n",
      "3690/3690 [==============================] - 13s 4ms/step - loss: 0.0588 - val_loss: 0.0199\n",
      "Epoch 2/24\n",
      "3690/3690 [==============================] - 2s 592us/step - loss: 0.0145 - val_loss: 0.0098\n",
      "Epoch 3/24\n",
      "3690/3690 [==============================] - 2s 591us/step - loss: 0.0063 - val_loss: 0.0021\n",
      "Epoch 4/24\n",
      "3690/3690 [==============================] - 2s 589us/step - loss: 0.0040 - val_loss: 0.0019\n",
      "Epoch 5/24\n",
      "3690/3690 [==============================] - 2s 593us/step - loss: 0.0027 - val_loss: 0.0055\n",
      "Epoch 6/24\n",
      "3690/3690 [==============================] - 2s 588us/step - loss: 0.0019 - val_loss: 0.0017\n",
      "Epoch 7/24\n",
      "3690/3690 [==============================] - 2s 588us/step - loss: 0.0014 - val_loss: 7.7194e-04\n",
      "Epoch 8/24\n",
      "3690/3690 [==============================] - 2s 591us/step - loss: 0.0012 - val_loss: 5.7744e-04\n",
      "Epoch 9/24\n",
      "3690/3690 [==============================] - 2s 588us/step - loss: 0.0011 - val_loss: 6.0016e-04\n",
      "Epoch 10/24\n",
      "3690/3690 [==============================] - 2s 587us/step - loss: 0.0011 - val_loss: 7.0645e-04\n",
      "Epoch 11/24\n",
      "3690/3690 [==============================] - 2s 593us/step - loss: 0.0010 - val_loss: 7.8811e-04\n",
      "Epoch 12/24\n",
      "3690/3690 [==============================] - 2s 588us/step - loss: 9.7723e-04 - val_loss: 7.3203e-04\n",
      "Epoch 13/24\n",
      "3690/3690 [==============================] - 2s 588us/step - loss: 9.6706e-04 - val_loss: 9.7325e-04\n",
      "Epoch 14/24\n",
      "3690/3690 [==============================] - 2s 589us/step - loss: 9.3104e-04 - val_loss: 8.4787e-04\n",
      "Epoch 15/24\n",
      "3690/3690 [==============================] - 2s 588us/step - loss: 9.4596e-04 - val_loss: 9.0092e-04\n",
      "Epoch 16/24\n",
      "3690/3690 [==============================] - 2s 592us/step - loss: 8.8893e-04 - val_loss: 8.9451e-04\n",
      "Epoch 17/24\n",
      "3690/3690 [==============================] - 2s 588us/step - loss: 8.6543e-04 - val_loss: 7.7164e-04\n",
      "Epoch 18/24\n",
      "3690/3690 [==============================] - 2s 591us/step - loss: 8.3942e-04 - val_loss: 9.5159e-04\n",
      "Epoch 19/24\n",
      "3690/3690 [==============================] - 2s 588us/step - loss: 8.1081e-04 - val_loss: 0.0011\n",
      "Epoch 20/24\n",
      "3690/3690 [==============================] - 2s 587us/step - loss: 8.5042e-04 - val_loss: 0.0023\n",
      "Epoch 21/24\n",
      "3690/3690 [==============================] - 2s 588us/step - loss: 9.2203e-04 - val_loss: 0.0011\n",
      "Epoch 22/24\n",
      "3690/3690 [==============================] - 2s 590us/step - loss: 8.4505e-04 - val_loss: 5.4929e-04\n",
      "Epoch 23/24\n",
      "3690/3690 [==============================] - 2s 591us/step - loss: 8.4014e-04 - val_loss: 5.2113e-04\n",
      "Epoch 24/24\n",
      "3690/3690 [==============================] - 2s 591us/step - loss: 8.6265e-04 - val_loss: 0.0011\n",
      "Train on 3690 samples, validate on 410 samples\n",
      "Epoch 1/24\n",
      "3690/3690 [==============================] - 13s 4ms/step - loss: 0.0870 - val_loss: 0.0141\n",
      "Epoch 2/24\n",
      "3690/3690 [==============================] - 2s 599us/step - loss: 0.0156 - val_loss: 0.0324\n",
      "Epoch 3/24\n",
      "3690/3690 [==============================] - 2s 606us/step - loss: 0.0105 - val_loss: 0.0074\n",
      "Epoch 4/24\n",
      "3690/3690 [==============================] - 2s 598us/step - loss: 0.0078 - val_loss: 0.0109\n",
      "Epoch 5/24\n",
      "3690/3690 [==============================] - 2s 601us/step - loss: 0.0072 - val_loss: 0.0109\n",
      "Epoch 6/24\n",
      "3690/3690 [==============================] - 2s 600us/step - loss: 0.0067 - val_loss: 0.0057\n",
      "Epoch 7/24\n",
      "3690/3690 [==============================] - 2s 604us/step - loss: 0.0048 - val_loss: 0.0049\n",
      "Epoch 8/24\n",
      "3690/3690 [==============================] - 2s 605us/step - loss: 0.0056 - val_loss: 0.0125\n",
      "Epoch 9/24\n",
      "3690/3690 [==============================] - 2s 602us/step - loss: 0.0058 - val_loss: 0.0044\n",
      "Epoch 10/24\n",
      "3690/3690 [==============================] - 2s 601us/step - loss: 0.0045 - val_loss: 0.0098\n",
      "Epoch 11/24\n",
      "3690/3690 [==============================] - 2s 603us/step - loss: 0.0057 - val_loss: 0.0040\n",
      "Epoch 12/24\n",
      "3690/3690 [==============================] - 2s 601us/step - loss: 0.0038 - val_loss: 0.0018\n",
      "Epoch 13/24\n",
      "3690/3690 [==============================] - 2s 598us/step - loss: 0.0037 - val_loss: 0.0060\n",
      "Epoch 14/24\n",
      "3690/3690 [==============================] - 2s 601us/step - loss: 0.0043 - val_loss: 0.0067\n",
      "Epoch 15/24\n",
      "3690/3690 [==============================] - 2s 601us/step - loss: 0.0054 - val_loss: 0.0029\n",
      "Epoch 16/24\n",
      "3690/3690 [==============================] - 2s 599us/step - loss: 0.0029 - val_loss: 0.0033\n",
      "Epoch 17/24\n",
      "3690/3690 [==============================] - 2s 599us/step - loss: 0.0041 - val_loss: 0.0041\n",
      "Epoch 18/24\n",
      "3690/3690 [==============================] - 2s 602us/step - loss: 0.0039 - val_loss: 0.0024\n",
      "Epoch 19/24\n",
      "3690/3690 [==============================] - 2s 608us/step - loss: 0.0030 - val_loss: 0.0029\n",
      "Epoch 20/24\n",
      "3690/3690 [==============================] - 2s 606us/step - loss: 0.0046 - val_loss: 0.0037\n",
      "Epoch 21/24\n",
      "3690/3690 [==============================] - 2s 611us/step - loss: 0.0029 - val_loss: 0.0029\n",
      "Epoch 22/24\n",
      "3690/3690 [==============================] - 2s 601us/step - loss: 0.0032 - val_loss: 0.0029\n",
      "Epoch 23/24\n",
      "3690/3690 [==============================] - 2s 604us/step - loss: 0.0040 - val_loss: 0.0040\n",
      "Epoch 24/24\n",
      "3690/3690 [==============================] - 2s 614us/step - loss: 0.0040 - val_loss: 0.0046\n",
      "Train on 3690 samples, validate on 410 samples\n",
      "Epoch 1/24\n",
      "3690/3690 [==============================] - 14s 4ms/step - loss: 0.0347 - val_loss: 0.0146\n",
      "Epoch 2/24\n",
      "3690/3690 [==============================] - 2s 435us/step - loss: 0.0080 - val_loss: 0.0047\n",
      "Epoch 3/24\n",
      "3690/3690 [==============================] - 2s 433us/step - loss: 0.0064 - val_loss: 0.0065\n",
      "Epoch 4/24\n",
      "3690/3690 [==============================] - 2s 432us/step - loss: 0.0062 - val_loss: 0.0055\n",
      "Epoch 5/24\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3690/3690 [==============================] - 2s 429us/step - loss: 0.0051 - val_loss: 0.0027\n",
      "Epoch 6/24\n",
      "3690/3690 [==============================] - 2s 414us/step - loss: 0.0031 - val_loss: 0.0011\n",
      "Epoch 7/24\n",
      "3690/3690 [==============================] - 2s 414us/step - loss: 0.0031 - val_loss: 0.0032\n",
      "Epoch 8/24\n",
      "3690/3690 [==============================] - 2s 411us/step - loss: 0.0060 - val_loss: 0.0085\n",
      "Epoch 9/24\n",
      "3690/3690 [==============================] - 2s 411us/step - loss: 0.0059 - val_loss: 0.0027\n",
      "Epoch 10/24\n",
      "3690/3690 [==============================] - 2s 410us/step - loss: 0.0038 - val_loss: 0.0029\n",
      "Epoch 11/24\n",
      "3690/3690 [==============================] - 2s 412us/step - loss: 0.0039 - val_loss: 0.0046\n",
      "Epoch 12/24\n",
      "3690/3690 [==============================] - 2s 418us/step - loss: 0.0055 - val_loss: 0.0122\n",
      "Epoch 13/24\n",
      "3690/3690 [==============================] - 2s 410us/step - loss: 0.0054 - val_loss: 0.0044\n",
      "Epoch 14/24\n",
      "3690/3690 [==============================] - 2s 412us/step - loss: 0.0046 - val_loss: 0.0049\n",
      "Epoch 15/24\n",
      "3690/3690 [==============================] - 2s 412us/step - loss: 0.0033 - val_loss: 0.0050\n",
      "Epoch 16/24\n",
      "3690/3690 [==============================] - 2s 409us/step - loss: 0.0038 - val_loss: 0.0030\n",
      "Epoch 17/24\n",
      "3690/3690 [==============================] - 2s 414us/step - loss: 0.0027 - val_loss: 0.0072\n",
      "Epoch 18/24\n",
      "3690/3690 [==============================] - 2s 411us/step - loss: 0.0050 - val_loss: 0.0028\n",
      "Epoch 19/24\n",
      "3690/3690 [==============================] - 2s 412us/step - loss: 0.0029 - val_loss: 0.0047\n",
      "Epoch 20/24\n",
      "3690/3690 [==============================] - 2s 410us/step - loss: 0.0040 - val_loss: 0.0079\n",
      "Epoch 21/24\n",
      "3690/3690 [==============================] - 2s 413us/step - loss: 0.0035 - val_loss: 8.2908e-04\n",
      "Epoch 22/24\n",
      "3690/3690 [==============================] - 2s 414us/step - loss: 0.0017 - val_loss: 7.4790e-04\n",
      "Epoch 23/24\n",
      "3690/3690 [==============================] - 2s 413us/step - loss: 0.0014 - val_loss: 9.0250e-04\n",
      "Epoch 24/24\n",
      "3690/3690 [==============================] - 2s 409us/step - loss: 0.0018 - val_loss: 0.0043\n",
      "Train on 3690 samples, validate on 410 samples\n",
      "Epoch 1/24\n",
      "3690/3690 [==============================] - 13s 4ms/step - loss: 0.1639 - val_loss: 0.0025\n",
      "Epoch 2/24\n",
      "3690/3690 [==============================] - 2s 412us/step - loss: 0.0106 - val_loss: 0.0021\n",
      "Epoch 3/24\n",
      "3690/3690 [==============================] - 2s 413us/step - loss: 0.0095 - val_loss: 0.0205\n",
      "Epoch 4/24\n",
      "3690/3690 [==============================] - 2s 412us/step - loss: 0.0100 - val_loss: 0.0105\n",
      "Epoch 5/24\n",
      "3690/3690 [==============================] - 2s 414us/step - loss: 0.0062 - val_loss: 0.0024\n",
      "Epoch 6/24\n",
      "3690/3690 [==============================] - 2s 411us/step - loss: 0.0067 - val_loss: 0.0095\n",
      "Epoch 7/24\n",
      "3690/3690 [==============================] - 2s 410us/step - loss: 0.0072 - val_loss: 0.0057\n",
      "Epoch 8/24\n",
      "3690/3690 [==============================] - 2s 414us/step - loss: 0.0056 - val_loss: 0.0027\n",
      "Epoch 9/24\n",
      "3690/3690 [==============================] - 2s 411us/step - loss: 0.0038 - val_loss: 8.6763e-04\n",
      "Epoch 10/24\n",
      "3690/3690 [==============================] - 2s 414us/step - loss: 0.0026 - val_loss: 0.0032\n",
      "Epoch 11/24\n",
      "3690/3690 [==============================] - 2s 415us/step - loss: 0.0050 - val_loss: 0.0028\n",
      "Epoch 12/24\n",
      "3690/3690 [==============================] - 2s 414us/step - loss: 0.0052 - val_loss: 0.0039\n",
      "Epoch 13/24\n",
      "3690/3690 [==============================] - 2s 413us/step - loss: 0.0056 - val_loss: 0.0038\n",
      "Epoch 14/24\n",
      "3690/3690 [==============================] - 2s 414us/step - loss: 0.0046 - val_loss: 0.0031\n",
      "Epoch 15/24\n",
      "3690/3690 [==============================] - 2s 412us/step - loss: 0.0046 - val_loss: 0.0032\n",
      "Epoch 16/24\n",
      "3690/3690 [==============================] - 2s 412us/step - loss: 0.0048 - val_loss: 0.0023\n",
      "Epoch 17/24\n",
      "3690/3690 [==============================] - 2s 410us/step - loss: 0.0033 - val_loss: 7.5648e-04\n",
      "Epoch 18/24\n",
      "3690/3690 [==============================] - 2s 409us/step - loss: 0.0021 - val_loss: 0.0022\n",
      "Epoch 19/24\n",
      "3690/3690 [==============================] - 2s 409us/step - loss: 0.0040 - val_loss: 0.0025\n",
      "Epoch 20/24\n",
      "3690/3690 [==============================] - 2s 414us/step - loss: 0.0043 - val_loss: 0.0046\n",
      "Epoch 21/24\n",
      "3690/3690 [==============================] - 2s 415us/step - loss: 0.0046 - val_loss: 0.0031\n",
      "Epoch 22/24\n",
      "3690/3690 [==============================] - 2s 416us/step - loss: 0.0029 - val_loss: 0.0021\n",
      "Epoch 23/24\n",
      "3690/3690 [==============================] - 2s 413us/step - loss: 0.0036 - val_loss: 0.0047\n",
      "Epoch 24/24\n",
      "3690/3690 [==============================] - 2s 412us/step - loss: 0.0052 - val_loss: 0.0018\n",
      "Train on 3690 samples, validate on 410 samples\n",
      "Epoch 1/24\n",
      "3690/3690 [==============================] - 14s 4ms/step - loss: 0.0679 - val_loss: 0.0045\n",
      "Epoch 2/24\n",
      "3690/3690 [==============================] - 2s 445us/step - loss: 0.0103 - val_loss: 0.0047\n",
      "Epoch 3/24\n",
      "3690/3690 [==============================] - 2s 443us/step - loss: 0.0063 - val_loss: 0.0041\n",
      "Epoch 4/24\n",
      "3690/3690 [==============================] - 2s 446us/step - loss: 0.0071 - val_loss: 0.0050\n",
      "Epoch 5/24\n",
      "3690/3690 [==============================] - 2s 446us/step - loss: 0.0058 - val_loss: 0.0018\n",
      "Epoch 6/24\n",
      "3690/3690 [==============================] - 2s 446us/step - loss: 0.0044 - val_loss: 0.0016\n",
      "Epoch 7/24\n",
      "3690/3690 [==============================] - 2s 446us/step - loss: 0.0051 - val_loss: 0.0077\n",
      "Epoch 8/24\n",
      "3690/3690 [==============================] - 2s 447us/step - loss: 0.0047 - val_loss: 8.0831e-04\n",
      "Epoch 9/24\n",
      "3690/3690 [==============================] - 2s 446us/step - loss: 0.0031 - val_loss: 0.0013\n",
      "Epoch 10/24\n",
      "3690/3690 [==============================] - 2s 445us/step - loss: 0.0032 - val_loss: 0.0027\n",
      "Epoch 11/24\n",
      "3690/3690 [==============================] - 2s 446us/step - loss: 0.0050 - val_loss: 0.0042\n",
      "Epoch 12/24\n",
      "3690/3690 [==============================] - 2s 451us/step - loss: 0.0062 - val_loss: 0.0057\n",
      "Epoch 13/24\n",
      "3690/3690 [==============================] - 2s 444us/step - loss: 0.0043 - val_loss: 0.0073\n",
      "Epoch 14/24\n",
      "3690/3690 [==============================] - 2s 442us/step - loss: 0.0054 - val_loss: 0.0012\n",
      "Epoch 15/24\n",
      "3690/3690 [==============================] - 2s 446us/step - loss: 0.0025 - val_loss: 0.0011\n",
      "Epoch 16/24\n",
      "3690/3690 [==============================] - 2s 443us/step - loss: 0.0037 - val_loss: 0.0045\n",
      "Epoch 17/24\n",
      "3690/3690 [==============================] - 2s 448us/step - loss: 0.0060 - val_loss: 0.0028\n",
      "Epoch 18/24\n",
      "3690/3690 [==============================] - 2s 444us/step - loss: 0.0034 - val_loss: 0.0049\n",
      "Epoch 19/24\n",
      "3690/3690 [==============================] - 2s 444us/step - loss: 0.0044 - val_loss: 0.0057\n",
      "Epoch 20/24\n",
      "3690/3690 [==============================] - 2s 445us/step - loss: 0.0035 - val_loss: 0.0032\n",
      "Epoch 21/24\n",
      "3690/3690 [==============================] - 2s 444us/step - loss: 0.0032 - val_loss: 0.0023\n",
      "Epoch 22/24\n",
      "3690/3690 [==============================] - 2s 446us/step - loss: 0.0029 - val_loss: 0.0028\n",
      "Epoch 23/24\n",
      "3690/3690 [==============================] - 2s 445us/step - loss: 0.0036 - val_loss: 0.0057\n",
      "Epoch 24/24\n",
      "3690/3690 [==============================] - 2s 445us/step - loss: 0.0037 - val_loss: 0.0053\n",
      "Train on 3690 samples, validate on 410 samples\n",
      "Epoch 1/24\n",
      "3690/3690 [==============================] - 14s 4ms/step - loss: 0.0566 - val_loss: 0.0086\n",
      "Epoch 2/24\n",
      "3690/3690 [==============================] - 2s 523us/step - loss: 0.0071 - val_loss: 0.0138\n",
      "Epoch 3/24\n",
      "3690/3690 [==============================] - 2s 522us/step - loss: 0.0074 - val_loss: 0.0172\n",
      "Epoch 4/24\n",
      "3690/3690 [==============================] - 2s 526us/step - loss: 0.0056 - val_loss: 0.0073\n",
      "Epoch 5/24\n",
      "3690/3690 [==============================] - 2s 522us/step - loss: 0.0034 - val_loss: 0.0044\n",
      "Epoch 6/24\n",
      "3690/3690 [==============================] - 2s 527us/step - loss: 0.0025 - val_loss: 0.0030\n",
      "Epoch 7/24\n",
      "3690/3690 [==============================] - 2s 523us/step - loss: 0.0024 - val_loss: 0.0123\n",
      "Epoch 8/24\n",
      "3690/3690 [==============================] - 2s 525us/step - loss: 0.0120 - val_loss: 0.0067\n",
      "Epoch 9/24\n",
      "3690/3690 [==============================] - 2s 529us/step - loss: 0.0028 - val_loss: 0.0062\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/24\n",
      "3690/3690 [==============================] - 2s 532us/step - loss: 0.0030 - val_loss: 0.0052\n",
      "Epoch 11/24\n",
      "3690/3690 [==============================] - 2s 523us/step - loss: 0.0028 - val_loss: 0.0086\n",
      "Epoch 12/24\n",
      "3690/3690 [==============================] - 2s 524us/step - loss: 0.0067 - val_loss: 0.0115\n",
      "Epoch 13/24\n",
      "3690/3690 [==============================] - 2s 522us/step - loss: 0.0049 - val_loss: 0.0096\n",
      "Epoch 14/24\n",
      "3690/3690 [==============================] - 2s 524us/step - loss: 0.0045 - val_loss: 0.0087\n",
      "Epoch 15/24\n",
      "3690/3690 [==============================] - 2s 521us/step - loss: 0.0041 - val_loss: 0.0094\n",
      "Epoch 16/24\n",
      "3690/3690 [==============================] - 2s 522us/step - loss: 0.0039 - val_loss: 0.0082\n",
      "Epoch 17/24\n",
      "3690/3690 [==============================] - 2s 523us/step - loss: 0.0036 - val_loss: 0.0032\n",
      "Epoch 18/24\n",
      "3690/3690 [==============================] - 2s 523us/step - loss: 0.0022 - val_loss: 0.0042\n",
      "Epoch 19/24\n",
      "3690/3690 [==============================] - 2s 524us/step - loss: 0.0026 - val_loss: 0.0056\n",
      "Epoch 20/24\n",
      "3690/3690 [==============================] - 2s 525us/step - loss: 0.0041 - val_loss: 0.0145\n",
      "Epoch 21/24\n",
      "3690/3690 [==============================] - 2s 524us/step - loss: 0.0051 - val_loss: 0.0110\n",
      "Epoch 22/24\n",
      "3690/3690 [==============================] - 2s 523us/step - loss: 0.0040 - val_loss: 0.0048\n",
      "Epoch 23/24\n",
      "3690/3690 [==============================] - 2s 525us/step - loss: 0.0019 - val_loss: 0.0055\n",
      "Epoch 24/24\n",
      "3690/3690 [==============================] - 2s 521us/step - loss: 0.0024 - val_loss: 0.0074\n",
      "Train on 3690 samples, validate on 410 samples\n",
      "Epoch 1/24\n",
      "3690/3690 [==============================] - 15s 4ms/step - loss: 0.0419 - val_loss: 0.0074\n",
      "Epoch 2/24\n",
      "3690/3690 [==============================] - 2s 572us/step - loss: 0.0124 - val_loss: 0.0089\n",
      "Epoch 3/24\n",
      "3690/3690 [==============================] - 2s 570us/step - loss: 0.0065 - val_loss: 0.0205\n",
      "Epoch 4/24\n",
      "3690/3690 [==============================] - 2s 574us/step - loss: 0.0088 - val_loss: 0.0091\n",
      "Epoch 5/24\n",
      "3690/3690 [==============================] - 2s 572us/step - loss: 0.0045 - val_loss: 0.0125\n",
      "Epoch 6/24\n",
      "3690/3690 [==============================] - 2s 576us/step - loss: 0.0068 - val_loss: 0.0122\n",
      "Epoch 7/24\n",
      "3690/3690 [==============================] - 2s 577us/step - loss: 0.0061 - val_loss: 0.0156\n",
      "Epoch 8/24\n",
      "3690/3690 [==============================] - 2s 569us/step - loss: 0.0056 - val_loss: 0.0064\n",
      "Epoch 9/24\n",
      "3690/3690 [==============================] - 2s 568us/step - loss: 0.0035 - val_loss: 0.0137\n",
      "Epoch 10/24\n",
      "3690/3690 [==============================] - 2s 571us/step - loss: 0.0064 - val_loss: 0.0128\n",
      "Epoch 11/24\n",
      "3690/3690 [==============================] - 2s 570us/step - loss: 0.0057 - val_loss: 0.0181\n",
      "Epoch 12/24\n",
      "3690/3690 [==============================] - 2s 576us/step - loss: 0.0063 - val_loss: 0.0106\n",
      "Epoch 13/24\n",
      "3690/3690 [==============================] - 2s 569us/step - loss: 0.0038 - val_loss: 0.0117\n",
      "Epoch 14/24\n",
      "3690/3690 [==============================] - 2s 573us/step - loss: 0.0042 - val_loss: 0.0133\n",
      "Epoch 15/24\n",
      "3690/3690 [==============================] - 2s 573us/step - loss: 0.0043 - val_loss: 0.0077\n",
      "Epoch 16/24\n",
      "3690/3690 [==============================] - 2s 570us/step - loss: 0.0028 - val_loss: 0.0094\n",
      "Epoch 17/24\n",
      "3690/3690 [==============================] - 2s 573us/step - loss: 0.0027 - val_loss: 0.0108\n",
      "Epoch 18/24\n",
      "3690/3690 [==============================] - 2s 566us/step - loss: 0.0051 - val_loss: 0.0082\n",
      "Epoch 19/24\n",
      "3690/3690 [==============================] - 2s 572us/step - loss: 0.0024 - val_loss: 0.0142\n",
      "Epoch 20/24\n",
      "3690/3690 [==============================] - 2s 575us/step - loss: 0.0046 - val_loss: 0.0101\n",
      "Epoch 21/24\n",
      "3690/3690 [==============================] - 2s 572us/step - loss: 0.0034 - val_loss: 0.0076\n",
      "Epoch 22/24\n",
      "3690/3690 [==============================] - 2s 573us/step - loss: 0.0020 - val_loss: 0.0098\n",
      "Epoch 23/24\n",
      "3690/3690 [==============================] - 2s 572us/step - loss: 0.0026 - val_loss: 0.0072\n",
      "Epoch 24/24\n",
      "3690/3690 [==============================] - 2s 576us/step - loss: 0.0022 - val_loss: 0.0094\n",
      "Train on 3690 samples, validate on 410 samples\n",
      "Epoch 1/24\n",
      "3690/3690 [==============================] - 14s 4ms/step - loss: 0.0697 - val_loss: 0.0039\n",
      "Epoch 2/24\n",
      "3690/3690 [==============================] - 2s 495us/step - loss: 0.0170 - val_loss: 0.0019\n",
      "Epoch 3/24\n",
      "3690/3690 [==============================] - 2s 496us/step - loss: 0.0058 - val_loss: 0.0311\n",
      "Epoch 4/24\n",
      "3690/3690 [==============================] - 2s 496us/step - loss: 0.0102 - val_loss: 0.0094\n",
      "Epoch 5/24\n",
      "3690/3690 [==============================] - 2s 496us/step - loss: 0.0090 - val_loss: 0.0025\n",
      "Epoch 6/24\n",
      "3690/3690 [==============================] - 2s 496us/step - loss: 0.0083 - val_loss: 0.0189\n",
      "Epoch 7/24\n",
      "3690/3690 [==============================] - 2s 494us/step - loss: 0.0112 - val_loss: 0.0016\n",
      "Epoch 8/24\n",
      "3690/3690 [==============================] - 2s 497us/step - loss: 0.0015 - val_loss: 6.7096e-04\n",
      "Epoch 9/24\n",
      "3690/3690 [==============================] - 2s 496us/step - loss: 0.0101 - val_loss: 0.0049\n",
      "Epoch 10/24\n",
      "3690/3690 [==============================] - 2s 496us/step - loss: 0.0039 - val_loss: 7.5669e-04\n",
      "Epoch 11/24\n",
      "3690/3690 [==============================] - 2s 496us/step - loss: 0.0060 - val_loss: 0.0034\n",
      "Epoch 12/24\n",
      "3690/3690 [==============================] - 2s 496us/step - loss: 0.0054 - val_loss: 6.7881e-04\n",
      "Epoch 13/24\n",
      "3690/3690 [==============================] - 2s 500us/step - loss: 0.0038 - val_loss: 0.0044\n",
      "Epoch 14/24\n",
      "3690/3690 [==============================] - 2s 495us/step - loss: 0.0096 - val_loss: 0.0014\n",
      "Epoch 15/24\n",
      "3690/3690 [==============================] - 2s 499us/step - loss: 0.0012 - val_loss: 6.7659e-04\n",
      "Epoch 16/24\n",
      "3690/3690 [==============================] - 2s 496us/step - loss: 0.0054 - val_loss: 9.8733e-04\n",
      "Epoch 17/24\n",
      "3690/3690 [==============================] - 2s 495us/step - loss: 0.0034 - val_loss: 0.0011\n",
      "Epoch 18/24\n",
      "3690/3690 [==============================] - 2s 497us/step - loss: 0.0044 - val_loss: 9.6041e-04\n",
      "Epoch 19/24\n",
      "3690/3690 [==============================] - 2s 494us/step - loss: 0.0035 - val_loss: 0.0018\n",
      "Epoch 20/24\n",
      "3690/3690 [==============================] - 2s 498us/step - loss: 0.0033 - val_loss: 0.0031\n",
      "Epoch 21/24\n",
      "3690/3690 [==============================] - 2s 499us/step - loss: 0.0053 - val_loss: 0.0011\n",
      "Epoch 22/24\n",
      "3690/3690 [==============================] - 2s 497us/step - loss: 0.0016 - val_loss: 0.0012\n",
      "Epoch 23/24\n",
      "3690/3690 [==============================] - 2s 498us/step - loss: 0.0022 - val_loss: 0.0078\n",
      "Epoch 24/24\n",
      "3690/3690 [==============================] - 2s 498us/step - loss: 0.0045 - val_loss: 8.8676e-04\n",
      "\n",
      "GENERATION 10\n",
      "\n",
      "Train on 3690 samples, validate on 410 samples\n",
      "Epoch 1/24\n",
      "3690/3690 [==============================] - 16s 4ms/step - loss: 0.1029 - val_loss: 0.0288\n",
      "Epoch 2/24\n",
      "3690/3690 [==============================] - 2s 602us/step - loss: 0.0306 - val_loss: 0.0026\n",
      "Epoch 3/24\n",
      "3690/3690 [==============================] - 2s 603us/step - loss: 0.0090 - val_loss: 0.0140\n",
      "Epoch 4/24\n",
      "3690/3690 [==============================] - 2s 605us/step - loss: 0.0053 - val_loss: 0.0023\n",
      "Epoch 5/24\n",
      "3690/3690 [==============================] - 2s 605us/step - loss: 0.0031 - val_loss: 0.0018\n",
      "Epoch 6/24\n",
      "3690/3690 [==============================] - 2s 600us/step - loss: 0.0026 - val_loss: 0.0014\n",
      "Epoch 7/24\n",
      "3690/3690 [==============================] - 2s 602us/step - loss: 0.0023 - val_loss: 0.0011\n",
      "Epoch 8/24\n",
      "3690/3690 [==============================] - 2s 601us/step - loss: 0.0021 - val_loss: 0.0014\n",
      "Epoch 9/24\n",
      "3690/3690 [==============================] - 2s 607us/step - loss: 0.0019 - val_loss: 0.0011\n",
      "Epoch 10/24\n",
      "3690/3690 [==============================] - 2s 604us/step - loss: 0.0017 - val_loss: 0.0015\n",
      "Epoch 11/24\n",
      "3690/3690 [==============================] - 2s 606us/step - loss: 0.0015 - val_loss: 0.0016\n",
      "Epoch 12/24\n",
      "3690/3690 [==============================] - 2s 605us/step - loss: 0.0015 - val_loss: 0.0012\n",
      "Epoch 13/24\n",
      "3690/3690 [==============================] - 2s 605us/step - loss: 0.0016 - val_loss: 0.0010\n",
      "Epoch 14/24\n",
      "3690/3690 [==============================] - 2s 601us/step - loss: 0.0014 - val_loss: 7.0731e-04\n",
      "Epoch 15/24\n",
      "3690/3690 [==============================] - 2s 604us/step - loss: 0.0013 - val_loss: 8.3683e-04\n",
      "Epoch 16/24\n",
      "3690/3690 [==============================] - 2s 604us/step - loss: 0.0013 - val_loss: 6.7450e-04\n",
      "Epoch 17/24\n",
      "3690/3690 [==============================] - 2s 605us/step - loss: 0.0012 - val_loss: 0.0011\n",
      "Epoch 18/24\n",
      "3690/3690 [==============================] - 2s 603us/step - loss: 0.0013 - val_loss: 7.3179e-04\n",
      "Epoch 19/24\n",
      "3690/3690 [==============================] - 2s 606us/step - loss: 0.0011 - val_loss: 5.9198e-04\n",
      "Epoch 20/24\n",
      "3690/3690 [==============================] - 2s 603us/step - loss: 0.0011 - val_loss: 0.0012\n",
      "Epoch 21/24\n",
      "3690/3690 [==============================] - 2s 602us/step - loss: 0.0012 - val_loss: 7.8772e-04\n",
      "Epoch 22/24\n",
      "3690/3690 [==============================] - 2s 602us/step - loss: 0.0010 - val_loss: 5.8783e-04\n",
      "Epoch 23/24\n",
      "3690/3690 [==============================] - 2s 606us/step - loss: 0.0011 - val_loss: 6.0506e-04\n",
      "Epoch 24/24\n",
      "3690/3690 [==============================] - 2s 604us/step - loss: 0.0011 - val_loss: 5.2874e-04\n",
      "Train on 3690 samples, validate on 410 samples\n",
      "Epoch 1/24\n",
      "3690/3690 [==============================] - 15s 4ms/step - loss: 0.0631 - val_loss: 0.0129\n",
      "Epoch 2/24\n",
      "3690/3690 [==============================] - 2s 603us/step - loss: 0.0156 - val_loss: 0.0134\n",
      "Epoch 3/24\n",
      "3690/3690 [==============================] - 2s 607us/step - loss: 0.0063 - val_loss: 0.0084\n",
      "Epoch 4/24\n",
      "3690/3690 [==============================] - 2s 604us/step - loss: 0.0028 - val_loss: 8.6822e-04\n",
      "Epoch 5/24\n",
      "3690/3690 [==============================] - 2s 602us/step - loss: 0.0016 - val_loss: 8.6315e-04\n",
      "Epoch 6/24\n",
      "3690/3690 [==============================] - 2s 602us/step - loss: 0.0014 - val_loss: 6.6274e-04\n",
      "Epoch 7/24\n",
      "3690/3690 [==============================] - 2s 601us/step - loss: 0.0013 - val_loss: 0.0020\n",
      "Epoch 8/24\n",
      "3690/3690 [==============================] - 2s 603us/step - loss: 0.0011 - val_loss: 0.0015\n",
      "Epoch 9/24\n",
      "3690/3690 [==============================] - 2s 605us/step - loss: 0.0010 - val_loss: 0.0014\n",
      "Epoch 10/24\n",
      "3690/3690 [==============================] - 2s 604us/step - loss: 9.9391e-04 - val_loss: 0.0010\n",
      "Epoch 11/24\n",
      "3690/3690 [==============================] - 2s 601us/step - loss: 9.3978e-04 - val_loss: 9.8721e-04\n",
      "Epoch 12/24\n",
      "3690/3690 [==============================] - 2s 604us/step - loss: 9.1925e-04 - val_loss: 9.1894e-04\n",
      "Epoch 13/24\n",
      "3690/3690 [==============================] - 2s 605us/step - loss: 9.2906e-04 - val_loss: 5.7169e-04\n",
      "Epoch 14/24\n",
      "3690/3690 [==============================] - 2s 603us/step - loss: 9.7087e-04 - val_loss: 5.2051e-04\n",
      "Epoch 15/24\n",
      "3690/3690 [==============================] - 2s 604us/step - loss: 9.7184e-04 - val_loss: 6.1164e-04\n",
      "Epoch 16/24\n",
      "3690/3690 [==============================] - 2s 602us/step - loss: 8.9615e-04 - val_loss: 8.0428e-04\n",
      "Epoch 17/24\n",
      "3690/3690 [==============================] - 2s 606us/step - loss: 8.5173e-04 - val_loss: 0.0011\n",
      "Epoch 18/24\n",
      "3690/3690 [==============================] - 2s 601us/step - loss: 8.4080e-04 - val_loss: 6.5472e-04\n",
      "Epoch 19/24\n",
      "3690/3690 [==============================] - 2s 605us/step - loss: 8.5169e-04 - val_loss: 8.7536e-04\n",
      "Epoch 20/24\n",
      "3690/3690 [==============================] - 2s 605us/step - loss: 8.3469e-04 - val_loss: 7.7332e-04\n",
      "Epoch 21/24\n",
      "3690/3690 [==============================] - 2s 605us/step - loss: 8.0661e-04 - val_loss: 6.1171e-04\n",
      "Epoch 22/24\n",
      "3690/3690 [==============================] - 2s 602us/step - loss: 8.1643e-04 - val_loss: 9.9432e-04\n",
      "Epoch 23/24\n",
      "3690/3690 [==============================] - 2s 604us/step - loss: 7.6821e-04 - val_loss: 9.1735e-04\n",
      "Epoch 24/24\n",
      "3690/3690 [==============================] - 2s 604us/step - loss: 7.9214e-04 - val_loss: 0.0017\n",
      "Train on 3690 samples, validate on 410 samples\n",
      "Epoch 1/24\n",
      "3690/3690 [==============================] - 16s 4ms/step - loss: 0.0487 - val_loss: 0.0124\n",
      "Epoch 2/24\n",
      "3690/3690 [==============================] - 2s 602us/step - loss: 0.0091 - val_loss: 0.0026\n",
      "Epoch 3/24\n",
      "3690/3690 [==============================] - 2s 599us/step - loss: 0.0046 - val_loss: 0.0026\n",
      "Epoch 4/24\n",
      "3690/3690 [==============================] - 2s 601us/step - loss: 0.0024 - val_loss: 0.0023\n",
      "Epoch 5/24\n",
      "3690/3690 [==============================] - 2s 599us/step - loss: 0.0014 - val_loss: 0.0010\n",
      "Epoch 6/24\n",
      "3690/3690 [==============================] - 2s 602us/step - loss: 0.0013 - val_loss: 6.8384e-04\n",
      "Epoch 7/24\n",
      "3690/3690 [==============================] - 2s 601us/step - loss: 0.0012 - val_loss: 8.7359e-04\n",
      "Epoch 8/24\n",
      "3690/3690 [==============================] - 2s 604us/step - loss: 0.0010 - val_loss: 0.0010\n",
      "Epoch 9/24\n",
      "3690/3690 [==============================] - 2s 601us/step - loss: 0.0010 - val_loss: 0.0018\n",
      "Epoch 10/24\n",
      "3690/3690 [==============================] - 2s 602us/step - loss: 0.0010 - val_loss: 0.0013\n",
      "Epoch 11/24\n",
      "3690/3690 [==============================] - 2s 602us/step - loss: 9.5896e-04 - val_loss: 0.0010\n",
      "Epoch 12/24\n",
      "3690/3690 [==============================] - 2s 604us/step - loss: 8.9646e-04 - val_loss: 0.0011\n",
      "Epoch 13/24\n",
      "3690/3690 [==============================] - 2s 603us/step - loss: 8.8087e-04 - val_loss: 8.0711e-04\n",
      "Epoch 14/24\n",
      "3690/3690 [==============================] - 2s 602us/step - loss: 8.8817e-04 - val_loss: 9.9264e-04\n",
      "Epoch 15/24\n",
      "3690/3690 [==============================] - 2s 600us/step - loss: 8.5802e-04 - val_loss: 0.0014\n",
      "Epoch 16/24\n",
      "3690/3690 [==============================] - 2s 599us/step - loss: 8.5866e-04 - val_loss: 7.9434e-04\n",
      "Epoch 17/24\n",
      "3690/3690 [==============================] - 2s 601us/step - loss: 8.4143e-04 - val_loss: 7.8432e-04\n",
      "Epoch 18/24\n",
      "3690/3690 [==============================] - 2s 610us/step - loss: 8.2787e-04 - val_loss: 0.0012\n",
      "Epoch 19/24\n",
      "3690/3690 [==============================] - 2s 605us/step - loss: 8.2631e-04 - val_loss: 6.2446e-04\n",
      "Epoch 20/24\n",
      "3690/3690 [==============================] - 2s 605us/step - loss: 7.8229e-04 - val_loss: 0.0012\n",
      "Epoch 21/24\n",
      "3690/3690 [==============================] - 2s 602us/step - loss: 7.9233e-04 - val_loss: 0.0014\n",
      "Epoch 22/24\n",
      "3690/3690 [==============================] - 2s 604us/step - loss: 8.0502e-04 - val_loss: 8.2785e-04\n",
      "Epoch 23/24\n",
      "3690/3690 [==============================] - 2s 600us/step - loss: 7.6520e-04 - val_loss: 6.9913e-04\n",
      "Epoch 24/24\n",
      "3690/3690 [==============================] - 2s 598us/step - loss: 7.3794e-04 - val_loss: 6.6185e-04\n",
      "Train on 3690 samples, validate on 410 samples\n",
      "Epoch 1/24\n",
      "3690/3690 [==============================] - 16s 4ms/step - loss: 1516154.5840 - val_loss: 108.8248\n",
      "Epoch 2/24\n",
      "3690/3690 [==============================] - 2s 578us/step - loss: 36852.4354 - val_loss: 853.2997\n",
      "Epoch 3/24\n",
      "3690/3690 [==============================] - 2s 599us/step - loss: 587.2401 - val_loss: 385.7169\n",
      "Epoch 4/24\n",
      "3690/3690 [==============================] - 2s 580us/step - loss: 266.2220 - val_loss: 231.3788\n",
      "Epoch 5/24\n",
      "3690/3690 [==============================] - 2s 578us/step - loss: 154.0337 - val_loss: 165.3513\n",
      "Epoch 6/24\n",
      "3690/3690 [==============================] - 2s 580us/step - loss: 124.2819 - val_loss: 130.5268\n",
      "Epoch 7/24\n",
      "3690/3690 [==============================] - 2s 578us/step - loss: 99.9961 - val_loss: 109.3844\n",
      "Epoch 8/24\n",
      "3690/3690 [==============================] - 2s 575us/step - loss: 84.6000 - val_loss: 94.6759\n",
      "Epoch 9/24\n",
      "3690/3690 [==============================] - 2s 577us/step - loss: 75.2261 - val_loss: 83.6903\n",
      "Epoch 10/24\n",
      "3690/3690 [==============================] - 2s 579us/step - loss: 69.7436 - val_loss: 74.7869\n",
      "Epoch 11/24\n",
      "3690/3690 [==============================] - 2s 580us/step - loss: 65.3855 - val_loss: 67.2660\n",
      "Epoch 12/24\n",
      "3690/3690 [==============================] - 2s 578us/step - loss: 60.2048 - val_loss: 60.8918\n",
      "Epoch 13/24\n",
      "3690/3690 [==============================] - 2s 579us/step - loss: 56.3513 - val_loss: 55.4312\n",
      "Epoch 14/24\n",
      "3690/3690 [==============================] - 2s 579us/step - loss: 54.5357 - val_loss: 50.7065\n",
      "Epoch 15/24\n",
      "3690/3690 [==============================] - 2s 578us/step - loss: 49.0340 - val_loss: 46.4107\n",
      "Epoch 16/24\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3690/3690 [==============================] - 2s 576us/step - loss: 46.7773 - val_loss: 42.7771\n",
      "Epoch 17/24\n",
      "3690/3690 [==============================] - 2s 578us/step - loss: 45.2671 - val_loss: 39.5644\n",
      "Epoch 18/24\n",
      "3690/3690 [==============================] - 2s 579us/step - loss: 42.8685 - val_loss: 36.6601\n",
      "Epoch 19/24\n",
      "3690/3690 [==============================] - 2s 577us/step - loss: 41.1054 - val_loss: 34.0220\n",
      "Epoch 20/24\n",
      "3690/3690 [==============================] - 2s 578us/step - loss: 41.3266 - val_loss: 31.5055\n",
      "Epoch 21/24\n",
      "3690/3690 [==============================] - 2s 578us/step - loss: 39.1966 - val_loss: 28.9974\n",
      "Epoch 22/24\n",
      "3690/3690 [==============================] - 2s 578us/step - loss: 36.1183 - val_loss: 26.1719\n",
      "Epoch 23/24\n",
      "3690/3690 [==============================] - 2s 576us/step - loss: 36.6083 - val_loss: 21.9451\n",
      "Epoch 24/24\n",
      "3690/3690 [==============================] - 2s 579us/step - loss: 33.0790 - val_loss: 13.8381\n",
      "Train on 3690 samples, validate on 410 samples\n",
      "Epoch 1/24\n",
      "3690/3690 [==============================] - 17s 5ms/step - loss: nan - val_loss: nan\n",
      "Epoch 2/24\n",
      "3690/3690 [==============================] - 2s 578us/step - loss: nan - val_loss: nan\n",
      "Epoch 3/24\n",
      "3690/3690 [==============================] - 2s 577us/step - loss: nan - val_loss: nan\n",
      "Epoch 4/24\n",
      "3690/3690 [==============================] - 2s 575us/step - loss: nan - val_loss: nan\n",
      "Epoch 5/24\n",
      "3690/3690 [==============================] - 2s 577us/step - loss: nan - val_loss: nan\n",
      "Epoch 6/24\n",
      "3690/3690 [==============================] - 2s 580us/step - loss: nan - val_loss: nan\n",
      "Epoch 7/24\n",
      "3690/3690 [==============================] - 2s 578us/step - loss: nan - val_loss: nan\n",
      "Epoch 8/24\n",
      "3690/3690 [==============================] - 2s 579us/step - loss: nan - val_loss: nan\n",
      "Epoch 9/24\n",
      "3690/3690 [==============================] - 2s 580us/step - loss: nan - val_loss: nan\n",
      "Epoch 10/24\n",
      "3690/3690 [==============================] - 2s 575us/step - loss: nan - val_loss: nan\n",
      "Epoch 11/24\n",
      "3690/3690 [==============================] - 2s 577us/step - loss: nan - val_loss: nan\n",
      "Epoch 12/24\n",
      "3690/3690 [==============================] - 2s 575us/step - loss: nan - val_loss: nan\n",
      "Epoch 13/24\n",
      "3690/3690 [==============================] - 2s 577us/step - loss: nan - val_loss: nan\n",
      "Epoch 14/24\n",
      "3690/3690 [==============================] - 2s 579us/step - loss: nan - val_loss: nan\n",
      "Epoch 15/24\n",
      "3690/3690 [==============================] - 2s 576us/step - loss: nan - val_loss: nan\n",
      "Epoch 16/24\n",
      "3690/3690 [==============================] - 2s 579us/step - loss: nan - val_loss: nan\n",
      "Epoch 17/24\n",
      "3690/3690 [==============================] - 2s 576us/step - loss: nan - val_loss: nan\n",
      "Epoch 18/24\n",
      "3690/3690 [==============================] - 2s 578us/step - loss: nan - val_loss: nan\n",
      "Epoch 19/24\n",
      "3690/3690 [==============================] - 2s 577us/step - loss: nan - val_loss: nan\n",
      "Epoch 20/24\n",
      "3690/3690 [==============================] - 2s 578us/step - loss: nan - val_loss: nan\n",
      "Epoch 21/24\n",
      "3690/3690 [==============================] - 2s 579us/step - loss: nan - val_loss: nan\n",
      "Epoch 22/24\n",
      "3690/3690 [==============================] - 2s 582us/step - loss: nan - val_loss: nan\n",
      "Epoch 23/24\n",
      "3690/3690 [==============================] - 2s 580us/step - loss: nan - val_loss: nan\n",
      "Epoch 24/24\n",
      "3690/3690 [==============================] - 2s 578us/step - loss: nan - val_loss: nan\n",
      "Train on 3690 samples, validate on 410 samples\n",
      "Epoch 1/24\n",
      "3690/3690 [==============================] - 17s 5ms/step - loss: nan - val_loss: nan\n",
      "Epoch 2/24\n",
      "3690/3690 [==============================] - 2s 579us/step - loss: nan - val_loss: nan\n",
      "Epoch 3/24\n",
      "3690/3690 [==============================] - 2s 583us/step - loss: nan - val_loss: nan\n",
      "Epoch 4/24\n",
      "3690/3690 [==============================] - 2s 582us/step - loss: nan - val_loss: nan\n",
      "Epoch 5/24\n",
      "3690/3690 [==============================] - 2s 576us/step - loss: nan - val_loss: nan\n",
      "Epoch 6/24\n",
      "3690/3690 [==============================] - 2s 581us/step - loss: nan - val_loss: nan\n",
      "Epoch 7/24\n",
      "3690/3690 [==============================] - 2s 577us/step - loss: nan - val_loss: nan\n",
      "Epoch 8/24\n",
      "3690/3690 [==============================] - 2s 582us/step - loss: nan - val_loss: nan\n",
      "Epoch 9/24\n",
      "3690/3690 [==============================] - 2s 581us/step - loss: nan - val_loss: nan\n",
      "Epoch 10/24\n",
      "3690/3690 [==============================] - 2s 580us/step - loss: nan - val_loss: nan\n",
      "Epoch 11/24\n",
      "3690/3690 [==============================] - 2s 579us/step - loss: nan - val_loss: nan\n",
      "Epoch 12/24\n",
      "3690/3690 [==============================] - 2s 578us/step - loss: nan - val_loss: nan\n",
      "Epoch 13/24\n",
      "3690/3690 [==============================] - 2s 584us/step - loss: nan - val_loss: nan\n",
      "Epoch 14/24\n",
      "3690/3690 [==============================] - 2s 580us/step - loss: nan - val_loss: nan\n",
      "Epoch 15/24\n",
      "3690/3690 [==============================] - 2s 581us/step - loss: nan - val_loss: nan\n",
      "Epoch 16/24\n",
      "3690/3690 [==============================] - 2s 581us/step - loss: nan - val_loss: nan\n",
      "Epoch 17/24\n",
      "3690/3690 [==============================] - 2s 580us/step - loss: nan - val_loss: nan\n",
      "Epoch 18/24\n",
      "3690/3690 [==============================] - 2s 583us/step - loss: nan - val_loss: nan\n",
      "Epoch 19/24\n",
      "3690/3690 [==============================] - 2s 575us/step - loss: nan - val_loss: nan\n",
      "Epoch 20/24\n",
      "3690/3690 [==============================] - 2s 583us/step - loss: nan - val_loss: nan\n",
      "Epoch 21/24\n",
      "3690/3690 [==============================] - 2s 581us/step - loss: nan - val_loss: nan\n",
      "Epoch 22/24\n",
      "3690/3690 [==============================] - 2s 580us/step - loss: nan - val_loss: nan\n",
      "Epoch 23/24\n",
      "3690/3690 [==============================] - 2s 578us/step - loss: nan - val_loss: nan\n",
      "Epoch 24/24\n",
      "3690/3690 [==============================] - 2s 576us/step - loss: nan - val_loss: nan\n",
      "Train on 3690 samples, validate on 410 samples\n",
      "Epoch 1/24\n",
      "3690/3690 [==============================] - 17s 5ms/step - loss: nan - val_loss: nan\n",
      "Epoch 2/24\n",
      "3690/3690 [==============================] - 2s 578us/step - loss: nan - val_loss: nan\n",
      "Epoch 3/24\n",
      "3690/3690 [==============================] - 2s 583us/step - loss: nan - val_loss: nan\n",
      "Epoch 4/24\n",
      "3690/3690 [==============================] - 2s 581us/step - loss: nan - val_loss: nan\n",
      "Epoch 5/24\n",
      "3690/3690 [==============================] - 2s 583us/step - loss: nan - val_loss: nan\n",
      "Epoch 6/24\n",
      "3690/3690 [==============================] - 2s 578us/step - loss: nan - val_loss: nan\n",
      "Epoch 7/24\n",
      "3690/3690 [==============================] - 2s 578us/step - loss: nan - val_loss: nan\n",
      "Epoch 8/24\n",
      "3690/3690 [==============================] - 2s 584us/step - loss: nan - val_loss: nan\n",
      "Epoch 9/24\n",
      "3690/3690 [==============================] - 2s 579us/step - loss: nan - val_loss: nan\n",
      "Epoch 10/24\n",
      "3690/3690 [==============================] - 2s 579us/step - loss: nan - val_loss: nan\n",
      "Epoch 11/24\n",
      "3690/3690 [==============================] - 2s 580us/step - loss: nan - val_loss: nan\n",
      "Epoch 12/24\n",
      "3690/3690 [==============================] - 2s 581us/step - loss: nan - val_loss: nan\n",
      "Epoch 13/24\n",
      "3690/3690 [==============================] - 2s 580us/step - loss: nan - val_loss: nan\n",
      "Epoch 14/24\n",
      "3690/3690 [==============================] - 2s 584us/step - loss: nan - val_loss: nan\n",
      "Epoch 15/24\n",
      "3690/3690 [==============================] - 2s 581us/step - loss: nan - val_loss: nan\n",
      "Epoch 16/24\n",
      "3690/3690 [==============================] - 2s 580us/step - loss: nan - val_loss: nan\n",
      "Epoch 17/24\n",
      "3690/3690 [==============================] - 2s 583us/step - loss: nan - val_loss: nan\n",
      "Epoch 18/24\n",
      "3690/3690 [==============================] - 2s 582us/step - loss: nan - val_loss: nan\n",
      "Epoch 19/24\n",
      "3690/3690 [==============================] - 2s 582us/step - loss: nan - val_loss: nan\n",
      "Epoch 20/24\n",
      "3690/3690 [==============================] - 2s 581us/step - loss: nan - val_loss: nan\n",
      "Epoch 21/24\n",
      "3690/3690 [==============================] - 2s 582us/step - loss: nan - val_loss: nan\n",
      "Epoch 22/24\n",
      "3690/3690 [==============================] - 2s 584us/step - loss: nan - val_loss: nan\n",
      "Epoch 23/24\n",
      "3690/3690 [==============================] - 2s 586us/step - loss: nan - val_loss: nan\n",
      "Epoch 24/24\n",
      "3690/3690 [==============================] - 2s 587us/step - loss: nan - val_loss: nan\n",
      "Train on 3690 samples, validate on 410 samples\n",
      "Epoch 1/24\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3690/3690 [==============================] - 18s 5ms/step - loss: nan - val_loss: nan\n",
      "Epoch 2/24\n",
      "3690/3690 [==============================] - 2s 578us/step - loss: nan - val_loss: nan\n",
      "Epoch 3/24\n",
      "3690/3690 [==============================] - 2s 579us/step - loss: nan - val_loss: nan\n",
      "Epoch 4/24\n",
      "3690/3690 [==============================] - 2s 574us/step - loss: nan - val_loss: nan\n",
      "Epoch 5/24\n",
      "3690/3690 [==============================] - 2s 578us/step - loss: nan - val_loss: nan\n",
      "Epoch 6/24\n",
      "3690/3690 [==============================] - 2s 578us/step - loss: nan - val_loss: nan\n",
      "Epoch 7/24\n",
      "3690/3690 [==============================] - 2s 577us/step - loss: nan - val_loss: nan\n",
      "Epoch 8/24\n",
      "3690/3690 [==============================] - 2s 578us/step - loss: nan - val_loss: nan\n",
      "Epoch 9/24\n",
      "3690/3690 [==============================] - 2s 578us/step - loss: nan - val_loss: nan\n",
      "Epoch 10/24\n",
      "3690/3690 [==============================] - 2s 580us/step - loss: nan - val_loss: nan\n",
      "Epoch 11/24\n",
      "3690/3690 [==============================] - 2s 578us/step - loss: nan - val_loss: nan\n",
      "Epoch 12/24\n",
      "3690/3690 [==============================] - 2s 578us/step - loss: nan - val_loss: nan\n",
      "Epoch 13/24\n",
      "3690/3690 [==============================] - 2s 579us/step - loss: nan - val_loss: nan\n",
      "Epoch 14/24\n",
      "3690/3690 [==============================] - 2s 582us/step - loss: nan - val_loss: nan\n",
      "Epoch 15/24\n",
      "3690/3690 [==============================] - 2s 575us/step - loss: nan - val_loss: nan\n",
      "Epoch 16/24\n",
      "3690/3690 [==============================] - 2s 575us/step - loss: nan - val_loss: nan\n",
      "Epoch 17/24\n",
      "3690/3690 [==============================] - 2s 579us/step - loss: nan - val_loss: nan\n",
      "Epoch 18/24\n",
      "3690/3690 [==============================] - 2s 577us/step - loss: nan - val_loss: nan\n",
      "Epoch 19/24\n",
      "3690/3690 [==============================] - 2s 579us/step - loss: nan - val_loss: nan\n",
      "Epoch 20/24\n",
      "3690/3690 [==============================] - 2s 576us/step - loss: nan - val_loss: nan\n",
      "Epoch 21/24\n",
      "3690/3690 [==============================] - 2s 582us/step - loss: nan - val_loss: nan\n",
      "Epoch 22/24\n",
      "3690/3690 [==============================] - 2s 577us/step - loss: nan - val_loss: nan\n",
      "Epoch 23/24\n",
      "3690/3690 [==============================] - 2s 579us/step - loss: nan - val_loss: nan\n",
      "Epoch 24/24\n",
      "3690/3690 [==============================] - 2s 577us/step - loss: nan - val_loss: nan\n",
      "Train on 3690 samples, validate on 410 samples\n",
      "Epoch 1/24\n",
      "3690/3690 [==============================] - 18s 5ms/step - loss: nan - val_loss: nan\n",
      "Epoch 2/24\n",
      "3690/3690 [==============================] - 2s 581us/step - loss: nan - val_loss: nan\n",
      "Epoch 3/24\n",
      "3690/3690 [==============================] - 2s 579us/step - loss: nan - val_loss: nan\n",
      "Epoch 4/24\n",
      "3690/3690 [==============================] - 2s 581us/step - loss: nan - val_loss: nan\n",
      "Epoch 5/24\n",
      "3690/3690 [==============================] - 2s 583us/step - loss: nan - val_loss: nan\n",
      "Epoch 6/24\n",
      "3690/3690 [==============================] - 2s 582us/step - loss: nan - val_loss: nan\n",
      "Epoch 7/24\n",
      "3690/3690 [==============================] - 2s 582us/step - loss: nan - val_loss: nan\n",
      "Epoch 8/24\n",
      "3690/3690 [==============================] - 2s 583us/step - loss: nan - val_loss: nan\n",
      "Epoch 9/24\n",
      "3690/3690 [==============================] - 2s 582us/step - loss: nan - val_loss: nan\n",
      "Epoch 10/24\n",
      "3690/3690 [==============================] - 2s 581us/step - loss: nan - val_loss: nan\n",
      "Epoch 11/24\n",
      "3690/3690 [==============================] - 2s 582us/step - loss: nan - val_loss: nan\n",
      "Epoch 12/24\n",
      "3690/3690 [==============================] - 2s 580us/step - loss: nan - val_loss: nan\n",
      "Epoch 13/24\n",
      "3690/3690 [==============================] - 2s 584us/step - loss: nan - val_loss: nan\n",
      "Epoch 14/24\n",
      "3690/3690 [==============================] - 2s 584us/step - loss: nan - val_loss: nan\n",
      "Epoch 15/24\n",
      "3690/3690 [==============================] - 2s 579us/step - loss: nan - val_loss: nan\n",
      "Epoch 16/24\n",
      "3690/3690 [==============================] - 2s 582us/step - loss: nan - val_loss: nan\n",
      "Epoch 17/24\n",
      "3690/3690 [==============================] - 2s 581us/step - loss: nan - val_loss: nan\n",
      "Epoch 18/24\n",
      "3690/3690 [==============================] - 2s 579us/step - loss: nan - val_loss: nan\n",
      "Epoch 19/24\n",
      "3690/3690 [==============================] - 2s 582us/step - loss: nan - val_loss: nan\n",
      "Epoch 20/24\n",
      "3690/3690 [==============================] - 2s 580us/step - loss: nan - val_loss: nan\n",
      "Epoch 21/24\n",
      "3690/3690 [==============================] - 2s 583us/step - loss: nan - val_loss: nan\n",
      "Epoch 22/24\n",
      "3690/3690 [==============================] - 2s 579us/step - loss: nan - val_loss: nan\n",
      "Epoch 23/24\n",
      "3690/3690 [==============================] - 2s 593us/step - loss: nan - val_loss: nan\n",
      "Epoch 24/24\n",
      "3690/3690 [==============================] - 2s 588us/step - loss: nan - val_loss: nan\n",
      "\n",
      "GENERATION 11\n",
      "\n",
      "Train on 3690 samples, validate on 410 samples\n",
      "Epoch 1/24\n",
      "3690/3690 [==============================] - 18s 5ms/step - loss: 0.0552 - val_loss: 0.0143\n",
      "Epoch 2/24\n",
      "3690/3690 [==============================] - 2s 604us/step - loss: 0.0109 - val_loss: 6.9372e-04\n",
      "Epoch 3/24\n",
      "3690/3690 [==============================] - 2s 605us/step - loss: 0.0056 - val_loss: 0.0014\n",
      "Epoch 4/24\n",
      "3690/3690 [==============================] - 2s 604us/step - loss: 0.0028 - val_loss: 0.0048\n",
      "Epoch 5/24\n",
      "3690/3690 [==============================] - 2s 602us/step - loss: 0.0016 - val_loss: 0.0032\n",
      "Epoch 6/24\n",
      "3690/3690 [==============================] - 2s 606us/step - loss: 0.0013 - val_loss: 0.0019\n",
      "Epoch 7/24\n",
      "3690/3690 [==============================] - 2s 608us/step - loss: 0.0012 - val_loss: 0.0024\n",
      "Epoch 8/24\n",
      "3690/3690 [==============================] - 2s 608us/step - loss: 0.0011 - val_loss: 0.0021\n",
      "Epoch 9/24\n",
      "3690/3690 [==============================] - 2s 609us/step - loss: 0.0010 - val_loss: 0.0019\n",
      "Epoch 10/24\n",
      "3690/3690 [==============================] - 2s 605us/step - loss: 0.0010 - val_loss: 0.0022\n",
      "Epoch 11/24\n",
      "3690/3690 [==============================] - 2s 606us/step - loss: 9.7891e-04 - val_loss: 0.0018\n",
      "Epoch 12/24\n",
      "3690/3690 [==============================] - 2s 606us/step - loss: 9.3992e-04 - val_loss: 0.0015\n",
      "Epoch 13/24\n",
      "3690/3690 [==============================] - 2s 607us/step - loss: 9.3553e-04 - val_loss: 0.0018\n",
      "Epoch 14/24\n",
      "3690/3690 [==============================] - 2s 605us/step - loss: 9.1252e-04 - val_loss: 9.9249e-04\n",
      "Epoch 15/24\n",
      "3690/3690 [==============================] - 2s 606us/step - loss: 8.5371e-04 - val_loss: 8.2678e-04\n",
      "Epoch 16/24\n",
      "3690/3690 [==============================] - 2s 604us/step - loss: 8.5436e-04 - val_loss: 9.9469e-04\n",
      "Epoch 17/24\n",
      "3690/3690 [==============================] - 2s 607us/step - loss: 8.2405e-04 - val_loss: 0.0010\n",
      "Epoch 18/24\n",
      "3690/3690 [==============================] - 2s 604us/step - loss: 8.0956e-04 - val_loss: 6.5096e-04\n",
      "Epoch 19/24\n",
      "3690/3690 [==============================] - 2s 604us/step - loss: 8.3894e-04 - val_loss: 9.5578e-04\n",
      "Epoch 20/24\n",
      "3690/3690 [==============================] - 2s 605us/step - loss: 8.0107e-04 - val_loss: 0.0010\n",
      "Epoch 21/24\n",
      "3690/3690 [==============================] - 2s 605us/step - loss: 7.8062e-04 - val_loss: 5.9832e-04\n",
      "Epoch 22/24\n",
      "3690/3690 [==============================] - 2s 607us/step - loss: 7.5968e-04 - val_loss: 0.0011\n",
      "Epoch 23/24\n",
      "3690/3690 [==============================] - 2s 604us/step - loss: 8.0022e-04 - val_loss: 0.0011\n",
      "Epoch 24/24\n",
      "3690/3690 [==============================] - 2s 605us/step - loss: 7.9764e-04 - val_loss: 0.0012\n",
      "Train on 3690 samples, validate on 410 samples\n",
      "Epoch 1/24\n",
      "3690/3690 [==============================] - 18s 5ms/step - loss: 0.0512 - val_loss: 0.0162\n",
      "Epoch 2/24\n",
      "3690/3690 [==============================] - 2s 608us/step - loss: 0.0100 - val_loss: 0.0033\n",
      "Epoch 3/24\n",
      "3690/3690 [==============================] - 2s 605us/step - loss: 0.0047 - val_loss: 0.0110\n",
      "Epoch 4/24\n",
      "3690/3690 [==============================] - 2s 603us/step - loss: 0.0030 - val_loss: 0.0034\n",
      "Epoch 5/24\n",
      "3690/3690 [==============================] - 2s 606us/step - loss: 0.0020 - val_loss: 0.0010\n",
      "Epoch 6/24\n",
      "3690/3690 [==============================] - 2s 606us/step - loss: 0.0015 - val_loss: 6.8982e-04\n",
      "Epoch 7/24\n",
      "3690/3690 [==============================] - 2s 604us/step - loss: 0.0012 - val_loss: 5.4142e-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/24\n",
      "3690/3690 [==============================] - 2s 605us/step - loss: 0.0012 - val_loss: 5.4277e-04\n",
      "Epoch 9/24\n",
      "3690/3690 [==============================] - 2s 605us/step - loss: 0.0011 - val_loss: 6.2035e-04\n",
      "Epoch 10/24\n",
      "3690/3690 [==============================] - 2s 604us/step - loss: 0.0010 - val_loss: 6.2512e-04\n",
      "Epoch 11/24\n",
      "3690/3690 [==============================] - 2s 605us/step - loss: 9.9375e-04 - val_loss: 0.0013\n",
      "Epoch 12/24\n",
      "3690/3690 [==============================] - 2s 606us/step - loss: 9.8392e-04 - val_loss: 0.0016\n",
      "Epoch 13/24\n",
      "3690/3690 [==============================] - 2s 606us/step - loss: 0.0010 - val_loss: 0.0023\n",
      "Epoch 14/24\n",
      "3690/3690 [==============================] - 2s 605us/step - loss: 0.0010 - val_loss: 0.0012\n",
      "Epoch 15/24\n",
      "3690/3690 [==============================] - 2s 607us/step - loss: 9.0524e-04 - val_loss: 8.0679e-04\n",
      "Epoch 16/24\n",
      "3690/3690 [==============================] - 2s 609us/step - loss: 8.6341e-04 - val_loss: 6.6562e-04\n",
      "Epoch 17/24\n",
      "3690/3690 [==============================] - 2s 608us/step - loss: 8.8214e-04 - val_loss: 7.1509e-04\n",
      "Epoch 18/24\n",
      "3690/3690 [==============================] - 2s 605us/step - loss: 8.6153e-04 - val_loss: 6.6729e-04\n",
      "Epoch 19/24\n",
      "3690/3690 [==============================] - 2s 605us/step - loss: 8.1870e-04 - val_loss: 8.1556e-04\n",
      "Epoch 20/24\n",
      "3690/3690 [==============================] - 2s 604us/step - loss: 7.8980e-04 - val_loss: 7.9196e-04\n",
      "Epoch 21/24\n",
      "3690/3690 [==============================] - 2s 612us/step - loss: 7.5601e-04 - val_loss: 0.0017\n",
      "Epoch 22/24\n",
      "3690/3690 [==============================] - 2s 612us/step - loss: 8.2960e-04 - val_loss: 0.0012\n",
      "Epoch 23/24\n",
      "3690/3690 [==============================] - 2s 609us/step - loss: 8.0699e-04 - val_loss: 4.6896e-04\n",
      "Epoch 24/24\n",
      "3690/3690 [==============================] - 2s 606us/step - loss: 8.5222e-04 - val_loss: 0.0014\n",
      "Train on 3690 samples, validate on 410 samples\n",
      "Epoch 1/24\n",
      "3690/3690 [==============================] - 19s 5ms/step - loss: 0.0816 - val_loss: 0.0017\n",
      "Epoch 2/24\n",
      "3690/3690 [==============================] - 2s 517us/step - loss: 0.0108 - val_loss: 0.0069\n",
      "Epoch 3/24\n",
      "3690/3690 [==============================] - 2s 521us/step - loss: 0.0062 - val_loss: 0.0074\n",
      "Epoch 4/24\n",
      "3690/3690 [==============================] - 2s 516us/step - loss: 0.0036 - val_loss: 0.0037\n",
      "Epoch 5/24\n",
      "3690/3690 [==============================] - 2s 515us/step - loss: 0.0028 - val_loss: 5.9392e-04\n",
      "Epoch 6/24\n",
      "3690/3690 [==============================] - 2s 516us/step - loss: 0.0022 - val_loss: 0.0014\n",
      "Epoch 7/24\n",
      "3690/3690 [==============================] - 2s 518us/step - loss: 0.0020 - val_loss: 0.0020\n",
      "Epoch 8/24\n",
      "3690/3690 [==============================] - 2s 515us/step - loss: 0.0019 - val_loss: 0.0012\n",
      "Epoch 9/24\n",
      "3690/3690 [==============================] - 2s 517us/step - loss: 0.0017 - val_loss: 9.9561e-04\n",
      "Epoch 10/24\n",
      "3690/3690 [==============================] - 2s 517us/step - loss: 0.0017 - val_loss: 0.0013\n",
      "Epoch 11/24\n",
      "3690/3690 [==============================] - 2s 520us/step - loss: 0.0016 - val_loss: 0.0011\n",
      "Epoch 12/24\n",
      "3690/3690 [==============================] - 2s 519us/step - loss: 0.0015 - val_loss: 0.0013\n",
      "Epoch 13/24\n",
      "3690/3690 [==============================] - 2s 517us/step - loss: 0.0015 - val_loss: 9.5712e-04\n",
      "Epoch 14/24\n",
      "3690/3690 [==============================] - 2s 518us/step - loss: 0.0014 - val_loss: 9.4095e-04\n",
      "Epoch 15/24\n",
      "3690/3690 [==============================] - 2s 518us/step - loss: 0.0013 - val_loss: 7.9425e-04\n",
      "Epoch 16/24\n",
      "3690/3690 [==============================] - 2s 517us/step - loss: 0.0013 - val_loss: 5.9340e-04\n",
      "Epoch 17/24\n",
      "3690/3690 [==============================] - 2s 518us/step - loss: 0.0013 - val_loss: 7.3338e-04\n",
      "Epoch 18/24\n",
      "3690/3690 [==============================] - 2s 516us/step - loss: 0.0013 - val_loss: 6.2771e-04\n",
      "Epoch 19/24\n",
      "3690/3690 [==============================] - 2s 523us/step - loss: 0.0012 - val_loss: 7.7521e-04\n",
      "Epoch 20/24\n",
      "3690/3690 [==============================] - 2s 518us/step - loss: 0.0012 - val_loss: 7.5227e-04\n",
      "Epoch 21/24\n",
      "3690/3690 [==============================] - 2s 518us/step - loss: 0.0012 - val_loss: 7.3747e-04\n",
      "Epoch 22/24\n",
      "3690/3690 [==============================] - 2s 520us/step - loss: 0.0011 - val_loss: 0.0012\n",
      "Epoch 23/24\n",
      "3690/3690 [==============================] - 2s 517us/step - loss: 0.0011 - val_loss: 8.9855e-04\n",
      "Epoch 24/24\n",
      "3690/3690 [==============================] - 2s 520us/step - loss: 0.0010 - val_loss: 0.0010\n",
      "Train on 3690 samples, validate on 410 samples\n",
      "Epoch 1/24\n",
      "3690/3690 [==============================] - 19s 5ms/step - loss: 0.0513 - val_loss: 0.0051\n",
      "Epoch 2/24\n",
      "3690/3690 [==============================] - 2s 519us/step - loss: 0.0070 - val_loss: 5.5968e-04\n",
      "Epoch 3/24\n",
      "3690/3690 [==============================] - 2s 519us/step - loss: 0.0035 - val_loss: 7.7107e-04\n",
      "Epoch 4/24\n",
      "3690/3690 [==============================] - 2s 519us/step - loss: 0.0028 - val_loss: 0.0022\n",
      "Epoch 5/24\n",
      "3690/3690 [==============================] - 2s 518us/step - loss: 0.0019 - val_loss: 0.0032\n",
      "Epoch 6/24\n",
      "3690/3690 [==============================] - 2s 518us/step - loss: 0.0017 - val_loss: 0.0014\n",
      "Epoch 7/24\n",
      "3690/3690 [==============================] - 2s 522us/step - loss: 0.0015 - val_loss: 5.2611e-04\n",
      "Epoch 8/24\n",
      "3690/3690 [==============================] - 2s 519us/step - loss: 0.0013 - val_loss: 5.5925e-04\n",
      "Epoch 9/24\n",
      "3690/3690 [==============================] - 2s 518us/step - loss: 0.0012 - val_loss: 5.4377e-04\n",
      "Epoch 10/24\n",
      "3690/3690 [==============================] - 2s 521us/step - loss: 0.0011 - val_loss: 6.5095e-04\n",
      "Epoch 11/24\n",
      "3690/3690 [==============================] - 2s 515us/step - loss: 0.0011 - val_loss: 5.4654e-04\n",
      "Epoch 12/24\n",
      "3690/3690 [==============================] - 2s 518us/step - loss: 0.0011 - val_loss: 5.5762e-04\n",
      "Epoch 13/24\n",
      "3690/3690 [==============================] - 2s 520us/step - loss: 0.0010 - val_loss: 8.9851e-04\n",
      "Epoch 14/24\n",
      "3690/3690 [==============================] - 2s 516us/step - loss: 0.0010 - val_loss: 7.4527e-04\n",
      "Epoch 15/24\n",
      "3690/3690 [==============================] - 2s 522us/step - loss: 9.9155e-04 - val_loss: 4.9546e-04\n",
      "Epoch 16/24\n",
      "3690/3690 [==============================] - 2s 520us/step - loss: 9.8425e-04 - val_loss: 5.2635e-04\n",
      "Epoch 17/24\n",
      "3690/3690 [==============================] - 2s 518us/step - loss: 9.1883e-04 - val_loss: 5.5083e-04\n",
      "Epoch 18/24\n",
      "3690/3690 [==============================] - 2s 520us/step - loss: 8.8737e-04 - val_loss: 5.6765e-04\n",
      "Epoch 19/24\n",
      "3690/3690 [==============================] - 2s 519us/step - loss: 8.5409e-04 - val_loss: 5.3959e-04\n",
      "Epoch 20/24\n",
      "3690/3690 [==============================] - 2s 518us/step - loss: 8.8078e-04 - val_loss: 4.5715e-04\n",
      "Epoch 21/24\n",
      "3690/3690 [==============================] - 2s 518us/step - loss: 8.7889e-04 - val_loss: 6.3533e-04\n",
      "Epoch 22/24\n",
      "3690/3690 [==============================] - 2s 519us/step - loss: 8.0676e-04 - val_loss: 4.0071e-04\n",
      "Epoch 23/24\n",
      "3690/3690 [==============================] - 2s 516us/step - loss: 8.9705e-04 - val_loss: 0.0011\n",
      "Epoch 24/24\n",
      "3690/3690 [==============================] - 2s 521us/step - loss: 8.8352e-04 - val_loss: 5.4293e-04\n",
      "Train on 3690 samples, validate on 410 samples\n",
      "Epoch 1/24\n",
      "3690/3690 [==============================] - 19s 5ms/step - loss: 0.0783 - val_loss: 0.0712\n",
      "Epoch 2/24\n",
      "3690/3690 [==============================] - 2s 447us/step - loss: 0.0718 - val_loss: 0.0383\n",
      "Epoch 3/24\n",
      "3690/3690 [==============================] - 2s 451us/step - loss: 0.0654 - val_loss: 0.0219\n",
      "Epoch 4/24\n",
      "3690/3690 [==============================] - 2s 449us/step - loss: 0.0562 - val_loss: 0.0223\n",
      "Epoch 5/24\n",
      "3690/3690 [==============================] - 2s 445us/step - loss: 0.0445 - val_loss: 0.0488\n",
      "Epoch 6/24\n",
      "3690/3690 [==============================] - 2s 445us/step - loss: 0.0486 - val_loss: 0.0468\n",
      "Epoch 7/24\n",
      "3690/3690 [==============================] - 2s 445us/step - loss: 0.0614 - val_loss: 0.0374\n",
      "Epoch 8/24\n",
      "3690/3690 [==============================] - 2s 445us/step - loss: 0.0665 - val_loss: 0.0413\n",
      "Epoch 9/24\n",
      "3690/3690 [==============================] - 2s 449us/step - loss: 0.0674 - val_loss: 0.0375\n",
      "Epoch 10/24\n",
      "3690/3690 [==============================] - 2s 447us/step - loss: 0.0672 - val_loss: 0.0382\n",
      "Epoch 11/24\n",
      "3690/3690 [==============================] - 2s 447us/step - loss: 0.0655 - val_loss: 0.0305\n",
      "Epoch 12/24\n",
      "3690/3690 [==============================] - 2s 447us/step - loss: 0.0588 - val_loss: 0.0093\n",
      "Epoch 13/24\n",
      "3690/3690 [==============================] - 2s 445us/step - loss: 0.0703 - val_loss: 0.0566\n",
      "Epoch 14/24\n",
      "3690/3690 [==============================] - 2s 444us/step - loss: 0.0568 - val_loss: 0.0244\n",
      "Epoch 15/24\n",
      "3690/3690 [==============================] - 2s 446us/step - loss: 0.0187 - val_loss: 0.0383\n",
      "Epoch 16/24\n",
      "3690/3690 [==============================] - 2s 446us/step - loss: 0.0124 - val_loss: 0.0567\n",
      "Epoch 17/24\n",
      "3690/3690 [==============================] - 2s 445us/step - loss: 0.0893 - val_loss: 0.0071\n",
      "Epoch 18/24\n",
      "3690/3690 [==============================] - 2s 444us/step - loss: 0.0852 - val_loss: 0.1198\n",
      "Epoch 19/24\n",
      "3690/3690 [==============================] - 2s 443us/step - loss: 0.0759 - val_loss: 0.0187\n",
      "Epoch 20/24\n",
      "3690/3690 [==============================] - 2s 443us/step - loss: 0.0735 - val_loss: 0.0273\n",
      "Epoch 21/24\n",
      "3690/3690 [==============================] - 2s 447us/step - loss: 0.0689 - val_loss: 0.0554\n",
      "Epoch 22/24\n",
      "3690/3690 [==============================] - 2s 447us/step - loss: 0.0693 - val_loss: 0.0402\n",
      "Epoch 23/24\n",
      "3690/3690 [==============================] - 2s 442us/step - loss: 0.0683 - val_loss: 0.0326\n",
      "Epoch 24/24\n",
      "3690/3690 [==============================] - 2s 447us/step - loss: 0.0682 - val_loss: 0.0400\n",
      "Train on 3690 samples, validate on 410 samples\n",
      "Epoch 1/24\n",
      "3690/3690 [==============================] - 20s 5ms/step - loss: 2.2540 - val_loss: 2.1298\n",
      "Epoch 2/24\n",
      "3690/3690 [==============================] - 2s 545us/step - loss: 1.4310 - val_loss: 1.4004\n",
      "Epoch 3/24\n",
      "3690/3690 [==============================] - 2s 534us/step - loss: 0.9007 - val_loss: 0.9227\n",
      "Epoch 4/24\n",
      "3690/3690 [==============================] - 2s 536us/step - loss: 0.5640 - val_loss: 0.6027\n",
      "Epoch 5/24\n",
      "3690/3690 [==============================] - 2s 537us/step - loss: 0.3513 - val_loss: 0.3898\n",
      "Epoch 6/24\n",
      "3690/3690 [==============================] - 2s 536us/step - loss: 0.2204 - val_loss: 0.2514\n",
      "Epoch 7/24\n",
      "3690/3690 [==============================] - 2s 538us/step - loss: 0.1443 - val_loss: 0.1630\n",
      "Epoch 8/24\n",
      "3690/3690 [==============================] - 2s 536us/step - loss: 0.1025 - val_loss: 0.1081\n",
      "Epoch 9/24\n",
      "3690/3690 [==============================] - 2s 540us/step - loss: 0.0816 - val_loss: 0.0749\n",
      "Epoch 10/24\n",
      "3690/3690 [==============================] - 2s 538us/step - loss: 0.0726 - val_loss: 0.0558\n",
      "Epoch 11/24\n",
      "3690/3690 [==============================] - 2s 540us/step - loss: 0.0692 - val_loss: 0.0452\n",
      "Epoch 12/24\n",
      "3690/3690 [==============================] - 2s 540us/step - loss: 0.0683 - val_loss: 0.0394\n",
      "Epoch 13/24\n",
      "3690/3690 [==============================] - 2s 535us/step - loss: 0.0681 - val_loss: 0.0368\n",
      "Epoch 14/24\n",
      "3690/3690 [==============================] - 2s 539us/step - loss: 0.0682 - val_loss: 0.0359\n",
      "Epoch 15/24\n",
      "3690/3690 [==============================] - 2s 537us/step - loss: 0.0682 - val_loss: 0.0359\n",
      "Epoch 16/24\n",
      "3690/3690 [==============================] - 2s 540us/step - loss: 0.0682 - val_loss: 0.0364\n",
      "Epoch 17/24\n",
      "3690/3690 [==============================] - 2s 541us/step - loss: 0.0682 - val_loss: 0.0370\n",
      "Epoch 18/24\n",
      "3690/3690 [==============================] - 2s 538us/step - loss: 0.0682 - val_loss: 0.0379\n",
      "Epoch 19/24\n",
      "3690/3690 [==============================] - 2s 539us/step - loss: 0.0681 - val_loss: 0.0379\n",
      "Epoch 20/24\n",
      "3690/3690 [==============================] - 2s 537us/step - loss: 0.0681 - val_loss: 0.0382\n",
      "Epoch 21/24\n",
      "3690/3690 [==============================] - 2s 538us/step - loss: 0.0681 - val_loss: 0.0384\n",
      "Epoch 22/24\n",
      "3690/3690 [==============================] - 2s 535us/step - loss: 0.0681 - val_loss: 0.0386\n",
      "Epoch 23/24\n",
      "3690/3690 [==============================] - 2s 539us/step - loss: 0.0681 - val_loss: 0.0386\n",
      "Epoch 24/24\n",
      "3690/3690 [==============================] - 2s 537us/step - loss: 0.0681 - val_loss: 0.0387\n",
      "Train on 3690 samples, validate on 410 samples\n",
      "Epoch 1/24\n",
      "3690/3690 [==============================] - 21s 6ms/step - loss: 0.4476 - val_loss: 8.9818e-04\n",
      "Epoch 2/24\n",
      "3690/3690 [==============================] - 2s 559us/step - loss: 0.0059 - val_loss: 5.9939e-04\n",
      "Epoch 3/24\n",
      "3690/3690 [==============================] - 2s 566us/step - loss: 0.0036 - val_loss: 0.0060\n",
      "Epoch 4/24\n",
      "3690/3690 [==============================] - 2s 575us/step - loss: 0.0150 - val_loss: 0.0094\n",
      "Epoch 5/24\n",
      "3690/3690 [==============================] - 2s 562us/step - loss: 0.0139 - val_loss: 0.0147\n",
      "Epoch 6/24\n",
      "3690/3690 [==============================] - 2s 569us/step - loss: 0.0172 - val_loss: 0.0220\n",
      "Epoch 7/24\n",
      "3690/3690 [==============================] - 2s 565us/step - loss: 0.0152 - val_loss: 0.0183\n",
      "Epoch 8/24\n",
      "3690/3690 [==============================] - 2s 558us/step - loss: 0.0207 - val_loss: 0.0503\n",
      "Epoch 9/24\n",
      "3690/3690 [==============================] - 2s 549us/step - loss: 0.0210 - val_loss: 0.0012\n",
      "Epoch 10/24\n",
      "3690/3690 [==============================] - 2s 535us/step - loss: 0.0039 - val_loss: 0.0365\n",
      "Epoch 11/24\n",
      "3690/3690 [==============================] - 2s 546us/step - loss: 0.0182 - val_loss: 0.0104\n",
      "Epoch 12/24\n",
      "3690/3690 [==============================] - 2s 558us/step - loss: 0.0109 - val_loss: 0.0061\n",
      "Epoch 13/24\n",
      "3690/3690 [==============================] - 2s 544us/step - loss: 0.0098 - val_loss: 0.0245\n",
      "Epoch 14/24\n",
      "3690/3690 [==============================] - 2s 534us/step - loss: 0.0199 - val_loss: 0.0015\n",
      "Epoch 15/24\n",
      "3690/3690 [==============================] - 2s 538us/step - loss: 0.0023 - val_loss: 4.9285e-04\n",
      "Epoch 16/24\n",
      "3690/3690 [==============================] - 2s 540us/step - loss: 0.0129 - val_loss: 0.0067\n",
      "Epoch 17/24\n",
      "3690/3690 [==============================] - 2s 535us/step - loss: 0.0069 - val_loss: 4.2060e-04\n",
      "Epoch 18/24\n",
      "3690/3690 [==============================] - 2s 540us/step - loss: 0.0076 - val_loss: 0.0038\n",
      "Epoch 19/24\n",
      "3690/3690 [==============================] - 2s 541us/step - loss: 0.0069 - val_loss: 0.0094\n",
      "Epoch 20/24\n",
      "3690/3690 [==============================] - 2s 538us/step - loss: 0.0108 - val_loss: 4.4662e-04\n",
      "Epoch 21/24\n",
      "3690/3690 [==============================] - 2s 534us/step - loss: 0.0029 - val_loss: 0.0049\n",
      "Epoch 22/24\n",
      "3690/3690 [==============================] - 2s 538us/step - loss: 0.0086 - val_loss: 8.1341e-04\n",
      "Epoch 23/24\n",
      "3690/3690 [==============================] - 2s 537us/step - loss: 0.0043 - val_loss: 0.0030\n",
      "Epoch 24/24\n",
      "3690/3690 [==============================] - 2s 537us/step - loss: 0.0069 - val_loss: 0.0193\n",
      "Train on 3690 samples, validate on 410 samples\n",
      "Epoch 1/24\n",
      "3690/3690 [==============================] - 20s 6ms/step - loss: 0.1393 - val_loss: 0.0057\n",
      "Epoch 2/24\n",
      "3690/3690 [==============================] - 2s 539us/step - loss: 0.0185 - val_loss: 0.0093\n",
      "Epoch 3/24\n",
      "3690/3690 [==============================] - 2s 539us/step - loss: 0.0140 - val_loss: 0.0066\n",
      "Epoch 4/24\n",
      "3690/3690 [==============================] - 2s 542us/step - loss: 0.0121 - val_loss: 0.0136\n",
      "Epoch 5/24\n",
      "3690/3690 [==============================] - 2s 538us/step - loss: 0.0115 - val_loss: 0.0014\n",
      "Epoch 6/24\n",
      "3690/3690 [==============================] - 2s 538us/step - loss: 0.0104 - val_loss: 0.0072\n",
      "Epoch 7/24\n",
      "3690/3690 [==============================] - 2s 539us/step - loss: 0.0075 - val_loss: 0.0082\n",
      "Epoch 8/24\n",
      "3690/3690 [==============================] - 2s 538us/step - loss: 0.0104 - val_loss: 0.0039\n",
      "Epoch 9/24\n",
      "3690/3690 [==============================] - 2s 541us/step - loss: 0.0072 - val_loss: 0.0054\n",
      "Epoch 10/24\n",
      "3690/3690 [==============================] - 2s 537us/step - loss: 0.0067 - val_loss: 0.0059\n",
      "Epoch 11/24\n",
      "3690/3690 [==============================] - 2s 539us/step - loss: 0.0062 - val_loss: 0.0050\n",
      "Epoch 12/24\n",
      "3690/3690 [==============================] - 2s 542us/step - loss: 0.0072 - val_loss: 0.0017\n",
      "Epoch 13/24\n",
      "3690/3690 [==============================] - 2s 542us/step - loss: 0.0045 - val_loss: 0.0205\n",
      "Epoch 14/24\n",
      "3690/3690 [==============================] - 2s 540us/step - loss: 0.0066 - val_loss: 9.5554e-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15/24\n",
      "3690/3690 [==============================] - 2s 539us/step - loss: 0.0052 - val_loss: 0.0076\n",
      "Epoch 16/24\n",
      "3690/3690 [==============================] - 2s 541us/step - loss: 0.0045 - val_loss: 0.0060\n",
      "Epoch 17/24\n",
      "3690/3690 [==============================] - 2s 539us/step - loss: 0.0049 - val_loss: 0.0086\n",
      "Epoch 18/24\n",
      "3690/3690 [==============================] - 2s 538us/step - loss: 0.0065 - val_loss: 0.0010\n",
      "Epoch 19/24\n",
      "3690/3690 [==============================] - 2s 536us/step - loss: 0.0042 - val_loss: 0.0011\n",
      "Epoch 20/24\n",
      "3690/3690 [==============================] - 2s 542us/step - loss: 0.0042 - val_loss: 0.0020\n",
      "Epoch 21/24\n",
      "3690/3690 [==============================] - 2s 538us/step - loss: 0.0037 - val_loss: 0.0082\n",
      "Epoch 22/24\n",
      "3690/3690 [==============================] - 2s 537us/step - loss: 0.0049 - val_loss: 0.0020\n",
      "Epoch 23/24\n",
      "3690/3690 [==============================] - 2s 538us/step - loss: 0.0041 - val_loss: 0.0014\n",
      "Epoch 24/24\n",
      "3690/3690 [==============================] - 2s 539us/step - loss: 0.0037 - val_loss: 0.0046\n",
      "Train on 3690 samples, validate on 410 samples\n",
      "Epoch 1/24\n",
      "3690/3690 [==============================] - 20s 5ms/step - loss: 0.1427 - val_loss: 0.0072\n",
      "Epoch 2/24\n",
      "3690/3690 [==============================] - 2s 538us/step - loss: 0.0073 - val_loss: 0.0047\n",
      "Epoch 3/24\n",
      "3690/3690 [==============================] - 2s 539us/step - loss: 0.0055 - val_loss: 0.0087\n",
      "Epoch 4/24\n",
      "3690/3690 [==============================] - 2s 535us/step - loss: 0.0046 - val_loss: 0.0086\n",
      "Epoch 5/24\n",
      "3690/3690 [==============================] - 2s 539us/step - loss: 0.0028 - val_loss: 0.0047\n",
      "Epoch 6/24\n",
      "3690/3690 [==============================] - 2s 536us/step - loss: 0.0021 - val_loss: 0.0070\n",
      "Epoch 7/24\n",
      "3690/3690 [==============================] - 2s 541us/step - loss: 0.0054 - val_loss: 0.0146\n",
      "Epoch 8/24\n",
      "3690/3690 [==============================] - 2s 541us/step - loss: 0.0048 - val_loss: 0.0036\n",
      "Epoch 9/24\n",
      "3690/3690 [==============================] - 2s 536us/step - loss: 0.0015 - val_loss: 0.0024\n",
      "Epoch 10/24\n",
      "3690/3690 [==============================] - 2s 538us/step - loss: 0.0017 - val_loss: 0.0016\n",
      "Epoch 11/24\n",
      "3690/3690 [==============================] - 2s 537us/step - loss: 0.0016 - val_loss: 0.0091\n",
      "Epoch 12/24\n",
      "3690/3690 [==============================] - 2s 535us/step - loss: 0.0050 - val_loss: 0.0051\n",
      "Epoch 13/24\n",
      "3690/3690 [==============================] - 2s 538us/step - loss: 0.0018 - val_loss: 0.0034\n",
      "Epoch 14/24\n",
      "3690/3690 [==============================] - 2s 538us/step - loss: 0.0014 - val_loss: 0.0023\n",
      "Epoch 15/24\n",
      "3690/3690 [==============================] - 2s 536us/step - loss: 0.0021 - val_loss: 0.0064\n",
      "Epoch 16/24\n",
      "3690/3690 [==============================] - 2s 540us/step - loss: 0.0040 - val_loss: 0.0027\n",
      "Epoch 17/24\n",
      "3690/3690 [==============================] - 2s 540us/step - loss: 0.0021 - val_loss: 0.0051\n",
      "Epoch 18/24\n",
      "3690/3690 [==============================] - 2s 537us/step - loss: 0.0035 - val_loss: 0.0019\n",
      "Epoch 19/24\n",
      "3690/3690 [==============================] - 2s 539us/step - loss: 0.0010 - val_loss: 5.8717e-04\n",
      "Epoch 20/24\n",
      "3690/3690 [==============================] - 2s 539us/step - loss: 0.0020 - val_loss: 0.0015\n",
      "Epoch 21/24\n",
      "3690/3690 [==============================] - 2s 534us/step - loss: 0.0025 - val_loss: 0.0013\n",
      "Epoch 22/24\n",
      "3690/3690 [==============================] - 2s 542us/step - loss: 0.0021 - val_loss: 7.5621e-04\n",
      "Epoch 23/24\n",
      "3690/3690 [==============================] - 2s 535us/step - loss: 0.0013 - val_loss: 0.0016\n",
      "Epoch 24/24\n",
      "3690/3690 [==============================] - 2s 539us/step - loss: 0.0031 - val_loss: 0.0010\n",
      "\n",
      "GENERATION 12\n",
      "\n",
      "Train on 3690 samples, validate on 410 samples\n",
      "Epoch 1/24\n",
      "3690/3690 [==============================] - 21s 6ms/step - loss: 0.5913 - val_loss: 0.0080\n",
      "Epoch 2/24\n",
      "3690/3690 [==============================] - 2s 425us/step - loss: 0.0916 - val_loss: 0.0200\n",
      "Epoch 3/24\n",
      "3690/3690 [==============================] - 2s 426us/step - loss: 0.0701 - val_loss: 0.0316\n",
      "Epoch 4/24\n",
      "3690/3690 [==============================] - 2s 425us/step - loss: 0.0685 - val_loss: 0.0352\n",
      "Epoch 5/24\n",
      "3690/3690 [==============================] - 2s 427us/step - loss: 0.0682 - val_loss: 0.0371\n",
      "Epoch 6/24\n",
      "3690/3690 [==============================] - 2s 423us/step - loss: 0.0681 - val_loss: 0.0412\n",
      "Epoch 7/24\n",
      "3690/3690 [==============================] - 2s 427us/step - loss: 0.0682 - val_loss: 0.0400\n",
      "Epoch 8/24\n",
      "3690/3690 [==============================] - 2s 426us/step - loss: 0.0681 - val_loss: 0.0367\n",
      "Epoch 9/24\n",
      "3690/3690 [==============================] - 2s 431us/step - loss: 0.0682 - val_loss: 0.0385\n",
      "Epoch 10/24\n",
      "3690/3690 [==============================] - 2s 424us/step - loss: 0.0682 - val_loss: 0.0397\n",
      "Epoch 11/24\n",
      "3690/3690 [==============================] - 2s 427us/step - loss: 0.0681 - val_loss: 0.0395\n",
      "Epoch 12/24\n",
      "3690/3690 [==============================] - 2s 426us/step - loss: 0.0682 - val_loss: 0.0402\n",
      "Epoch 13/24\n",
      "3690/3690 [==============================] - 2s 428us/step - loss: 0.0682 - val_loss: 0.0387\n",
      "Epoch 14/24\n",
      "3690/3690 [==============================] - 2s 425us/step - loss: 0.0681 - val_loss: 0.0392\n",
      "Epoch 15/24\n",
      "3690/3690 [==============================] - 2s 427us/step - loss: 0.0681 - val_loss: 0.0395\n",
      "Epoch 16/24\n",
      "3690/3690 [==============================] - 2s 430us/step - loss: 0.0681 - val_loss: 0.0396\n",
      "Epoch 17/24\n",
      "3690/3690 [==============================] - 2s 427us/step - loss: 0.0681 - val_loss: 0.0418\n",
      "Epoch 18/24\n",
      "3690/3690 [==============================] - 2s 427us/step - loss: 0.0681 - val_loss: 0.0381\n",
      "Epoch 19/24\n",
      "3690/3690 [==============================] - 2s 439us/step - loss: 0.0681 - val_loss: 0.0410\n",
      "Epoch 20/24\n",
      "3690/3690 [==============================] - 2s 438us/step - loss: 0.0681 - val_loss: 0.0429\n",
      "Epoch 21/24\n",
      "3690/3690 [==============================] - 2s 431us/step - loss: 0.0681 - val_loss: 0.0407\n",
      "Epoch 22/24\n",
      "3690/3690 [==============================] - 2s 423us/step - loss: 0.0679 - val_loss: 0.0388\n",
      "Epoch 23/24\n",
      "3690/3690 [==============================] - 2s 425us/step - loss: 0.0681 - val_loss: 0.0386\n",
      "Epoch 24/24\n",
      "3690/3690 [==============================] - 2s 431us/step - loss: 0.0678 - val_loss: 0.0377\n",
      "Train on 3690 samples, validate on 410 samples\n",
      "Epoch 1/24\n",
      "3690/3690 [==============================] - 21s 6ms/step - loss: 0.0563 - val_loss: 0.0030\n",
      "Epoch 2/24\n",
      "3690/3690 [==============================] - 2s 523us/step - loss: 0.0066 - val_loss: 0.0018\n",
      "Epoch 3/24\n",
      "3690/3690 [==============================] - 2s 520us/step - loss: 0.0036 - val_loss: 5.1136e-04\n",
      "Epoch 4/24\n",
      "3690/3690 [==============================] - 2s 522us/step - loss: 0.0025 - val_loss: 0.0049\n",
      "Epoch 5/24\n",
      "3690/3690 [==============================] - 2s 522us/step - loss: 0.0021 - val_loss: 0.0016\n",
      "Epoch 6/24\n",
      "3690/3690 [==============================] - 2s 522us/step - loss: 0.0017 - val_loss: 5.9899e-04\n",
      "Epoch 7/24\n",
      "3690/3690 [==============================] - 2s 519us/step - loss: 0.0014 - val_loss: 5.4242e-04\n",
      "Epoch 8/24\n",
      "3690/3690 [==============================] - 2s 519us/step - loss: 0.0013 - val_loss: 9.2024e-04\n",
      "Epoch 9/24\n",
      "3690/3690 [==============================] - 2s 521us/step - loss: 0.0012 - val_loss: 0.0012\n",
      "Epoch 10/24\n",
      "3690/3690 [==============================] - 2s 522us/step - loss: 0.0011 - val_loss: 9.5283e-04\n",
      "Epoch 11/24\n",
      "3690/3690 [==============================] - 2s 520us/step - loss: 0.0011 - val_loss: 0.0011\n",
      "Epoch 12/24\n",
      "3690/3690 [==============================] - 2s 522us/step - loss: 0.0011 - val_loss: 0.0011\n",
      "Epoch 13/24\n",
      "3690/3690 [==============================] - 2s 520us/step - loss: 0.0011 - val_loss: 9.7724e-04\n",
      "Epoch 14/24\n",
      "3690/3690 [==============================] - 2s 524us/step - loss: 9.9749e-04 - val_loss: 9.6092e-04\n",
      "Epoch 15/24\n",
      "3690/3690 [==============================] - 2s 521us/step - loss: 9.8722e-04 - val_loss: 5.6044e-04\n",
      "Epoch 16/24\n",
      "3690/3690 [==============================] - 2s 520us/step - loss: 9.5752e-04 - val_loss: 4.0386e-04\n",
      "Epoch 17/24\n",
      "3690/3690 [==============================] - 2s 521us/step - loss: 8.8553e-04 - val_loss: 5.3849e-04\n",
      "Epoch 18/24\n",
      "3690/3690 [==============================] - 2s 523us/step - loss: 9.0773e-04 - val_loss: 7.5199e-04\n",
      "Epoch 19/24\n",
      "3690/3690 [==============================] - 2s 521us/step - loss: 8.7308e-04 - val_loss: 4.1714e-04\n",
      "Epoch 20/24\n",
      "3690/3690 [==============================] - 2s 522us/step - loss: 8.4422e-04 - val_loss: 4.1839e-04\n",
      "Epoch 21/24\n",
      "3690/3690 [==============================] - 2s 523us/step - loss: 8.2048e-04 - val_loss: 5.2629e-04\n",
      "Epoch 22/24\n",
      "3690/3690 [==============================] - 2s 524us/step - loss: 7.5509e-04 - val_loss: 4.0564e-04\n",
      "Epoch 23/24\n",
      "3690/3690 [==============================] - 2s 519us/step - loss: 7.8497e-04 - val_loss: 3.7924e-04\n",
      "Epoch 24/24\n",
      "3690/3690 [==============================] - 2s 520us/step - loss: 7.7521e-04 - val_loss: 4.7637e-04\n",
      "Train on 3690 samples, validate on 410 samples\n",
      "Epoch 1/24\n",
      "3690/3690 [==============================] - 22s 6ms/step - loss: 2.2146 - val_loss: 0.0200\n",
      "Epoch 2/24\n",
      "3690/3690 [==============================] - 2s 523us/step - loss: 0.1654 - val_loss: 0.0359\n",
      "Epoch 3/24\n",
      "3690/3690 [==============================] - 2s 520us/step - loss: 0.0878 - val_loss: 0.1017\n",
      "Epoch 4/24\n",
      "3690/3690 [==============================] - 2s 519us/step - loss: 0.0724 - val_loss: 0.0176\n",
      "Epoch 5/24\n",
      "3690/3690 [==============================] - 2s 523us/step - loss: 0.0742 - val_loss: 0.0281\n",
      "Epoch 6/24\n",
      "3690/3690 [==============================] - 2s 522us/step - loss: 0.0685 - val_loss: 0.0545\n",
      "Epoch 7/24\n",
      "3690/3690 [==============================] - 2s 521us/step - loss: 0.0695 - val_loss: 0.0429\n",
      "Epoch 8/24\n",
      "3690/3690 [==============================] - 2s 521us/step - loss: 0.0681 - val_loss: 0.0326\n",
      "Epoch 9/24\n",
      "3690/3690 [==============================] - 2s 522us/step - loss: 0.0682 - val_loss: 0.0393\n",
      "Epoch 10/24\n",
      "3690/3690 [==============================] - 2s 523us/step - loss: 0.0679 - val_loss: 0.0410\n",
      "Epoch 11/24\n",
      "3690/3690 [==============================] - 2s 526us/step - loss: 0.0675 - val_loss: 0.0366\n",
      "Epoch 12/24\n",
      "3690/3690 [==============================] - 2s 524us/step - loss: 0.0661 - val_loss: 0.0352\n",
      "Epoch 13/24\n",
      "3690/3690 [==============================] - 2s 522us/step - loss: 0.0641 - val_loss: 0.0309\n",
      "Epoch 14/24\n",
      "3690/3690 [==============================] - 2s 522us/step - loss: 0.0680 - val_loss: 0.0395\n",
      "Epoch 15/24\n",
      "3690/3690 [==============================] - 2s 522us/step - loss: 0.0658 - val_loss: 0.0380\n",
      "Epoch 16/24\n",
      "3690/3690 [==============================] - 2s 524us/step - loss: 0.0581 - val_loss: 0.0216\n",
      "Epoch 17/24\n",
      "3690/3690 [==============================] - 2s 522us/step - loss: 0.0428 - val_loss: 0.0058\n",
      "Epoch 18/24\n",
      "3690/3690 [==============================] - 2s 520us/step - loss: 0.0250 - val_loss: 0.0488\n",
      "Epoch 19/24\n",
      "3690/3690 [==============================] - 2s 524us/step - loss: 0.0256 - val_loss: 0.0974\n",
      "Epoch 20/24\n",
      "3690/3690 [==============================] - 2s 523us/step - loss: 0.0173 - val_loss: 0.0449\n",
      "Epoch 21/24\n",
      "3690/3690 [==============================] - 2s 521us/step - loss: 0.0133 - val_loss: 0.0497\n",
      "Epoch 22/24\n",
      "3690/3690 [==============================] - 2s 521us/step - loss: 0.0119 - val_loss: 0.0326\n",
      "Epoch 23/24\n",
      "3690/3690 [==============================] - 2s 526us/step - loss: 0.0089 - val_loss: 0.0462\n",
      "Epoch 24/24\n",
      "3690/3690 [==============================] - 2s 522us/step - loss: 0.0075 - val_loss: 0.0296\n",
      "Train on 3690 samples, validate on 410 samples\n",
      "Epoch 1/24\n",
      "3690/3690 [==============================] - 22s 6ms/step - loss: 1.1877 - val_loss: 0.2283\n",
      "Epoch 2/24\n",
      "3690/3690 [==============================] - 2s 519us/step - loss: 0.2032 - val_loss: 0.0744\n",
      "Epoch 3/24\n",
      "3690/3690 [==============================] - 2s 519us/step - loss: 0.0984 - val_loss: 0.1396\n",
      "Epoch 4/24\n",
      "3690/3690 [==============================] - 2s 524us/step - loss: 0.0859 - val_loss: 0.0455\n",
      "Epoch 5/24\n",
      "3690/3690 [==============================] - 2s 521us/step - loss: 0.0660 - val_loss: 0.0092\n",
      "Epoch 6/24\n",
      "3690/3690 [==============================] - 2s 522us/step - loss: 0.0573 - val_loss: 0.0027\n",
      "Epoch 7/24\n",
      "3690/3690 [==============================] - 2s 524us/step - loss: 0.0187 - val_loss: 0.0488\n",
      "Epoch 8/24\n",
      "3690/3690 [==============================] - 2s 517us/step - loss: 0.0119 - val_loss: 0.0740\n",
      "Epoch 9/24\n",
      "3690/3690 [==============================] - 2s 519us/step - loss: 0.0083 - val_loss: 0.0201\n",
      "Epoch 10/24\n",
      "3690/3690 [==============================] - 2s 522us/step - loss: 0.0052 - val_loss: 0.0268\n",
      "Epoch 11/24\n",
      "3690/3690 [==============================] - 2s 520us/step - loss: 0.0036 - val_loss: 0.0028\n",
      "Epoch 12/24\n",
      "3690/3690 [==============================] - 2s 519us/step - loss: 0.0024 - val_loss: 0.0045\n",
      "Epoch 13/24\n",
      "3690/3690 [==============================] - 2s 521us/step - loss: 0.0021 - val_loss: 0.0010\n",
      "Epoch 14/24\n",
      "3690/3690 [==============================] - 2s 518us/step - loss: 0.0019 - val_loss: 0.0012\n",
      "Epoch 15/24\n",
      "3690/3690 [==============================] - 2s 522us/step - loss: 0.0018 - val_loss: 0.0011\n",
      "Epoch 16/24\n",
      "3690/3690 [==============================] - 2s 523us/step - loss: 0.0017 - val_loss: 0.0018\n",
      "Epoch 17/24\n",
      "3690/3690 [==============================] - 2s 523us/step - loss: 0.0017 - val_loss: 0.0014\n",
      "Epoch 18/24\n",
      "3690/3690 [==============================] - 2s 520us/step - loss: 0.0015 - val_loss: 0.0010\n",
      "Epoch 19/24\n",
      "3690/3690 [==============================] - 2s 517us/step - loss: 0.0015 - val_loss: 0.0010\n",
      "Epoch 20/24\n",
      "3690/3690 [==============================] - 2s 525us/step - loss: 0.0014 - val_loss: 8.2909e-04\n",
      "Epoch 21/24\n",
      "3690/3690 [==============================] - 2s 519us/step - loss: 0.0014 - val_loss: 9.1586e-04\n",
      "Epoch 22/24\n",
      "3690/3690 [==============================] - 2s 522us/step - loss: 0.0014 - val_loss: 9.5512e-04\n",
      "Epoch 23/24\n",
      "3690/3690 [==============================] - 2s 524us/step - loss: 0.0013 - val_loss: 0.0014\n",
      "Epoch 24/24\n",
      "3690/3690 [==============================] - 2s 522us/step - loss: 0.0013 - val_loss: 0.0014\n",
      "Train on 3690 samples, validate on 410 samples\n",
      "Epoch 1/24\n",
      "3690/3690 [==============================] - 22s 6ms/step - loss: nan - val_loss: nan\n",
      "Epoch 2/24\n",
      "3690/3690 [==============================] - 2s 520us/step - loss: nan - val_loss: nan\n",
      "Epoch 3/24\n",
      "3690/3690 [==============================] - 2s 520us/step - loss: nan - val_loss: nan\n",
      "Epoch 4/24\n",
      "3690/3690 [==============================] - 2s 516us/step - loss: nan - val_loss: nan\n",
      "Epoch 5/24\n",
      "3690/3690 [==============================] - 2s 518us/step - loss: nan - val_loss: nan\n",
      "Epoch 6/24\n",
      "3690/3690 [==============================] - 2s 517us/step - loss: nan - val_loss: nan\n",
      "Epoch 7/24\n",
      "3690/3690 [==============================] - 2s 518us/step - loss: nan - val_loss: nan\n",
      "Epoch 8/24\n",
      "3690/3690 [==============================] - 2s 527us/step - loss: nan - val_loss: nan\n",
      "Epoch 9/24\n",
      "3690/3690 [==============================] - 2s 519us/step - loss: nan - val_loss: nan\n",
      "Epoch 10/24\n",
      "3690/3690 [==============================] - 2s 517us/step - loss: nan - val_loss: nan\n",
      "Epoch 11/24\n",
      "3690/3690 [==============================] - 2s 517us/step - loss: nan - val_loss: nan\n",
      "Epoch 12/24\n",
      "3690/3690 [==============================] - 2s 519us/step - loss: nan - val_loss: nan\n",
      "Epoch 13/24\n",
      "3690/3690 [==============================] - 2s 519us/step - loss: nan - val_loss: nan\n",
      "Epoch 14/24\n",
      "3690/3690 [==============================] - 2s 518us/step - loss: nan - val_loss: nan\n",
      "Epoch 15/24\n",
      "3690/3690 [==============================] - 2s 521us/step - loss: nan - val_loss: nan\n",
      "Epoch 16/24\n",
      "3690/3690 [==============================] - 2s 521us/step - loss: nan - val_loss: nan\n",
      "Epoch 17/24\n",
      "3690/3690 [==============================] - 2s 521us/step - loss: nan - val_loss: nan\n",
      "Epoch 18/24\n",
      "3690/3690 [==============================] - 2s 525us/step - loss: nan - val_loss: nan\n",
      "Epoch 19/24\n",
      "3690/3690 [==============================] - 2s 518us/step - loss: nan - val_loss: nan\n",
      "Epoch 20/24\n",
      "3690/3690 [==============================] - 2s 520us/step - loss: nan - val_loss: nan\n",
      "Epoch 21/24\n",
      "3690/3690 [==============================] - 2s 521us/step - loss: nan - val_loss: nan\n",
      "Epoch 22/24\n",
      "3690/3690 [==============================] - 2s 520us/step - loss: nan - val_loss: nan\n",
      "Epoch 23/24\n",
      "3690/3690 [==============================] - 2s 520us/step - loss: nan - val_loss: nan\n",
      "Epoch 24/24\n",
      "3690/3690 [==============================] - 2s 520us/step - loss: nan - val_loss: nan\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 3690 samples, validate on 410 samples\n",
      "Epoch 1/24\n",
      "3690/3690 [==============================] - 23s 6ms/step - loss: nan - val_loss: nan\n",
      "Epoch 2/24\n",
      "3690/3690 [==============================] - 2s 522us/step - loss: nan - val_loss: nan\n",
      "Epoch 3/24\n",
      "3690/3690 [==============================] - 2s 522us/step - loss: nan - val_loss: nan\n",
      "Epoch 4/24\n",
      "3690/3690 [==============================] - 2s 519us/step - loss: nan - val_loss: nan\n",
      "Epoch 5/24\n",
      "3690/3690 [==============================] - 2s 520us/step - loss: nan - val_loss: nan\n",
      "Epoch 6/24\n",
      "3690/3690 [==============================] - 2s 520us/step - loss: nan - val_loss: nan\n",
      "Epoch 7/24\n",
      "3690/3690 [==============================] - 2s 520us/step - loss: nan - val_loss: nan\n",
      "Epoch 8/24\n",
      "3690/3690 [==============================] - 2s 523us/step - loss: nan - val_loss: nan\n",
      "Epoch 9/24\n",
      "3690/3690 [==============================] - 2s 522us/step - loss: nan - val_loss: nan\n",
      "Epoch 10/24\n",
      "3690/3690 [==============================] - 2s 519us/step - loss: nan - val_loss: nan\n",
      "Epoch 11/24\n",
      "3690/3690 [==============================] - 2s 520us/step - loss: nan - val_loss: nan\n",
      "Epoch 12/24\n",
      "3690/3690 [==============================] - 2s 520us/step - loss: nan - val_loss: nan\n",
      "Epoch 13/24\n",
      "3690/3690 [==============================] - 2s 520us/step - loss: nan - val_loss: nan\n",
      "Epoch 14/24\n",
      "3690/3690 [==============================] - 2s 520us/step - loss: nan - val_loss: nan\n",
      "Epoch 15/24\n",
      "3690/3690 [==============================] - 2s 527us/step - loss: nan - val_loss: nan\n",
      "Epoch 16/24\n",
      "3690/3690 [==============================] - 2s 521us/step - loss: nan - val_loss: nan\n",
      "Epoch 17/24\n",
      "3690/3690 [==============================] - 2s 521us/step - loss: nan - val_loss: nan\n",
      "Epoch 18/24\n",
      "3690/3690 [==============================] - 2s 520us/step - loss: nan - val_loss: nan\n",
      "Epoch 19/24\n",
      "3690/3690 [==============================] - 2s 519us/step - loss: nan - val_loss: nan\n",
      "Epoch 20/24\n",
      "3690/3690 [==============================] - 2s 520us/step - loss: nan - val_loss: nan\n",
      "Epoch 21/24\n",
      "3690/3690 [==============================] - 2s 520us/step - loss: nan - val_loss: nan\n",
      "Epoch 22/24\n",
      "3690/3690 [==============================] - 2s 522us/step - loss: nan - val_loss: nan\n",
      "Epoch 23/24\n",
      "3690/3690 [==============================] - 2s 518us/step - loss: nan - val_loss: nan\n",
      "Epoch 24/24\n",
      "3690/3690 [==============================] - 2s 528us/step - loss: nan - val_loss: nan\n",
      "Train on 3690 samples, validate on 410 samples\n",
      "Epoch 1/24\n",
      "3690/3690 [==============================] - 23s 6ms/step - loss: nan - val_loss: nan\n",
      "Epoch 2/24\n",
      "3690/3690 [==============================] - 2s 431us/step - loss: nan - val_loss: nan\n",
      "Epoch 3/24\n",
      "3690/3690 [==============================] - 2s 434us/step - loss: nan - val_loss: nan\n",
      "Epoch 4/24\n",
      "3690/3690 [==============================] - 2s 437us/step - loss: nan - val_loss: nan\n",
      "Epoch 5/24\n",
      "3690/3690 [==============================] - 2s 432us/step - loss: nan - val_loss: nan\n",
      "Epoch 6/24\n",
      "3690/3690 [==============================] - 2s 435us/step - loss: nan - val_loss: nan\n",
      "Epoch 7/24\n",
      "3690/3690 [==============================] - 2s 435us/step - loss: nan - val_loss: nan\n",
      "Epoch 8/24\n",
      "3690/3690 [==============================] - 2s 434us/step - loss: nan - val_loss: nan\n",
      "Epoch 9/24\n",
      "3690/3690 [==============================] - 2s 438us/step - loss: nan - val_loss: nan\n",
      "Epoch 10/24\n",
      "3690/3690 [==============================] - 2s 436us/step - loss: nan - val_loss: nan\n",
      "Epoch 11/24\n",
      "3690/3690 [==============================] - 2s 431us/step - loss: nan - val_loss: nan\n",
      "Epoch 12/24\n",
      "3690/3690 [==============================] - 2s 434us/step - loss: nan - val_loss: nan\n",
      "Epoch 13/24\n",
      "3690/3690 [==============================] - 2s 433us/step - loss: nan - val_loss: nan\n",
      "Epoch 14/24\n",
      "3690/3690 [==============================] - 2s 439us/step - loss: nan - val_loss: nan\n",
      "Epoch 15/24\n",
      "3690/3690 [==============================] - 2s 435us/step - loss: nan - val_loss: nan\n",
      "Epoch 16/24\n",
      "3690/3690 [==============================] - 2s 433us/step - loss: nan - val_loss: nan\n",
      "Epoch 17/24\n",
      "3690/3690 [==============================] - 2s 432us/step - loss: nan - val_loss: nan\n",
      "Epoch 18/24\n",
      "3690/3690 [==============================] - 2s 434us/step - loss: nan - val_loss: nan\n",
      "Epoch 19/24\n",
      "3690/3690 [==============================] - 2s 439us/step - loss: nan - val_loss: nan\n",
      "Epoch 20/24\n",
      "3690/3690 [==============================] - 2s 435us/step - loss: nan - val_loss: nan\n",
      "Epoch 21/24\n",
      "3690/3690 [==============================] - 2s 433us/step - loss: nan - val_loss: nan\n",
      "Epoch 22/24\n",
      "3690/3690 [==============================] - 2s 433us/step - loss: nan - val_loss: nan\n",
      "Epoch 23/24\n",
      "3690/3690 [==============================] - 2s 434us/step - loss: nan - val_loss: nan\n",
      "Epoch 24/24\n",
      "3690/3690 [==============================] - 2s 432us/step - loss: nan - val_loss: nan\n",
      "Train on 3690 samples, validate on 410 samples\n",
      "Epoch 1/24\n",
      "3690/3690 [==============================] - 23s 6ms/step - loss: nan - val_loss: nan\n",
      "Epoch 2/24\n",
      "3690/3690 [==============================] - 2s 432us/step - loss: nan - val_loss: nan\n",
      "Epoch 3/24\n",
      "3690/3690 [==============================] - 2s 434us/step - loss: nan - val_loss: nan\n",
      "Epoch 4/24\n",
      "3690/3690 [==============================] - 2s 432us/step - loss: nan - val_loss: nan\n",
      "Epoch 5/24\n",
      "3690/3690 [==============================] - 2s 433us/step - loss: nan - val_loss: nan\n",
      "Epoch 6/24\n",
      "3690/3690 [==============================] - 2s 433us/step - loss: nan - val_loss: nan\n",
      "Epoch 7/24\n",
      "3690/3690 [==============================] - 2s 435us/step - loss: nan - val_loss: nan\n",
      "Epoch 8/24\n",
      "3690/3690 [==============================] - 2s 436us/step - loss: nan - val_loss: nan\n",
      "Epoch 9/24\n",
      "3690/3690 [==============================] - 2s 433us/step - loss: nan - val_loss: nan\n",
      "Epoch 10/24\n",
      "3690/3690 [==============================] - 2s 432us/step - loss: nan - val_loss: nan\n",
      "Epoch 11/24\n",
      "3690/3690 [==============================] - 2s 434us/step - loss: nan - val_loss: nan\n",
      "Epoch 12/24\n",
      "3690/3690 [==============================] - 2s 436us/step - loss: nan - val_loss: nan\n",
      "Epoch 13/24\n",
      "3690/3690 [==============================] - 2s 437us/step - loss: nan - val_loss: nan\n",
      "Epoch 14/24\n",
      "3690/3690 [==============================] - 2s 436us/step - loss: nan - val_loss: nan\n",
      "Epoch 15/24\n",
      "3690/3690 [==============================] - 2s 436us/step - loss: nan - val_loss: nan\n",
      "Epoch 16/24\n",
      "3690/3690 [==============================] - 2s 431us/step - loss: nan - val_loss: nan\n",
      "Epoch 17/24\n",
      "3690/3690 [==============================] - 2s 431us/step - loss: nan - val_loss: nan\n",
      "Epoch 18/24\n",
      "3690/3690 [==============================] - 2s 435us/step - loss: nan - val_loss: nan\n",
      "Epoch 19/24\n",
      "3690/3690 [==============================] - 2s 435us/step - loss: nan - val_loss: nan\n",
      "Epoch 20/24\n",
      "3690/3690 [==============================] - 2s 432us/step - loss: nan - val_loss: nan\n",
      "Epoch 21/24\n",
      "3690/3690 [==============================] - 2s 435us/step - loss: nan - val_loss: nan\n",
      "Epoch 22/24\n",
      "3690/3690 [==============================] - 2s 432us/step - loss: nan - val_loss: nan\n",
      "Epoch 23/24\n",
      "3690/3690 [==============================] - 2s 438us/step - loss: nan - val_loss: nan\n",
      "Epoch 24/24\n",
      "3690/3690 [==============================] - 2s 430us/step - loss: nan - val_loss: nan\n",
      "Train on 3690 samples, validate on 410 samples\n",
      "Epoch 1/24\n",
      "3690/3690 [==============================] - 24s 6ms/step - loss: nan - val_loss: nan\n",
      "Epoch 2/24\n",
      "3690/3690 [==============================] - 2s 438us/step - loss: nan - val_loss: nan\n",
      "Epoch 3/24\n",
      "3690/3690 [==============================] - 2s 438us/step - loss: nan - val_loss: nan\n",
      "Epoch 4/24\n",
      "3690/3690 [==============================] - 2s 434us/step - loss: nan - val_loss: nan\n",
      "Epoch 5/24\n",
      "3690/3690 [==============================] - 2s 438us/step - loss: nan - val_loss: nan\n",
      "Epoch 6/24\n",
      "3690/3690 [==============================] - 2s 438us/step - loss: nan - val_loss: nan\n",
      "Epoch 7/24\n",
      "3690/3690 [==============================] - 2s 437us/step - loss: nan - val_loss: nan\n",
      "Epoch 8/24\n",
      "3690/3690 [==============================] - 2s 435us/step - loss: nan - val_loss: nan\n",
      "Epoch 9/24\n",
      "3690/3690 [==============================] - 2s 438us/step - loss: nan - val_loss: nan\n",
      "Epoch 10/24\n",
      "3690/3690 [==============================] - 2s 435us/step - loss: nan - val_loss: nan\n",
      "Epoch 11/24\n",
      "3690/3690 [==============================] - 2s 436us/step - loss: nan - val_loss: nan\n",
      "Epoch 12/24\n",
      "3690/3690 [==============================] - 2s 435us/step - loss: nan - val_loss: nan\n",
      "Epoch 13/24\n",
      "3690/3690 [==============================] - 2s 440us/step - loss: nan - val_loss: nan\n",
      "Epoch 14/24\n",
      "3690/3690 [==============================] - 2s 435us/step - loss: nan - val_loss: nan\n",
      "Epoch 15/24\n",
      "3690/3690 [==============================] - 2s 439us/step - loss: nan - val_loss: nan\n",
      "Epoch 16/24\n",
      "3690/3690 [==============================] - 2s 448us/step - loss: nan - val_loss: nan\n",
      "Epoch 17/24\n",
      "3690/3690 [==============================] - 2s 435us/step - loss: nan - val_loss: nan\n",
      "Epoch 18/24\n",
      "3690/3690 [==============================] - 2s 434us/step - loss: nan - val_loss: nan\n",
      "Epoch 19/24\n",
      "3690/3690 [==============================] - 2s 436us/step - loss: nan - val_loss: nan\n",
      "Epoch 20/24\n",
      "3690/3690 [==============================] - 2s 434us/step - loss: nan - val_loss: nan\n",
      "Epoch 21/24\n",
      "3690/3690 [==============================] - 2s 437us/step - loss: nan - val_loss: nan\n",
      "Epoch 22/24\n",
      "3690/3690 [==============================] - 2s 438us/step - loss: nan - val_loss: nan\n",
      "Epoch 23/24\n",
      "3690/3690 [==============================] - 2s 436us/step - loss: nan - val_loss: nan\n",
      "Epoch 24/24\n",
      "3690/3690 [==============================] - 2s 433us/step - loss: nan - val_loss: nan\n",
      "\n",
      "GENERATION 13\n",
      "\n",
      "Train on 3690 samples, validate on 410 samples\n",
      "Epoch 1/24\n",
      "3690/3690 [==============================] - 25s 7ms/step - loss: 0.0851 - val_loss: 0.0525\n",
      "Epoch 2/24\n",
      "3690/3690 [==============================] - 2s 535us/step - loss: 0.0203 - val_loss: 0.0149\n",
      "Epoch 3/24\n",
      "3690/3690 [==============================] - 2s 534us/step - loss: 0.0077 - val_loss: 0.0029\n",
      "Epoch 4/24\n",
      "3690/3690 [==============================] - 2s 534us/step - loss: 0.0035 - val_loss: 7.7748e-04\n",
      "Epoch 5/24\n",
      "3690/3690 [==============================] - 2s 535us/step - loss: 0.0026 - val_loss: 0.0029\n",
      "Epoch 6/24\n",
      "3690/3690 [==============================] - 2s 538us/step - loss: 0.0022 - val_loss: 0.0011\n",
      "Epoch 7/24\n",
      "3690/3690 [==============================] - 2s 534us/step - loss: 0.0017 - val_loss: 5.9208e-04\n",
      "Epoch 8/24\n",
      "3690/3690 [==============================] - 2s 531us/step - loss: 0.0015 - val_loss: 6.6637e-04\n",
      "Epoch 9/24\n",
      "3690/3690 [==============================] - 2s 536us/step - loss: 0.0015 - val_loss: 5.7542e-04\n",
      "Epoch 10/24\n",
      "3690/3690 [==============================] - 2s 535us/step - loss: 0.0014 - val_loss: 7.2510e-04\n",
      "Epoch 11/24\n",
      "3690/3690 [==============================] - 2s 535us/step - loss: 0.0014 - val_loss: 5.8554e-04\n",
      "Epoch 12/24\n",
      "3690/3690 [==============================] - 2s 536us/step - loss: 0.0012 - val_loss: 6.2615e-04\n",
      "Epoch 13/24\n",
      "3690/3690 [==============================] - 2s 535us/step - loss: 0.0012 - val_loss: 7.2237e-04\n",
      "Epoch 14/24\n",
      "3690/3690 [==============================] - 2s 537us/step - loss: 0.0011 - val_loss: 8.0548e-04\n",
      "Epoch 15/24\n",
      "3690/3690 [==============================] - 2s 535us/step - loss: 0.0011 - val_loss: 0.0010\n",
      "Epoch 16/24\n",
      "3690/3690 [==============================] - 2s 534us/step - loss: 0.0011 - val_loss: 0.0019\n",
      "Epoch 17/24\n",
      "3690/3690 [==============================] - 2s 534us/step - loss: 0.0010 - val_loss: 0.0015\n",
      "Epoch 18/24\n",
      "3690/3690 [==============================] - 2s 537us/step - loss: 0.0010 - val_loss: 0.0021\n",
      "Epoch 19/24\n",
      "3690/3690 [==============================] - 2s 540us/step - loss: 9.3206e-04 - val_loss: 0.0017\n",
      "Epoch 20/24\n",
      "3690/3690 [==============================] - 2s 533us/step - loss: 9.4503e-04 - val_loss: 0.0028\n",
      "Epoch 21/24\n",
      "3690/3690 [==============================] - 2s 535us/step - loss: 9.6236e-04 - val_loss: 0.0022\n",
      "Epoch 22/24\n",
      "3690/3690 [==============================] - 2s 538us/step - loss: 8.5453e-04 - val_loss: 0.0026\n",
      "Epoch 23/24\n",
      "3690/3690 [==============================] - 2s 536us/step - loss: 9.0570e-04 - val_loss: 0.0032\n",
      "Epoch 24/24\n",
      "3690/3690 [==============================] - 2s 538us/step - loss: 8.1534e-04 - val_loss: 0.0030\n",
      "Train on 3690 samples, validate on 410 samples\n",
      "Epoch 1/24\n",
      "3690/3690 [==============================] - 25s 7ms/step - loss: 0.0458 - val_loss: 0.0174\n",
      "Epoch 2/24\n",
      "3690/3690 [==============================] - 2s 524us/step - loss: 0.0086 - val_loss: 0.0013\n",
      "Epoch 3/24\n",
      "3690/3690 [==============================] - 2s 524us/step - loss: 0.0042 - val_loss: 9.0809e-04\n",
      "Epoch 4/24\n",
      "3690/3690 [==============================] - 2s 524us/step - loss: 0.0034 - val_loss: 0.0018\n",
      "Epoch 5/24\n",
      "3690/3690 [==============================] - 2s 528us/step - loss: 0.0026 - val_loss: 0.0044\n",
      "Epoch 6/24\n",
      "3690/3690 [==============================] - 2s 525us/step - loss: 0.0021 - val_loss: 0.0021\n",
      "Epoch 7/24\n",
      "3690/3690 [==============================] - 2s 524us/step - loss: 0.0018 - val_loss: 0.0011\n",
      "Epoch 8/24\n",
      "3690/3690 [==============================] - 2s 520us/step - loss: 0.0016 - val_loss: 7.7334e-04\n",
      "Epoch 9/24\n",
      "3690/3690 [==============================] - 2s 521us/step - loss: 0.0015 - val_loss: 7.8233e-04\n",
      "Epoch 10/24\n",
      "3690/3690 [==============================] - 2s 523us/step - loss: 0.0015 - val_loss: 6.5925e-04\n",
      "Epoch 11/24\n",
      "3690/3690 [==============================] - 2s 524us/step - loss: 0.0013 - val_loss: 5.6781e-04\n",
      "Epoch 12/24\n",
      "3690/3690 [==============================] - 2s 523us/step - loss: 0.0013 - val_loss: 5.0711e-04\n",
      "Epoch 13/24\n",
      "3690/3690 [==============================] - 2s 521us/step - loss: 0.0013 - val_loss: 4.6730e-04\n",
      "Epoch 14/24\n",
      "3690/3690 [==============================] - 2s 522us/step - loss: 0.0012 - val_loss: 5.6013e-04\n",
      "Epoch 15/24\n",
      "3690/3690 [==============================] - 2s 523us/step - loss: 0.0012 - val_loss: 6.3451e-04\n",
      "Epoch 16/24\n",
      "3690/3690 [==============================] - 2s 523us/step - loss: 0.0012 - val_loss: 6.7592e-04\n",
      "Epoch 17/24\n",
      "3690/3690 [==============================] - 2s 524us/step - loss: 0.0011 - val_loss: 0.0013\n",
      "Epoch 18/24\n",
      "3690/3690 [==============================] - 2s 525us/step - loss: 0.0011 - val_loss: 5.9991e-04\n",
      "Epoch 19/24\n",
      "3690/3690 [==============================] - 2s 527us/step - loss: 0.0011 - val_loss: 6.4769e-04\n",
      "Epoch 20/24\n",
      "3690/3690 [==============================] - 2s 526us/step - loss: 0.0010 - val_loss: 9.4973e-04\n",
      "Epoch 21/24\n",
      "3690/3690 [==============================] - 2s 525us/step - loss: 0.0010 - val_loss: 8.9301e-04\n",
      "Epoch 22/24\n",
      "3690/3690 [==============================] - 2s 524us/step - loss: 9.9512e-04 - val_loss: 5.2497e-04\n",
      "Epoch 23/24\n",
      "3690/3690 [==============================] - 2s 524us/step - loss: 9.2019e-04 - val_loss: 6.2927e-04\n",
      "Epoch 24/24\n",
      "3690/3690 [==============================] - 2s 525us/step - loss: 9.4335e-04 - val_loss: 8.2993e-04\n",
      "Train on 3690 samples, validate on 410 samples\n",
      "Epoch 1/24\n",
      "3690/3690 [==============================] - 26s 7ms/step - loss: 0.0745 - val_loss: 0.0023\n",
      "Epoch 2/24\n",
      "3690/3690 [==============================] - 2s 522us/step - loss: 0.0091 - val_loss: 0.0044\n",
      "Epoch 3/24\n",
      "3690/3690 [==============================] - 2s 522us/step - loss: 0.0048 - val_loss: 5.9067e-04\n",
      "Epoch 4/24\n",
      "3690/3690 [==============================] - 2s 522us/step - loss: 0.0031 - val_loss: 0.0049\n",
      "Epoch 5/24\n",
      "3690/3690 [==============================] - 2s 524us/step - loss: 0.0027 - val_loss: 0.0010\n",
      "Epoch 6/24\n",
      "3690/3690 [==============================] - 2s 523us/step - loss: 0.0022 - val_loss: 6.2821e-04\n",
      "Epoch 7/24\n",
      "3690/3690 [==============================] - 2s 524us/step - loss: 0.0019 - val_loss: 9.0390e-04\n",
      "Epoch 8/24\n",
      "3690/3690 [==============================] - 2s 526us/step - loss: 0.0016 - val_loss: 0.0011\n",
      "Epoch 9/24\n",
      "3690/3690 [==============================] - 2s 520us/step - loss: 0.0017 - val_loss: 0.0014\n",
      "Epoch 10/24\n",
      "3690/3690 [==============================] - 2s 522us/step - loss: 0.0016 - val_loss: 0.0013\n",
      "Epoch 11/24\n",
      "3690/3690 [==============================] - 2s 522us/step - loss: 0.0015 - val_loss: 0.0010\n",
      "Epoch 12/24\n",
      "3690/3690 [==============================] - 2s 522us/step - loss: 0.0014 - val_loss: 5.2736e-04\n",
      "Epoch 13/24\n",
      "3690/3690 [==============================] - 2s 523us/step - loss: 0.0014 - val_loss: 7.2971e-04\n",
      "Epoch 14/24\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3690/3690 [==============================] - 2s 523us/step - loss: 0.0013 - val_loss: 6.7290e-04\n",
      "Epoch 15/24\n",
      "3690/3690 [==============================] - 2s 520us/step - loss: 0.0012 - val_loss: 6.8460e-04\n",
      "Epoch 16/24\n",
      "3690/3690 [==============================] - 2s 524us/step - loss: 0.0012 - val_loss: 6.6587e-04\n",
      "Epoch 17/24\n",
      "3690/3690 [==============================] - 2s 524us/step - loss: 0.0012 - val_loss: 5.9076e-04\n",
      "Epoch 18/24\n",
      "3690/3690 [==============================] - 2s 525us/step - loss: 0.0011 - val_loss: 5.2210e-04\n",
      "Epoch 19/24\n",
      "3690/3690 [==============================] - 2s 524us/step - loss: 0.0011 - val_loss: 4.6488e-04\n",
      "Epoch 20/24\n",
      "3690/3690 [==============================] - 2s 521us/step - loss: 0.0011 - val_loss: 7.4703e-04\n",
      "Epoch 21/24\n",
      "3690/3690 [==============================] - 2s 525us/step - loss: 0.0010 - val_loss: 4.2469e-04\n",
      "Epoch 22/24\n",
      "3690/3690 [==============================] - 2s 523us/step - loss: 0.0010 - val_loss: 4.2590e-04\n",
      "Epoch 23/24\n",
      "3690/3690 [==============================] - 2s 521us/step - loss: 0.0011 - val_loss: 0.0010\n",
      "Epoch 24/24\n",
      "3690/3690 [==============================] - 2s 522us/step - loss: 0.0010 - val_loss: 5.9039e-04\n",
      "Train on 3690 samples, validate on 410 samples\n",
      "Epoch 1/24\n",
      "3690/3690 [==============================] - 25s 7ms/step - loss: 0.0511 - val_loss: 0.0198\n",
      "Epoch 2/24\n",
      "3690/3690 [==============================] - 2s 523us/step - loss: 0.0088 - val_loss: 0.0035\n",
      "Epoch 3/24\n",
      "3690/3690 [==============================] - 2s 523us/step - loss: 0.0043 - val_loss: 9.2660e-04\n",
      "Epoch 4/24\n",
      "3690/3690 [==============================] - 2s 525us/step - loss: 0.0030 - val_loss: 5.1686e-04\n",
      "Epoch 5/24\n",
      "3690/3690 [==============================] - 2s 528us/step - loss: 0.0026 - val_loss: 0.0023\n",
      "Epoch 6/24\n",
      "3690/3690 [==============================] - 2s 522us/step - loss: 0.0020 - val_loss: 0.0018\n",
      "Epoch 7/24\n",
      "3690/3690 [==============================] - 2s 522us/step - loss: 0.0018 - val_loss: 9.3268e-04\n",
      "Epoch 8/24\n",
      "3690/3690 [==============================] - 2s 523us/step - loss: 0.0016 - val_loss: 7.9683e-04\n",
      "Epoch 9/24\n",
      "3690/3690 [==============================] - 2s 522us/step - loss: 0.0017 - val_loss: 8.4165e-04\n",
      "Epoch 10/24\n",
      "3690/3690 [==============================] - 2s 524us/step - loss: 0.0016 - val_loss: 7.2745e-04\n",
      "Epoch 11/24\n",
      "3690/3690 [==============================] - 2s 525us/step - loss: 0.0015 - val_loss: 9.5218e-04\n",
      "Epoch 12/24\n",
      "3690/3690 [==============================] - 2s 525us/step - loss: 0.0014 - val_loss: 6.0746e-04\n",
      "Epoch 13/24\n",
      "3690/3690 [==============================] - 2s 523us/step - loss: 0.0014 - val_loss: 5.3974e-04\n",
      "Epoch 14/24\n",
      "3690/3690 [==============================] - 2s 525us/step - loss: 0.0014 - val_loss: 4.5619e-04\n",
      "Epoch 15/24\n",
      "3690/3690 [==============================] - 2s 522us/step - loss: 0.0013 - val_loss: 4.1859e-04\n",
      "Epoch 16/24\n",
      "3690/3690 [==============================] - 2s 523us/step - loss: 0.0014 - val_loss: 4.5705e-04\n",
      "Epoch 17/24\n",
      "3690/3690 [==============================] - 2s 522us/step - loss: 0.0013 - val_loss: 9.4965e-04\n",
      "Epoch 18/24\n",
      "3690/3690 [==============================] - 2s 524us/step - loss: 0.0012 - val_loss: 6.8499e-04\n",
      "Epoch 19/24\n",
      "3690/3690 [==============================] - 2s 524us/step - loss: 0.0012 - val_loss: 4.0910e-04\n",
      "Epoch 20/24\n",
      "3690/3690 [==============================] - 2s 524us/step - loss: 0.0011 - val_loss: 4.6519e-04\n",
      "Epoch 21/24\n",
      "3690/3690 [==============================] - 2s 525us/step - loss: 0.0011 - val_loss: 4.3589e-04\n",
      "Epoch 22/24\n",
      "3690/3690 [==============================] - 2s 520us/step - loss: 0.0011 - val_loss: 4.7328e-04\n",
      "Epoch 23/24\n",
      "3690/3690 [==============================] - 2s 524us/step - loss: 0.0010 - val_loss: 4.0591e-04\n",
      "Epoch 24/24\n",
      "3690/3690 [==============================] - 2s 523us/step - loss: 0.0011 - val_loss: 0.0017\n",
      "Train on 3690 samples, validate on 410 samples\n",
      "Epoch 1/24\n",
      "3690/3690 [==============================] - 26s 7ms/step - loss: 0.1079 - val_loss: 0.0344\n",
      "Epoch 2/24\n",
      "3690/3690 [==============================] - 2s 552us/step - loss: 0.0230 - val_loss: 0.0244\n",
      "Epoch 3/24\n",
      "3690/3690 [==============================] - 2s 549us/step - loss: 0.0096 - val_loss: 0.0025\n",
      "Epoch 4/24\n",
      "3690/3690 [==============================] - 2s 548us/step - loss: 0.0035 - val_loss: 0.0012\n",
      "Epoch 5/24\n",
      "3690/3690 [==============================] - 2s 549us/step - loss: 0.0028 - val_loss: 0.0027\n",
      "Epoch 6/24\n",
      "3690/3690 [==============================] - 2s 558us/step - loss: 0.0025 - val_loss: 0.0012\n",
      "Epoch 7/24\n",
      "3690/3690 [==============================] - 2s 551us/step - loss: 0.0021 - val_loss: 0.0011\n",
      "Epoch 8/24\n",
      "3690/3690 [==============================] - 2s 550us/step - loss: 0.0019 - val_loss: 8.0024e-04\n",
      "Epoch 9/24\n",
      "3690/3690 [==============================] - 2s 550us/step - loss: 0.0017 - val_loss: 6.7313e-04\n",
      "Epoch 10/24\n",
      "3690/3690 [==============================] - 2s 550us/step - loss: 0.0016 - val_loss: 8.4626e-04\n",
      "Epoch 11/24\n",
      "3690/3690 [==============================] - 2s 550us/step - loss: 0.0016 - val_loss: 7.5397e-04\n",
      "Epoch 12/24\n",
      "3690/3690 [==============================] - 2s 550us/step - loss: 0.0016 - val_loss: 0.0013\n",
      "Epoch 13/24\n",
      "3690/3690 [==============================] - 2s 551us/step - loss: 0.0015 - val_loss: 0.0015\n",
      "Epoch 14/24\n",
      "3690/3690 [==============================] - 2s 548us/step - loss: 0.0014 - val_loss: 0.0017\n",
      "Epoch 15/24\n",
      "3690/3690 [==============================] - 2s 553us/step - loss: 0.0012 - val_loss: 0.0018\n",
      "Epoch 16/24\n",
      "3690/3690 [==============================] - 2s 555us/step - loss: 0.0013 - val_loss: 0.0013\n",
      "Epoch 17/24\n",
      "3690/3690 [==============================] - 2s 553us/step - loss: 0.0013 - val_loss: 0.0022\n",
      "Epoch 18/24\n",
      "3690/3690 [==============================] - 2s 550us/step - loss: 0.0013 - val_loss: 0.0038\n",
      "Epoch 19/24\n",
      "3690/3690 [==============================] - 2s 553us/step - loss: 0.0012 - val_loss: 0.0036\n",
      "Epoch 20/24\n",
      "3690/3690 [==============================] - 2s 553us/step - loss: 0.0011 - val_loss: 0.0039\n",
      "Epoch 21/24\n",
      "3690/3690 [==============================] - 2s 552us/step - loss: 0.0011 - val_loss: 0.0052\n",
      "Epoch 22/24\n",
      "3690/3690 [==============================] - 2s 553us/step - loss: 0.0011 - val_loss: 0.0030\n",
      "Epoch 23/24\n",
      "3690/3690 [==============================] - 2s 551us/step - loss: 0.0011 - val_loss: 0.0076\n",
      "Epoch 24/24\n",
      "3690/3690 [==============================] - 2s 554us/step - loss: 0.0010 - val_loss: 0.0041\n",
      "Train on 3690 samples, validate on 410 samples\n",
      "Epoch 1/24\n",
      "3690/3690 [==============================] - 27s 7ms/step - loss: 0.1285 - val_loss: 0.0267\n",
      "Epoch 2/24\n",
      "3690/3690 [==============================] - 2s 551us/step - loss: 0.0204 - val_loss: 0.0166\n",
      "Epoch 3/24\n",
      "3690/3690 [==============================] - 2s 549us/step - loss: 0.0075 - val_loss: 9.8411e-04\n",
      "Epoch 4/24\n",
      "3690/3690 [==============================] - 2s 552us/step - loss: 0.0033 - val_loss: 0.0029\n",
      "Epoch 5/24\n",
      "3690/3690 [==============================] - 2s 553us/step - loss: 0.0028 - val_loss: 0.0014\n",
      "Epoch 6/24\n",
      "3690/3690 [==============================] - 2s 552us/step - loss: 0.0021 - val_loss: 0.0010\n",
      "Epoch 7/24\n",
      "3690/3690 [==============================] - 2s 550us/step - loss: 0.0019 - val_loss: 7.9872e-04\n",
      "Epoch 8/24\n",
      "3690/3690 [==============================] - 2s 547us/step - loss: 0.0017 - val_loss: 0.0012\n",
      "Epoch 9/24\n",
      "3690/3690 [==============================] - 2s 551us/step - loss: 0.0016 - val_loss: 0.0011\n",
      "Epoch 10/24\n",
      "3690/3690 [==============================] - 2s 552us/step - loss: 0.0015 - val_loss: 0.0011\n",
      "Epoch 11/24\n",
      "3690/3690 [==============================] - 2s 550us/step - loss: 0.0015 - val_loss: 0.0012\n",
      "Epoch 12/24\n",
      "3690/3690 [==============================] - 2s 552us/step - loss: 0.0014 - val_loss: 0.0011\n",
      "Epoch 13/24\n",
      "3690/3690 [==============================] - 2s 550us/step - loss: 0.0014 - val_loss: 0.0011\n",
      "Epoch 14/24\n",
      "3690/3690 [==============================] - 2s 549us/step - loss: 0.0014 - val_loss: 0.0015\n",
      "Epoch 15/24\n",
      "3690/3690 [==============================] - 2s 551us/step - loss: 0.0013 - val_loss: 0.0026\n",
      "Epoch 16/24\n",
      "3690/3690 [==============================] - 2s 549us/step - loss: 0.0013 - val_loss: 0.0043\n",
      "Epoch 17/24\n",
      "3690/3690 [==============================] - 2s 552us/step - loss: 0.0014 - val_loss: 0.0044\n",
      "Epoch 18/24\n",
      "3690/3690 [==============================] - 2s 552us/step - loss: 0.0013 - val_loss: 0.0034\n",
      "Epoch 19/24\n",
      "3690/3690 [==============================] - 2s 554us/step - loss: 0.0012 - val_loss: 0.0030\n",
      "Epoch 20/24\n",
      "3690/3690 [==============================] - 2s 549us/step - loss: 0.0012 - val_loss: 0.0020\n",
      "Epoch 21/24\n",
      "3690/3690 [==============================] - 2s 550us/step - loss: 0.0011 - val_loss: 0.0039\n",
      "Epoch 22/24\n",
      "3690/3690 [==============================] - 2s 552us/step - loss: 0.0011 - val_loss: 0.0041\n",
      "Epoch 23/24\n",
      "3690/3690 [==============================] - 2s 553us/step - loss: 0.0011 - val_loss: 0.0041\n",
      "Epoch 24/24\n",
      "3690/3690 [==============================] - 2s 552us/step - loss: 0.0010 - val_loss: 0.0058\n",
      "Train on 3690 samples, validate on 410 samples\n",
      "Epoch 1/24\n",
      "3690/3690 [==============================] - 26s 7ms/step - loss: 0.1206 - val_loss: 0.0176\n",
      "Epoch 2/24\n",
      "3690/3690 [==============================] - 2s 542us/step - loss: 0.0196 - val_loss: 0.0171\n",
      "Epoch 3/24\n",
      "3690/3690 [==============================] - 2s 541us/step - loss: 0.0083 - val_loss: 0.0055\n",
      "Epoch 4/24\n",
      "3690/3690 [==============================] - 2s 541us/step - loss: 0.0042 - val_loss: 9.8527e-04\n",
      "Epoch 5/24\n",
      "3690/3690 [==============================] - 2s 549us/step - loss: 0.0027 - val_loss: 0.0017\n",
      "Epoch 6/24\n",
      "3690/3690 [==============================] - 2s 541us/step - loss: 0.0023 - val_loss: 0.0012\n",
      "Epoch 7/24\n",
      "3690/3690 [==============================] - 2s 541us/step - loss: 0.0018 - val_loss: 0.0016\n",
      "Epoch 8/24\n",
      "3690/3690 [==============================] - 2s 543us/step - loss: 0.0018 - val_loss: 8.2161e-04\n",
      "Epoch 9/24\n",
      "3690/3690 [==============================] - 2s 541us/step - loss: 0.0016 - val_loss: 8.3745e-04\n",
      "Epoch 10/24\n",
      "3690/3690 [==============================] - 2s 544us/step - loss: 0.0015 - val_loss: 0.0015\n",
      "Epoch 11/24\n",
      "3690/3690 [==============================] - 2s 544us/step - loss: 0.0015 - val_loss: 0.0019\n",
      "Epoch 12/24\n",
      "3690/3690 [==============================] - 2s 541us/step - loss: 0.0013 - val_loss: 0.0016\n",
      "Epoch 13/24\n",
      "3690/3690 [==============================] - 2s 543us/step - loss: 0.0014 - val_loss: 0.0022\n",
      "Epoch 14/24\n",
      "3690/3690 [==============================] - 2s 542us/step - loss: 0.0013 - val_loss: 0.0024\n",
      "Epoch 15/24\n",
      "3690/3690 [==============================] - 2s 542us/step - loss: 0.0013 - val_loss: 0.0028\n",
      "Epoch 16/24\n",
      "3690/3690 [==============================] - 2s 542us/step - loss: 0.0013 - val_loss: 0.0030\n",
      "Epoch 17/24\n",
      "3690/3690 [==============================] - 2s 541us/step - loss: 0.0012 - val_loss: 0.0028\n",
      "Epoch 18/24\n",
      "3690/3690 [==============================] - 2s 541us/step - loss: 0.0012 - val_loss: 0.0045\n",
      "Epoch 19/24\n",
      "3690/3690 [==============================] - 2s 540us/step - loss: 0.0012 - val_loss: 0.0038\n",
      "Epoch 20/24\n",
      "3690/3690 [==============================] - 2s 543us/step - loss: 0.0012 - val_loss: 0.0026\n",
      "Epoch 21/24\n",
      "3690/3690 [==============================] - 2s 544us/step - loss: 0.0012 - val_loss: 0.0043\n",
      "Epoch 22/24\n",
      "3690/3690 [==============================] - 2s 542us/step - loss: 0.0012 - val_loss: 0.0061\n",
      "Epoch 23/24\n",
      "3690/3690 [==============================] - 2s 541us/step - loss: 0.0011 - val_loss: 0.0047\n",
      "Epoch 24/24\n",
      "3690/3690 [==============================] - 2s 541us/step - loss: 0.0011 - val_loss: 0.0040\n",
      "Train on 3690 samples, validate on 410 samples\n",
      "Epoch 1/24\n",
      "3690/3690 [==============================] - 27s 7ms/step - loss: 0.0887 - val_loss: 0.0565\n",
      "Epoch 2/24\n",
      "3690/3690 [==============================] - 2s 539us/step - loss: 0.0198 - val_loss: 0.0083\n",
      "Epoch 3/24\n",
      "3690/3690 [==============================] - 2s 542us/step - loss: 0.0075 - val_loss: 7.6408e-04\n",
      "Epoch 4/24\n",
      "3690/3690 [==============================] - 2s 537us/step - loss: 0.0037 - val_loss: 0.0034\n",
      "Epoch 5/24\n",
      "3690/3690 [==============================] - 2s 535us/step - loss: 0.0029 - val_loss: 6.4492e-04\n",
      "Epoch 6/24\n",
      "3690/3690 [==============================] - 2s 539us/step - loss: 0.0021 - val_loss: 9.8738e-04\n",
      "Epoch 7/24\n",
      "3690/3690 [==============================] - 2s 540us/step - loss: 0.0022 - val_loss: 8.6889e-04\n",
      "Epoch 8/24\n",
      "3690/3690 [==============================] - 2s 539us/step - loss: 0.0017 - val_loss: 0.0021\n",
      "Epoch 9/24\n",
      "3690/3690 [==============================] - 2s 535us/step - loss: 0.0017 - val_loss: 0.0019\n",
      "Epoch 10/24\n",
      "3690/3690 [==============================] - 2s 537us/step - loss: 0.0015 - val_loss: 0.0019\n",
      "Epoch 11/24\n",
      "3690/3690 [==============================] - 2s 539us/step - loss: 0.0015 - val_loss: 0.0028\n",
      "Epoch 12/24\n",
      "3690/3690 [==============================] - 2s 538us/step - loss: 0.0015 - val_loss: 0.0037\n",
      "Epoch 13/24\n",
      "3690/3690 [==============================] - 2s 539us/step - loss: 0.0014 - val_loss: 0.0026\n",
      "Epoch 14/24\n",
      "3690/3690 [==============================] - 2s 538us/step - loss: 0.0013 - val_loss: 0.0030\n",
      "Epoch 15/24\n",
      "3690/3690 [==============================] - 2s 540us/step - loss: 0.0013 - val_loss: 0.0034\n",
      "Epoch 16/24\n",
      "3690/3690 [==============================] - 2s 540us/step - loss: 0.0013 - val_loss: 0.0054\n",
      "Epoch 17/24\n",
      "3690/3690 [==============================] - 2s 540us/step - loss: 0.0012 - val_loss: 0.0056\n",
      "Epoch 18/24\n",
      "3690/3690 [==============================] - 2s 539us/step - loss: 0.0012 - val_loss: 0.0038\n",
      "Epoch 19/24\n",
      "3690/3690 [==============================] - 2s 540us/step - loss: 0.0012 - val_loss: 0.0042\n",
      "Epoch 20/24\n",
      "3690/3690 [==============================] - 2s 538us/step - loss: 0.0012 - val_loss: 0.0093\n",
      "Epoch 21/24\n",
      "3690/3690 [==============================] - 2s 542us/step - loss: 0.0013 - val_loss: 0.0070\n",
      "Epoch 22/24\n",
      "3690/3690 [==============================] - 2s 541us/step - loss: 0.0011 - val_loss: 0.0053\n",
      "Epoch 23/24\n",
      "3690/3690 [==============================] - 2s 542us/step - loss: 0.0011 - val_loss: 0.0050\n",
      "Epoch 24/24\n",
      "3690/3690 [==============================] - 2s 539us/step - loss: 0.0010 - val_loss: 0.0076\n",
      "Train on 3690 samples, validate on 410 samples\n",
      "Epoch 1/24\n",
      "3690/3690 [==============================] - 27s 7ms/step - loss: 0.1026 - val_loss: 0.0087\n",
      "Epoch 2/24\n",
      "3690/3690 [==============================] - 2s 542us/step - loss: 0.0195 - val_loss: 0.0260\n",
      "Epoch 3/24\n",
      "3690/3690 [==============================] - 2s 541us/step - loss: 0.0153 - val_loss: 0.0117\n",
      "Epoch 4/24\n",
      "3690/3690 [==============================] - 2s 541us/step - loss: 0.0127 - val_loss: 0.0025\n",
      "Epoch 5/24\n",
      "3690/3690 [==============================] - 2s 543us/step - loss: 0.0072 - val_loss: 0.0119\n",
      "Epoch 6/24\n",
      "3690/3690 [==============================] - 2s 544us/step - loss: 0.0108 - val_loss: 0.0120\n",
      "Epoch 7/24\n",
      "3690/3690 [==============================] - 2s 544us/step - loss: 0.0081 - val_loss: 0.0083\n",
      "Epoch 8/24\n",
      "3690/3690 [==============================] - 2s 544us/step - loss: 0.0068 - val_loss: 0.0114\n",
      "Epoch 9/24\n",
      "3690/3690 [==============================] - 2s 547us/step - loss: 0.0072 - val_loss: 0.0128\n",
      "Epoch 10/24\n",
      "3690/3690 [==============================] - 2s 544us/step - loss: 0.0055 - val_loss: 0.0062\n",
      "Epoch 11/24\n",
      "3690/3690 [==============================] - 2s 545us/step - loss: 0.0054 - val_loss: 0.0075\n",
      "Epoch 12/24\n",
      "3690/3690 [==============================] - 2s 543us/step - loss: 0.0046 - val_loss: 0.0044\n",
      "Epoch 13/24\n",
      "3690/3690 [==============================] - 2s 543us/step - loss: 0.0054 - val_loss: 0.0018\n",
      "Epoch 14/24\n",
      "3690/3690 [==============================] - 2s 543us/step - loss: 0.0040 - val_loss: 0.0152\n",
      "Epoch 15/24\n",
      "3690/3690 [==============================] - 2s 547us/step - loss: 0.0077 - val_loss: 0.0074\n",
      "Epoch 16/24\n",
      "3690/3690 [==============================] - 2s 543us/step - loss: 0.0043 - val_loss: 0.0094\n",
      "Epoch 17/24\n",
      "3690/3690 [==============================] - 2s 548us/step - loss: 0.0053 - val_loss: 0.0085\n",
      "Epoch 18/24\n",
      "3690/3690 [==============================] - 2s 542us/step - loss: 0.0068 - val_loss: 0.0047\n",
      "Epoch 19/24\n",
      "3690/3690 [==============================] - 2s 544us/step - loss: 0.0042 - val_loss: 0.0074\n",
      "Epoch 20/24\n",
      "3690/3690 [==============================] - 2s 543us/step - loss: 0.0045 - val_loss: 0.0072\n",
      "Epoch 21/24\n",
      "3690/3690 [==============================] - 2s 545us/step - loss: 0.0044 - val_loss: 0.0034\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 22/24\n",
      "3690/3690 [==============================] - 2s 543us/step - loss: 0.0046 - val_loss: 0.0066\n",
      "Epoch 23/24\n",
      "3690/3690 [==============================] - 2s 545us/step - loss: 0.0040 - val_loss: 0.0085\n",
      "Epoch 24/24\n",
      "3690/3690 [==============================] - 2s 542us/step - loss: 0.0047 - val_loss: 0.0071\n",
      "\n",
      "GENERATION 14\n",
      "\n",
      "Train on 3690 samples, validate on 410 samples\n",
      "Epoch 1/24\n",
      "3690/3690 [==============================] - 28s 8ms/step - loss: 0.0801 - val_loss: 0.0037\n",
      "Epoch 2/24\n",
      "3690/3690 [==============================] - 2s 572us/step - loss: 0.0095 - val_loss: 0.0046\n",
      "Epoch 3/24\n",
      "3690/3690 [==============================] - 2s 572us/step - loss: 0.0064 - val_loss: 0.0140\n",
      "Epoch 4/24\n",
      "3690/3690 [==============================] - 2s 571us/step - loss: 0.0036 - val_loss: 5.6322e-04\n",
      "Epoch 5/24\n",
      "3690/3690 [==============================] - 2s 573us/step - loss: 0.0025 - val_loss: 0.0014\n",
      "Epoch 6/24\n",
      "3690/3690 [==============================] - 2s 574us/step - loss: 0.0020 - val_loss: 0.0031\n",
      "Epoch 7/24\n",
      "3690/3690 [==============================] - 2s 574us/step - loss: 0.0017 - val_loss: 7.0117e-04\n",
      "Epoch 8/24\n",
      "3690/3690 [==============================] - 2s 573us/step - loss: 0.0014 - val_loss: 7.5748e-04\n",
      "Epoch 9/24\n",
      "3690/3690 [==============================] - 2s 577us/step - loss: 0.0012 - val_loss: 9.7218e-04\n",
      "Epoch 10/24\n",
      "3690/3690 [==============================] - 2s 572us/step - loss: 0.0012 - val_loss: 0.0012\n",
      "Epoch 11/24\n",
      "3690/3690 [==============================] - 2s 572us/step - loss: 0.0011 - val_loss: 6.2273e-04\n",
      "Epoch 12/24\n",
      "3690/3690 [==============================] - 2s 572us/step - loss: 0.0011 - val_loss: 7.8629e-04\n",
      "Epoch 13/24\n",
      "3690/3690 [==============================] - 2s 571us/step - loss: 0.0010 - val_loss: 9.1728e-04\n",
      "Epoch 14/24\n",
      "3690/3690 [==============================] - 2s 570us/step - loss: 0.0011 - val_loss: 7.4599e-04\n",
      "Epoch 15/24\n",
      "3690/3690 [==============================] - 2s 574us/step - loss: 9.9151e-04 - val_loss: 6.4186e-04\n",
      "Epoch 16/24\n",
      "3690/3690 [==============================] - 2s 574us/step - loss: 9.9685e-04 - val_loss: 6.2973e-04\n",
      "Epoch 17/24\n",
      "3690/3690 [==============================] - 2s 574us/step - loss: 9.5730e-04 - val_loss: 7.7784e-04\n",
      "Epoch 18/24\n",
      "3690/3690 [==============================] - 2s 575us/step - loss: 8.9054e-04 - val_loss: 6.8064e-04\n",
      "Epoch 19/24\n",
      "3690/3690 [==============================] - 2s 573us/step - loss: 8.8620e-04 - val_loss: 7.7934e-04\n",
      "Epoch 20/24\n",
      "3690/3690 [==============================] - 2s 576us/step - loss: 8.5905e-04 - val_loss: 7.7458e-04\n",
      "Epoch 21/24\n",
      "3690/3690 [==============================] - 2s 575us/step - loss: 8.4895e-04 - val_loss: 8.7054e-04\n",
      "Epoch 22/24\n",
      "3690/3690 [==============================] - 2s 574us/step - loss: 8.3812e-04 - val_loss: 5.3195e-04\n",
      "Epoch 23/24\n",
      "3690/3690 [==============================] - 2s 574us/step - loss: 7.8996e-04 - val_loss: 4.9632e-04\n",
      "Epoch 24/24\n",
      "3690/3690 [==============================] - 2s 577us/step - loss: 8.1225e-04 - val_loss: 4.7331e-04\n",
      "Train on 3690 samples, validate on 410 samples\n",
      "Epoch 1/24\n",
      "3690/3690 [==============================] - 27s 7ms/step - loss: 0.0626 - val_loss: 0.0172\n",
      "Epoch 2/24\n",
      "3690/3690 [==============================] - 2s 518us/step - loss: 0.0108 - val_loss: 8.6402e-04\n",
      "Epoch 3/24\n",
      "3690/3690 [==============================] - 2s 517us/step - loss: 0.0048 - val_loss: 0.0020\n",
      "Epoch 4/24\n",
      "3690/3690 [==============================] - 2s 516us/step - loss: 0.0038 - val_loss: 0.0019\n",
      "Epoch 5/24\n",
      "3690/3690 [==============================] - 2s 516us/step - loss: 0.0027 - val_loss: 0.0038\n",
      "Epoch 6/24\n",
      "3690/3690 [==============================] - 2s 513us/step - loss: 0.0023 - val_loss: 7.8405e-04\n",
      "Epoch 7/24\n",
      "3690/3690 [==============================] - 2s 518us/step - loss: 0.0020 - val_loss: 5.1074e-04\n",
      "Epoch 8/24\n",
      "3690/3690 [==============================] - 2s 516us/step - loss: 0.0018 - val_loss: 0.0012\n",
      "Epoch 9/24\n",
      "3690/3690 [==============================] - 2s 515us/step - loss: 0.0017 - val_loss: 0.0015\n",
      "Epoch 10/24\n",
      "3690/3690 [==============================] - 2s 517us/step - loss: 0.0015 - val_loss: 6.3020e-04\n",
      "Epoch 11/24\n",
      "3690/3690 [==============================] - 2s 516us/step - loss: 0.0015 - val_loss: 4.9167e-04\n",
      "Epoch 12/24\n",
      "3690/3690 [==============================] - 2s 513us/step - loss: 0.0015 - val_loss: 5.6483e-04\n",
      "Epoch 13/24\n",
      "3690/3690 [==============================] - 2s 516us/step - loss: 0.0014 - val_loss: 5.4732e-04\n",
      "Epoch 14/24\n",
      "3690/3690 [==============================] - 2s 516us/step - loss: 0.0013 - val_loss: 5.5569e-04\n",
      "Epoch 15/24\n",
      "3690/3690 [==============================] - 2s 516us/step - loss: 0.0013 - val_loss: 5.3851e-04\n",
      "Epoch 16/24\n",
      "3690/3690 [==============================] - 2s 514us/step - loss: 0.0013 - val_loss: 6.5748e-04\n",
      "Epoch 17/24\n",
      "3690/3690 [==============================] - 2s 515us/step - loss: 0.0012 - val_loss: 7.0735e-04\n",
      "Epoch 18/24\n",
      "3690/3690 [==============================] - 2s 520us/step - loss: 0.0012 - val_loss: 9.8285e-04\n",
      "Epoch 19/24\n",
      "3690/3690 [==============================] - 2s 515us/step - loss: 0.0012 - val_loss: 4.8612e-04\n",
      "Epoch 20/24\n",
      "3690/3690 [==============================] - 2s 516us/step - loss: 0.0012 - val_loss: 4.2590e-04\n",
      "Epoch 21/24\n",
      "3690/3690 [==============================] - 2s 517us/step - loss: 0.0011 - val_loss: 5.0172e-04\n",
      "Epoch 22/24\n",
      "3690/3690 [==============================] - 2s 515us/step - loss: 0.0011 - val_loss: 5.0851e-04\n",
      "Epoch 23/24\n",
      "3690/3690 [==============================] - 2s 516us/step - loss: 0.0011 - val_loss: 5.1669e-04\n",
      "Epoch 24/24\n",
      "3690/3690 [==============================] - 2s 517us/step - loss: 0.0012 - val_loss: 4.3015e-04\n",
      "Train on 3690 samples, validate on 410 samples\n",
      "Epoch 1/24\n",
      "3690/3690 [==============================] - 28s 8ms/step - loss: 1.4644 - val_loss: 0.2795\n",
      "Epoch 2/24\n",
      "3690/3690 [==============================] - 2s 516us/step - loss: 0.1838 - val_loss: 0.1247\n",
      "Epoch 3/24\n",
      "3690/3690 [==============================] - 2s 517us/step - loss: 0.1111 - val_loss: 0.0894\n",
      "Epoch 4/24\n",
      "3690/3690 [==============================] - 2s 521us/step - loss: 0.0695 - val_loss: 0.0069\n",
      "Epoch 5/24\n",
      "3690/3690 [==============================] - 2s 516us/step - loss: 0.0650 - val_loss: 0.0233\n",
      "Epoch 6/24\n",
      "3690/3690 [==============================] - 2s 516us/step - loss: 0.0297 - val_loss: 0.0036\n",
      "Epoch 7/24\n",
      "3690/3690 [==============================] - 2s 517us/step - loss: 0.0083 - val_loss: 0.0032\n",
      "Epoch 8/24\n",
      "3690/3690 [==============================] - 2s 518us/step - loss: 0.0053 - val_loss: 0.0039\n",
      "Epoch 9/24\n",
      "3690/3690 [==============================] - 2s 518us/step - loss: 0.0038 - val_loss: 0.0047\n",
      "Epoch 10/24\n",
      "3690/3690 [==============================] - 2s 515us/step - loss: 0.0036 - val_loss: 0.0026\n",
      "Epoch 11/24\n",
      "3690/3690 [==============================] - 2s 517us/step - loss: 0.0031 - val_loss: 0.0017\n",
      "Epoch 12/24\n",
      "3690/3690 [==============================] - 2s 522us/step - loss: 0.0030 - val_loss: 8.3663e-04\n",
      "Epoch 13/24\n",
      "3690/3690 [==============================] - 2s 516us/step - loss: 0.0029 - val_loss: 7.6297e-04\n",
      "Epoch 14/24\n",
      "3690/3690 [==============================] - 2s 520us/step - loss: 0.0028 - val_loss: 8.6303e-04\n",
      "Epoch 15/24\n",
      "3690/3690 [==============================] - 2s 523us/step - loss: 0.0025 - val_loss: 0.0017\n",
      "Epoch 16/24\n",
      "3690/3690 [==============================] - 2s 519us/step - loss: 0.0027 - val_loss: 7.7199e-04\n",
      "Epoch 17/24\n",
      "3690/3690 [==============================] - 2s 517us/step - loss: 0.0024 - val_loss: 8.7696e-04\n",
      "Epoch 18/24\n",
      "3690/3690 [==============================] - 2s 519us/step - loss: 0.0024 - val_loss: 7.3521e-04\n",
      "Epoch 19/24\n",
      "3690/3690 [==============================] - 2s 516us/step - loss: 0.0023 - val_loss: 0.0011\n",
      "Epoch 20/24\n",
      "3690/3690 [==============================] - 2s 521us/step - loss: 0.0022 - val_loss: 0.0012\n",
      "Epoch 21/24\n",
      "3690/3690 [==============================] - 2s 516us/step - loss: 0.0021 - val_loss: 7.7100e-04\n",
      "Epoch 22/24\n",
      "3690/3690 [==============================] - 2s 518us/step - loss: 0.0021 - val_loss: 0.0012\n",
      "Epoch 23/24\n",
      "3690/3690 [==============================] - 2s 518us/step - loss: 0.0021 - val_loss: 6.8439e-04\n",
      "Epoch 24/24\n",
      "3690/3690 [==============================] - 2s 518us/step - loss: 0.0021 - val_loss: 6.7153e-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 3690 samples, validate on 410 samples\n",
      "Epoch 1/24\n",
      "3690/3690 [==============================] - 28s 8ms/step - loss: 0.0955 - val_loss: 0.0813\n",
      "Epoch 2/24\n",
      "3690/3690 [==============================] - 2s 516us/step - loss: 0.0686 - val_loss: 0.0320\n",
      "Epoch 3/24\n",
      "3690/3690 [==============================] - 2s 517us/step - loss: 0.0622 - val_loss: 0.0306\n",
      "Epoch 4/24\n",
      "3690/3690 [==============================] - 2s 517us/step - loss: 0.0538 - val_loss: 0.0046\n",
      "Epoch 5/24\n",
      "3690/3690 [==============================] - 2s 516us/step - loss: 0.0742 - val_loss: 0.0038\n",
      "Epoch 6/24\n",
      "3690/3690 [==============================] - 2s 517us/step - loss: 0.0390 - val_loss: 0.0223\n",
      "Epoch 7/24\n",
      "3690/3690 [==============================] - 2s 518us/step - loss: 0.0258 - val_loss: 0.0772\n",
      "Epoch 8/24\n",
      "3690/3690 [==============================] - 2s 519us/step - loss: 0.0191 - val_loss: 0.0270\n",
      "Epoch 9/24\n",
      "3690/3690 [==============================] - 2s 518us/step - loss: 0.0064 - val_loss: 0.0280\n",
      "Epoch 10/24\n",
      "3690/3690 [==============================] - 2s 517us/step - loss: 0.0084 - val_loss: 0.0383\n",
      "Epoch 11/24\n",
      "3690/3690 [==============================] - 2s 516us/step - loss: 0.0112 - val_loss: 0.0496\n",
      "Epoch 12/24\n",
      "3690/3690 [==============================] - 2s 517us/step - loss: 0.0091 - val_loss: 0.0298\n",
      "Epoch 13/24\n",
      "3690/3690 [==============================] - 2s 519us/step - loss: 0.0083 - val_loss: 0.0408\n",
      "Epoch 14/24\n",
      "3690/3690 [==============================] - 2s 519us/step - loss: 0.0091 - val_loss: 0.0386\n",
      "Epoch 15/24\n",
      "3690/3690 [==============================] - 2s 518us/step - loss: 0.0083 - val_loss: 0.0318\n",
      "Epoch 16/24\n",
      "3690/3690 [==============================] - 2s 521us/step - loss: 0.0088 - val_loss: 0.0291\n",
      "Epoch 17/24\n",
      "3690/3690 [==============================] - 2s 517us/step - loss: 0.0086 - val_loss: 0.0298\n",
      "Epoch 18/24\n",
      "3690/3690 [==============================] - 2s 519us/step - loss: 0.0064 - val_loss: 0.0279\n",
      "Epoch 19/24\n",
      "3690/3690 [==============================] - 2s 517us/step - loss: 0.0066 - val_loss: 0.0400\n",
      "Epoch 20/24\n",
      "3690/3690 [==============================] - 2s 518us/step - loss: 0.0106 - val_loss: 0.0339\n",
      "Epoch 21/24\n",
      "3690/3690 [==============================] - 2s 522us/step - loss: 0.0083 - val_loss: 0.0356\n",
      "Epoch 22/24\n",
      "3690/3690 [==============================] - 2s 517us/step - loss: 0.0091 - val_loss: 0.0310\n",
      "Epoch 23/24\n",
      "3690/3690 [==============================] - 2s 520us/step - loss: 0.0068 - val_loss: 0.0233\n",
      "Epoch 24/24\n",
      "3690/3690 [==============================] - 2s 518us/step - loss: 0.0050 - val_loss: 0.0190\n",
      "Train on 3690 samples, validate on 410 samples\n",
      "Epoch 1/24\n",
      "3690/3690 [==============================] - 29s 8ms/step - loss: 0.0819 - val_loss: 0.0314\n",
      "Epoch 2/24\n",
      "3690/3690 [==============================] - 2s 521us/step - loss: 0.0603 - val_loss: 0.0224\n",
      "Epoch 3/24\n",
      "3690/3690 [==============================] - 2s 520us/step - loss: 0.0552 - val_loss: 0.1495\n",
      "Epoch 4/24\n",
      "3690/3690 [==============================] - 2s 521us/step - loss: 0.0681 - val_loss: 0.0293\n",
      "Epoch 5/24\n",
      "3690/3690 [==============================] - 2s 519us/step - loss: 0.0528 - val_loss: 0.0480\n",
      "Epoch 6/24\n",
      "3690/3690 [==============================] - 2s 519us/step - loss: 0.0244 - val_loss: 0.0101\n",
      "Epoch 7/24\n",
      "3690/3690 [==============================] - 2s 516us/step - loss: 0.0177 - val_loss: 0.0047\n",
      "Epoch 8/24\n",
      "3690/3690 [==============================] - 2s 516us/step - loss: 0.0097 - val_loss: 0.0035\n",
      "Epoch 9/24\n",
      "3690/3690 [==============================] - 2s 521us/step - loss: 0.0077 - val_loss: 0.0076\n",
      "Epoch 10/24\n",
      "3690/3690 [==============================] - 2s 518us/step - loss: 0.0125 - val_loss: 0.0076\n",
      "Epoch 11/24\n",
      "3690/3690 [==============================] - 2s 520us/step - loss: 0.0131 - val_loss: 0.0060\n",
      "Epoch 12/24\n",
      "3690/3690 [==============================] - 2s 516us/step - loss: 0.0112 - val_loss: 0.0026\n",
      "Epoch 13/24\n",
      "3690/3690 [==============================] - 2s 519us/step - loss: 0.0079 - val_loss: 0.0038\n",
      "Epoch 14/24\n",
      "3690/3690 [==============================] - 2s 520us/step - loss: 0.0104 - val_loss: 0.0024\n",
      "Epoch 15/24\n",
      "3690/3690 [==============================] - 2s 518us/step - loss: 0.0098 - val_loss: 0.0031\n",
      "Epoch 16/24\n",
      "3690/3690 [==============================] - 2s 516us/step - loss: 0.0112 - val_loss: 0.0083\n",
      "Epoch 17/24\n",
      "3690/3690 [==============================] - 2s 519us/step - loss: 0.0089 - val_loss: 0.0020\n",
      "Epoch 18/24\n",
      "3690/3690 [==============================] - 2s 518us/step - loss: 0.0069 - val_loss: 0.0025\n",
      "Epoch 19/24\n",
      "3690/3690 [==============================] - 2s 518us/step - loss: 0.0123 - val_loss: 0.0081\n",
      "Epoch 20/24\n",
      "3690/3690 [==============================] - 2s 519us/step - loss: 0.0114 - val_loss: 0.0019\n",
      "Epoch 21/24\n",
      "3690/3690 [==============================] - 2s 521us/step - loss: 0.0063 - val_loss: 0.0020\n",
      "Epoch 22/24\n",
      "3690/3690 [==============================] - 2s 518us/step - loss: 0.0064 - val_loss: 0.0027\n",
      "Epoch 23/24\n",
      "3690/3690 [==============================] - 2s 518us/step - loss: 0.0098 - val_loss: 0.0036\n",
      "Epoch 24/24\n",
      "3690/3690 [==============================] - 2s 519us/step - loss: 0.0069 - val_loss: 0.0019\n",
      "Train on 3690 samples, validate on 410 samples\n",
      "Epoch 1/24\n",
      "3690/3690 [==============================] - 28s 8ms/step - loss: 2.3428 - val_loss: 0.0254\n",
      "Epoch 2/24\n",
      "3690/3690 [==============================] - 2s 442us/step - loss: 0.0619 - val_loss: 0.2918\n",
      "Epoch 3/24\n",
      "3690/3690 [==============================] - 2s 444us/step - loss: 0.0824 - val_loss: 0.0023\n",
      "Epoch 4/24\n",
      "3690/3690 [==============================] - 2s 443us/step - loss: 0.0258 - val_loss: 0.0404\n",
      "Epoch 5/24\n",
      "3690/3690 [==============================] - 2s 445us/step - loss: 0.0648 - val_loss: 0.0116\n",
      "Epoch 6/24\n",
      "3690/3690 [==============================] - 2s 446us/step - loss: 0.0297 - val_loss: 0.0081\n",
      "Epoch 7/24\n",
      "3690/3690 [==============================] - 2s 444us/step - loss: 0.0304 - val_loss: 0.0119\n",
      "Epoch 8/24\n",
      "3690/3690 [==============================] - 2s 444us/step - loss: 0.0298 - val_loss: 0.0120\n",
      "Epoch 9/24\n",
      "3690/3690 [==============================] - 2s 447us/step - loss: 0.0316 - val_loss: 0.0125\n",
      "Epoch 10/24\n",
      "3690/3690 [==============================] - 2s 442us/step - loss: 0.0344 - val_loss: 0.0226\n",
      "Epoch 11/24\n",
      "3690/3690 [==============================] - 2s 445us/step - loss: 0.0216 - val_loss: 0.0112\n",
      "Epoch 12/24\n",
      "3690/3690 [==============================] - 2s 445us/step - loss: 0.0260 - val_loss: 0.0203\n",
      "Epoch 13/24\n",
      "3690/3690 [==============================] - 2s 444us/step - loss: 0.0263 - val_loss: 0.0097\n",
      "Epoch 14/24\n",
      "3690/3690 [==============================] - 2s 442us/step - loss: 0.0267 - val_loss: 0.0348\n",
      "Epoch 15/24\n",
      "3690/3690 [==============================] - 2s 443us/step - loss: 0.0277 - val_loss: 0.0086\n",
      "Epoch 16/24\n",
      "3690/3690 [==============================] - 2s 449us/step - loss: 0.0174 - val_loss: 0.0042\n",
      "Epoch 17/24\n",
      "3690/3690 [==============================] - 2s 442us/step - loss: 0.0174 - val_loss: 0.0212\n",
      "Epoch 18/24\n",
      "3690/3690 [==============================] - 2s 444us/step - loss: 0.0229 - val_loss: 0.0138\n",
      "Epoch 19/24\n",
      "3690/3690 [==============================] - 2s 445us/step - loss: 0.0190 - val_loss: 0.0243\n",
      "Epoch 20/24\n",
      "3690/3690 [==============================] - 2s 446us/step - loss: 0.0224 - val_loss: 0.0160\n",
      "Epoch 21/24\n",
      "3690/3690 [==============================] - 2s 445us/step - loss: 0.0198 - val_loss: 0.0187\n",
      "Epoch 22/24\n",
      "3690/3690 [==============================] - 2s 442us/step - loss: 0.0183 - val_loss: 0.0121\n",
      "Epoch 23/24\n",
      "3690/3690 [==============================] - 2s 442us/step - loss: 0.0150 - val_loss: 0.0133\n",
      "Epoch 24/24\n",
      "3690/3690 [==============================] - 2s 446us/step - loss: 0.0141 - val_loss: 0.0208\n",
      "Train on 3690 samples, validate on 410 samples\n",
      "Epoch 1/24\n",
      "3690/3690 [==============================] - 31s 8ms/step - loss: 1.3770 - val_loss: 0.0275\n",
      "Epoch 2/24\n",
      "3690/3690 [==============================] - 2s 454us/step - loss: 0.0727 - val_loss: 0.0507\n",
      "Epoch 3/24\n",
      "3690/3690 [==============================] - 2s 449us/step - loss: 0.0498 - val_loss: 0.0408\n",
      "Epoch 4/24\n",
      "3690/3690 [==============================] - 2s 454us/step - loss: 0.0370 - val_loss: 0.0109\n",
      "Epoch 5/24\n",
      "3690/3690 [==============================] - 2s 452us/step - loss: 0.0332 - val_loss: 0.0279\n",
      "Epoch 6/24\n",
      "3690/3690 [==============================] - 2s 452us/step - loss: 0.0310 - val_loss: 0.0141\n",
      "Epoch 7/24\n",
      "3690/3690 [==============================] - 2s 451us/step - loss: 0.0248 - val_loss: 0.0103\n",
      "Epoch 8/24\n",
      "3690/3690 [==============================] - 2s 452us/step - loss: 0.0223 - val_loss: 0.0101\n",
      "Epoch 9/24\n",
      "3690/3690 [==============================] - 2s 456us/step - loss: 0.0202 - val_loss: 0.0034\n",
      "Epoch 10/24\n",
      "3690/3690 [==============================] - 2s 452us/step - loss: 0.0137 - val_loss: 0.0111\n",
      "Epoch 11/24\n",
      "3690/3690 [==============================] - 2s 451us/step - loss: 0.0238 - val_loss: 0.0215\n",
      "Epoch 12/24\n",
      "3690/3690 [==============================] - 2s 449us/step - loss: 0.0195 - val_loss: 0.0072\n",
      "Epoch 13/24\n",
      "3690/3690 [==============================] - 2s 452us/step - loss: 0.0131 - val_loss: 0.0208\n",
      "Epoch 14/24\n",
      "3690/3690 [==============================] - 2s 452us/step - loss: 0.0192 - val_loss: 0.0242\n",
      "Epoch 15/24\n",
      "3690/3690 [==============================] - 2s 450us/step - loss: 0.0194 - val_loss: 0.0111\n",
      "Epoch 16/24\n",
      "3690/3690 [==============================] - 2s 451us/step - loss: 0.0152 - val_loss: 0.0095\n",
      "Epoch 17/24\n",
      "3690/3690 [==============================] - 2s 452us/step - loss: 0.0113 - val_loss: 0.0096\n",
      "Epoch 18/24\n",
      "3690/3690 [==============================] - 2s 458us/step - loss: 0.0174 - val_loss: 0.0178\n",
      "Epoch 19/24\n",
      "3690/3690 [==============================] - 2s 455us/step - loss: 0.0166 - val_loss: 0.0245\n",
      "Epoch 20/24\n",
      "3690/3690 [==============================] - 2s 452us/step - loss: 0.0151 - val_loss: 0.0062\n",
      "Epoch 21/24\n",
      "3690/3690 [==============================] - 2s 452us/step - loss: 0.0102 - val_loss: 0.0104\n",
      "Epoch 22/24\n",
      "3690/3690 [==============================] - 2s 451us/step - loss: 0.0127 - val_loss: 0.0068\n",
      "Epoch 23/24\n",
      "3690/3690 [==============================] - 2s 452us/step - loss: 0.0083 - val_loss: 0.0101\n",
      "Epoch 24/24\n",
      "3690/3690 [==============================] - 2s 451us/step - loss: 0.0115 - val_loss: 0.0186\n",
      "Train on 3690 samples, validate on 410 samples\n",
      "Epoch 1/24\n",
      "3690/3690 [==============================] - 29s 8ms/step - loss: 0.2493 - val_loss: 0.1175\n",
      "Epoch 2/24\n",
      "3690/3690 [==============================] - 2s 451us/step - loss: 0.0783 - val_loss: 0.0552\n",
      "Epoch 3/24\n",
      "3690/3690 [==============================] - 2s 457us/step - loss: 0.0690 - val_loss: 0.0435\n",
      "Epoch 4/24\n",
      "3690/3690 [==============================] - 2s 452us/step - loss: 0.0684 - val_loss: 0.0406\n",
      "Epoch 5/24\n",
      "3690/3690 [==============================] - 2s 456us/step - loss: 0.0683 - val_loss: 0.0387\n",
      "Epoch 6/24\n",
      "3690/3690 [==============================] - 2s 452us/step - loss: 0.0683 - val_loss: 0.0388\n",
      "Epoch 7/24\n",
      "3690/3690 [==============================] - 2s 451us/step - loss: 0.0683 - val_loss: 0.0411\n",
      "Epoch 8/24\n",
      "3690/3690 [==============================] - 2s 457us/step - loss: 0.0683 - val_loss: 0.0371\n",
      "Epoch 9/24\n",
      "3690/3690 [==============================] - 2s 453us/step - loss: 0.0683 - val_loss: 0.0357\n",
      "Epoch 10/24\n",
      "3690/3690 [==============================] - 2s 449us/step - loss: 0.0683 - val_loss: 0.0390\n",
      "Epoch 11/24\n",
      "3690/3690 [==============================] - 2s 452us/step - loss: 0.0683 - val_loss: 0.0390\n",
      "Epoch 12/24\n",
      "3690/3690 [==============================] - 2s 458us/step - loss: 0.0683 - val_loss: 0.0391\n",
      "Epoch 13/24\n",
      "3690/3690 [==============================] - 2s 453us/step - loss: 0.0683 - val_loss: 0.0408\n",
      "Epoch 14/24\n",
      "3690/3690 [==============================] - 2s 449us/step - loss: 0.0683 - val_loss: 0.0386\n",
      "Epoch 15/24\n",
      "3690/3690 [==============================] - 2s 453us/step - loss: 0.0683 - val_loss: 0.0405\n",
      "Epoch 16/24\n",
      "3690/3690 [==============================] - 2s 454us/step - loss: 0.0683 - val_loss: 0.0413\n",
      "Epoch 17/24\n",
      "3690/3690 [==============================] - 2s 453us/step - loss: 0.0683 - val_loss: 0.0381\n",
      "Epoch 18/24\n",
      "3690/3690 [==============================] - 2s 453us/step - loss: 0.0683 - val_loss: 0.0361\n",
      "Epoch 19/24\n",
      "3690/3690 [==============================] - 2s 451us/step - loss: 0.0683 - val_loss: 0.0380\n",
      "Epoch 20/24\n",
      "3690/3690 [==============================] - 2s 451us/step - loss: 0.0683 - val_loss: 0.0389\n",
      "Epoch 21/24\n",
      "3690/3690 [==============================] - 2s 456us/step - loss: 0.0683 - val_loss: 0.0371\n",
      "Epoch 22/24\n",
      "3690/3690 [==============================] - 2s 455us/step - loss: 0.0683 - val_loss: 0.0380\n",
      "Epoch 23/24\n",
      "3690/3690 [==============================] - 2s 450us/step - loss: 0.0683 - val_loss: 0.0403\n",
      "Epoch 24/24\n",
      "3690/3690 [==============================] - 2s 453us/step - loss: 0.0683 - val_loss: 0.0363\n",
      "Train on 3690 samples, validate on 410 samples\n",
      "Epoch 1/24\n",
      "3690/3690 [==============================] - 30s 8ms/step - loss: 0.0945 - val_loss: 0.0398\n",
      "Epoch 2/24\n",
      "3690/3690 [==============================] - 2s 450us/step - loss: 0.0425 - val_loss: 0.0203\n",
      "Epoch 3/24\n",
      "3690/3690 [==============================] - 2s 454us/step - loss: 0.0372 - val_loss: 0.0158\n",
      "Epoch 4/24\n",
      "3690/3690 [==============================] - 2s 462us/step - loss: 0.0335 - val_loss: 0.0130\n",
      "Epoch 5/24\n",
      "3690/3690 [==============================] - 2s 458us/step - loss: 0.0306 - val_loss: 0.0123\n",
      "Epoch 6/24\n",
      "3690/3690 [==============================] - 2s 447us/step - loss: 0.0276 - val_loss: 0.0105\n",
      "Epoch 7/24\n",
      "3690/3690 [==============================] - 2s 456us/step - loss: 0.0247 - val_loss: 0.0088\n",
      "Epoch 8/24\n",
      "3690/3690 [==============================] - 2s 451us/step - loss: 0.0219 - val_loss: 0.0074\n",
      "Epoch 9/24\n",
      "3690/3690 [==============================] - 2s 452us/step - loss: 0.0193 - val_loss: 0.0059\n",
      "Epoch 10/24\n",
      "3690/3690 [==============================] - 2s 452us/step - loss: 0.0170 - val_loss: 0.0051\n",
      "Epoch 11/24\n",
      "3690/3690 [==============================] - 2s 458us/step - loss: 0.0148 - val_loss: 0.0042\n",
      "Epoch 12/24\n",
      "3690/3690 [==============================] - 2s 454us/step - loss: 0.0130 - val_loss: 0.0032\n",
      "Epoch 13/24\n",
      "3690/3690 [==============================] - 2s 452us/step - loss: 0.0114 - val_loss: 0.0027\n",
      "Epoch 14/24\n",
      "3690/3690 [==============================] - 2s 450us/step - loss: 0.0101 - val_loss: 0.0020\n",
      "Epoch 15/24\n",
      "3690/3690 [==============================] - 2s 452us/step - loss: 0.0088 - val_loss: 0.0018\n",
      "Epoch 16/24\n",
      "3690/3690 [==============================] - 2s 447us/step - loss: 0.0080 - val_loss: 0.0015\n",
      "Epoch 17/24\n",
      "3690/3690 [==============================] - 2s 454us/step - loss: 0.0070 - val_loss: 0.0014\n",
      "Epoch 18/24\n",
      "3690/3690 [==============================] - 2s 452us/step - loss: 0.0064 - val_loss: 0.0015\n",
      "Epoch 19/24\n",
      "3690/3690 [==============================] - 2s 453us/step - loss: 0.0059 - val_loss: 0.0016\n",
      "Epoch 20/24\n",
      "3690/3690 [==============================] - 2s 454us/step - loss: 0.0056 - val_loss: 0.0017\n",
      "Epoch 21/24\n",
      "3690/3690 [==============================] - 2s 450us/step - loss: 0.0052 - val_loss: 0.0018\n",
      "Epoch 22/24\n",
      "3690/3690 [==============================] - 2s 451us/step - loss: 0.0049 - val_loss: 0.0020\n",
      "Epoch 23/24\n",
      "3690/3690 [==============================] - 2s 450us/step - loss: 0.0047 - val_loss: 0.0022\n",
      "Epoch 24/24\n",
      "3690/3690 [==============================] - 2s 457us/step - loss: 0.0046 - val_loss: 0.0025\n",
      "\n",
      "GENERATION 15\n",
      "\n",
      "Train on 3690 samples, validate on 410 samples\n",
      "Epoch 1/24\n",
      "3690/3690 [==============================] - 31s 8ms/step - loss: 0.2354 - val_loss: 0.0162\n",
      "Epoch 2/24\n",
      "3690/3690 [==============================] - 2s 527us/step - loss: 0.0871 - val_loss: 0.0921\n",
      "Epoch 3/24\n",
      "3690/3690 [==============================] - 2s 531us/step - loss: 0.0654 - val_loss: 0.0044\n",
      "Epoch 4/24\n",
      "3690/3690 [==============================] - 2s 527us/step - loss: 0.0378 - val_loss: 0.0252\n",
      "Epoch 5/24\n",
      "3690/3690 [==============================] - 2s 528us/step - loss: 0.0108 - val_loss: 0.0185\n",
      "Epoch 6/24\n",
      "3690/3690 [==============================] - 2s 527us/step - loss: 0.0062 - val_loss: 0.0119\n",
      "Epoch 7/24\n",
      "3690/3690 [==============================] - 2s 528us/step - loss: 0.0033 - val_loss: 0.0116\n",
      "Epoch 8/24\n",
      "3690/3690 [==============================] - 2s 527us/step - loss: 0.0027 - val_loss: 0.0043\n",
      "Epoch 9/24\n",
      "3690/3690 [==============================] - 2s 530us/step - loss: 0.0021 - val_loss: 0.0018\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/24\n",
      "3690/3690 [==============================] - 2s 528us/step - loss: 0.0019 - val_loss: 0.0014\n",
      "Epoch 11/24\n",
      "3690/3690 [==============================] - 2s 527us/step - loss: 0.0020 - val_loss: 0.0021\n",
      "Epoch 12/24\n",
      "3690/3690 [==============================] - 2s 524us/step - loss: 0.0017 - val_loss: 0.0021\n",
      "Epoch 13/24\n",
      "3690/3690 [==============================] - 2s 524us/step - loss: 0.0017 - val_loss: 0.0016\n",
      "Epoch 14/24\n",
      "3690/3690 [==============================] - 2s 529us/step - loss: 0.0016 - val_loss: 0.0016\n",
      "Epoch 15/24\n",
      "3690/3690 [==============================] - 2s 538us/step - loss: 0.0015 - val_loss: 0.0015\n",
      "Epoch 16/24\n",
      "3690/3690 [==============================] - 2s 537us/step - loss: 0.0015 - val_loss: 0.0021\n",
      "Epoch 17/24\n",
      "3690/3690 [==============================] - 2s 527us/step - loss: 0.0014 - val_loss: 8.0266e-04\n",
      "Epoch 18/24\n",
      "3690/3690 [==============================] - 2s 525us/step - loss: 0.0015 - val_loss: 0.0016\n",
      "Epoch 19/24\n",
      "3690/3690 [==============================] - 2s 527us/step - loss: 0.0014 - val_loss: 0.0016\n",
      "Epoch 20/24\n",
      "3690/3690 [==============================] - 2s 533us/step - loss: 0.0014 - val_loss: 7.4546e-04\n",
      "Epoch 21/24\n",
      "3690/3690 [==============================] - 2s 535us/step - loss: 0.0013 - val_loss: 0.0015\n",
      "Epoch 22/24\n",
      "3690/3690 [==============================] - 2s 531us/step - loss: 0.0012 - val_loss: 6.8155e-04\n",
      "Epoch 23/24\n",
      "3690/3690 [==============================] - 2s 527us/step - loss: 0.0012 - val_loss: 9.7257e-04\n",
      "Epoch 24/24\n",
      "3690/3690 [==============================] - 2s 526us/step - loss: 0.0011 - val_loss: 6.2067e-04\n",
      "Train on 3690 samples, validate on 410 samples\n",
      "Epoch 1/24\n",
      "3690/3690 [==============================] - 33s 9ms/step - loss: 0.0721 - val_loss: 0.0087\n",
      "Epoch 2/24\n",
      "3690/3690 [==============================] - 2s 531us/step - loss: 0.0092 - val_loss: 0.0078\n",
      "Epoch 3/24\n",
      "3690/3690 [==============================] - 2s 528us/step - loss: 0.0058 - val_loss: 0.0051\n",
      "Epoch 4/24\n",
      "3690/3690 [==============================] - 2s 533us/step - loss: 0.0029 - val_loss: 0.0035\n",
      "Epoch 5/24\n",
      "3690/3690 [==============================] - 2s 530us/step - loss: 0.0021 - val_loss: 5.2627e-04\n",
      "Epoch 6/24\n",
      "3690/3690 [==============================] - 2s 532us/step - loss: 0.0018 - val_loss: 0.0017\n",
      "Epoch 7/24\n",
      "3690/3690 [==============================] - 2s 529us/step - loss: 0.0017 - val_loss: 0.0020\n",
      "Epoch 8/24\n",
      "3690/3690 [==============================] - 2s 528us/step - loss: 0.0016 - val_loss: 7.3095e-04\n",
      "Epoch 9/24\n",
      "3690/3690 [==============================] - 2s 532us/step - loss: 0.0014 - val_loss: 5.2016e-04\n",
      "Epoch 10/24\n",
      "3690/3690 [==============================] - 2s 532us/step - loss: 0.0013 - val_loss: 6.3664e-04\n",
      "Epoch 11/24\n",
      "3690/3690 [==============================] - 2s 529us/step - loss: 0.0013 - val_loss: 6.8358e-04\n",
      "Epoch 12/24\n",
      "3690/3690 [==============================] - 2s 531us/step - loss: 0.0012 - val_loss: 0.0012\n",
      "Epoch 13/24\n",
      "3690/3690 [==============================] - 2s 531us/step - loss: 0.0012 - val_loss: 0.0011\n",
      "Epoch 14/24\n",
      "3690/3690 [==============================] - 2s 529us/step - loss: 0.0012 - val_loss: 9.2739e-04\n",
      "Epoch 15/24\n",
      "3690/3690 [==============================] - 2s 532us/step - loss: 0.0011 - val_loss: 0.0011\n",
      "Epoch 16/24\n",
      "3690/3690 [==============================] - 2s 534us/step - loss: 0.0011 - val_loss: 8.2004e-04\n",
      "Epoch 17/24\n",
      "3690/3690 [==============================] - 2s 526us/step - loss: 0.0011 - val_loss: 8.4929e-04\n",
      "Epoch 18/24\n",
      "3690/3690 [==============================] - 2s 531us/step - loss: 9.9060e-04 - val_loss: 8.8667e-04\n",
      "Epoch 19/24\n",
      "3690/3690 [==============================] - 2s 529us/step - loss: 9.9047e-04 - val_loss: 6.7845e-04\n",
      "Epoch 20/24\n",
      "3690/3690 [==============================] - 2s 531us/step - loss: 9.4119e-04 - val_loss: 5.9886e-04\n",
      "Epoch 21/24\n",
      "3690/3690 [==============================] - 2s 526us/step - loss: 9.1525e-04 - val_loss: 6.8220e-04\n",
      "Epoch 22/24\n",
      "3690/3690 [==============================] - 2s 529us/step - loss: 9.3783e-04 - val_loss: 5.1370e-04\n",
      "Epoch 23/24\n",
      "3690/3690 [==============================] - 2s 528us/step - loss: 8.6902e-04 - val_loss: 4.7532e-04\n",
      "Epoch 24/24\n",
      "3690/3690 [==============================] - 2s 529us/step - loss: 8.7472e-04 - val_loss: 4.7000e-04\n",
      "Train on 3690 samples, validate on 410 samples\n",
      "Epoch 1/24\n",
      "3690/3690 [==============================] - 32s 9ms/step - loss: 1.1646 - val_loss: 0.1892\n",
      "Epoch 2/24\n",
      "3690/3690 [==============================] - 2s 532us/step - loss: 0.1291 - val_loss: 0.1737\n",
      "Epoch 3/24\n",
      "3690/3690 [==============================] - 2s 530us/step - loss: 0.1007 - val_loss: 0.0315\n",
      "Epoch 4/24\n",
      "3690/3690 [==============================] - 2s 528us/step - loss: 0.0724 - val_loss: 0.0124\n",
      "Epoch 5/24\n",
      "3690/3690 [==============================] - 2s 529us/step - loss: 0.0593 - val_loss: 0.0384\n",
      "Epoch 6/24\n",
      "3690/3690 [==============================] - 2s 530us/step - loss: 0.0361 - val_loss: 0.0025\n",
      "Epoch 7/24\n",
      "3690/3690 [==============================] - 2s 526us/step - loss: 0.0082 - val_loss: 0.0150\n",
      "Epoch 8/24\n",
      "3690/3690 [==============================] - 2s 530us/step - loss: 0.0057 - val_loss: 0.0104\n",
      "Epoch 9/24\n",
      "3690/3690 [==============================] - 2s 526us/step - loss: 0.0040 - val_loss: 0.0063\n",
      "Epoch 10/24\n",
      "3690/3690 [==============================] - 2s 529us/step - loss: 0.0028 - val_loss: 0.0030\n",
      "Epoch 11/24\n",
      "3690/3690 [==============================] - 2s 530us/step - loss: 0.0025 - val_loss: 0.0019\n",
      "Epoch 12/24\n",
      "3690/3690 [==============================] - 2s 525us/step - loss: 0.0021 - val_loss: 0.0016\n",
      "Epoch 13/24\n",
      "3690/3690 [==============================] - 2s 528us/step - loss: 0.0020 - val_loss: 0.0016\n",
      "Epoch 14/24\n",
      "3690/3690 [==============================] - 2s 528us/step - loss: 0.0020 - val_loss: 0.0016\n",
      "Epoch 15/24\n",
      "3690/3690 [==============================] - 2s 524us/step - loss: 0.0019 - val_loss: 8.6387e-04\n",
      "Epoch 16/24\n",
      "3690/3690 [==============================] - 2s 532us/step - loss: 0.0018 - val_loss: 9.9936e-04\n",
      "Epoch 17/24\n",
      "3690/3690 [==============================] - 2s 533us/step - loss: 0.0017 - val_loss: 8.1127e-04\n",
      "Epoch 18/24\n",
      "3690/3690 [==============================] - 2s 527us/step - loss: 0.0019 - val_loss: 0.0019\n",
      "Epoch 19/24\n",
      "3690/3690 [==============================] - 2s 528us/step - loss: 0.0018 - val_loss: 9.1258e-04\n",
      "Epoch 20/24\n",
      "3690/3690 [==============================] - 2s 528us/step - loss: 0.0016 - val_loss: 0.0010\n",
      "Epoch 21/24\n",
      "3690/3690 [==============================] - 2s 531us/step - loss: 0.0016 - val_loss: 9.0587e-04\n",
      "Epoch 22/24\n",
      "3690/3690 [==============================] - 2s 532us/step - loss: 0.0016 - val_loss: 7.2293e-04\n",
      "Epoch 23/24\n",
      "3690/3690 [==============================] - 2s 529us/step - loss: 0.0015 - val_loss: 7.7281e-04\n",
      "Epoch 24/24\n",
      "3690/3690 [==============================] - 2s 527us/step - loss: 0.0016 - val_loss: 7.1516e-04\n",
      "Train on 3690 samples, validate on 410 samples\n",
      "Epoch 1/24\n",
      "3690/3690 [==============================] - 33s 9ms/step - loss: 0.1076 - val_loss: 0.0148\n",
      "Epoch 2/24\n",
      "3690/3690 [==============================] - 2s 548us/step - loss: 0.0421 - val_loss: 0.0024\n",
      "Epoch 3/24\n",
      "3690/3690 [==============================] - 2s 545us/step - loss: 0.0078 - val_loss: 0.0015\n",
      "Epoch 4/24\n",
      "3690/3690 [==============================] - 2s 544us/step - loss: 0.0033 - val_loss: 0.0032\n",
      "Epoch 5/24\n",
      "3690/3690 [==============================] - 2s 545us/step - loss: 0.0022 - val_loss: 0.0103\n",
      "Epoch 6/24\n",
      "3690/3690 [==============================] - 2s 544us/step - loss: 0.0019 - val_loss: 0.0042\n",
      "Epoch 7/24\n",
      "3690/3690 [==============================] - 2s 544us/step - loss: 0.0016 - val_loss: 0.0027\n",
      "Epoch 8/24\n",
      "3690/3690 [==============================] - 2s 547us/step - loss: 0.0014 - val_loss: 0.0011\n",
      "Epoch 9/24\n",
      "3690/3690 [==============================] - 2s 540us/step - loss: 0.0012 - val_loss: 0.0016\n",
      "Epoch 10/24\n",
      "3690/3690 [==============================] - 2s 548us/step - loss: 0.0011 - val_loss: 0.0010\n",
      "Epoch 11/24\n",
      "3690/3690 [==============================] - 2s 545us/step - loss: 0.0011 - val_loss: 8.2085e-04\n",
      "Epoch 12/24\n",
      "3690/3690 [==============================] - 2s 545us/step - loss: 0.0010 - val_loss: 7.7827e-04\n",
      "Epoch 13/24\n",
      "3690/3690 [==============================] - 2s 543us/step - loss: 9.5985e-04 - val_loss: 6.0337e-04\n",
      "Epoch 14/24\n",
      "3690/3690 [==============================] - 2s 545us/step - loss: 9.6929e-04 - val_loss: 5.6426e-04\n",
      "Epoch 15/24\n",
      "3690/3690 [==============================] - 2s 549us/step - loss: 9.3956e-04 - val_loss: 5.3616e-04\n",
      "Epoch 16/24\n",
      "3690/3690 [==============================] - 2s 549us/step - loss: 9.5882e-04 - val_loss: 5.3094e-04\n",
      "Epoch 17/24\n",
      "3690/3690 [==============================] - 2s 544us/step - loss: 8.8169e-04 - val_loss: 7.3584e-04\n",
      "Epoch 18/24\n",
      "3690/3690 [==============================] - 2s 549us/step - loss: 8.5590e-04 - val_loss: 0.0013\n",
      "Epoch 19/24\n",
      "3690/3690 [==============================] - 2s 545us/step - loss: 9.1451e-04 - val_loss: 0.0012\n",
      "Epoch 20/24\n",
      "3690/3690 [==============================] - 2s 543us/step - loss: 8.9892e-04 - val_loss: 6.8614e-04\n",
      "Epoch 21/24\n",
      "3690/3690 [==============================] - 2s 545us/step - loss: 8.0422e-04 - val_loss: 5.2848e-04\n",
      "Epoch 22/24\n",
      "3690/3690 [==============================] - 2s 543us/step - loss: 7.4820e-04 - val_loss: 7.5856e-04\n",
      "Epoch 23/24\n",
      "3690/3690 [==============================] - 2s 545us/step - loss: 7.6739e-04 - val_loss: 4.5968e-04\n",
      "Epoch 24/24\n",
      "3690/3690 [==============================] - 2s 544us/step - loss: 7.2208e-04 - val_loss: 4.5974e-04\n",
      "Train on 3690 samples, validate on 410 samples\n",
      "Epoch 1/24\n",
      "3690/3690 [==============================] - 33s 9ms/step - loss: nan - val_loss: nan\n",
      "Epoch 2/24\n",
      "3690/3690 [==============================] - 2s 602us/step - loss: nan - val_loss: nan\n",
      "Epoch 3/24\n",
      "3690/3690 [==============================] - 2s 604us/step - loss: nan - val_loss: nan\n",
      "Epoch 4/24\n",
      "3690/3690 [==============================] - 2s 607us/step - loss: nan - val_loss: nan\n",
      "Epoch 5/24\n",
      "3690/3690 [==============================] - 2s 605us/step - loss: nan - val_loss: nan\n",
      "Epoch 6/24\n",
      "3690/3690 [==============================] - 2s 602us/step - loss: nan - val_loss: nan\n",
      "Epoch 7/24\n",
      "3690/3690 [==============================] - 2s 602us/step - loss: nan - val_loss: nan\n",
      "Epoch 8/24\n",
      "3690/3690 [==============================] - 2s 602us/step - loss: nan - val_loss: nan\n",
      "Epoch 9/24\n",
      "3690/3690 [==============================] - 2s 604us/step - loss: nan - val_loss: nan\n",
      "Epoch 10/24\n",
      "3690/3690 [==============================] - 2s 602us/step - loss: nan - val_loss: nan\n",
      "Epoch 11/24\n",
      "3690/3690 [==============================] - 2s 605us/step - loss: nan - val_loss: nan\n",
      "Epoch 12/24\n",
      "3690/3690 [==============================] - 2s 604us/step - loss: nan - val_loss: nan\n",
      "Epoch 13/24\n",
      "3690/3690 [==============================] - 2s 611us/step - loss: nan - val_loss: nan\n",
      "Epoch 14/24\n",
      "3690/3690 [==============================] - 2s 605us/step - loss: nan - val_loss: nan\n",
      "Epoch 15/24\n",
      "3690/3690 [==============================] - 2s 603us/step - loss: nan - val_loss: nan\n",
      "Epoch 16/24\n",
      "3690/3690 [==============================] - 2s 602us/step - loss: nan - val_loss: nan\n",
      "Epoch 17/24\n",
      "3690/3690 [==============================] - 2s 604us/step - loss: nan - val_loss: nan\n",
      "Epoch 18/24\n",
      "3690/3690 [==============================] - 2s 603us/step - loss: nan - val_loss: nan\n",
      "Epoch 19/24\n",
      "3690/3690 [==============================] - 2s 607us/step - loss: nan - val_loss: nan\n",
      "Epoch 20/24\n",
      "3690/3690 [==============================] - 2s 601us/step - loss: nan - val_loss: nan\n",
      "Epoch 21/24\n",
      "3690/3690 [==============================] - 2s 602us/step - loss: nan - val_loss: nan\n",
      "Epoch 22/24\n",
      "3690/3690 [==============================] - 2s 601us/step - loss: nan - val_loss: nan\n",
      "Epoch 23/24\n",
      "3690/3690 [==============================] - 2s 605us/step - loss: nan - val_loss: nan\n",
      "Epoch 24/24\n",
      "3690/3690 [==============================] - 2s 605us/step - loss: nan - val_loss: nan\n",
      "Train on 3690 samples, validate on 410 samples\n",
      "Epoch 1/24\n",
      "3690/3690 [==============================] - 34s 9ms/step - loss: nan - val_loss: nan\n",
      "Epoch 2/24\n",
      "3690/3690 [==============================] - 2s 608us/step - loss: nan - val_loss: nan\n",
      "Epoch 3/24\n",
      "3690/3690 [==============================] - 2s 607us/step - loss: nan - val_loss: nan\n",
      "Epoch 4/24\n",
      "3690/3690 [==============================] - 2s 609us/step - loss: nan - val_loss: nan\n",
      "Epoch 5/24\n",
      "3690/3690 [==============================] - 2s 606us/step - loss: nan - val_loss: nan\n",
      "Epoch 6/24\n",
      "3690/3690 [==============================] - 2s 608us/step - loss: nan - val_loss: nan\n",
      "Epoch 7/24\n",
      "3690/3690 [==============================] - 2s 608us/step - loss: nan - val_loss: nan\n",
      "Epoch 8/24\n",
      "3690/3690 [==============================] - 2s 608us/step - loss: nan - val_loss: nan\n",
      "Epoch 9/24\n",
      "3690/3690 [==============================] - 2s 608us/step - loss: nan - val_loss: nan\n",
      "Epoch 10/24\n",
      "3690/3690 [==============================] - 2s 601us/step - loss: nan - val_loss: nan\n",
      "Epoch 11/24\n",
      "3690/3690 [==============================] - 2s 603us/step - loss: nan - val_loss: nan\n",
      "Epoch 12/24\n",
      "3690/3690 [==============================] - 2s 602us/step - loss: nan - val_loss: nan\n",
      "Epoch 13/24\n",
      "3690/3690 [==============================] - 2s 602us/step - loss: nan - val_loss: nan\n",
      "Epoch 14/24\n",
      "3690/3690 [==============================] - 2s 603us/step - loss: nan - val_loss: nan\n",
      "Epoch 15/24\n",
      "3690/3690 [==============================] - 2s 605us/step - loss: nan - val_loss: nan\n",
      "Epoch 16/24\n",
      "3690/3690 [==============================] - 2s 605us/step - loss: nan - val_loss: nan\n",
      "Epoch 17/24\n",
      "3690/3690 [==============================] - 2s 603us/step - loss: nan - val_loss: nan\n",
      "Epoch 18/24\n",
      "3690/3690 [==============================] - 2s 605us/step - loss: nan - val_loss: nan\n",
      "Epoch 19/24\n",
      "3690/3690 [==============================] - 2s 603us/step - loss: nan - val_loss: nan\n",
      "Epoch 20/24\n",
      "3690/3690 [==============================] - 2s 606us/step - loss: nan - val_loss: nan\n",
      "Epoch 21/24\n",
      "3690/3690 [==============================] - 2s 604us/step - loss: nan - val_loss: nan\n",
      "Epoch 22/24\n",
      "3690/3690 [==============================] - 2s 606us/step - loss: nan - val_loss: nan\n",
      "Epoch 23/24\n",
      "3690/3690 [==============================] - 2s 605us/step - loss: nan - val_loss: nan\n",
      "Epoch 24/24\n",
      "3690/3690 [==============================] - 2s 604us/step - loss: nan - val_loss: nan\n",
      "Train on 3690 samples, validate on 410 samples\n",
      "Epoch 1/24\n",
      "3690/3690 [==============================] - 35s 9ms/step - loss: nan - val_loss: nan\n",
      "Epoch 2/24\n",
      "3690/3690 [==============================] - 2s 607us/step - loss: nan - val_loss: nan\n",
      "Epoch 3/24\n",
      "3690/3690 [==============================] - 2s 610us/step - loss: nan - val_loss: nan\n",
      "Epoch 4/24\n",
      "3690/3690 [==============================] - 2s 618us/step - loss: nan - val_loss: nan\n",
      "Epoch 5/24\n",
      "3690/3690 [==============================] - 2s 605us/step - loss: nan - val_loss: nan\n",
      "Epoch 6/24\n",
      "3690/3690 [==============================] - 2s 609us/step - loss: nan - val_loss: nan\n",
      "Epoch 7/24\n",
      "3690/3690 [==============================] - 2s 612us/step - loss: nan - val_loss: nan\n",
      "Epoch 8/24\n",
      "3690/3690 [==============================] - 2s 607us/step - loss: nan - val_loss: nan\n",
      "Epoch 9/24\n",
      "3690/3690 [==============================] - 2s 608us/step - loss: nan - val_loss: nan\n",
      "Epoch 10/24\n",
      "3690/3690 [==============================] - 2s 609us/step - loss: nan - val_loss: nan\n",
      "Epoch 11/24\n",
      "3690/3690 [==============================] - 2s 606us/step - loss: nan - val_loss: nan\n",
      "Epoch 12/24\n",
      "3690/3690 [==============================] - 2s 609us/step - loss: nan - val_loss: nan\n",
      "Epoch 13/24\n",
      "3690/3690 [==============================] - 2s 608us/step - loss: nan - val_loss: nan\n",
      "Epoch 14/24\n",
      "3690/3690 [==============================] - 2s 609us/step - loss: nan - val_loss: nan\n",
      "Epoch 15/24\n",
      "3690/3690 [==============================] - 2s 611us/step - loss: nan - val_loss: nan\n",
      "Epoch 16/24\n",
      "3690/3690 [==============================] - 2s 608us/step - loss: nan - val_loss: nan\n",
      "Epoch 17/24\n",
      "3690/3690 [==============================] - 2s 609us/step - loss: nan - val_loss: nan\n",
      "Epoch 18/24\n",
      "3690/3690 [==============================] - 2s 607us/step - loss: nan - val_loss: nan\n",
      "Epoch 19/24\n",
      "3690/3690 [==============================] - 2s 610us/step - loss: nan - val_loss: nan\n",
      "Epoch 20/24\n",
      "3690/3690 [==============================] - 2s 608us/step - loss: nan - val_loss: nan\n",
      "Epoch 21/24\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3690/3690 [==============================] - 2s 610us/step - loss: nan - val_loss: nan\n",
      "Epoch 22/24\n",
      "3690/3690 [==============================] - 2s 611us/step - loss: nan - val_loss: nan\n",
      "Epoch 23/24\n",
      "3690/3690 [==============================] - 2s 606us/step - loss: nan - val_loss: nan\n",
      "Epoch 24/24\n",
      "3690/3690 [==============================] - 2s 609us/step - loss: nan - val_loss: nan\n",
      "Train on 3690 samples, validate on 410 samples\n",
      "Epoch 1/24\n",
      "3690/3690 [==============================] - 36s 10ms/step - loss: 0.0567 - val_loss: 0.0028\n",
      "Epoch 2/24\n",
      "3690/3690 [==============================] - 2s 624us/step - loss: 0.0038 - val_loss: 0.0019\n",
      "Epoch 3/24\n",
      "3690/3690 [==============================] - 2s 619us/step - loss: 0.0027 - val_loss: 0.0034\n",
      "Epoch 4/24\n",
      "3690/3690 [==============================] - 2s 633us/step - loss: 0.0022 - val_loss: 0.0025\n",
      "Epoch 5/24\n",
      "3690/3690 [==============================] - 2s 634us/step - loss: 0.0019 - val_loss: 9.4641e-04\n",
      "Epoch 6/24\n",
      "3690/3690 [==============================] - 2s 621us/step - loss: 0.0017 - val_loss: 8.3158e-04\n",
      "Epoch 7/24\n",
      "3690/3690 [==============================] - 2s 614us/step - loss: 0.0017 - val_loss: 0.0015\n",
      "Epoch 8/24\n",
      "3690/3690 [==============================] - 2s 617us/step - loss: 0.0016 - val_loss: 0.0018\n",
      "Epoch 9/24\n",
      "3690/3690 [==============================] - 2s 612us/step - loss: 0.0015 - val_loss: 0.0022\n",
      "Epoch 10/24\n",
      "3690/3690 [==============================] - 2s 612us/step - loss: 0.0014 - val_loss: 0.0032\n",
      "Epoch 11/24\n",
      "3690/3690 [==============================] - 2s 612us/step - loss: 0.0014 - val_loss: 0.0036\n",
      "Epoch 12/24\n",
      "3690/3690 [==============================] - 2s 612us/step - loss: 0.0013 - val_loss: 0.0032\n",
      "Epoch 13/24\n",
      "3690/3690 [==============================] - 2s 608us/step - loss: 0.0013 - val_loss: 0.0029\n",
      "Epoch 14/24\n",
      "3690/3690 [==============================] - 2s 613us/step - loss: 0.0013 - val_loss: 0.0037\n",
      "Epoch 15/24\n",
      "3690/3690 [==============================] - 2s 610us/step - loss: 0.0012 - val_loss: 0.0038\n",
      "Epoch 16/24\n",
      "3690/3690 [==============================] - 2s 609us/step - loss: 0.0012 - val_loss: 0.0046\n",
      "Epoch 17/24\n",
      "3690/3690 [==============================] - 2s 612us/step - loss: 0.0012 - val_loss: 0.0048\n",
      "Epoch 18/24\n",
      "3690/3690 [==============================] - 2s 613us/step - loss: 0.0012 - val_loss: 0.0040\n",
      "Epoch 19/24\n",
      "3690/3690 [==============================] - 2s 610us/step - loss: 0.0011 - val_loss: 0.0040\n",
      "Epoch 20/24\n",
      "3690/3690 [==============================] - 2s 611us/step - loss: 0.0011 - val_loss: 0.0051\n",
      "Epoch 21/24\n",
      "3690/3690 [==============================] - 2s 612us/step - loss: 0.0012 - val_loss: 0.0061\n",
      "Epoch 22/24\n",
      "3690/3690 [==============================] - 2s 610us/step - loss: 0.0012 - val_loss: 0.0090\n",
      "Epoch 23/24\n",
      "3690/3690 [==============================] - 2s 613us/step - loss: 0.0013 - val_loss: 0.0071\n",
      "Epoch 24/24\n",
      "3690/3690 [==============================] - 2s 613us/step - loss: 0.0011 - val_loss: 0.0062\n",
      "Train on 3690 samples, validate on 410 samples\n",
      "Epoch 1/24\n",
      "3690/3690 [==============================] - 36s 10ms/step - loss: 2.0588 - val_loss: 0.0107\n",
      "Epoch 2/24\n",
      "3690/3690 [==============================] - 2s 611us/step - loss: 0.0071 - val_loss: 0.0142\n",
      "Epoch 3/24\n",
      "3690/3690 [==============================] - 2s 614us/step - loss: 0.0058 - val_loss: 0.0070\n",
      "Epoch 4/24\n",
      "3690/3690 [==============================] - 2s 611us/step - loss: 0.0040 - val_loss: 0.0021\n",
      "Epoch 5/24\n",
      "3690/3690 [==============================] - 2s 609us/step - loss: 0.0032 - val_loss: 0.0045\n",
      "Epoch 6/24\n",
      "3690/3690 [==============================] - 2s 609us/step - loss: 0.0041 - val_loss: 0.0124\n",
      "Epoch 7/24\n",
      "3690/3690 [==============================] - 2s 609us/step - loss: 0.0039 - val_loss: 0.0014\n",
      "Epoch 8/24\n",
      "3690/3690 [==============================] - 2s 609us/step - loss: 0.0027 - val_loss: 0.0043\n",
      "Epoch 9/24\n",
      "3690/3690 [==============================] - 2s 613us/step - loss: 0.0028 - val_loss: 0.0030\n",
      "Epoch 10/24\n",
      "3690/3690 [==============================] - 2s 608us/step - loss: 0.0026 - val_loss: 0.0027\n",
      "Epoch 11/24\n",
      "3690/3690 [==============================] - 2s 609us/step - loss: 0.0023 - val_loss: 6.7939e-04\n",
      "Epoch 12/24\n",
      "3690/3690 [==============================] - 2s 614us/step - loss: 0.0031 - val_loss: 6.9612e-04\n",
      "Epoch 13/24\n",
      "3690/3690 [==============================] - 2s 613us/step - loss: 0.0023 - val_loss: 8.7702e-04\n",
      "Epoch 14/24\n",
      "3690/3690 [==============================] - 2s 610us/step - loss: 0.0021 - val_loss: 6.9637e-04\n",
      "Epoch 15/24\n",
      "3690/3690 [==============================] - 2s 613us/step - loss: 0.0026 - val_loss: 8.5082e-04\n",
      "Epoch 16/24\n",
      "3690/3690 [==============================] - 2s 611us/step - loss: 0.0024 - val_loss: 6.9836e-04\n",
      "Epoch 17/24\n",
      "3690/3690 [==============================] - 2s 610us/step - loss: 0.0022 - val_loss: 6.9947e-04\n",
      "Epoch 18/24\n",
      "3690/3690 [==============================] - 2s 613us/step - loss: 0.0023 - val_loss: 0.0012\n",
      "Epoch 19/24\n",
      "3690/3690 [==============================] - 2s 616us/step - loss: 0.0022 - val_loss: 6.2659e-04\n",
      "Epoch 20/24\n",
      "3690/3690 [==============================] - 2s 609us/step - loss: 0.0021 - val_loss: 6.2360e-04\n",
      "Epoch 21/24\n",
      "3690/3690 [==============================] - 2s 612us/step - loss: 0.0024 - val_loss: 6.4674e-04\n",
      "Epoch 22/24\n",
      "3690/3690 [==============================] - 2s 610us/step - loss: 0.0020 - val_loss: 6.7465e-04\n",
      "Epoch 23/24\n",
      "3690/3690 [==============================] - 2s 609us/step - loss: 0.0019 - val_loss: 9.4757e-04\n",
      "Epoch 24/24\n",
      "3690/3690 [==============================] - 2s 613us/step - loss: 0.0019 - val_loss: 0.0019\n",
      "\n",
      "GENERATION 16\n",
      "\n",
      "Train on 3690 samples, validate on 410 samples\n",
      "Epoch 1/24\n",
      "3690/3690 [==============================] - 36s 10ms/step - loss: 1.8043 - val_loss: 0.1973\n",
      "Epoch 2/24\n",
      "3690/3690 [==============================] - 2s 559us/step - loss: 0.1657 - val_loss: 0.3508\n",
      "Epoch 3/24\n",
      "3690/3690 [==============================] - 2s 548us/step - loss: 0.1556 - val_loss: 0.0225\n",
      "Epoch 4/24\n",
      "3690/3690 [==============================] - 2s 553us/step - loss: 0.0929 - val_loss: 0.0069\n",
      "Epoch 5/24\n",
      "3690/3690 [==============================] - 2s 547us/step - loss: 0.0670 - val_loss: 0.0771\n",
      "Epoch 6/24\n",
      "3690/3690 [==============================] - 2s 548us/step - loss: 0.0500 - val_loss: 0.0078\n",
      "Epoch 7/24\n",
      "3690/3690 [==============================] - 2s 548us/step - loss: 0.0185 - val_loss: 0.0209\n",
      "Epoch 8/24\n",
      "3690/3690 [==============================] - 2s 546us/step - loss: 0.0065 - val_loss: 0.0056\n",
      "Epoch 9/24\n",
      "3690/3690 [==============================] - 2s 542us/step - loss: 0.0046 - val_loss: 0.0012\n",
      "Epoch 10/24\n",
      "3690/3690 [==============================] - 2s 548us/step - loss: 0.0030 - val_loss: 0.0010\n",
      "Epoch 11/24\n",
      "3690/3690 [==============================] - 2s 545us/step - loss: 0.0022 - val_loss: 0.0010\n",
      "Epoch 12/24\n",
      "3690/3690 [==============================] - 2s 545us/step - loss: 0.0019 - val_loss: 8.7211e-04\n",
      "Epoch 13/24\n",
      "3690/3690 [==============================] - 2s 547us/step - loss: 0.0018 - val_loss: 9.0940e-04\n",
      "Epoch 14/24\n",
      "3690/3690 [==============================] - 2s 546us/step - loss: 0.0016 - val_loss: 8.4785e-04\n",
      "Epoch 15/24\n",
      "3690/3690 [==============================] - 2s 548us/step - loss: 0.0015 - val_loss: 9.5533e-04\n",
      "Epoch 16/24\n",
      "3690/3690 [==============================] - 2s 549us/step - loss: 0.0014 - val_loss: 7.4663e-04\n",
      "Epoch 17/24\n",
      "3690/3690 [==============================] - 2s 545us/step - loss: 0.0014 - val_loss: 7.6746e-04\n",
      "Epoch 18/24\n",
      "3690/3690 [==============================] - 2s 548us/step - loss: 0.0013 - val_loss: 8.8814e-04\n",
      "Epoch 19/24\n",
      "3690/3690 [==============================] - 2s 547us/step - loss: 0.0014 - val_loss: 7.8111e-04\n",
      "Epoch 20/24\n",
      "3690/3690 [==============================] - 2s 552us/step - loss: 0.0014 - val_loss: 0.0011\n",
      "Epoch 21/24\n",
      "3690/3690 [==============================] - 2s 545us/step - loss: 0.0013 - val_loss: 0.0011\n",
      "Epoch 22/24\n",
      "3690/3690 [==============================] - 2s 545us/step - loss: 0.0013 - val_loss: 7.1570e-04\n",
      "Epoch 23/24\n",
      "3690/3690 [==============================] - 2s 546us/step - loss: 0.0013 - val_loss: 7.1958e-04\n",
      "Epoch 24/24\n",
      "3690/3690 [==============================] - 2s 545us/step - loss: 0.0013 - val_loss: 8.0940e-04\n",
      "Train on 3690 samples, validate on 410 samples\n",
      "Epoch 1/24\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3690/3690 [==============================] - 36s 10ms/step - loss: 0.3280 - val_loss: 0.0068\n",
      "Epoch 2/24\n",
      "3690/3690 [==============================] - 2s 536us/step - loss: 0.0879 - val_loss: 0.1307\n",
      "Epoch 3/24\n",
      "3690/3690 [==============================] - 2s 528us/step - loss: 0.0711 - val_loss: 0.0050\n",
      "Epoch 4/24\n",
      "3690/3690 [==============================] - 2s 531us/step - loss: 0.0534 - val_loss: 0.0029\n",
      "Epoch 5/24\n",
      "3690/3690 [==============================] - 2s 530us/step - loss: 0.0100 - val_loss: 0.0085\n",
      "Epoch 6/24\n",
      "3690/3690 [==============================] - 2s 531us/step - loss: 0.0091 - val_loss: 0.0086\n",
      "Epoch 7/24\n",
      "3690/3690 [==============================] - 2s 528us/step - loss: 0.0045 - val_loss: 0.0048\n",
      "Epoch 8/24\n",
      "3690/3690 [==============================] - 2s 531us/step - loss: 0.0032 - val_loss: 0.0108\n",
      "Epoch 9/24\n",
      "3690/3690 [==============================] - 2s 533us/step - loss: 0.0026 - val_loss: 0.0066\n",
      "Epoch 10/24\n",
      "3690/3690 [==============================] - 2s 531us/step - loss: 0.0022 - val_loss: 0.0034\n",
      "Epoch 11/24\n",
      "3690/3690 [==============================] - 2s 529us/step - loss: 0.0020 - val_loss: 0.0018\n",
      "Epoch 12/24\n",
      "3690/3690 [==============================] - 2s 529us/step - loss: 0.0018 - val_loss: 0.0014\n",
      "Epoch 13/24\n",
      "3690/3690 [==============================] - 2s 525us/step - loss: 0.0017 - val_loss: 9.1818e-04\n",
      "Epoch 14/24\n",
      "3690/3690 [==============================] - 2s 529us/step - loss: 0.0017 - val_loss: 8.6550e-04\n",
      "Epoch 15/24\n",
      "3690/3690 [==============================] - 2s 529us/step - loss: 0.0016 - val_loss: 0.0010\n",
      "Epoch 16/24\n",
      "3690/3690 [==============================] - 2s 532us/step - loss: 0.0016 - val_loss: 0.0010\n",
      "Epoch 17/24\n",
      "3690/3690 [==============================] - 2s 538us/step - loss: 0.0015 - val_loss: 7.8992e-04\n",
      "Epoch 18/24\n",
      "3690/3690 [==============================] - 2s 532us/step - loss: 0.0015 - val_loss: 0.0012\n",
      "Epoch 19/24\n",
      "3690/3690 [==============================] - 2s 534us/step - loss: 0.0015 - val_loss: 8.7395e-04\n",
      "Epoch 20/24\n",
      "3690/3690 [==============================] - 2s 533us/step - loss: 0.0014 - val_loss: 0.0012\n",
      "Epoch 21/24\n",
      "3690/3690 [==============================] - 2s 533us/step - loss: 0.0014 - val_loss: 0.0011\n",
      "Epoch 22/24\n",
      "3690/3690 [==============================] - 2s 536us/step - loss: 0.0014 - val_loss: 0.0010\n",
      "Epoch 23/24\n",
      "3690/3690 [==============================] - 2s 533us/step - loss: 0.0014 - val_loss: 6.8869e-04\n",
      "Epoch 24/24\n",
      "3690/3690 [==============================] - 2s 536us/step - loss: 0.0014 - val_loss: 7.5106e-04\n",
      "Train on 3690 samples, validate on 410 samples\n",
      "Epoch 1/24\n",
      "3690/3690 [==============================] - 36s 10ms/step - loss: 2.1733 - val_loss: 0.0030\n",
      "Epoch 2/24\n",
      "3690/3690 [==============================] - 2s 534us/step - loss: 0.0231 - val_loss: 0.0196\n",
      "Epoch 3/24\n",
      "3690/3690 [==============================] - 2s 533us/step - loss: 0.0277 - val_loss: 0.0101\n",
      "Epoch 4/24\n",
      "3690/3690 [==============================] - 2s 537us/step - loss: 0.0205 - val_loss: 0.0557\n",
      "Epoch 5/24\n",
      "3690/3690 [==============================] - 2s 536us/step - loss: 0.0345 - val_loss: 0.0126\n",
      "Epoch 6/24\n",
      "3690/3690 [==============================] - 2s 534us/step - loss: 0.0264 - val_loss: 0.0325\n",
      "Epoch 7/24\n",
      "3690/3690 [==============================] - 2s 531us/step - loss: 0.0406 - val_loss: 0.0459\n",
      "Epoch 8/24\n",
      "3690/3690 [==============================] - 2s 534us/step - loss: 0.0607 - val_loss: 0.0610\n",
      "Epoch 9/24\n",
      "3690/3690 [==============================] - 2s 538us/step - loss: 0.0576 - val_loss: 0.0530\n",
      "Epoch 10/24\n",
      "3690/3690 [==============================] - 2s 533us/step - loss: 0.0728 - val_loss: 0.0621\n",
      "Epoch 11/24\n",
      "3690/3690 [==============================] - 2s 534us/step - loss: 0.0688 - val_loss: 0.0716\n",
      "Epoch 12/24\n",
      "3690/3690 [==============================] - 2s 534us/step - loss: 0.0717 - val_loss: 0.0318\n",
      "Epoch 13/24\n",
      "3690/3690 [==============================] - 2s 529us/step - loss: 0.0450 - val_loss: 0.0756\n",
      "Epoch 14/24\n",
      "3690/3690 [==============================] - 2s 534us/step - loss: 0.0859 - val_loss: 0.1210\n",
      "Epoch 15/24\n",
      "3690/3690 [==============================] - 2s 535us/step - loss: 0.0790 - val_loss: 0.0294\n",
      "Epoch 16/24\n",
      "3690/3690 [==============================] - 2s 531us/step - loss: 0.0291 - val_loss: 0.0650\n",
      "Epoch 17/24\n",
      "3690/3690 [==============================] - 2s 536us/step - loss: 0.0519 - val_loss: 0.0679\n",
      "Epoch 18/24\n",
      "3690/3690 [==============================] - 2s 532us/step - loss: 0.0514 - val_loss: 0.0477\n",
      "Epoch 19/24\n",
      "3690/3690 [==============================] - 2s 533us/step - loss: 0.0344 - val_loss: 0.0463\n",
      "Epoch 20/24\n",
      "3690/3690 [==============================] - 2s 534us/step - loss: 0.0383 - val_loss: 0.0867\n",
      "Epoch 21/24\n",
      "3690/3690 [==============================] - 2s 531us/step - loss: 0.0542 - val_loss: 0.0484\n",
      "Epoch 22/24\n",
      "3690/3690 [==============================] - 2s 533us/step - loss: 0.0352 - val_loss: 0.0679\n",
      "Epoch 23/24\n",
      "3690/3690 [==============================] - 2s 535us/step - loss: 0.0417 - val_loss: 0.0428\n",
      "Epoch 24/24\n",
      "3690/3690 [==============================] - 2s 534us/step - loss: 0.0264 - val_loss: 0.0450\n",
      "Train on 3690 samples, validate on 410 samples\n",
      "Epoch 1/24\n",
      "3690/3690 [==============================] - 37s 10ms/step - loss: nan - val_loss: nan\n",
      "Epoch 2/24\n",
      "3690/3690 [==============================] - 2s 535us/step - loss: nan - val_loss: nan\n",
      "Epoch 3/24\n",
      "3690/3690 [==============================] - 2s 535us/step - loss: nan - val_loss: nan\n",
      "Epoch 4/24\n",
      "3690/3690 [==============================] - 2s 534us/step - loss: nan - val_loss: nan\n",
      "Epoch 5/24\n",
      "3690/3690 [==============================] - 2s 534us/step - loss: nan - val_loss: nan\n",
      "Epoch 6/24\n",
      "3690/3690 [==============================] - 2s 535us/step - loss: nan - val_loss: nan\n",
      "Epoch 7/24\n",
      "3690/3690 [==============================] - 2s 537us/step - loss: nan - val_loss: nan\n",
      "Epoch 8/24\n",
      "3690/3690 [==============================] - 2s 536us/step - loss: nan - val_loss: nan\n",
      "Epoch 9/24\n",
      "3690/3690 [==============================] - 2s 536us/step - loss: nan - val_loss: nan\n",
      "Epoch 10/24\n",
      "3690/3690 [==============================] - 2s 537us/step - loss: nan - val_loss: nan\n",
      "Epoch 11/24\n",
      "3690/3690 [==============================] - 2s 540us/step - loss: nan - val_loss: nan\n",
      "Epoch 12/24\n",
      "3690/3690 [==============================] - 2s 574us/step - loss: nan - val_loss: nan\n",
      "Epoch 13/24\n",
      "3690/3690 [==============================] - 2s 542us/step - loss: nan - val_loss: nan\n",
      "Epoch 14/24\n",
      "3690/3690 [==============================] - 2s 547us/step - loss: nan - val_loss: nan\n",
      "Epoch 15/24\n",
      "3690/3690 [==============================] - 2s 557us/step - loss: nan - val_loss: nan\n",
      "Epoch 16/24\n",
      "3690/3690 [==============================] - 2s 546us/step - loss: nan - val_loss: nan\n",
      "Epoch 17/24\n",
      "3690/3690 [==============================] - 2s 535us/step - loss: nan - val_loss: nan\n",
      "Epoch 18/24\n",
      "3690/3690 [==============================] - 2s 535us/step - loss: nan - val_loss: nan\n",
      "Epoch 19/24\n",
      "3690/3690 [==============================] - 2s 533us/step - loss: nan - val_loss: nan\n",
      "Epoch 20/24\n",
      "3690/3690 [==============================] - 1s 168us/step - loss: nan - val_loss: nan\n",
      "Epoch 21/24\n",
      "3690/3690 [==============================] - 2s 536us/step - loss: nan - val_loss: nan\n",
      "Epoch 22/24\n",
      "3690/3690 [==============================] - 2s 534us/step - loss: nan - val_loss: nan\n",
      "Epoch 23/24\n",
      "3690/3690 [==============================] - 2s 539us/step - loss: nan - val_loss: nan\n",
      "Epoch 24/24\n",
      "3690/3690 [==============================] - 2s 536us/step - loss: nan - val_loss: nan\n",
      "Train on 3690 samples, validate on 410 samples\n",
      "Epoch 1/24\n",
      "3690/3690 [==============================] - 37s 10ms/step - loss: nan - val_loss: nan\n",
      "Epoch 2/24\n",
      "3690/3690 [==============================] - 2s 538us/step - loss: nan - val_loss: nan\n",
      "Epoch 3/24\n",
      "3690/3690 [==============================] - 2s 536us/step - loss: nan - val_loss: nan\n",
      "Epoch 4/24\n",
      "3690/3690 [==============================] - 2s 533us/step - loss: nan - val_loss: nan\n",
      "Epoch 5/24\n",
      "3690/3690 [==============================] - 2s 535us/step - loss: nan - val_loss: nan\n",
      "Epoch 6/24\n",
      "3690/3690 [==============================] - 2s 535us/step - loss: nan - val_loss: nan\n",
      "Epoch 7/24\n",
      "3690/3690 [==============================] - 2s 534us/step - loss: nan - val_loss: nan\n",
      "Epoch 8/24\n",
      "3690/3690 [==============================] - 2s 530us/step - loss: nan - val_loss: nan\n",
      "Epoch 9/24\n",
      "3690/3690 [==============================] - 2s 530us/step - loss: nan - val_loss: nan\n",
      "Epoch 10/24\n",
      "3690/3690 [==============================] - 2s 537us/step - loss: nan - val_loss: nan\n",
      "Epoch 11/24\n",
      "3690/3690 [==============================] - 2s 534us/step - loss: nan - val_loss: nan\n",
      "Epoch 12/24\n",
      "3690/3690 [==============================] - 2s 530us/step - loss: nan - val_loss: nan\n",
      "Epoch 13/24\n",
      "3690/3690 [==============================] - 2s 533us/step - loss: nan - val_loss: nan\n",
      "Epoch 14/24\n",
      "3690/3690 [==============================] - 2s 539us/step - loss: nan - val_loss: nan\n",
      "Epoch 15/24\n",
      "3690/3690 [==============================] - 2s 530us/step - loss: nan - val_loss: nan\n",
      "Epoch 16/24\n",
      "3690/3690 [==============================] - 2s 532us/step - loss: nan - val_loss: nan\n",
      "Epoch 17/24\n",
      "3690/3690 [==============================] - 2s 531us/step - loss: nan - val_loss: nan\n",
      "Epoch 18/24\n",
      "3690/3690 [==============================] - 2s 535us/step - loss: nan - val_loss: nan\n",
      "Epoch 19/24\n",
      "3690/3690 [==============================] - 2s 537us/step - loss: nan - val_loss: nan\n",
      "Epoch 20/24\n",
      "3690/3690 [==============================] - 2s 530us/step - loss: nan - val_loss: nan\n",
      "Epoch 21/24\n",
      "3690/3690 [==============================] - 2s 532us/step - loss: nan - val_loss: nan\n",
      "Epoch 22/24\n",
      "3690/3690 [==============================] - 2s 537us/step - loss: nan - val_loss: nan\n",
      "Epoch 23/24\n",
      "3690/3690 [==============================] - 2s 538us/step - loss: nan - val_loss: nan\n",
      "Epoch 24/24\n",
      "3690/3690 [==============================] - 2s 539us/step - loss: nan - val_loss: nan\n",
      "Train on 3690 samples, validate on 410 samples\n",
      "Epoch 1/24\n",
      "3690/3690 [==============================] - 37s 10ms/step - loss: 3.1097 - val_loss: 0.0025\n",
      "Epoch 2/24\n",
      "3690/3690 [==============================] - 2s 419us/step - loss: 0.0125 - val_loss: 0.0014\n",
      "Epoch 3/24\n",
      "3690/3690 [==============================] - 2s 418us/step - loss: 0.0078 - val_loss: 0.0012\n",
      "Epoch 4/24\n",
      "3690/3690 [==============================] - 2s 419us/step - loss: 0.0058 - val_loss: 0.0012\n",
      "Epoch 5/24\n",
      "3690/3690 [==============================] - 2s 420us/step - loss: 0.0049 - val_loss: 0.0010\n",
      "Epoch 6/24\n",
      "3690/3690 [==============================] - 2s 415us/step - loss: 0.0046 - val_loss: 0.0012\n",
      "Epoch 7/24\n",
      "3690/3690 [==============================] - 2s 419us/step - loss: 0.0043 - val_loss: 0.0014\n",
      "Epoch 8/24\n",
      "3690/3690 [==============================] - 2s 421us/step - loss: 0.0044 - val_loss: 0.0014\n",
      "Epoch 9/24\n",
      "3690/3690 [==============================] - 2s 418us/step - loss: 0.0042 - val_loss: 0.0012\n",
      "Epoch 10/24\n",
      "3690/3690 [==============================] - 2s 418us/step - loss: 0.0040 - val_loss: 0.0011\n",
      "Epoch 11/24\n",
      "3690/3690 [==============================] - 2s 416us/step - loss: 0.0039 - val_loss: 0.0015\n",
      "Epoch 12/24\n",
      "3690/3690 [==============================] - 2s 416us/step - loss: 0.0037 - val_loss: 0.0010\n",
      "Epoch 13/24\n",
      "3690/3690 [==============================] - 2s 418us/step - loss: 0.0038 - val_loss: 0.0018\n",
      "Epoch 14/24\n",
      "3690/3690 [==============================] - 2s 421us/step - loss: 0.0037 - val_loss: 0.0013\n",
      "Epoch 15/24\n",
      "3690/3690 [==============================] - 2s 431us/step - loss: 0.0034 - val_loss: 0.0022\n",
      "Epoch 16/24\n",
      "3690/3690 [==============================] - 2s 434us/step - loss: 0.0034 - val_loss: 0.0012\n",
      "Epoch 17/24\n",
      "3690/3690 [==============================] - 2s 416us/step - loss: 0.0035 - val_loss: 0.0020\n",
      "Epoch 18/24\n",
      "3690/3690 [==============================] - 2s 419us/step - loss: 0.0033 - val_loss: 0.0014\n",
      "Epoch 19/24\n",
      "3690/3690 [==============================] - 2s 418us/step - loss: 0.0032 - val_loss: 0.0014\n",
      "Epoch 20/24\n",
      "3690/3690 [==============================] - 2s 420us/step - loss: 0.0032 - val_loss: 0.0015\n",
      "Epoch 21/24\n",
      "3690/3690 [==============================] - 2s 414us/step - loss: 0.0030 - val_loss: 9.9374e-04\n",
      "Epoch 22/24\n",
      "3690/3690 [==============================] - 2s 421us/step - loss: 0.0030 - val_loss: 0.0011\n",
      "Epoch 23/24\n",
      "3690/3690 [==============================] - 2s 425us/step - loss: 0.0030 - val_loss: 8.9996e-04\n",
      "Epoch 24/24\n",
      "3690/3690 [==============================] - 2s 436us/step - loss: 0.0030 - val_loss: 8.1754e-04\n",
      "Train on 3690 samples, validate on 410 samples\n",
      "Epoch 1/24\n",
      "3690/3690 [==============================] - 39s 10ms/step - loss: 3.8723 - val_loss: 0.0044\n",
      "Epoch 2/24\n",
      "3690/3690 [==============================] - 2s 419us/step - loss: 0.0111 - val_loss: 0.0010\n",
      "Epoch 3/24\n",
      "3690/3690 [==============================] - 2s 416us/step - loss: 0.0047 - val_loss: 0.0010\n",
      "Epoch 4/24\n",
      "3690/3690 [==============================] - 2s 435us/step - loss: 0.0034 - val_loss: 0.0025\n",
      "Epoch 5/24\n",
      "3690/3690 [==============================] - 2s 437us/step - loss: 0.0032 - val_loss: 0.0023\n",
      "Epoch 6/24\n",
      "3690/3690 [==============================] - 2s 429us/step - loss: 0.0030 - val_loss: 7.7556e-04\n",
      "Epoch 7/24\n",
      "3690/3690 [==============================] - 2s 436us/step - loss: 0.0030 - val_loss: 0.0025\n",
      "Epoch 8/24\n",
      "3690/3690 [==============================] - 2s 432us/step - loss: 0.0030 - val_loss: 0.0014\n",
      "Epoch 9/24\n",
      "3690/3690 [==============================] - 2s 447us/step - loss: 0.0028 - val_loss: 9.3949e-04\n",
      "Epoch 10/24\n",
      "3690/3690 [==============================] - 2s 426us/step - loss: 0.0027 - val_loss: 0.0017\n",
      "Epoch 11/24\n",
      "3690/3690 [==============================] - 2s 416us/step - loss: 0.0026 - val_loss: 0.0025\n",
      "Epoch 12/24\n",
      "3690/3690 [==============================] - 2s 422us/step - loss: 0.0026 - val_loss: 0.0011\n",
      "Epoch 13/24\n",
      "3690/3690 [==============================] - 2s 419us/step - loss: 0.0025 - val_loss: 7.9842e-04\n",
      "Epoch 14/24\n",
      "3690/3690 [==============================] - 2s 419us/step - loss: 0.0025 - val_loss: 8.7686e-04\n",
      "Epoch 15/24\n",
      "3690/3690 [==============================] - 2s 419us/step - loss: 0.0024 - val_loss: 0.0015\n",
      "Epoch 16/24\n",
      "3690/3690 [==============================] - 2s 420us/step - loss: 0.0024 - val_loss: 0.0015\n",
      "Epoch 17/24\n",
      "3690/3690 [==============================] - 2s 423us/step - loss: 0.0023 - val_loss: 8.9919e-04\n",
      "Epoch 18/24\n",
      "3690/3690 [==============================] - 2s 421us/step - loss: 0.0024 - val_loss: 0.0011\n",
      "Epoch 19/24\n",
      "3690/3690 [==============================] - 2s 419us/step - loss: 0.0022 - val_loss: 0.0010\n",
      "Epoch 20/24\n",
      "3690/3690 [==============================] - 2s 420us/step - loss: 0.0022 - val_loss: 0.0012\n",
      "Epoch 21/24\n",
      "3690/3690 [==============================] - 2s 420us/step - loss: 0.0022 - val_loss: 0.0012\n",
      "Epoch 22/24\n",
      "3690/3690 [==============================] - 2s 420us/step - loss: 0.0021 - val_loss: 8.5339e-04\n",
      "Epoch 23/24\n",
      "3690/3690 [==============================] - 2s 418us/step - loss: 0.0021 - val_loss: 0.0013\n",
      "Epoch 24/24\n",
      "3690/3690 [==============================] - 2s 417us/step - loss: 0.0020 - val_loss: 0.0012\n",
      "Train on 3690 samples, validate on 410 samples\n",
      "Epoch 1/24\n",
      "3690/3690 [==============================] - 39s 11ms/step - loss: 0.1140 - val_loss: 7.9326e-04\n",
      "Epoch 2/24\n",
      "3690/3690 [==============================] - 2s 605us/step - loss: 0.0028 - val_loss: 0.0014\n",
      "Epoch 3/24\n",
      "3690/3690 [==============================] - 2s 600us/step - loss: 0.0021 - val_loss: 0.0012\n",
      "Epoch 4/24\n",
      "3690/3690 [==============================] - 2s 603us/step - loss: 0.0021 - val_loss: 0.0020\n",
      "Epoch 5/24\n",
      "3690/3690 [==============================] - 2s 600us/step - loss: 0.0019 - val_loss: 0.0015\n",
      "Epoch 6/24\n",
      "3690/3690 [==============================] - 2s 603us/step - loss: 0.0021 - val_loss: 0.0058\n",
      "Epoch 7/24\n",
      "3690/3690 [==============================] - 2s 605us/step - loss: 0.0112 - val_loss: 0.0112\n",
      "Epoch 8/24\n",
      "3690/3690 [==============================] - 2s 605us/step - loss: 0.0071 - val_loss: 0.0100\n",
      "Epoch 9/24\n",
      "3690/3690 [==============================] - 2s 604us/step - loss: 0.0065 - val_loss: 0.0150\n",
      "Epoch 10/24\n",
      "3690/3690 [==============================] - 2s 602us/step - loss: 0.0085 - val_loss: 0.0114\n",
      "Epoch 11/24\n",
      "3690/3690 [==============================] - 2s 607us/step - loss: 0.0065 - val_loss: 0.0041\n",
      "Epoch 12/24\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3690/3690 [==============================] - 2s 610us/step - loss: 0.0032 - val_loss: 0.0112\n",
      "Epoch 13/24\n",
      "3690/3690 [==============================] - 2s 606us/step - loss: 0.0100 - val_loss: 0.0229\n",
      "Epoch 14/24\n",
      "3690/3690 [==============================] - 2s 603us/step - loss: 0.0097 - val_loss: 0.0090\n",
      "Epoch 15/24\n",
      "3690/3690 [==============================] - 2s 604us/step - loss: 0.0057 - val_loss: 0.0163\n",
      "Epoch 16/24\n",
      "3690/3690 [==============================] - 2s 602us/step - loss: 0.0096 - val_loss: 0.0122\n",
      "Epoch 17/24\n",
      "3690/3690 [==============================] - 2s 602us/step - loss: 0.0078 - val_loss: 0.0078\n",
      "Epoch 18/24\n",
      "3690/3690 [==============================] - 2s 602us/step - loss: 0.0050 - val_loss: 0.0091\n",
      "Epoch 19/24\n",
      "3690/3690 [==============================] - 2s 605us/step - loss: 0.0054 - val_loss: 0.0050\n",
      "Epoch 20/24\n",
      "3690/3690 [==============================] - 2s 605us/step - loss: 0.0034 - val_loss: 0.0064\n",
      "Epoch 21/24\n",
      "3690/3690 [==============================] - 2s 607us/step - loss: 0.0056 - val_loss: 0.0127\n",
      "Epoch 22/24\n",
      "3690/3690 [==============================] - 2s 602us/step - loss: 0.0070 - val_loss: 0.0116\n",
      "Epoch 23/24\n",
      "3690/3690 [==============================] - 2s 603us/step - loss: 0.0061 - val_loss: 0.0281\n",
      "Epoch 24/24\n",
      "3690/3690 [==============================] - 2s 605us/step - loss: 0.0091 - val_loss: 0.0096\n",
      "Train on 3690 samples, validate on 410 samples\n",
      "Epoch 1/24\n",
      "3690/3690 [==============================] - 39s 11ms/step - loss: 0.4195 - val_loss: 0.0048\n",
      "Epoch 2/24\n",
      "3690/3690 [==============================] - 2s 569us/step - loss: 0.0129 - val_loss: 0.0246\n",
      "Epoch 3/24\n",
      "3690/3690 [==============================] - 2s 567us/step - loss: 0.0129 - val_loss: 0.0047\n",
      "Epoch 4/24\n",
      "3690/3690 [==============================] - 2s 562us/step - loss: 0.0036 - val_loss: 0.0032\n",
      "Epoch 5/24\n",
      "3690/3690 [==============================] - 2s 564us/step - loss: 0.0040 - val_loss: 0.0082\n",
      "Epoch 6/24\n",
      "3690/3690 [==============================] - 2s 563us/step - loss: 0.0048 - val_loss: 0.0021\n",
      "Epoch 7/24\n",
      "3690/3690 [==============================] - 2s 563us/step - loss: 0.0023 - val_loss: 0.0032\n",
      "Epoch 8/24\n",
      "3690/3690 [==============================] - 2s 567us/step - loss: 0.0024 - val_loss: 9.1396e-04\n",
      "Epoch 9/24\n",
      "3690/3690 [==============================] - 2s 567us/step - loss: 0.0018 - val_loss: 8.9374e-04\n",
      "Epoch 10/24\n",
      "3690/3690 [==============================] - 2s 563us/step - loss: 0.0018 - val_loss: 9.1506e-04\n",
      "Epoch 11/24\n",
      "3690/3690 [==============================] - 2s 564us/step - loss: 0.0017 - val_loss: 6.3281e-04\n",
      "Epoch 12/24\n",
      "3690/3690 [==============================] - 2s 564us/step - loss: 0.0016 - val_loss: 6.3427e-04\n",
      "Epoch 13/24\n",
      "3690/3690 [==============================] - 2s 564us/step - loss: 0.0016 - val_loss: 6.4276e-04\n",
      "Epoch 14/24\n",
      "3690/3690 [==============================] - 2s 568us/step - loss: 0.0016 - val_loss: 0.0016\n",
      "Epoch 15/24\n",
      "3690/3690 [==============================] - 2s 564us/step - loss: 0.0020 - val_loss: 9.7968e-04\n",
      "Epoch 16/24\n",
      "3690/3690 [==============================] - 2s 568us/step - loss: 0.0020 - val_loss: 0.0018\n",
      "Epoch 17/24\n",
      "3690/3690 [==============================] - 2s 566us/step - loss: 0.0026 - val_loss: 0.0011\n",
      "Epoch 18/24\n",
      "3690/3690 [==============================] - 2s 564us/step - loss: 0.0016 - val_loss: 8.2124e-04\n",
      "Epoch 19/24\n",
      "3690/3690 [==============================] - 2s 566us/step - loss: 0.0015 - val_loss: 7.5265e-04\n",
      "Epoch 20/24\n",
      "3690/3690 [==============================] - 2s 563us/step - loss: 0.0014 - val_loss: 6.3325e-04\n",
      "Epoch 21/24\n",
      "3690/3690 [==============================] - 2s 563us/step - loss: 0.0015 - val_loss: 0.0015\n",
      "Epoch 22/24\n",
      "3690/3690 [==============================] - 2s 566us/step - loss: 0.0016 - val_loss: 5.7874e-04\n",
      "Epoch 23/24\n",
      "3690/3690 [==============================] - 2s 567us/step - loss: 0.0014 - val_loss: 6.2411e-04\n",
      "Epoch 24/24\n",
      "3690/3690 [==============================] - 2s 564us/step - loss: 0.0015 - val_loss: 5.7815e-04\n",
      "\n",
      "GENERATION 17\n",
      "\n",
      "Train on 3690 samples, validate on 410 samples\n",
      "Epoch 1/24\n",
      "3690/3690 [==============================] - 39s 11ms/step - loss: 0.1090 - val_loss: 0.0040\n",
      "Epoch 2/24\n",
      "3690/3690 [==============================] - 2s 554us/step - loss: 0.0431 - val_loss: 0.0191\n",
      "Epoch 3/24\n",
      "3690/3690 [==============================] - 2s 553us/step - loss: 0.0119 - val_loss: 0.0388\n",
      "Epoch 4/24\n",
      "3690/3690 [==============================] - 2s 547us/step - loss: 0.0064 - val_loss: 0.0010\n",
      "Epoch 5/24\n",
      "3690/3690 [==============================] - 2s 551us/step - loss: 0.0036 - val_loss: 0.0017\n",
      "Epoch 6/24\n",
      "3690/3690 [==============================] - 2s 550us/step - loss: 0.0026 - val_loss: 0.0016\n",
      "Epoch 7/24\n",
      "3690/3690 [==============================] - 2s 552us/step - loss: 0.0018 - val_loss: 0.0019\n",
      "Epoch 8/24\n",
      "3690/3690 [==============================] - 2s 554us/step - loss: 0.0015 - val_loss: 0.0018\n",
      "Epoch 9/24\n",
      "3690/3690 [==============================] - 2s 552us/step - loss: 0.0014 - val_loss: 0.0012\n",
      "Epoch 10/24\n",
      "3690/3690 [==============================] - 2s 551us/step - loss: 0.0012 - val_loss: 0.0012\n",
      "Epoch 11/24\n",
      "3690/3690 [==============================] - 2s 557us/step - loss: 0.0012 - val_loss: 9.6269e-04\n",
      "Epoch 12/24\n",
      "3690/3690 [==============================] - 2s 554us/step - loss: 0.0011 - val_loss: 9.4040e-04\n",
      "Epoch 13/24\n",
      "3690/3690 [==============================] - 2s 548us/step - loss: 0.0011 - val_loss: 0.0010\n",
      "Epoch 14/24\n",
      "3690/3690 [==============================] - 2s 552us/step - loss: 0.0011 - val_loss: 9.3065e-04\n",
      "Epoch 15/24\n",
      "3690/3690 [==============================] - 2s 550us/step - loss: 9.9763e-04 - val_loss: 6.2581e-04\n",
      "Epoch 16/24\n",
      "3690/3690 [==============================] - 2s 548us/step - loss: 9.9822e-04 - val_loss: 5.9997e-04\n",
      "Epoch 17/24\n",
      "3690/3690 [==============================] - 2s 550us/step - loss: 9.7493e-04 - val_loss: 8.7101e-04\n",
      "Epoch 18/24\n",
      "3690/3690 [==============================] - 2s 555us/step - loss: 9.6903e-04 - val_loss: 5.8356e-04\n",
      "Epoch 19/24\n",
      "3690/3690 [==============================] - 2s 552us/step - loss: 9.2955e-04 - val_loss: 8.2783e-04\n",
      "Epoch 20/24\n",
      "3690/3690 [==============================] - 2s 552us/step - loss: 8.7490e-04 - val_loss: 5.6616e-04\n",
      "Epoch 21/24\n",
      "3690/3690 [==============================] - 2s 551us/step - loss: 8.2615e-04 - val_loss: 5.1464e-04\n",
      "Epoch 22/24\n",
      "3690/3690 [==============================] - 2s 553us/step - loss: 7.9858e-04 - val_loss: 5.1176e-04\n",
      "Epoch 23/24\n",
      "3690/3690 [==============================] - 2s 557us/step - loss: 8.2841e-04 - val_loss: 7.1710e-04\n",
      "Epoch 24/24\n",
      "3690/3690 [==============================] - 2s 552us/step - loss: 8.1280e-04 - val_loss: 4.9916e-04\n",
      "Train on 3690 samples, validate on 410 samples\n",
      "Epoch 1/24\n",
      "3690/3690 [==============================] - 40s 11ms/step - loss: 0.3850 - val_loss: 0.0332\n",
      "Epoch 2/24\n",
      "3690/3690 [==============================] - 2s 555us/step - loss: 0.1132 - val_loss: 0.1219\n",
      "Epoch 3/24\n",
      "3690/3690 [==============================] - 2s 552us/step - loss: 0.0715 - val_loss: 0.0049\n",
      "Epoch 4/24\n",
      "3690/3690 [==============================] - 2s 555us/step - loss: 0.0573 - val_loss: 0.0072\n",
      "Epoch 5/24\n",
      "3690/3690 [==============================] - 2s 551us/step - loss: 0.0095 - val_loss: 0.0073\n",
      "Epoch 6/24\n",
      "3690/3690 [==============================] - 2s 552us/step - loss: 0.0076 - val_loss: 0.0206\n",
      "Epoch 7/24\n",
      "3690/3690 [==============================] - 2s 556us/step - loss: 0.0037 - val_loss: 0.0149\n",
      "Epoch 8/24\n",
      "3690/3690 [==============================] - 2s 591us/step - loss: 0.0023 - val_loss: 0.0020\n",
      "Epoch 9/24\n",
      "3690/3690 [==============================] - 2s 570us/step - loss: 0.0018 - val_loss: 0.0024\n",
      "Epoch 10/24\n",
      "3690/3690 [==============================] - 2s 557us/step - loss: 0.0017 - val_loss: 0.0027\n",
      "Epoch 11/24\n",
      "3690/3690 [==============================] - 2s 560us/step - loss: 0.0015 - val_loss: 9.5511e-04\n",
      "Epoch 12/24\n",
      "3690/3690 [==============================] - 2s 558us/step - loss: 0.0015 - val_loss: 0.0025\n",
      "Epoch 13/24\n",
      "3690/3690 [==============================] - 2s 546us/step - loss: 0.0014 - val_loss: 0.0012\n",
      "Epoch 14/24\n",
      "3690/3690 [==============================] - 2s 551us/step - loss: 0.0014 - val_loss: 0.0011\n",
      "Epoch 15/24\n",
      "3690/3690 [==============================] - 2s 550us/step - loss: 0.0013 - val_loss: 0.0012\n",
      "Epoch 16/24\n",
      "3690/3690 [==============================] - 2s 555us/step - loss: 0.0013 - val_loss: 9.5441e-04\n",
      "Epoch 17/24\n",
      "3690/3690 [==============================] - 2s 555us/step - loss: 0.0012 - val_loss: 0.0014\n",
      "Epoch 18/24\n",
      "3690/3690 [==============================] - 2s 544us/step - loss: 0.0012 - val_loss: 7.0714e-04\n",
      "Epoch 19/24\n",
      "3690/3690 [==============================] - 2s 552us/step - loss: 0.0013 - val_loss: 8.1750e-04\n",
      "Epoch 20/24\n",
      "3690/3690 [==============================] - 2s 551us/step - loss: 0.0012 - val_loss: 6.6848e-04\n",
      "Epoch 21/24\n",
      "3690/3690 [==============================] - 2s 549us/step - loss: 0.0012 - val_loss: 7.8858e-04\n",
      "Epoch 22/24\n",
      "3690/3690 [==============================] - 2s 560us/step - loss: 0.0011 - val_loss: 0.0023\n",
      "Epoch 23/24\n",
      "3690/3690 [==============================] - 2s 547us/step - loss: 0.0011 - val_loss: 8.6520e-04\n",
      "Epoch 24/24\n",
      "3690/3690 [==============================] - 2s 546us/step - loss: 0.0011 - val_loss: 6.3002e-04\n",
      "Train on 3690 samples, validate on 410 samples\n",
      "Epoch 1/24\n",
      "3690/3690 [==============================] - 40s 11ms/step - loss: 0.1365 - val_loss: 0.0097\n",
      "Epoch 2/24\n",
      "3690/3690 [==============================] - 2s 544us/step - loss: 0.0545 - val_loss: 0.0031\n",
      "Epoch 3/24\n",
      "3690/3690 [==============================] - 2s 546us/step - loss: 0.0115 - val_loss: 0.0412\n",
      "Epoch 4/24\n",
      "3690/3690 [==============================] - 2s 546us/step - loss: 0.0037 - val_loss: 0.0037\n",
      "Epoch 5/24\n",
      "3690/3690 [==============================] - 2s 549us/step - loss: 0.0024 - val_loss: 0.0046\n",
      "Epoch 6/24\n",
      "3690/3690 [==============================] - 2s 548us/step - loss: 0.0018 - val_loss: 0.0012\n",
      "Epoch 7/24\n",
      "3690/3690 [==============================] - 2s 547us/step - loss: 0.0015 - val_loss: 8.1232e-04\n",
      "Epoch 8/24\n",
      "3690/3690 [==============================] - 2s 546us/step - loss: 0.0014 - val_loss: 0.0020\n",
      "Epoch 9/24\n",
      "3690/3690 [==============================] - 2s 548us/step - loss: 0.0013 - val_loss: 9.5450e-04\n",
      "Epoch 10/24\n",
      "3690/3690 [==============================] - 2s 546us/step - loss: 0.0012 - val_loss: 6.7558e-04\n",
      "Epoch 11/24\n",
      "3690/3690 [==============================] - 2s 546us/step - loss: 0.0011 - val_loss: 6.4533e-04\n",
      "Epoch 12/24\n",
      "3690/3690 [==============================] - 2s 545us/step - loss: 0.0010 - val_loss: 7.7380e-04\n",
      "Epoch 13/24\n",
      "3690/3690 [==============================] - 2s 544us/step - loss: 0.0010 - val_loss: 5.9472e-04\n",
      "Epoch 14/24\n",
      "3690/3690 [==============================] - 2s 545us/step - loss: 9.8900e-04 - val_loss: 5.5541e-04\n",
      "Epoch 15/24\n",
      "3690/3690 [==============================] - 2s 545us/step - loss: 9.6740e-04 - val_loss: 0.0012\n",
      "Epoch 16/24\n",
      "3690/3690 [==============================] - 2s 547us/step - loss: 0.0011 - val_loss: 5.4866e-04\n",
      "Epoch 17/24\n",
      "3690/3690 [==============================] - 2s 546us/step - loss: 9.9127e-04 - val_loss: 5.7018e-04\n",
      "Epoch 18/24\n",
      "3690/3690 [==============================] - 2s 546us/step - loss: 8.7716e-04 - val_loss: 5.6297e-04\n",
      "Epoch 19/24\n",
      "3690/3690 [==============================] - 2s 546us/step - loss: 8.1226e-04 - val_loss: 5.8928e-04\n",
      "Epoch 20/24\n",
      "3690/3690 [==============================] - 2s 548us/step - loss: 8.4346e-04 - val_loss: 7.4188e-04\n",
      "Epoch 21/24\n",
      "3690/3690 [==============================] - 2s 545us/step - loss: 8.4939e-04 - val_loss: 9.1735e-04\n",
      "Epoch 22/24\n",
      "3690/3690 [==============================] - 2s 549us/step - loss: 8.8536e-04 - val_loss: 0.0010\n",
      "Epoch 23/24\n",
      "3690/3690 [==============================] - 2s 546us/step - loss: 8.8569e-04 - val_loss: 4.9615e-04\n",
      "Epoch 24/24\n",
      "3690/3690 [==============================] - 2s 545us/step - loss: 8.1514e-04 - val_loss: 6.7013e-04\n",
      "Train on 3690 samples, validate on 410 samples\n",
      "Epoch 1/24\n",
      "3690/3690 [==============================] - 41s 11ms/step - loss: 0.9690 - val_loss: 0.8042\n",
      "Epoch 2/24\n",
      "3690/3690 [==============================] - 2s 544us/step - loss: 0.4189 - val_loss: 0.3543\n",
      "Epoch 3/24\n",
      "3690/3690 [==============================] - 2s 545us/step - loss: 0.1727 - val_loss: 0.1419\n",
      "Epoch 4/24\n",
      "3690/3690 [==============================] - 2s 547us/step - loss: 0.0845 - val_loss: 0.0537\n",
      "Epoch 5/24\n",
      "3690/3690 [==============================] - 2s 546us/step - loss: 0.0688 - val_loss: 0.0243\n",
      "Epoch 6/24\n",
      "3690/3690 [==============================] - 2s 545us/step - loss: 0.0716 - val_loss: 0.0187\n",
      "Epoch 7/24\n",
      "3690/3690 [==============================] - 2s 545us/step - loss: 0.0723 - val_loss: 0.0221\n",
      "Epoch 8/24\n",
      "3690/3690 [==============================] - 2s 565us/step - loss: 0.0701 - val_loss: 0.0302\n",
      "Epoch 9/24\n",
      "3690/3690 [==============================] - 2s 547us/step - loss: 0.0684 - val_loss: 0.0376\n",
      "Epoch 10/24\n",
      "3690/3690 [==============================] - 2s 547us/step - loss: 0.0682 - val_loss: 0.0420\n",
      "Epoch 11/24\n",
      "3690/3690 [==============================] - 2s 547us/step - loss: 0.0682 - val_loss: 0.0419\n",
      "Epoch 12/24\n",
      "3690/3690 [==============================] - 2s 547us/step - loss: 0.0682 - val_loss: 0.0403\n",
      "Epoch 13/24\n",
      "3690/3690 [==============================] - 2s 547us/step - loss: 0.0681 - val_loss: 0.0387\n",
      "Epoch 14/24\n",
      "3690/3690 [==============================] - 2s 547us/step - loss: 0.0681 - val_loss: 0.0377\n",
      "Epoch 15/24\n",
      "3690/3690 [==============================] - 2s 547us/step - loss: 0.0681 - val_loss: 0.0377\n",
      "Epoch 16/24\n",
      "3690/3690 [==============================] - 2s 548us/step - loss: 0.0681 - val_loss: 0.0379\n",
      "Epoch 17/24\n",
      "3690/3690 [==============================] - 2s 544us/step - loss: 0.0681 - val_loss: 0.0379\n",
      "Epoch 18/24\n",
      "3690/3690 [==============================] - 2s 546us/step - loss: 0.0681 - val_loss: 0.0381\n",
      "Epoch 19/24\n",
      "3690/3690 [==============================] - 2s 545us/step - loss: 0.0681 - val_loss: 0.0379\n",
      "Epoch 20/24\n",
      "3690/3690 [==============================] - 2s 542us/step - loss: 0.0681 - val_loss: 0.0381\n",
      "Epoch 21/24\n",
      "3690/3690 [==============================] - 2s 549us/step - loss: 0.0681 - val_loss: 0.0384\n",
      "Epoch 22/24\n",
      "3690/3690 [==============================] - 2s 549us/step - loss: 0.0681 - val_loss: 0.0380\n",
      "Epoch 23/24\n",
      "3690/3690 [==============================] - 2s 546us/step - loss: 0.0681 - val_loss: 0.0386\n",
      "Epoch 24/24\n",
      "3690/3690 [==============================] - 2s 548us/step - loss: 0.0681 - val_loss: 0.0390\n",
      "Train on 3690 samples, validate on 410 samples\n",
      "Epoch 1/24\n",
      "3690/3690 [==============================] - 42s 11ms/step - loss: 0.2434 - val_loss: 0.1120\n",
      "Epoch 2/24\n",
      "3690/3690 [==============================] - 2s 581us/step - loss: 0.0736 - val_loss: 0.0120\n",
      "Epoch 3/24\n",
      "3690/3690 [==============================] - 2s 580us/step - loss: 0.0840 - val_loss: 0.0104\n",
      "Epoch 4/24\n",
      "3690/3690 [==============================] - 2s 585us/step - loss: 0.0750 - val_loss: 0.0333\n",
      "Epoch 5/24\n",
      "3690/3690 [==============================] - 2s 584us/step - loss: 0.0690 - val_loss: 0.0564\n",
      "Epoch 6/24\n",
      "3690/3690 [==============================] - 2s 580us/step - loss: 0.0699 - val_loss: 0.0488\n",
      "Epoch 7/24\n",
      "3690/3690 [==============================] - 2s 582us/step - loss: 0.0684 - val_loss: 0.0360\n",
      "Epoch 8/24\n",
      "3690/3690 [==============================] - 2s 585us/step - loss: 0.0684 - val_loss: 0.0329\n",
      "Epoch 9/24\n",
      "3690/3690 [==============================] - 2s 584us/step - loss: 0.0682 - val_loss: 0.0391\n",
      "Epoch 10/24\n",
      "3690/3690 [==============================] - 2s 588us/step - loss: 0.0682 - val_loss: 0.0411\n",
      "Epoch 11/24\n",
      "3690/3690 [==============================] - 2s 581us/step - loss: 0.0681 - val_loss: 0.0377\n",
      "Epoch 12/24\n",
      "3690/3690 [==============================] - 2s 580us/step - loss: 0.0682 - val_loss: 0.0362\n",
      "Epoch 13/24\n",
      "3690/3690 [==============================] - 2s 581us/step - loss: 0.0680 - val_loss: 0.0399\n",
      "Epoch 14/24\n",
      "3690/3690 [==============================] - 2s 581us/step - loss: 0.0679 - val_loss: 0.0398\n",
      "Epoch 15/24\n",
      "3690/3690 [==============================] - 2s 579us/step - loss: 0.0674 - val_loss: 0.0393\n",
      "Epoch 16/24\n",
      "3690/3690 [==============================] - 2s 583us/step - loss: 0.0666 - val_loss: 0.0348\n",
      "Epoch 17/24\n",
      "3690/3690 [==============================] - 2s 581us/step - loss: 0.0682 - val_loss: 0.0391\n",
      "Epoch 18/24\n",
      "3690/3690 [==============================] - 2s 582us/step - loss: 0.0681 - val_loss: 0.0403\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19/24\n",
      "3690/3690 [==============================] - 2s 581us/step - loss: 0.0681 - val_loss: 0.0386\n",
      "Epoch 20/24\n",
      "3690/3690 [==============================] - 2s 580us/step - loss: 0.0676 - val_loss: 0.0424\n",
      "Epoch 21/24\n",
      "3690/3690 [==============================] - 2s 584us/step - loss: 0.0645 - val_loss: 0.0326\n",
      "Epoch 22/24\n",
      "3690/3690 [==============================] - 2s 582us/step - loss: 0.0585 - val_loss: 0.0204\n",
      "Epoch 23/24\n",
      "3690/3690 [==============================] - 2s 585us/step - loss: 0.0639 - val_loss: 0.0417\n",
      "Epoch 24/24\n",
      "3690/3690 [==============================] - 2s 580us/step - loss: 0.0681 - val_loss: 0.0332\n",
      "Train on 3690 samples, validate on 410 samples\n",
      "Epoch 1/24\n",
      "3690/3690 [==============================] - 42s 11ms/step - loss: 0.0627 - val_loss: 0.0206\n",
      "Epoch 2/24\n",
      "3690/3690 [==============================] - 2s 580us/step - loss: 0.0307 - val_loss: 0.0137\n",
      "Epoch 3/24\n",
      "3690/3690 [==============================] - 2s 581us/step - loss: 0.0087 - val_loss: 0.0424\n",
      "Epoch 4/24\n",
      "3690/3690 [==============================] - 2s 582us/step - loss: 0.0037 - val_loss: 0.0028\n",
      "Epoch 5/24\n",
      "3690/3690 [==============================] - 2s 581us/step - loss: 0.0020 - val_loss: 0.0015\n",
      "Epoch 6/24\n",
      "3690/3690 [==============================] - 2s 583us/step - loss: 0.0016 - val_loss: 9.5892e-04\n",
      "Epoch 7/24\n",
      "3690/3690 [==============================] - 2s 581us/step - loss: 0.0013 - val_loss: 0.0010\n",
      "Epoch 8/24\n",
      "3690/3690 [==============================] - 2s 581us/step - loss: 0.0013 - val_loss: 0.0014\n",
      "Epoch 9/24\n",
      "3690/3690 [==============================] - 2s 579us/step - loss: 0.0012 - val_loss: 0.0017\n",
      "Epoch 10/24\n",
      "3690/3690 [==============================] - 2s 582us/step - loss: 9.8331e-04 - val_loss: 5.5290e-04\n",
      "Epoch 11/24\n",
      "3690/3690 [==============================] - 2s 581us/step - loss: 9.1866e-04 - val_loss: 8.5344e-04\n",
      "Epoch 12/24\n",
      "3690/3690 [==============================] - 2s 581us/step - loss: 8.0314e-04 - val_loss: 7.7218e-04\n",
      "Epoch 13/24\n",
      "3690/3690 [==============================] - 2s 582us/step - loss: 8.2454e-04 - val_loss: 0.0022\n",
      "Epoch 14/24\n",
      "3690/3690 [==============================] - 2s 578us/step - loss: 8.4520e-04 - val_loss: 0.0019\n",
      "Epoch 15/24\n",
      "3690/3690 [==============================] - 2s 583us/step - loss: 8.7274e-04 - val_loss: 4.9038e-04\n",
      "Epoch 16/24\n",
      "3690/3690 [==============================] - 2s 582us/step - loss: 8.3185e-04 - val_loss: 6.9285e-04\n",
      "Epoch 17/24\n",
      "3690/3690 [==============================] - 2s 583us/step - loss: 8.0182e-04 - val_loss: 0.0011\n",
      "Epoch 18/24\n",
      "3690/3690 [==============================] - 2s 582us/step - loss: 6.7474e-04 - val_loss: 0.0010\n",
      "Epoch 19/24\n",
      "3690/3690 [==============================] - 2s 583us/step - loss: 6.7204e-04 - val_loss: 0.0012\n",
      "Epoch 20/24\n",
      "3690/3690 [==============================] - 2s 578us/step - loss: 6.9604e-04 - val_loss: 5.4689e-04\n",
      "Epoch 21/24\n",
      "3690/3690 [==============================] - 2s 578us/step - loss: 6.2588e-04 - val_loss: 7.4253e-04\n",
      "Epoch 22/24\n",
      "3690/3690 [==============================] - 2s 580us/step - loss: 8.4670e-04 - val_loss: 5.7186e-04\n",
      "Epoch 23/24\n",
      "3690/3690 [==============================] - 2s 583us/step - loss: 7.6819e-04 - val_loss: 0.0017\n",
      "Epoch 24/24\n",
      "3690/3690 [==============================] - 2s 581us/step - loss: 7.6546e-04 - val_loss: 7.4221e-04\n",
      "Train on 3690 samples, validate on 410 samples\n",
      "Epoch 1/24\n",
      "3690/3690 [==============================] - 42s 11ms/step - loss: 0.3152 - val_loss: 0.0569\n",
      "Epoch 2/24\n",
      "3690/3690 [==============================] - 2s 581us/step - loss: 0.0805 - val_loss: 0.0062\n",
      "Epoch 3/24\n",
      "3690/3690 [==============================] - 2s 579us/step - loss: 0.0882 - val_loss: 0.0252\n",
      "Epoch 4/24\n",
      "3690/3690 [==============================] - 2s 580us/step - loss: 0.0695 - val_loss: 0.0656\n",
      "Epoch 5/24\n",
      "3690/3690 [==============================] - 2s 580us/step - loss: 0.0711 - val_loss: 0.0454\n",
      "Epoch 6/24\n",
      "3690/3690 [==============================] - 2s 581us/step - loss: 0.0684 - val_loss: 0.0280\n",
      "Epoch 7/24\n",
      "3690/3690 [==============================] - 2s 579us/step - loss: 0.0687 - val_loss: 0.0354\n",
      "Epoch 8/24\n",
      "3690/3690 [==============================] - 2s 579us/step - loss: 0.0679 - val_loss: 0.0428\n",
      "Epoch 9/24\n",
      "3690/3690 [==============================] - 2s 580us/step - loss: 0.0677 - val_loss: 0.0396\n",
      "Epoch 10/24\n",
      "3690/3690 [==============================] - 2s 581us/step - loss: 0.0662 - val_loss: 0.0342\n",
      "Epoch 11/24\n",
      "3690/3690 [==============================] - 2s 579us/step - loss: 0.0642 - val_loss: 0.0379\n",
      "Epoch 12/24\n",
      "3690/3690 [==============================] - 2s 578us/step - loss: 0.0595 - val_loss: 0.0285\n",
      "Epoch 13/24\n",
      "3690/3690 [==============================] - 2s 579us/step - loss: 0.0468 - val_loss: 0.0099\n",
      "Epoch 14/24\n",
      "3690/3690 [==============================] - 2s 582us/step - loss: 0.0412 - val_loss: 0.0066\n",
      "Epoch 15/24\n",
      "3690/3690 [==============================] - 2s 581us/step - loss: 0.0771 - val_loss: 0.0750\n",
      "Epoch 16/24\n",
      "3690/3690 [==============================] - 2s 582us/step - loss: 0.0709 - val_loss: 0.0308\n",
      "Epoch 17/24\n",
      "3690/3690 [==============================] - 2s 581us/step - loss: 0.0684 - val_loss: 0.0321\n",
      "Epoch 18/24\n",
      "3690/3690 [==============================] - 2s 577us/step - loss: 0.0677 - val_loss: 0.0438\n",
      "Epoch 19/24\n",
      "3690/3690 [==============================] - 2s 580us/step - loss: 0.0677 - val_loss: 0.0377\n",
      "Epoch 20/24\n",
      "3690/3690 [==============================] - 2s 579us/step - loss: 0.0674 - val_loss: 0.0366\n",
      "Epoch 21/24\n",
      "3690/3690 [==============================] - 2s 580us/step - loss: 0.0666 - val_loss: 0.0406\n",
      "Epoch 22/24\n",
      "3690/3690 [==============================] - 2s 581us/step - loss: 0.0660 - val_loss: 0.0400\n",
      "Epoch 23/24\n",
      "3690/3690 [==============================] - 2s 579us/step - loss: 0.0644 - val_loss: 0.0401\n",
      "Epoch 24/24\n",
      "3690/3690 [==============================] - 2s 579us/step - loss: 0.0580 - val_loss: 0.0300\n",
      "Train on 3690 samples, validate on 410 samples\n",
      "Epoch 1/24\n",
      "3690/3690 [==============================] - 43s 12ms/step - loss: nan - val_loss: nan\n",
      "Epoch 2/24\n",
      "3690/3690 [==============================] - 2s 578us/step - loss: nan - val_loss: nan\n",
      "Epoch 3/24\n",
      "3690/3690 [==============================] - 2s 576us/step - loss: nan - val_loss: nan\n",
      "Epoch 4/24\n",
      "3690/3690 [==============================] - 2s 576us/step - loss: nan - val_loss: nan\n",
      "Epoch 5/24\n",
      "3690/3690 [==============================] - 2s 578us/step - loss: nan - val_loss: nan\n",
      "Epoch 6/24\n",
      "3690/3690 [==============================] - 2s 575us/step - loss: nan - val_loss: nan\n",
      "Epoch 7/24\n",
      "3690/3690 [==============================] - 2s 577us/step - loss: nan - val_loss: nan\n",
      "Epoch 8/24\n",
      "3690/3690 [==============================] - 2s 576us/step - loss: nan - val_loss: nan\n",
      "Epoch 9/24\n",
      "3690/3690 [==============================] - 2s 576us/step - loss: nan - val_loss: nan\n",
      "Epoch 10/24\n",
      "3690/3690 [==============================] - 2s 578us/step - loss: nan - val_loss: nan\n",
      "Epoch 11/24\n",
      "3690/3690 [==============================] - 2s 576us/step - loss: nan - val_loss: nan\n",
      "Epoch 12/24\n",
      "3690/3690 [==============================] - 2s 578us/step - loss: nan - val_loss: nan\n",
      "Epoch 13/24\n",
      "3690/3690 [==============================] - 2s 580us/step - loss: nan - val_loss: nan\n",
      "Epoch 14/24\n",
      "3690/3690 [==============================] - 2s 576us/step - loss: nan - val_loss: nan\n",
      "Epoch 15/24\n",
      "3690/3690 [==============================] - 2s 577us/step - loss: nan - val_loss: nan\n",
      "Epoch 16/24\n",
      "3690/3690 [==============================] - 2s 574us/step - loss: nan - val_loss: nan\n",
      "Epoch 17/24\n",
      "3690/3690 [==============================] - 2s 577us/step - loss: nan - val_loss: nan\n",
      "Epoch 18/24\n",
      "3690/3690 [==============================] - 2s 577us/step - loss: nan - val_loss: nan\n",
      "Epoch 19/24\n",
      "3690/3690 [==============================] - 2s 576us/step - loss: nan - val_loss: nan\n",
      "Epoch 20/24\n",
      "3690/3690 [==============================] - 2s 578us/step - loss: nan - val_loss: nan\n",
      "Epoch 21/24\n",
      "3690/3690 [==============================] - 2s 575us/step - loss: nan - val_loss: nan\n",
      "Epoch 22/24\n",
      "3690/3690 [==============================] - 2s 578us/step - loss: nan - val_loss: nan\n",
      "Epoch 23/24\n",
      "3690/3690 [==============================] - 2s 579us/step - loss: nan - val_loss: nan\n",
      "Epoch 24/24\n",
      "3690/3690 [==============================] - 2s 578us/step - loss: nan - val_loss: nan\n",
      "Train on 3690 samples, validate on 410 samples\n",
      "Epoch 1/24\n",
      "3690/3690 [==============================] - 43s 12ms/step - loss: nan - val_loss: nan\n",
      "Epoch 2/24\n",
      "3690/3690 [==============================] - 2s 614us/step - loss: nan - val_loss: nan\n",
      "Epoch 3/24\n",
      "3690/3690 [==============================] - 2s 623us/step - loss: nan - val_loss: nan\n",
      "Epoch 4/24\n",
      "3690/3690 [==============================] - 2s 614us/step - loss: nan - val_loss: nan\n",
      "Epoch 5/24\n",
      "3690/3690 [==============================] - 2s 611us/step - loss: nan - val_loss: nan\n",
      "Epoch 6/24\n",
      "3690/3690 [==============================] - 2s 613us/step - loss: nan - val_loss: nan\n",
      "Epoch 7/24\n",
      "3690/3690 [==============================] - 2s 612us/step - loss: nan - val_loss: nan\n",
      "Epoch 8/24\n",
      "3690/3690 [==============================] - 2s 612us/step - loss: nan - val_loss: nan\n",
      "Epoch 9/24\n",
      "3690/3690 [==============================] - 2s 614us/step - loss: nan - val_loss: nan\n",
      "Epoch 10/24\n",
      "3690/3690 [==============================] - 2s 616us/step - loss: nan - val_loss: nan\n",
      "Epoch 11/24\n",
      "3690/3690 [==============================] - 2s 614us/step - loss: nan - val_loss: nan\n",
      "Epoch 12/24\n",
      "3690/3690 [==============================] - 2s 615us/step - loss: nan - val_loss: nan\n",
      "Epoch 13/24\n",
      "3690/3690 [==============================] - 2s 613us/step - loss: nan - val_loss: nan\n",
      "Epoch 14/24\n",
      "3690/3690 [==============================] - 2s 614us/step - loss: nan - val_loss: nan\n",
      "Epoch 15/24\n",
      "3690/3690 [==============================] - 2s 613us/step - loss: nan - val_loss: nan\n",
      "Epoch 16/24\n",
      "3690/3690 [==============================] - 2s 611us/step - loss: nan - val_loss: nan\n",
      "Epoch 17/24\n",
      "3690/3690 [==============================] - 2s 614us/step - loss: nan - val_loss: nan\n",
      "Epoch 18/24\n",
      "3690/3690 [==============================] - 2s 614us/step - loss: nan - val_loss: nan\n",
      "Epoch 19/24\n",
      "3690/3690 [==============================] - 2s 612us/step - loss: nan - val_loss: nan\n",
      "Epoch 20/24\n",
      "3690/3690 [==============================] - 2s 612us/step - loss: nan - val_loss: nan\n",
      "Epoch 21/24\n",
      "3690/3690 [==============================] - 2s 612us/step - loss: nan - val_loss: nan\n",
      "Epoch 22/24\n",
      "3690/3690 [==============================] - 2s 612us/step - loss: nan - val_loss: nan\n",
      "Epoch 23/24\n",
      "3690/3690 [==============================] - 2s 612us/step - loss: nan - val_loss: nan\n",
      "Epoch 24/24\n",
      "3690/3690 [==============================] - 2s 613us/step - loss: nan - val_loss: nan\n",
      "\n",
      "GENERATION 18\n",
      "\n",
      "Train on 3690 samples, validate on 410 samples\n",
      "Epoch 1/24\n",
      "3690/3690 [==============================] - 42s 11ms/step - loss: 1.7122 - val_loss: 0.3233\n",
      "Epoch 2/24\n",
      "3690/3690 [==============================] - 2s 546us/step - loss: 0.2103 - val_loss: 0.0370\n",
      "Epoch 3/24\n",
      "3690/3690 [==============================] - 2s 544us/step - loss: 0.0826 - val_loss: 0.1196\n",
      "Epoch 4/24\n",
      "3690/3690 [==============================] - 2s 544us/step - loss: 0.0788 - val_loss: 0.0323\n",
      "Epoch 5/24\n",
      "3690/3690 [==============================] - 2s 548us/step - loss: 0.0647 - val_loss: 0.0126\n",
      "Epoch 6/24\n",
      "3690/3690 [==============================] - 2s 547us/step - loss: 0.0565 - val_loss: 0.0202\n",
      "Epoch 7/24\n",
      "3690/3690 [==============================] - 2s 544us/step - loss: 0.0307 - val_loss: 0.0092\n",
      "Epoch 8/24\n",
      "3690/3690 [==============================] - 2s 543us/step - loss: 0.0068 - val_loss: 0.0115\n",
      "Epoch 9/24\n",
      "3690/3690 [==============================] - 2s 545us/step - loss: 0.0058 - val_loss: 0.0150\n",
      "Epoch 10/24\n",
      "3690/3690 [==============================] - 2s 541us/step - loss: 0.0035 - val_loss: 0.0079\n",
      "Epoch 11/24\n",
      "3690/3690 [==============================] - 2s 541us/step - loss: 0.0024 - val_loss: 0.0030\n",
      "Epoch 12/24\n",
      "3690/3690 [==============================] - 2s 544us/step - loss: 0.0023 - val_loss: 0.0024\n",
      "Epoch 13/24\n",
      "3690/3690 [==============================] - 2s 545us/step - loss: 0.0019 - val_loss: 0.0027\n",
      "Epoch 14/24\n",
      "3690/3690 [==============================] - 2s 544us/step - loss: 0.0018 - val_loss: 0.0031\n",
      "Epoch 15/24\n",
      "3690/3690 [==============================] - 2s 544us/step - loss: 0.0016 - val_loss: 0.0026\n",
      "Epoch 16/24\n",
      "3690/3690 [==============================] - 2s 543us/step - loss: 0.0015 - val_loss: 0.0016\n",
      "Epoch 17/24\n",
      "3690/3690 [==============================] - 2s 541us/step - loss: 0.0015 - val_loss: 0.0010\n",
      "Epoch 18/24\n",
      "3690/3690 [==============================] - 2s 547us/step - loss: 0.0014 - val_loss: 7.2292e-04\n",
      "Epoch 19/24\n",
      "3690/3690 [==============================] - 2s 545us/step - loss: 0.0015 - val_loss: 8.9185e-04\n",
      "Epoch 20/24\n",
      "3690/3690 [==============================] - 2s 544us/step - loss: 0.0014 - val_loss: 9.1145e-04\n",
      "Epoch 21/24\n",
      "3690/3690 [==============================] - 2s 548us/step - loss: 0.0014 - val_loss: 6.7747e-04\n",
      "Epoch 22/24\n",
      "3690/3690 [==============================] - 2s 545us/step - loss: 0.0013 - val_loss: 6.9327e-04\n",
      "Epoch 23/24\n",
      "3690/3690 [==============================] - 2s 544us/step - loss: 0.0013 - val_loss: 0.0012\n",
      "Epoch 24/24\n",
      "3690/3690 [==============================] - 2s 544us/step - loss: 0.0013 - val_loss: 6.5544e-04\n",
      "Train on 3690 samples, validate on 410 samples\n",
      "Epoch 1/24\n",
      "3690/3690 [==============================] - 44s 12ms/step - loss: 0.2954 - val_loss: 0.1676\n",
      "Epoch 2/24\n",
      "3690/3690 [==============================] - 2s 544us/step - loss: 0.1131 - val_loss: 0.0188\n",
      "Epoch 3/24\n",
      "3690/3690 [==============================] - 2s 545us/step - loss: 0.0717 - val_loss: 0.0053\n",
      "Epoch 4/24\n",
      "3690/3690 [==============================] - 2s 545us/step - loss: 0.0160 - val_loss: 0.0378\n",
      "Epoch 5/24\n",
      "3690/3690 [==============================] - 2s 544us/step - loss: 0.0106 - val_loss: 0.0051\n",
      "Epoch 6/24\n",
      "3690/3690 [==============================] - 2s 539us/step - loss: 0.0027 - val_loss: 0.0029\n",
      "Epoch 7/24\n",
      "3690/3690 [==============================] - 2s 540us/step - loss: 0.0023 - val_loss: 0.0055\n",
      "Epoch 8/24\n",
      "3690/3690 [==============================] - 2s 544us/step - loss: 0.0021 - val_loss: 0.0026\n",
      "Epoch 9/24\n",
      "3690/3690 [==============================] - 2s 546us/step - loss: 0.0016 - val_loss: 0.0011\n",
      "Epoch 10/24\n",
      "3690/3690 [==============================] - 2s 542us/step - loss: 0.0016 - val_loss: 8.1971e-04\n",
      "Epoch 11/24\n",
      "3690/3690 [==============================] - 2s 547us/step - loss: 0.0016 - val_loss: 0.0022\n",
      "Epoch 12/24\n",
      "3690/3690 [==============================] - 2s 542us/step - loss: 0.0015 - val_loss: 7.1378e-04\n",
      "Epoch 13/24\n",
      "3690/3690 [==============================] - 2s 539us/step - loss: 0.0014 - val_loss: 9.2882e-04\n",
      "Epoch 14/24\n",
      "3690/3690 [==============================] - 2s 540us/step - loss: 0.0013 - val_loss: 0.0019\n",
      "Epoch 15/24\n",
      "3690/3690 [==============================] - 2s 544us/step - loss: 0.0014 - val_loss: 7.8764e-04\n",
      "Epoch 16/24\n",
      "3690/3690 [==============================] - 2s 545us/step - loss: 0.0014 - val_loss: 0.0026\n",
      "Epoch 17/24\n",
      "3690/3690 [==============================] - 2s 542us/step - loss: 0.0013 - val_loss: 6.2689e-04\n",
      "Epoch 18/24\n",
      "3690/3690 [==============================] - 2s 541us/step - loss: 0.0012 - val_loss: 0.0010\n",
      "Epoch 19/24\n",
      "3690/3690 [==============================] - 2s 548us/step - loss: 0.0011 - val_loss: 0.0011\n",
      "Epoch 20/24\n",
      "3690/3690 [==============================] - 2s 542us/step - loss: 0.0011 - val_loss: 7.8161e-04\n",
      "Epoch 21/24\n",
      "3690/3690 [==============================] - 2s 540us/step - loss: 0.0010 - val_loss: 0.0011\n",
      "Epoch 22/24\n",
      "3690/3690 [==============================] - 2s 543us/step - loss: 0.0011 - val_loss: 7.9386e-04\n",
      "Epoch 23/24\n",
      "3690/3690 [==============================] - 2s 539us/step - loss: 0.0010 - val_loss: 5.8644e-04\n",
      "Epoch 24/24\n",
      "3690/3690 [==============================] - 2s 541us/step - loss: 0.0010 - val_loss: 8.8182e-04\n",
      "Train on 3690 samples, validate on 410 samples\n",
      "Epoch 1/24\n",
      "3690/3690 [==============================] - 44s 12ms/step - loss: 1.1503 - val_loss: 1.0111\n",
      "Epoch 2/24\n",
      "3690/3690 [==============================] - 2s 548us/step - loss: 0.5736 - val_loss: 0.5286\n",
      "Epoch 3/24\n",
      "3690/3690 [==============================] - 2s 543us/step - loss: 0.2793 - val_loss: 0.2684\n",
      "Epoch 4/24\n",
      "3690/3690 [==============================] - 2s 541us/step - loss: 0.1408 - val_loss: 0.1312\n",
      "Epoch 5/24\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3690/3690 [==============================] - 2s 542us/step - loss: 0.0850 - val_loss: 0.0655\n",
      "Epoch 6/24\n",
      "3690/3690 [==============================] - 2s 543us/step - loss: 0.0696 - val_loss: 0.0376\n",
      "Epoch 7/24\n",
      "3690/3690 [==============================] - 2s 541us/step - loss: 0.0684 - val_loss: 0.0280\n",
      "Epoch 8/24\n",
      "3690/3690 [==============================] - 2s 541us/step - loss: 0.0694 - val_loss: 0.0266\n",
      "Epoch 9/24\n",
      "3690/3690 [==============================] - 2s 545us/step - loss: 0.0693 - val_loss: 0.0292\n",
      "Epoch 10/24\n",
      "3690/3690 [==============================] - 2s 543us/step - loss: 0.0687 - val_loss: 0.0330\n",
      "Epoch 11/24\n",
      "3690/3690 [==============================] - 2s 539us/step - loss: 0.0683 - val_loss: 0.0364\n",
      "Epoch 12/24\n",
      "3690/3690 [==============================] - 2s 542us/step - loss: 0.0681 - val_loss: 0.0387\n",
      "Epoch 13/24\n",
      "3690/3690 [==============================] - 2s 541us/step - loss: 0.0681 - val_loss: 0.0397\n",
      "Epoch 14/24\n",
      "3690/3690 [==============================] - 2s 543us/step - loss: 0.0682 - val_loss: 0.0394\n",
      "Epoch 15/24\n",
      "3690/3690 [==============================] - 2s 542us/step - loss: 0.0681 - val_loss: 0.0392\n",
      "Epoch 16/24\n",
      "3690/3690 [==============================] - 2s 543us/step - loss: 0.0681 - val_loss: 0.0382\n",
      "Epoch 17/24\n",
      "3690/3690 [==============================] - 2s 542us/step - loss: 0.0681 - val_loss: 0.0383\n",
      "Epoch 18/24\n",
      "3690/3690 [==============================] - 2s 548us/step - loss: 0.0681 - val_loss: 0.0383\n",
      "Epoch 19/24\n",
      "3690/3690 [==============================] - 2s 544us/step - loss: 0.0681 - val_loss: 0.0383\n",
      "Epoch 20/24\n",
      "3690/3690 [==============================] - 2s 544us/step - loss: 0.0681 - val_loss: 0.0381\n",
      "Epoch 21/24\n",
      "3690/3690 [==============================] - 2s 542us/step - loss: 0.0681 - val_loss: 0.0382\n",
      "Epoch 22/24\n",
      "3690/3690 [==============================] - 2s 540us/step - loss: 0.0682 - val_loss: 0.0373\n",
      "Epoch 23/24\n",
      "3690/3690 [==============================] - 2s 542us/step - loss: 0.0682 - val_loss: 0.0371\n",
      "Epoch 24/24\n",
      "3690/3690 [==============================] - 2s 545us/step - loss: 0.0681 - val_loss: 0.0376\n",
      "Train on 3690 samples, validate on 410 samples\n",
      "Epoch 1/24\n",
      "3690/3690 [==============================] - 45s 12ms/step - loss: 0.1890 - val_loss: 0.0794\n",
      "Epoch 2/24\n",
      "3690/3690 [==============================] - 2s 542us/step - loss: 0.0734 - val_loss: 0.0100\n",
      "Epoch 3/24\n",
      "3690/3690 [==============================] - 2s 539us/step - loss: 0.0816 - val_loss: 0.0163\n",
      "Epoch 4/24\n",
      "3690/3690 [==============================] - 2s 542us/step - loss: 0.0704 - val_loss: 0.0468\n",
      "Epoch 5/24\n",
      "3690/3690 [==============================] - 2s 543us/step - loss: 0.0692 - val_loss: 0.0535\n",
      "Epoch 6/24\n",
      "3690/3690 [==============================] - 2s 542us/step - loss: 0.0685 - val_loss: 0.0398\n",
      "Epoch 7/24\n",
      "3690/3690 [==============================] - 2s 545us/step - loss: 0.0668 - val_loss: 0.0335\n",
      "Epoch 8/24\n",
      "3690/3690 [==============================] - 2s 541us/step - loss: 0.0653 - val_loss: 0.0321\n",
      "Epoch 9/24\n",
      "3690/3690 [==============================] - 2s 540us/step - loss: 0.0682 - val_loss: 0.0394\n",
      "Epoch 10/24\n",
      "3690/3690 [==============================] - 2s 539us/step - loss: 0.0682 - val_loss: 0.0436\n",
      "Epoch 11/24\n",
      "3690/3690 [==============================] - 2s 541us/step - loss: 0.0679 - val_loss: 0.0395\n",
      "Epoch 12/24\n",
      "3690/3690 [==============================] - 2s 538us/step - loss: 0.0656 - val_loss: 0.0414\n",
      "Epoch 13/24\n",
      "3690/3690 [==============================] - 2s 541us/step - loss: 0.0592 - val_loss: 0.0234\n",
      "Epoch 14/24\n",
      "3690/3690 [==============================] - 2s 542us/step - loss: 0.0693 - val_loss: 0.0384\n",
      "Epoch 15/24\n",
      "3690/3690 [==============================] - 2s 547us/step - loss: 0.0683 - val_loss: 0.0467\n",
      "Epoch 16/24\n",
      "3690/3690 [==============================] - 2s 544us/step - loss: 0.0679 - val_loss: 0.0345\n",
      "Epoch 17/24\n",
      "3690/3690 [==============================] - 2s 541us/step - loss: 0.0677 - val_loss: 0.0391\n",
      "Epoch 18/24\n",
      "3690/3690 [==============================] - 2s 542us/step - loss: 0.0668 - val_loss: 0.0414\n",
      "Epoch 19/24\n",
      "3690/3690 [==============================] - 2s 542us/step - loss: 0.0587 - val_loss: 0.0221\n",
      "Epoch 20/24\n",
      "3690/3690 [==============================] - 2s 541us/step - loss: 0.0419 - val_loss: 0.0064\n",
      "Epoch 21/24\n",
      "3690/3690 [==============================] - 2s 542us/step - loss: 0.0622 - val_loss: 0.0199\n",
      "Epoch 22/24\n",
      "3690/3690 [==============================] - 2s 538us/step - loss: 0.0700 - val_loss: 0.0375\n",
      "Epoch 23/24\n",
      "3690/3690 [==============================] - 2s 541us/step - loss: 0.0429 - val_loss: 0.0058\n",
      "Epoch 24/24\n",
      "3690/3690 [==============================] - 2s 539us/step - loss: 0.0242 - val_loss: 0.0095\n",
      "Train on 3690 samples, validate on 410 samples\n",
      "Epoch 1/24\n",
      "3690/3690 [==============================] - 45s 12ms/step - loss: 1.0298 - val_loss: 0.9089\n",
      "Epoch 2/24\n",
      "3690/3690 [==============================] - 2s 537us/step - loss: 0.5037 - val_loss: 0.4543\n",
      "Epoch 3/24\n",
      "3690/3690 [==============================] - 2s 538us/step - loss: 0.2326 - val_loss: 0.2149\n",
      "Epoch 4/24\n",
      "3690/3690 [==============================] - 2s 539us/step - loss: 0.1154 - val_loss: 0.0983\n",
      "Epoch 5/24\n",
      "3690/3690 [==============================] - 2s 538us/step - loss: 0.0755 - val_loss: 0.0489\n",
      "Epoch 6/24\n",
      "3690/3690 [==============================] - 2s 537us/step - loss: 0.0683 - val_loss: 0.0305\n",
      "Epoch 7/24\n",
      "3690/3690 [==============================] - 2s 538us/step - loss: 0.0693 - val_loss: 0.0256\n",
      "Epoch 8/24\n",
      "3690/3690 [==============================] - 2s 537us/step - loss: 0.0697 - val_loss: 0.0268\n",
      "Epoch 9/24\n",
      "3690/3690 [==============================] - 2s 538us/step - loss: 0.0692 - val_loss: 0.0303\n",
      "Epoch 10/24\n",
      "3690/3690 [==============================] - 2s 536us/step - loss: 0.0686 - val_loss: 0.0340\n",
      "Epoch 11/24\n",
      "3690/3690 [==============================] - 2s 535us/step - loss: 0.0682 - val_loss: 0.0375\n",
      "Epoch 12/24\n",
      "3690/3690 [==============================] - 2s 539us/step - loss: 0.0682 - val_loss: 0.0395\n",
      "Epoch 13/24\n",
      "3690/3690 [==============================] - 2s 539us/step - loss: 0.0682 - val_loss: 0.0402\n",
      "Epoch 14/24\n",
      "3690/3690 [==============================] - 2s 540us/step - loss: 0.0682 - val_loss: 0.0398\n",
      "Epoch 15/24\n",
      "3690/3690 [==============================] - 2s 537us/step - loss: 0.0681 - val_loss: 0.0392\n",
      "Epoch 16/24\n",
      "3690/3690 [==============================] - 2s 539us/step - loss: 0.0681 - val_loss: 0.0389\n",
      "Epoch 17/24\n",
      "3690/3690 [==============================] - 2s 538us/step - loss: 0.0681 - val_loss: 0.0385\n",
      "Epoch 18/24\n",
      "3690/3690 [==============================] - 2s 539us/step - loss: 0.0681 - val_loss: 0.0381\n",
      "Epoch 19/24\n",
      "3690/3690 [==============================] - 2s 539us/step - loss: 0.0681 - val_loss: 0.0379\n",
      "Epoch 20/24\n",
      "3690/3690 [==============================] - 2s 542us/step - loss: 0.0681 - val_loss: 0.0381\n",
      "Epoch 21/24\n",
      "3690/3690 [==============================] - 2s 538us/step - loss: 0.0681 - val_loss: 0.0382\n",
      "Epoch 22/24\n",
      "3690/3690 [==============================] - 2s 538us/step - loss: 0.0681 - val_loss: 0.0390\n",
      "Epoch 23/24\n",
      "3690/3690 [==============================] - 2s 538us/step - loss: 0.0681 - val_loss: 0.0383\n",
      "Epoch 24/24\n",
      "3690/3690 [==============================] - 2s 537us/step - loss: 0.0681 - val_loss: 0.0375\n",
      "Train on 3690 samples, validate on 410 samples\n",
      "Epoch 1/24\n",
      "3690/3690 [==============================] - 46s 12ms/step - loss: 2.6149 - val_loss: 0.0038\n",
      "Epoch 2/24\n",
      "3690/3690 [==============================] - 2s 544us/step - loss: 0.0146 - val_loss: 0.0016\n",
      "Epoch 3/24\n",
      "3690/3690 [==============================] - 2s 547us/step - loss: 0.0047 - val_loss: 0.0014\n",
      "Epoch 4/24\n",
      "3690/3690 [==============================] - 2s 541us/step - loss: 0.0027 - val_loss: 0.0015\n",
      "Epoch 5/24\n",
      "3690/3690 [==============================] - 2s 546us/step - loss: 0.0024 - val_loss: 0.0021\n",
      "Epoch 6/24\n",
      "3690/3690 [==============================] - 2s 546us/step - loss: 0.0023 - val_loss: 0.0020\n",
      "Epoch 7/24\n",
      "3690/3690 [==============================] - 2s 542us/step - loss: 0.0022 - val_loss: 0.0015\n",
      "Epoch 8/24\n",
      "3690/3690 [==============================] - 2s 550us/step - loss: 0.0022 - val_loss: 0.0020\n",
      "Epoch 9/24\n",
      "3690/3690 [==============================] - 2s 546us/step - loss: 0.0020 - val_loss: 0.0015\n",
      "Epoch 10/24\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3690/3690 [==============================] - 2s 547us/step - loss: 0.0020 - val_loss: 0.0019\n",
      "Epoch 11/24\n",
      "3690/3690 [==============================] - 2s 545us/step - loss: 0.0020 - val_loss: 0.0015\n",
      "Epoch 12/24\n",
      "3690/3690 [==============================] - 2s 544us/step - loss: 0.0019 - val_loss: 0.0019\n",
      "Epoch 13/24\n",
      "3690/3690 [==============================] - 2s 544us/step - loss: 0.0020 - val_loss: 0.0019\n",
      "Epoch 14/24\n",
      "3690/3690 [==============================] - 2s 542us/step - loss: 0.0018 - val_loss: 0.0018\n",
      "Epoch 15/24\n",
      "3690/3690 [==============================] - 2s 545us/step - loss: 0.0019 - val_loss: 0.0012\n",
      "Epoch 16/24\n",
      "3690/3690 [==============================] - 2s 546us/step - loss: 0.0018 - val_loss: 0.0017\n",
      "Epoch 17/24\n",
      "3690/3690 [==============================] - 2s 544us/step - loss: 0.0018 - val_loss: 0.0016\n",
      "Epoch 18/24\n",
      "3690/3690 [==============================] - 2s 540us/step - loss: 0.0017 - val_loss: 0.0014\n",
      "Epoch 19/24\n",
      "3690/3690 [==============================] - 2s 544us/step - loss: 0.0017 - val_loss: 0.0012\n",
      "Epoch 20/24\n",
      "3690/3690 [==============================] - 2s 545us/step - loss: 0.0017 - val_loss: 0.0015\n",
      "Epoch 21/24\n",
      "3690/3690 [==============================] - 2s 544us/step - loss: 0.0017 - val_loss: 0.0018\n",
      "Epoch 22/24\n",
      "3690/3690 [==============================] - 2s 544us/step - loss: 0.0016 - val_loss: 0.0014\n",
      "Epoch 23/24\n",
      "3690/3690 [==============================] - 2s 544us/step - loss: 0.0016 - val_loss: 0.0011\n",
      "Epoch 24/24\n",
      "3690/3690 [==============================] - 2s 545us/step - loss: 0.0016 - val_loss: 0.0012\n",
      "Train on 3690 samples, validate on 410 samples\n",
      "Epoch 1/24\n",
      "3690/3690 [==============================] - 46s 12ms/step - loss: 0.0867 - val_loss: 0.0126\n",
      "Epoch 2/24\n",
      "3690/3690 [==============================] - 2s 537us/step - loss: 0.0316 - val_loss: 0.0079\n",
      "Epoch 3/24\n",
      "3690/3690 [==============================] - 2s 538us/step - loss: 0.0215 - val_loss: 0.0055\n",
      "Epoch 4/24\n",
      "3690/3690 [==============================] - 2s 539us/step - loss: 0.0150 - val_loss: 0.0042\n",
      "Epoch 5/24\n",
      "3690/3690 [==============================] - 2s 541us/step - loss: 0.0111 - val_loss: 0.0018\n",
      "Epoch 6/24\n",
      "3690/3690 [==============================] - 2s 535us/step - loss: 0.0079 - val_loss: 0.0016\n",
      "Epoch 7/24\n",
      "3690/3690 [==============================] - 2s 537us/step - loss: 0.0062 - val_loss: 0.0010\n",
      "Epoch 8/24\n",
      "3690/3690 [==============================] - 2s 539us/step - loss: 0.0051 - val_loss: 8.9486e-04\n",
      "Epoch 9/24\n",
      "3690/3690 [==============================] - 2s 534us/step - loss: 0.0044 - val_loss: 8.2437e-04\n",
      "Epoch 10/24\n",
      "3690/3690 [==============================] - 2s 536us/step - loss: 0.0039 - val_loss: 9.4322e-04\n",
      "Epoch 11/24\n",
      "3690/3690 [==============================] - 2s 537us/step - loss: 0.0036 - val_loss: 0.0010\n",
      "Epoch 12/24\n",
      "3690/3690 [==============================] - 2s 539us/step - loss: 0.0032 - val_loss: 8.3930e-04\n",
      "Epoch 13/24\n",
      "3690/3690 [==============================] - 2s 538us/step - loss: 0.0033 - val_loss: 0.0010\n",
      "Epoch 14/24\n",
      "3690/3690 [==============================] - 2s 537us/step - loss: 0.0034 - val_loss: 0.0012\n",
      "Epoch 15/24\n",
      "3690/3690 [==============================] - 2s 537us/step - loss: 0.0031 - val_loss: 0.0011\n",
      "Epoch 16/24\n",
      "3690/3690 [==============================] - 2s 536us/step - loss: 0.0032 - val_loss: 9.9502e-04\n",
      "Epoch 17/24\n",
      "3690/3690 [==============================] - 2s 535us/step - loss: 0.0030 - val_loss: 0.0013\n",
      "Epoch 18/24\n",
      "3690/3690 [==============================] - 2s 536us/step - loss: 0.0029 - val_loss: 0.0015\n",
      "Epoch 19/24\n",
      "3690/3690 [==============================] - 2s 537us/step - loss: 0.0030 - val_loss: 0.0013\n",
      "Epoch 20/24\n",
      "3690/3690 [==============================] - 2s 535us/step - loss: 0.0030 - val_loss: 0.0013\n",
      "Epoch 21/24\n",
      "3690/3690 [==============================] - 2s 540us/step - loss: 0.0030 - val_loss: 0.0014\n",
      "Epoch 22/24\n",
      "3690/3690 [==============================] - 2s 534us/step - loss: 0.0030 - val_loss: 0.0013\n",
      "Epoch 23/24\n",
      "3690/3690 [==============================] - 2s 536us/step - loss: 0.0029 - val_loss: 0.0014\n",
      "Epoch 24/24\n",
      "3690/3690 [==============================] - 2s 536us/step - loss: 0.0028 - val_loss: 0.0012\n",
      "Train on 3690 samples, validate on 410 samples\n",
      "Epoch 1/24\n",
      "3690/3690 [==============================] - 47s 13ms/step - loss: 0.1401 - val_loss: 0.0018\n",
      "Epoch 2/24\n",
      "3690/3690 [==============================] - 2s 581us/step - loss: 0.0152 - val_loss: 0.0165\n",
      "Epoch 3/24\n",
      "3690/3690 [==============================] - 2s 582us/step - loss: 0.0060 - val_loss: 0.0050\n",
      "Epoch 4/24\n",
      "3690/3690 [==============================] - 2s 582us/step - loss: 0.0031 - val_loss: 0.0037\n",
      "Epoch 5/24\n",
      "3690/3690 [==============================] - 2s 581us/step - loss: 0.0021 - val_loss: 9.0902e-04\n",
      "Epoch 6/24\n",
      "3690/3690 [==============================] - 2s 580us/step - loss: 0.0018 - val_loss: 8.7141e-04\n",
      "Epoch 7/24\n",
      "3690/3690 [==============================] - 2s 582us/step - loss: 0.0017 - val_loss: 7.1718e-04\n",
      "Epoch 8/24\n",
      "3690/3690 [==============================] - 2s 581us/step - loss: 0.0016 - val_loss: 6.8779e-04\n",
      "Epoch 9/24\n",
      "3690/3690 [==============================] - 2s 581us/step - loss: 0.0016 - val_loss: 8.2124e-04\n",
      "Epoch 10/24\n",
      "3690/3690 [==============================] - 2s 582us/step - loss: 0.0015 - val_loss: 6.8406e-04\n",
      "Epoch 11/24\n",
      "3690/3690 [==============================] - 2s 581us/step - loss: 0.0014 - val_loss: 7.0174e-04\n",
      "Epoch 12/24\n",
      "3690/3690 [==============================] - 2s 585us/step - loss: 0.0014 - val_loss: 6.9272e-04\n",
      "Epoch 13/24\n",
      "3690/3690 [==============================] - 2s 581us/step - loss: 0.0014 - val_loss: 6.8047e-04\n",
      "Epoch 14/24\n",
      "3690/3690 [==============================] - 2s 584us/step - loss: 0.0014 - val_loss: 6.9273e-04\n",
      "Epoch 15/24\n",
      "3690/3690 [==============================] - 2s 586us/step - loss: 0.0014 - val_loss: 6.6436e-04\n",
      "Epoch 16/24\n",
      "3690/3690 [==============================] - 2s 580us/step - loss: 0.0014 - val_loss: 6.9243e-04\n",
      "Epoch 17/24\n",
      "3690/3690 [==============================] - 2s 581us/step - loss: 0.0014 - val_loss: 7.0592e-04\n",
      "Epoch 18/24\n",
      "3690/3690 [==============================] - 2s 579us/step - loss: 0.0014 - val_loss: 6.6563e-04\n",
      "Epoch 19/24\n",
      "3690/3690 [==============================] - 2s 580us/step - loss: 0.0013 - val_loss: 7.1552e-04\n",
      "Epoch 20/24\n",
      "3690/3690 [==============================] - 2s 581us/step - loss: 0.0013 - val_loss: 6.6234e-04\n",
      "Epoch 21/24\n",
      "3690/3690 [==============================] - 2s 578us/step - loss: 0.0013 - val_loss: 7.8828e-04\n",
      "Epoch 22/24\n",
      "3690/3690 [==============================] - 2s 582us/step - loss: 0.0013 - val_loss: 6.5527e-04\n",
      "Epoch 23/24\n",
      "3690/3690 [==============================] - 2s 582us/step - loss: 0.0013 - val_loss: 7.7298e-04\n",
      "Epoch 24/24\n",
      "3690/3690 [==============================] - 2s 583us/step - loss: 0.0012 - val_loss: 7.8428e-04\n",
      "Train on 3690 samples, validate on 410 samples\n",
      "Epoch 1/24\n",
      "3690/3690 [==============================] - 47s 13ms/step - loss: 0.3031 - val_loss: 0.0090\n",
      "Epoch 2/24\n",
      "3690/3690 [==============================] - 2s 583us/step - loss: 0.0729 - val_loss: 0.0790\n",
      "Epoch 3/24\n",
      "3690/3690 [==============================] - 2s 583us/step - loss: 0.0800 - val_loss: 0.0900\n",
      "Epoch 4/24\n",
      "3690/3690 [==============================] - 2s 580us/step - loss: 0.0765 - val_loss: 0.0621\n",
      "Epoch 5/24\n",
      "3690/3690 [==============================] - 2s 584us/step - loss: 0.0695 - val_loss: 0.0391\n",
      "Epoch 6/24\n",
      "3690/3690 [==============================] - 2s 581us/step - loss: 0.0683 - val_loss: 0.0312\n",
      "Epoch 7/24\n",
      "3690/3690 [==============================] - 2s 582us/step - loss: 0.0686 - val_loss: 0.0317\n",
      "Epoch 8/24\n",
      "3690/3690 [==============================] - 2s 586us/step - loss: 0.0684 - val_loss: 0.0352\n",
      "Epoch 9/24\n",
      "3690/3690 [==============================] - 2s 584us/step - loss: 0.0682 - val_loss: 0.0386\n",
      "Epoch 10/24\n",
      "3690/3690 [==============================] - 2s 582us/step - loss: 0.0681 - val_loss: 0.0391\n",
      "Epoch 11/24\n",
      "3690/3690 [==============================] - 2s 582us/step - loss: 0.0681 - val_loss: 0.0398\n",
      "Epoch 12/24\n",
      "3690/3690 [==============================] - 2s 584us/step - loss: 0.0682 - val_loss: 0.0386\n",
      "Epoch 13/24\n",
      "3690/3690 [==============================] - 2s 581us/step - loss: 0.0681 - val_loss: 0.0383\n",
      "Epoch 14/24\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3690/3690 [==============================] - 2s 584us/step - loss: 0.0681 - val_loss: 0.0385\n",
      "Epoch 15/24\n",
      "3690/3690 [==============================] - 2s 586us/step - loss: 0.0681 - val_loss: 0.0384\n",
      "Epoch 16/24\n",
      "3690/3690 [==============================] - 2s 582us/step - loss: 0.0681 - val_loss: 0.0381\n",
      "Epoch 17/24\n",
      "3690/3690 [==============================] - 2s 582us/step - loss: 0.0680 - val_loss: 0.0387\n",
      "Epoch 18/24\n",
      "3690/3690 [==============================] - 2s 582us/step - loss: 0.0680 - val_loss: 0.0386\n",
      "Epoch 19/24\n",
      "3690/3690 [==============================] - 2s 584us/step - loss: 0.0678 - val_loss: 0.0375\n",
      "Epoch 20/24\n",
      "3690/3690 [==============================] - 2s 582us/step - loss: 0.0676 - val_loss: 0.0360\n",
      "Epoch 21/24\n",
      "3690/3690 [==============================] - 2s 584us/step - loss: 0.0672 - val_loss: 0.0361\n",
      "Epoch 22/24\n",
      "3690/3690 [==============================] - 2s 585us/step - loss: 0.0666 - val_loss: 0.0364\n",
      "Epoch 23/24\n",
      "3690/3690 [==============================] - 2s 584us/step - loss: 0.0659 - val_loss: 0.0361\n",
      "Epoch 24/24\n",
      "3690/3690 [==============================] - 2s 588us/step - loss: 0.0651 - val_loss: 0.0417\n",
      "\n",
      "GENERATION 19\n",
      "\n",
      "Train on 3690 samples, validate on 410 samples\n",
      "Epoch 1/24\n",
      "3690/3690 [==============================] - 48s 13ms/step - loss: 0.3005 - val_loss: 0.0058\n",
      "Epoch 2/24\n",
      "3690/3690 [==============================] - 2s 546us/step - loss: 0.0840 - val_loss: 0.1308\n",
      "Epoch 3/24\n",
      "3690/3690 [==============================] - 2s 545us/step - loss: 0.0773 - val_loss: 0.0144\n",
      "Epoch 4/24\n",
      "3690/3690 [==============================] - 2s 547us/step - loss: 0.0629 - val_loss: 0.0063\n",
      "Epoch 5/24\n",
      "3690/3690 [==============================] - 2s 551us/step - loss: 0.0194 - val_loss: 0.0384\n",
      "Epoch 6/24\n",
      "3690/3690 [==============================] - 2s 546us/step - loss: 0.0089 - val_loss: 0.0048\n",
      "Epoch 7/24\n",
      "3690/3690 [==============================] - 2s 548us/step - loss: 0.0040 - val_loss: 0.0034\n",
      "Epoch 8/24\n",
      "3690/3690 [==============================] - 2s 549us/step - loss: 0.0026 - val_loss: 0.0038\n",
      "Epoch 9/24\n",
      "3690/3690 [==============================] - 2s 548us/step - loss: 0.0021 - val_loss: 0.0011\n",
      "Epoch 10/24\n",
      "3690/3690 [==============================] - 2s 545us/step - loss: 0.0019 - val_loss: 0.0029\n",
      "Epoch 11/24\n",
      "3690/3690 [==============================] - 2s 545us/step - loss: 0.0017 - val_loss: 0.0011\n",
      "Epoch 12/24\n",
      "3690/3690 [==============================] - 2s 544us/step - loss: 0.0016 - val_loss: 0.0014\n",
      "Epoch 13/24\n",
      "3690/3690 [==============================] - 2s 546us/step - loss: 0.0015 - val_loss: 9.4494e-04\n",
      "Epoch 14/24\n",
      "3690/3690 [==============================] - 2s 546us/step - loss: 0.0015 - val_loss: 0.0013\n",
      "Epoch 15/24\n",
      "3690/3690 [==============================] - 2s 549us/step - loss: 0.0014 - val_loss: 0.0012\n",
      "Epoch 16/24\n",
      "3690/3690 [==============================] - 2s 547us/step - loss: 0.0014 - val_loss: 8.6578e-04\n",
      "Epoch 17/24\n",
      "3690/3690 [==============================] - 2s 545us/step - loss: 0.0013 - val_loss: 9.4966e-04\n",
      "Epoch 18/24\n",
      "3690/3690 [==============================] - 2s 547us/step - loss: 0.0012 - val_loss: 6.3844e-04\n",
      "Epoch 19/24\n",
      "3690/3690 [==============================] - 2s 548us/step - loss: 0.0013 - val_loss: 0.0010\n",
      "Epoch 20/24\n",
      "3690/3690 [==============================] - 2s 547us/step - loss: 0.0012 - val_loss: 0.0012\n",
      "Epoch 21/24\n",
      "3690/3690 [==============================] - 2s 545us/step - loss: 0.0011 - val_loss: 8.0663e-04\n",
      "Epoch 22/24\n",
      "3690/3690 [==============================] - 2s 548us/step - loss: 0.0011 - val_loss: 0.0010\n",
      "Epoch 23/24\n",
      "3690/3690 [==============================] - 2s 549us/step - loss: 0.0011 - val_loss: 6.9317e-04\n",
      "Epoch 24/24\n",
      "3690/3690 [==============================] - 2s 550us/step - loss: 9.9260e-04 - val_loss: 5.3829e-04\n",
      "Train on 3690 samples, validate on 410 samples\n",
      "Epoch 1/24\n",
      "3690/3690 [==============================] - 50s 13ms/step - loss: 0.3970 - val_loss: 0.2010\n",
      "Epoch 2/24\n",
      "3690/3690 [==============================] - 2s 543us/step - loss: 0.0882 - val_loss: 0.0063\n",
      "Epoch 3/24\n",
      "3690/3690 [==============================] - 2s 540us/step - loss: 0.0668 - val_loss: 0.0739\n",
      "Epoch 4/24\n",
      "3690/3690 [==============================] - 2s 541us/step - loss: 0.0455 - val_loss: 0.0187\n",
      "Epoch 5/24\n",
      "3690/3690 [==============================] - 2s 541us/step - loss: 0.0147 - val_loss: 0.0087\n",
      "Epoch 6/24\n",
      "3690/3690 [==============================] - 2s 542us/step - loss: 0.0051 - val_loss: 0.0176\n",
      "Epoch 7/24\n",
      "3690/3690 [==============================] - 2s 541us/step - loss: 0.0040 - val_loss: 0.0121\n",
      "Epoch 8/24\n",
      "3690/3690 [==============================] - 2s 536us/step - loss: 0.0024 - val_loss: 0.0043\n",
      "Epoch 9/24\n",
      "3690/3690 [==============================] - 2s 541us/step - loss: 0.0019 - val_loss: 0.0021\n",
      "Epoch 10/24\n",
      "3690/3690 [==============================] - 2s 539us/step - loss: 0.0015 - val_loss: 0.0025\n",
      "Epoch 11/24\n",
      "3690/3690 [==============================] - 2s 538us/step - loss: 0.0013 - val_loss: 0.0017\n",
      "Epoch 12/24\n",
      "3690/3690 [==============================] - 2s 542us/step - loss: 0.0012 - val_loss: 8.1195e-04\n",
      "Epoch 13/24\n",
      "3690/3690 [==============================] - 2s 544us/step - loss: 0.0012 - val_loss: 9.9109e-04\n",
      "Epoch 14/24\n",
      "3690/3690 [==============================] - 2s 541us/step - loss: 0.0011 - val_loss: 7.8818e-04\n",
      "Epoch 15/24\n",
      "3690/3690 [==============================] - 2s 545us/step - loss: 0.0011 - val_loss: 0.0010\n",
      "Epoch 16/24\n",
      "3690/3690 [==============================] - 2s 541us/step - loss: 0.0010 - val_loss: 8.0067e-04\n",
      "Epoch 17/24\n",
      "3690/3690 [==============================] - 2s 540us/step - loss: 0.0010 - val_loss: 9.6054e-04\n",
      "Epoch 18/24\n",
      "3690/3690 [==============================] - 2s 542us/step - loss: 9.7644e-04 - val_loss: 7.4788e-04\n",
      "Epoch 19/24\n",
      "3690/3690 [==============================] - 2s 540us/step - loss: 9.4027e-04 - val_loss: 8.8202e-04\n",
      "Epoch 20/24\n",
      "3690/3690 [==============================] - 2s 537us/step - loss: 9.4664e-04 - val_loss: 5.9185e-04\n",
      "Epoch 21/24\n",
      "3690/3690 [==============================] - 2s 539us/step - loss: 9.1029e-04 - val_loss: 5.3127e-04\n",
      "Epoch 22/24\n",
      "3690/3690 [==============================] - 2s 541us/step - loss: 9.6977e-04 - val_loss: 0.0019\n",
      "Epoch 23/24\n",
      "3690/3690 [==============================] - 2s 544us/step - loss: 9.8173e-04 - val_loss: 5.6826e-04\n",
      "Epoch 24/24\n",
      "3690/3690 [==============================] - 2s 542us/step - loss: 9.1303e-04 - val_loss: 7.2065e-04\n",
      "Train on 3690 samples, validate on 410 samples\n",
      "Epoch 1/24\n",
      "3690/3690 [==============================] - 49s 13ms/step - loss: 0.7822 - val_loss: 0.1180\n",
      "Epoch 2/24\n",
      "3690/3690 [==============================] - 2s 551us/step - loss: 0.1243 - val_loss: 0.1001\n",
      "Epoch 3/24\n",
      "3690/3690 [==============================] - 2s 548us/step - loss: 0.0897 - val_loss: 0.0703\n",
      "Epoch 4/24\n",
      "3690/3690 [==============================] - 2s 546us/step - loss: 0.0618 - val_loss: 0.0076\n",
      "Epoch 5/24\n",
      "3690/3690 [==============================] - 2s 546us/step - loss: 0.0459 - val_loss: 0.0071\n",
      "Epoch 6/24\n",
      "3690/3690 [==============================] - 2s 546us/step - loss: 0.0092 - val_loss: 0.0150\n",
      "Epoch 7/24\n",
      "3690/3690 [==============================] - 2s 549us/step - loss: 0.0078 - val_loss: 0.0026\n",
      "Epoch 8/24\n",
      "3690/3690 [==============================] - 2s 549us/step - loss: 0.0039 - val_loss: 0.0010\n",
      "Epoch 9/24\n",
      "3690/3690 [==============================] - 2s 547us/step - loss: 0.0030 - val_loss: 9.6538e-04\n",
      "Epoch 10/24\n",
      "3690/3690 [==============================] - 2s 549us/step - loss: 0.0027 - val_loss: 0.0014\n",
      "Epoch 11/24\n",
      "3690/3690 [==============================] - 2s 546us/step - loss: 0.0024 - val_loss: 0.0022\n",
      "Epoch 12/24\n",
      "3690/3690 [==============================] - 2s 545us/step - loss: 0.0023 - val_loss: 0.0023\n",
      "Epoch 13/24\n",
      "3690/3690 [==============================] - 2s 546us/step - loss: 0.0023 - val_loss: 0.0014\n",
      "Epoch 14/24\n",
      "3690/3690 [==============================] - 2s 550us/step - loss: 0.0021 - val_loss: 0.0019\n",
      "Epoch 15/24\n",
      "3690/3690 [==============================] - 2s 552us/step - loss: 0.0021 - val_loss: 0.0012\n",
      "Epoch 16/24\n",
      "3690/3690 [==============================] - 2s 546us/step - loss: 0.0020 - val_loss: 8.2213e-04\n",
      "Epoch 17/24\n",
      "3690/3690 [==============================] - 2s 547us/step - loss: 0.0020 - val_loss: 0.0023\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18/24\n",
      "3690/3690 [==============================] - 2s 547us/step - loss: 0.0019 - val_loss: 0.0012\n",
      "Epoch 19/24\n",
      "3690/3690 [==============================] - 2s 547us/step - loss: 0.0017 - val_loss: 7.4691e-04\n",
      "Epoch 20/24\n",
      "3690/3690 [==============================] - 2s 546us/step - loss: 0.0018 - val_loss: 9.4973e-04\n",
      "Epoch 21/24\n",
      "3690/3690 [==============================] - 2s 545us/step - loss: 0.0018 - val_loss: 0.0011\n",
      "Epoch 22/24\n",
      "3690/3690 [==============================] - 2s 547us/step - loss: 0.0017 - val_loss: 7.9220e-04\n",
      "Epoch 23/24\n",
      "3690/3690 [==============================] - 2s 552us/step - loss: 0.0017 - val_loss: 9.8549e-04\n",
      "Epoch 24/24\n",
      "3690/3690 [==============================] - 2s 547us/step - loss: 0.0016 - val_loss: 8.4080e-04\n",
      "Train on 3690 samples, validate on 410 samples\n",
      "Epoch 1/24\n",
      "3690/3690 [==============================] - 51s 14ms/step - loss: 0.0747 - val_loss: 0.0137\n",
      "Epoch 2/24\n",
      "3690/3690 [==============================] - 2s 546us/step - loss: 0.0094 - val_loss: 6.0641e-04\n",
      "Epoch 3/24\n",
      "3690/3690 [==============================] - 2s 545us/step - loss: 0.0050 - val_loss: 0.0110\n",
      "Epoch 4/24\n",
      "3690/3690 [==============================] - 2s 549us/step - loss: 0.0039 - val_loss: 5.7257e-04\n",
      "Epoch 5/24\n",
      "3690/3690 [==============================] - 2s 547us/step - loss: 0.0027 - val_loss: 0.0030\n",
      "Epoch 6/24\n",
      "3690/3690 [==============================] - 2s 551us/step - loss: 0.0022 - val_loss: 0.0017\n",
      "Epoch 7/24\n",
      "3690/3690 [==============================] - 2s 545us/step - loss: 0.0021 - val_loss: 7.1800e-04\n",
      "Epoch 8/24\n",
      "3690/3690 [==============================] - 2s 551us/step - loss: 0.0019 - val_loss: 0.0016\n",
      "Epoch 9/24\n",
      "3690/3690 [==============================] - 2s 546us/step - loss: 0.0017 - val_loss: 0.0014\n",
      "Epoch 10/24\n",
      "3690/3690 [==============================] - 2s 547us/step - loss: 0.0016 - val_loss: 9.7074e-04\n",
      "Epoch 11/24\n",
      "3690/3690 [==============================] - 2s 548us/step - loss: 0.0015 - val_loss: 0.0011\n",
      "Epoch 12/24\n",
      "3690/3690 [==============================] - 2s 548us/step - loss: 0.0015 - val_loss: 8.2678e-04\n",
      "Epoch 13/24\n",
      "3690/3690 [==============================] - 2s 546us/step - loss: 0.0014 - val_loss: 9.3233e-04\n",
      "Epoch 14/24\n",
      "3690/3690 [==============================] - 2s 549us/step - loss: 0.0014 - val_loss: 7.7558e-04\n",
      "Epoch 15/24\n",
      "3690/3690 [==============================] - 2s 544us/step - loss: 0.0013 - val_loss: 5.9875e-04\n",
      "Epoch 16/24\n",
      "3690/3690 [==============================] - 2s 545us/step - loss: 0.0012 - val_loss: 7.6222e-04\n",
      "Epoch 17/24\n",
      "3690/3690 [==============================] - 2s 546us/step - loss: 0.0013 - val_loss: 7.0046e-04\n",
      "Epoch 18/24\n",
      "3690/3690 [==============================] - 2s 547us/step - loss: 0.0012 - val_loss: 9.3171e-04\n",
      "Epoch 19/24\n",
      "3690/3690 [==============================] - 2s 545us/step - loss: 0.0012 - val_loss: 7.6770e-04\n",
      "Epoch 20/24\n",
      "3690/3690 [==============================] - 2s 546us/step - loss: 0.0011 - val_loss: 9.4146e-04\n",
      "Epoch 21/24\n",
      "3690/3690 [==============================] - 2s 548us/step - loss: 0.0011 - val_loss: 8.1860e-04\n",
      "Epoch 22/24\n",
      "3690/3690 [==============================] - 2s 547us/step - loss: 0.0012 - val_loss: 4.6266e-04\n",
      "Epoch 23/24\n",
      "3690/3690 [==============================] - 2s 546us/step - loss: 0.0010 - val_loss: 5.2201e-04\n",
      "Epoch 24/24\n",
      "3690/3690 [==============================] - 2s 543us/step - loss: 0.0010 - val_loss: 7.1494e-04\n",
      "Train on 3690 samples, validate on 410 samples\n",
      "Epoch 1/24\n",
      "3690/3690 [==============================] - 51s 14ms/step - loss: nan - val_loss: nan\n",
      "Epoch 2/24\n",
      "3690/3690 [==============================] - 2s 547us/step - loss: nan - val_loss: nan\n",
      "Epoch 3/24\n",
      "3690/3690 [==============================] - 2s 548us/step - loss: nan - val_loss: nan\n",
      "Epoch 4/24\n",
      "3690/3690 [==============================] - 2s 548us/step - loss: nan - val_loss: nan\n",
      "Epoch 5/24\n",
      "3690/3690 [==============================] - 2s 547us/step - loss: nan - val_loss: nan\n",
      "Epoch 6/24\n",
      "3690/3690 [==============================] - 2s 547us/step - loss: nan - val_loss: nan\n",
      "Epoch 7/24\n",
      "3690/3690 [==============================] - 2s 551us/step - loss: nan - val_loss: nan\n",
      "Epoch 8/24\n",
      "3690/3690 [==============================] - 2s 552us/step - loss: nan - val_loss: nan\n",
      "Epoch 9/24\n",
      "3690/3690 [==============================] - 2s 551us/step - loss: nan - val_loss: nan\n",
      "Epoch 10/24\n",
      "3690/3690 [==============================] - 2s 549us/step - loss: nan - val_loss: nan\n",
      "Epoch 11/24\n",
      "3690/3690 [==============================] - 2s 547us/step - loss: nan - val_loss: nan\n",
      "Epoch 12/24\n",
      "3690/3690 [==============================] - 2s 551us/step - loss: nan - val_loss: nan\n",
      "Epoch 13/24\n",
      "3690/3690 [==============================] - 2s 548us/step - loss: nan - val_loss: nan\n",
      "Epoch 14/24\n",
      "3690/3690 [==============================] - 2s 547us/step - loss: nan - val_loss: nan\n",
      "Epoch 15/24\n",
      "3690/3690 [==============================] - 2s 548us/step - loss: nan - val_loss: nan\n",
      "Epoch 16/24\n",
      "3690/3690 [==============================] - 2s 547us/step - loss: nan - val_loss: nan\n",
      "Epoch 17/24\n",
      "3690/3690 [==============================] - 2s 548us/step - loss: nan - val_loss: nan\n",
      "Epoch 18/24\n",
      "3690/3690 [==============================] - 2s 552us/step - loss: nan - val_loss: nan\n",
      "Epoch 19/24\n",
      "3690/3690 [==============================] - 2s 550us/step - loss: nan - val_loss: nan\n",
      "Epoch 20/24\n",
      "3690/3690 [==============================] - 2s 548us/step - loss: nan - val_loss: nan\n",
      "Epoch 21/24\n",
      "3690/3690 [==============================] - 2s 547us/step - loss: nan - val_loss: nan\n",
      "Epoch 22/24\n",
      "3690/3690 [==============================] - 2s 547us/step - loss: nan - val_loss: nan\n",
      "Epoch 23/24\n",
      "3690/3690 [==============================] - 2s 548us/step - loss: nan - val_loss: nan\n",
      "Epoch 24/24\n",
      "3690/3690 [==============================] - 2s 550us/step - loss: nan - val_loss: nan\n",
      "Train on 3690 samples, validate on 410 samples\n",
      "Epoch 1/24\n",
      "3690/3690 [==============================] - 51s 14ms/step - loss: 0.0512 - val_loss: 0.0019\n",
      "Epoch 2/24\n",
      "3690/3690 [==============================] - 2s 547us/step - loss: 0.0075 - val_loss: 0.0086\n",
      "Epoch 3/24\n",
      "3690/3690 [==============================] - 2s 549us/step - loss: 0.0089 - val_loss: 0.0091\n",
      "Epoch 4/24\n",
      "3690/3690 [==============================] - 2s 546us/step - loss: 0.0058 - val_loss: 0.0096\n",
      "Epoch 5/24\n",
      "3690/3690 [==============================] - 2s 542us/step - loss: 0.0074 - val_loss: 0.0095\n",
      "Epoch 6/24\n",
      "3690/3690 [==============================] - 2s 544us/step - loss: 0.0060 - val_loss: 0.0112\n",
      "Epoch 7/24\n",
      "3690/3690 [==============================] - 2s 544us/step - loss: 0.0072 - val_loss: 0.0178\n",
      "Epoch 8/24\n",
      "3690/3690 [==============================] - 2s 547us/step - loss: 0.0058 - val_loss: 0.0072\n",
      "Epoch 9/24\n",
      "3690/3690 [==============================] - 2s 555us/step - loss: 0.0033 - val_loss: 0.0076\n",
      "Epoch 10/24\n",
      "3690/3690 [==============================] - 2s 547us/step - loss: 0.0051 - val_loss: 0.0118\n",
      "Epoch 11/24\n",
      "3690/3690 [==============================] - 2s 548us/step - loss: 0.0052 - val_loss: 0.0139\n",
      "Epoch 12/24\n",
      "3690/3690 [==============================] - 2s 548us/step - loss: 0.0044 - val_loss: 0.0107\n",
      "Epoch 13/24\n",
      "3690/3690 [==============================] - 2s 546us/step - loss: 0.0031 - val_loss: 0.0113\n",
      "Epoch 14/24\n",
      "3690/3690 [==============================] - 2s 546us/step - loss: 0.0041 - val_loss: 0.0105\n",
      "Epoch 15/24\n",
      "3690/3690 [==============================] - 2s 546us/step - loss: 0.0031 - val_loss: 0.0090\n",
      "Epoch 16/24\n",
      "3690/3690 [==============================] - 2s 544us/step - loss: 0.0035 - val_loss: 0.0123\n",
      "Epoch 17/24\n",
      "3690/3690 [==============================] - 2s 549us/step - loss: 0.0038 - val_loss: 0.0103\n",
      "Epoch 18/24\n",
      "3690/3690 [==============================] - 2s 546us/step - loss: 0.0024 - val_loss: 0.0059\n",
      "Epoch 19/24\n",
      "3690/3690 [==============================] - 2s 546us/step - loss: 0.0020 - val_loss: 0.0083\n",
      "Epoch 20/24\n",
      "3690/3690 [==============================] - 2s 548us/step - loss: 0.0022 - val_loss: 0.0073\n",
      "Epoch 21/24\n",
      "3690/3690 [==============================] - 2s 543us/step - loss: 0.0026 - val_loss: 0.0116\n",
      "Epoch 22/24\n",
      "3690/3690 [==============================] - 2s 547us/step - loss: 0.0029 - val_loss: 0.0107\n",
      "Epoch 23/24\n",
      "3690/3690 [==============================] - 2s 547us/step - loss: 0.0026 - val_loss: 0.0089\n",
      "Epoch 24/24\n",
      "3690/3690 [==============================] - 2s 540us/step - loss: 0.0030 - val_loss: 0.0112\n",
      "Train on 3690 samples, validate on 410 samples\n",
      "Epoch 1/24\n",
      "3690/3690 [==============================] - 55s 15ms/step - loss: 0.0778 - val_loss: 0.0016\n",
      "Epoch 2/24\n",
      "3690/3690 [==============================] - 2s 618us/step - loss: 0.0166 - val_loss: 0.0013\n",
      "Epoch 3/24\n",
      "3690/3690 [==============================] - 2s 618us/step - loss: 0.0051 - val_loss: 0.0167\n",
      "Epoch 4/24\n",
      "3690/3690 [==============================] - 2s 644us/step - loss: 0.0128 - val_loss: 0.0033\n",
      "Epoch 5/24\n",
      "3690/3690 [==============================] - 2s 638us/step - loss: 0.0096 - val_loss: 0.0075\n",
      "Epoch 6/24\n",
      "3690/3690 [==============================] - 2s 672us/step - loss: 0.0106 - val_loss: 0.0042\n",
      "Epoch 7/24\n",
      "3690/3690 [==============================] - 2s 653us/step - loss: 0.0046 - val_loss: 0.0104\n",
      "Epoch 8/24\n",
      "3690/3690 [==============================] - 2s 651us/step - loss: 0.0083 - val_loss: 0.0028\n",
      "Epoch 9/24\n",
      "3690/3690 [==============================] - 2s 670us/step - loss: 0.0061 - val_loss: 0.0087\n",
      "Epoch 10/24\n",
      "3690/3690 [==============================] - 2s 647us/step - loss: 0.0071 - val_loss: 7.6375e-04\n",
      "Epoch 11/24\n",
      "3690/3690 [==============================] - 2s 641us/step - loss: 0.0041 - val_loss: 0.0022\n",
      "Epoch 12/24\n",
      "3690/3690 [==============================] - 2s 627us/step - loss: 0.0077 - val_loss: 0.0050\n",
      "Epoch 13/24\n",
      "3690/3690 [==============================] - 2s 627us/step - loss: 0.0049 - val_loss: 0.0030\n",
      "Epoch 14/24\n",
      "3690/3690 [==============================] - 2s 645us/step - loss: 0.0023 - val_loss: 0.0118\n",
      "Epoch 15/24\n",
      "3690/3690 [==============================] - 2s 660us/step - loss: 0.0072 - val_loss: 5.5385e-04\n",
      "Epoch 16/24\n",
      "3690/3690 [==============================] - 2s 631us/step - loss: 0.0016 - val_loss: 0.0065\n",
      "Epoch 17/24\n",
      "3690/3690 [==============================] - 2s 619us/step - loss: 0.0069 - val_loss: 0.0013\n",
      "Epoch 18/24\n",
      "3690/3690 [==============================] - 2s 623us/step - loss: 0.0023 - val_loss: 5.4902e-04\n",
      "Epoch 19/24\n",
      "3690/3690 [==============================] - 2s 663us/step - loss: 0.0044 - val_loss: 0.0064\n",
      "Epoch 20/24\n",
      "3690/3690 [==============================] - 2s 628us/step - loss: 0.0036 - val_loss: 5.9113e-04\n",
      "Epoch 21/24\n",
      "3690/3690 [==============================] - 2s 624us/step - loss: 0.0024 - val_loss: 0.0010\n",
      "Epoch 22/24\n",
      "3690/3690 [==============================] - 2s 621us/step - loss: 0.0052 - val_loss: 6.7918e-04\n",
      "Epoch 23/24\n",
      "3690/3690 [==============================] - 2s 620us/step - loss: 0.0015 - val_loss: 7.3543e-04\n",
      "Epoch 24/24\n",
      "3690/3690 [==============================] - 2s 619us/step - loss: 0.0040 - val_loss: 0.0129\n",
      "Train on 3690 samples, validate on 410 samples\n",
      "Epoch 1/24\n",
      "3690/3690 [==============================] - 53s 14ms/step - loss: 0.0507 - val_loss: 0.0014\n",
      "Epoch 2/24\n",
      "3690/3690 [==============================] - 2s 630us/step - loss: 0.0064 - val_loss: 0.0045\n",
      "Epoch 3/24\n",
      "3690/3690 [==============================] - 2s 619us/step - loss: 0.0080 - val_loss: 0.0010\n",
      "Epoch 4/24\n",
      "3690/3690 [==============================] - 2s 619us/step - loss: 0.0043 - val_loss: 9.3874e-04\n",
      "Epoch 5/24\n",
      "3690/3690 [==============================] - 2s 621us/step - loss: 0.0071 - val_loss: 0.0016\n",
      "Epoch 6/24\n",
      "3690/3690 [==============================] - 2s 618us/step - loss: 0.0061 - val_loss: 0.0022\n",
      "Epoch 7/24\n",
      "3690/3690 [==============================] - 2s 619us/step - loss: 0.0073 - val_loss: 0.0014\n",
      "Epoch 8/24\n",
      "3690/3690 [==============================] - 2s 619us/step - loss: 0.0062 - val_loss: 7.8153e-04\n",
      "Epoch 9/24\n",
      "3690/3690 [==============================] - 2s 617us/step - loss: 0.0038 - val_loss: 7.2654e-04\n",
      "Epoch 10/24\n",
      "3690/3690 [==============================] - 2s 618us/step - loss: 0.0061 - val_loss: 0.0046\n",
      "Epoch 11/24\n",
      "3690/3690 [==============================] - 2s 619us/step - loss: 0.0078 - val_loss: 0.0010\n",
      "Epoch 12/24\n",
      "3690/3690 [==============================] - 2s 617us/step - loss: 0.0037 - val_loss: 0.0013\n",
      "Epoch 13/24\n",
      "3690/3690 [==============================] - 2s 619us/step - loss: 0.0028 - val_loss: 8.2822e-04\n",
      "Epoch 14/24\n",
      "3690/3690 [==============================] - 2s 622us/step - loss: 0.0058 - val_loss: 0.0020\n",
      "Epoch 15/24\n",
      "3690/3690 [==============================] - 2s 617us/step - loss: 0.0051 - val_loss: 6.8644e-04\n",
      "Epoch 16/24\n",
      "3690/3690 [==============================] - 2s 626us/step - loss: 0.0025 - val_loss: 6.3833e-04\n",
      "Epoch 17/24\n",
      "3690/3690 [==============================] - 2s 620us/step - loss: 0.0028 - val_loss: 0.0022\n",
      "Epoch 18/24\n",
      "3690/3690 [==============================] - 2s 620us/step - loss: 0.0064 - val_loss: 0.0030\n",
      "Epoch 19/24\n",
      "3690/3690 [==============================] - 2s 618us/step - loss: 0.0045 - val_loss: 7.4740e-04\n",
      "Epoch 20/24\n",
      "3690/3690 [==============================] - 2s 618us/step - loss: 0.0029 - val_loss: 0.0012\n",
      "Epoch 21/24\n",
      "3690/3690 [==============================] - 2s 618us/step - loss: 0.0031 - val_loss: 7.0666e-04\n",
      "Epoch 22/24\n",
      "3690/3690 [==============================] - 2s 623us/step - loss: 0.0029 - val_loss: 5.9322e-04\n",
      "Epoch 23/24\n",
      "3690/3690 [==============================] - 2s 628us/step - loss: 0.0028 - val_loss: 6.5382e-04\n",
      "Epoch 24/24\n",
      "3690/3690 [==============================] - 2s 619us/step - loss: 0.0023 - val_loss: 6.1775e-04\n",
      "Train on 3690 samples, validate on 410 samples\n",
      "Epoch 1/24\n",
      "3690/3690 [==============================] - 54s 15ms/step - loss: 0.1210 - val_loss: 0.0014\n",
      "Epoch 2/24\n",
      "3690/3690 [==============================] - 2s 622us/step - loss: 0.0139 - val_loss: 0.0054\n",
      "Epoch 3/24\n",
      "3690/3690 [==============================] - 2s 623us/step - loss: 0.0084 - val_loss: 0.0012\n",
      "Epoch 4/24\n",
      "3690/3690 [==============================] - 2s 616us/step - loss: 0.0036 - val_loss: 0.0012\n",
      "Epoch 5/24\n",
      "3690/3690 [==============================] - 2s 624us/step - loss: 0.0021 - val_loss: 6.2774e-04\n",
      "Epoch 6/24\n",
      "3690/3690 [==============================] - 2s 622us/step - loss: 0.0018 - val_loss: 0.0022\n",
      "Epoch 7/24\n",
      "3690/3690 [==============================] - 2s 621us/step - loss: 0.0016 - val_loss: 7.2756e-04\n",
      "Epoch 8/24\n",
      "3690/3690 [==============================] - 2s 626us/step - loss: 0.0015 - val_loss: 0.0016\n",
      "Epoch 9/24\n",
      "3690/3690 [==============================] - 2s 618us/step - loss: 0.0015 - val_loss: 7.9881e-04\n",
      "Epoch 10/24\n",
      "3690/3690 [==============================] - 2s 623us/step - loss: 0.0016 - val_loss: 0.0012\n",
      "Epoch 11/24\n",
      "3690/3690 [==============================] - 2s 621us/step - loss: 0.0014 - val_loss: 8.9277e-04\n",
      "Epoch 12/24\n",
      "3690/3690 [==============================] - 2s 625us/step - loss: 0.0015 - val_loss: 9.6037e-04\n",
      "Epoch 13/24\n",
      "3690/3690 [==============================] - 2s 621us/step - loss: 0.0014 - val_loss: 9.9092e-04\n",
      "Epoch 14/24\n",
      "3690/3690 [==============================] - 2s 620us/step - loss: 0.0014 - val_loss: 9.2724e-04\n",
      "Epoch 15/24\n",
      "3690/3690 [==============================] - 2s 622us/step - loss: 0.0013 - val_loss: 9.1581e-04\n",
      "Epoch 16/24\n",
      "3690/3690 [==============================] - 2s 619us/step - loss: 0.0013 - val_loss: 8.4746e-04\n",
      "Epoch 17/24\n",
      "3690/3690 [==============================] - 2s 621us/step - loss: 0.0013 - val_loss: 9.3605e-04\n",
      "Epoch 18/24\n",
      "3690/3690 [==============================] - 2s 621us/step - loss: 0.0013 - val_loss: 7.8880e-04\n",
      "Epoch 19/24\n",
      "3690/3690 [==============================] - 2s 624us/step - loss: 0.0013 - val_loss: 8.1348e-04\n",
      "Epoch 20/24\n",
      "3690/3690 [==============================] - 2s 618us/step - loss: 0.0013 - val_loss: 8.6219e-04\n",
      "Epoch 21/24\n",
      "3690/3690 [==============================] - 2s 619us/step - loss: 0.0012 - val_loss: 7.2438e-04\n",
      "Epoch 22/24\n",
      "3690/3690 [==============================] - 2s 619us/step - loss: 0.0012 - val_loss: 8.1535e-04\n",
      "Epoch 23/24\n",
      "3690/3690 [==============================] - 2s 620us/step - loss: 0.0012 - val_loss: 7.7297e-04\n",
      "Epoch 24/24\n",
      "3690/3690 [==============================] - 2s 621us/step - loss: 0.0012 - val_loss: 7.2289e-04\n"
     ]
    }
   ],
   "source": [
    "best_models = breed_population(X_train, Y_train, generations=20, population_size=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.0148102892562747,\n",
       " 0.002357694087550044,\n",
       " 0.0015180862974375486,\n",
       " 0.0032058616634458303,\n",
       " 0.010299623012542725,\n",
       " 0.004211158491671085,\n",
       " 0.0026668920181691647,\n",
       " 0.0010865309741348028,\n",
       " 0.0015870343195274472,\n",
       " 0.001020119059830904,\n",
       " 0.000820847402792424,\n",
       " 0.0007782719330862164,\n",
       " 0.0006033737445250154,\n",
       " 0.0005642649484798312,\n",
       " 0.0005361604853533208,\n",
       " 0.0005309363477863371,\n",
       " 0.0007358361035585403,\n",
       " 0.0013018675381317735,\n",
       " 0.0011685275239869952,\n",
       " 0.0006861375877633691,\n",
       " 0.0005284797516651452,\n",
       " 0.0007585635758005083,\n",
       " 0.0004596828075591475,\n",
       " 0.0004597422666847706]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_models[0][1].history['val_loss']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "optimal parameters\n",
      "_______________________\n",
      "optimizer: adam\n",
      "shuffle: True\n",
      "dropout: 0.1\n",
      "full_density: True\n",
      "twice: True\n",
      "density: 230\n",
      "activation: softplus\n",
      "lstmsize: 150\n"
     ]
    }
   ],
   "source": [
    "print('optimal parameters')\n",
    "print('_______________________')\n",
    "for key,val in best_models[0][2].items():\n",
    "    if key != 'x' and key != 'y': print(f'{key}: {val}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_140\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_278 (LSTM)              (None, 92, 150)           93600     \n",
      "_________________________________________________________________\n",
      "dropout_278 (Dropout)        (None, 92, 150)           0         \n",
      "_________________________________________________________________\n",
      "lstm_279 (LSTM)              (None, 150)               180600    \n",
      "_________________________________________________________________\n",
      "dropout_279 (Dropout)        (None, 150)               0         \n",
      "_________________________________________________________________\n",
      "dense_702 (Dense)            (None, 230)               34730     \n",
      "_________________________________________________________________\n",
      "dense_703 (Dense)            (None, 115)               26565     \n",
      "_________________________________________________________________\n",
      "dense_704 (Dense)            (None, 57)                6612      \n",
      "_________________________________________________________________\n",
      "dense_705 (Dense)            (None, 28)                1624      \n",
      "_________________________________________________________________\n",
      "dense_706 (Dense)            (None, 14)                406       \n",
      "_________________________________________________________________\n",
      "dense_707 (Dense)            (None, 1)                 15        \n",
      "=================================================================\n",
      "Total params: 344,152\n",
      "Trainable params: 344,152\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "best_models[0][0].summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "ibm_checkpoint = ModelCheckpoint(\n",
    "    filepath='./checkpoints/IBM.{epoch}-{val_loss}.hdf5',\n",
    "    save_weights_only=True,\n",
    "    monitor='val_loss',\n",
    "    mode='min',\n",
    "    save_best_only=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = best_models[0][2]\n",
    "params['x'] = X_train\n",
    "params['y'] = Y_train\n",
    "params['epochs'] = 2000\n",
    "params['callbacks'] = [ibm_checkpoint]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 3690 samples, validate on 410 samples\n",
      "Epoch 1/2000\n",
      "3690/3690 [==============================] - 58s 16ms/step - loss: 0.1711 - val_loss: 0.1110\n",
      "Epoch 2/2000\n",
      "3690/3690 [==============================] - 2s 570us/step - loss: 0.0828 - val_loss: 0.0038\n",
      "Epoch 3/2000\n",
      "3690/3690 [==============================] - 2s 555us/step - loss: 0.0427 - val_loss: 0.0146\n",
      "Epoch 4/2000\n",
      "3690/3690 [==============================] - 2s 554us/step - loss: 0.0089 - val_loss: 0.0075\n",
      "Epoch 5/2000\n",
      "3690/3690 [==============================] - 2s 549us/step - loss: 0.0061 - val_loss: 0.0011\n",
      "Epoch 6/2000\n",
      "3690/3690 [==============================] - 2s 554us/step - loss: 0.0039 - val_loss: 0.0023\n",
      "Epoch 7/2000\n",
      "3690/3690 [==============================] - 2s 554us/step - loss: 0.0023 - val_loss: 0.0074\n",
      "Epoch 8/2000\n",
      "3690/3690 [==============================] - 2s 554us/step - loss: 0.0017 - val_loss: 0.0052\n",
      "Epoch 9/2000\n",
      "3690/3690 [==============================] - 2s 555us/step - loss: 0.0017 - val_loss: 0.0018\n",
      "Epoch 10/2000\n",
      "3690/3690 [==============================] - 2s 556us/step - loss: 0.0014 - val_loss: 0.0012\n",
      "Epoch 11/2000\n",
      "3690/3690 [==============================] - 2s 555us/step - loss: 0.0012 - val_loss: 0.0014\n",
      "Epoch 12/2000\n",
      "3690/3690 [==============================] - 2s 556us/step - loss: 0.0012 - val_loss: 0.0017\n",
      "Epoch 13/2000\n",
      "3690/3690 [==============================] - 2s 553us/step - loss: 0.0012 - val_loss: 0.0012\n",
      "Epoch 14/2000\n",
      "3690/3690 [==============================] - 2s 553us/step - loss: 0.0012 - val_loss: 8.8368e-04\n",
      "Epoch 15/2000\n",
      "3690/3690 [==============================] - 2s 553us/step - loss: 0.0011 - val_loss: 7.6415e-04\n",
      "Epoch 16/2000\n",
      "3690/3690 [==============================] - 2s 552us/step - loss: 0.0010 - val_loss: 9.5876e-04\n",
      "Epoch 17/2000\n",
      "3690/3690 [==============================] - 2s 552us/step - loss: 0.0010 - val_loss: 9.2300e-04\n",
      "Epoch 18/2000\n",
      "3690/3690 [==============================] - 2s 556us/step - loss: 0.0010 - val_loss: 0.0011\n",
      "Epoch 19/2000\n",
      "3690/3690 [==============================] - 2s 553us/step - loss: 9.9647e-04 - val_loss: 6.1302e-04\n",
      "Epoch 20/2000\n",
      "3690/3690 [==============================] - 2s 552us/step - loss: 9.9846e-04 - val_loss: 0.0011\n",
      "Epoch 21/2000\n",
      "3690/3690 [==============================] - 2s 550us/step - loss: 9.2000e-04 - val_loss: 5.7912e-04\n",
      "Epoch 22/2000\n",
      "3690/3690 [==============================] - 2s 553us/step - loss: 8.8526e-04 - val_loss: 9.6260e-04\n",
      "Epoch 23/2000\n",
      "3690/3690 [==============================] - 2s 554us/step - loss: 9.0014e-04 - val_loss: 5.6634e-04\n",
      "Epoch 24/2000\n",
      "3690/3690 [==============================] - 2s 554us/step - loss: 9.0765e-04 - val_loss: 5.2765e-04\n",
      "Epoch 25/2000\n",
      "3690/3690 [==============================] - 2s 558us/step - loss: 8.4646e-04 - val_loss: 5.9876e-04\n",
      "Epoch 26/2000\n",
      "3690/3690 [==============================] - 2s 553us/step - loss: 8.0101e-04 - val_loss: 6.7024e-04\n",
      "Epoch 27/2000\n",
      "3690/3690 [==============================] - 2s 552us/step - loss: 8.2226e-04 - val_loss: 8.8728e-04\n",
      "Epoch 28/2000\n",
      "3690/3690 [==============================] - 2s 554us/step - loss: 8.5552e-04 - val_loss: 5.0892e-04\n",
      "Epoch 29/2000\n",
      "3690/3690 [==============================] - 2s 554us/step - loss: 8.9672e-04 - val_loss: 9.0327e-04\n",
      "Epoch 30/2000\n",
      "3690/3690 [==============================] - 2s 555us/step - loss: 9.0970e-04 - val_loss: 7.3791e-04\n",
      "Epoch 31/2000\n",
      "3690/3690 [==============================] - 2s 551us/step - loss: 7.4899e-04 - val_loss: 5.9799e-04\n",
      "Epoch 32/2000\n",
      "3690/3690 [==============================] - 2s 557us/step - loss: 8.5228e-04 - val_loss: 0.0010\n",
      "Epoch 33/2000\n",
      "3690/3690 [==============================] - 2s 558us/step - loss: 7.8603e-04 - val_loss: 5.0177e-04\n",
      "Epoch 34/2000\n",
      "3690/3690 [==============================] - 2s 555us/step - loss: 7.5130e-04 - val_loss: 5.5354e-04\n",
      "Epoch 35/2000\n",
      "3690/3690 [==============================] - 2s 554us/step - loss: 7.0476e-04 - val_loss: 6.3547e-04\n",
      "Epoch 36/2000\n",
      "3690/3690 [==============================] - 2s 556us/step - loss: 7.2822e-04 - val_loss: 4.9658e-04\n",
      "Epoch 37/2000\n",
      "3690/3690 [==============================] - 2s 555us/step - loss: 6.9828e-04 - val_loss: 4.4044e-04\n",
      "Epoch 38/2000\n",
      "3690/3690 [==============================] - 2s 556us/step - loss: 7.0986e-04 - val_loss: 8.9891e-04\n",
      "Epoch 39/2000\n",
      "3690/3690 [==============================] - 2s 556us/step - loss: 7.2033e-04 - val_loss: 4.8896e-04\n",
      "Epoch 40/2000\n",
      "3690/3690 [==============================] - 2s 552us/step - loss: 6.7064e-04 - val_loss: 4.2137e-04\n",
      "Epoch 41/2000\n",
      "3690/3690 [==============================] - 2s 559us/step - loss: 6.6740e-04 - val_loss: 5.1079e-04\n",
      "Epoch 42/2000\n",
      "3690/3690 [==============================] - 2s 555us/step - loss: 6.4321e-04 - val_loss: 5.2115e-04\n",
      "Epoch 43/2000\n",
      "3690/3690 [==============================] - 2s 557us/step - loss: 6.7066e-04 - val_loss: 6.0602e-04\n",
      "Epoch 44/2000\n",
      "3690/3690 [==============================] - 2s 556us/step - loss: 6.3142e-04 - val_loss: 7.6657e-04\n",
      "Epoch 45/2000\n",
      "3690/3690 [==============================] - 2s 554us/step - loss: 6.6851e-04 - val_loss: 5.8569e-04\n",
      "Epoch 46/2000\n",
      "3690/3690 [==============================] - 2s 554us/step - loss: 6.1323e-04 - val_loss: 4.1039e-04\n",
      "Epoch 47/2000\n",
      "3690/3690 [==============================] - 2s 553us/step - loss: 6.2199e-04 - val_loss: 6.4851e-04\n",
      "Epoch 48/2000\n",
      "3690/3690 [==============================] - 2s 558us/step - loss: 6.1185e-04 - val_loss: 6.7287e-04\n",
      "Epoch 49/2000\n",
      "3690/3690 [==============================] - 2s 553us/step - loss: 6.1039e-04 - val_loss: 3.9537e-04\n",
      "Epoch 50/2000\n",
      "3690/3690 [==============================] - 2s 555us/step - loss: 5.6587e-04 - val_loss: 7.4289e-04\n",
      "Epoch 51/2000\n",
      "3690/3690 [==============================] - 2s 552us/step - loss: 6.0446e-04 - val_loss: 6.6609e-04\n",
      "Epoch 52/2000\n",
      "3690/3690 [==============================] - 2s 552us/step - loss: 6.5612e-04 - val_loss: 6.9339e-04\n",
      "Epoch 53/2000\n",
      "3690/3690 [==============================] - 2s 552us/step - loss: 6.0209e-04 - val_loss: 4.0988e-04\n",
      "Epoch 54/2000\n",
      "3690/3690 [==============================] - 2s 553us/step - loss: 5.7113e-04 - val_loss: 3.9752e-04\n",
      "Epoch 55/2000\n",
      "3690/3690 [==============================] - 2s 552us/step - loss: 5.6491e-04 - val_loss: 4.8379e-04\n",
      "Epoch 56/2000\n",
      "3690/3690 [==============================] - 2s 555us/step - loss: 5.9018e-04 - val_loss: 4.4978e-04\n",
      "Epoch 57/2000\n",
      "3690/3690 [==============================] - 2s 551us/step - loss: 6.3440e-04 - val_loss: 3.8668e-04\n",
      "Epoch 58/2000\n",
      "3690/3690 [==============================] - 2s 552us/step - loss: 5.5995e-04 - val_loss: 3.7448e-04\n",
      "Epoch 59/2000\n",
      "3690/3690 [==============================] - 2s 549us/step - loss: 5.3726e-04 - val_loss: 3.8966e-04\n",
      "Epoch 60/2000\n",
      "3690/3690 [==============================] - 2s 561us/step - loss: 5.3082e-04 - val_loss: 3.8601e-04\n",
      "Epoch 61/2000\n",
      "3690/3690 [==============================] - 2s 555us/step - loss: 5.5781e-04 - val_loss: 3.7168e-04\n",
      "Epoch 62/2000\n",
      "3690/3690 [==============================] - 2s 550us/step - loss: 5.5327e-04 - val_loss: 4.1906e-04\n",
      "Epoch 63/2000\n",
      "3690/3690 [==============================] - 2s 551us/step - loss: 5.3306e-04 - val_loss: 3.8617e-04\n",
      "Epoch 64/2000\n",
      "3690/3690 [==============================] - 2s 559us/step - loss: 5.2679e-04 - val_loss: 4.1283e-04\n",
      "Epoch 65/2000\n",
      "3690/3690 [==============================] - 2s 554us/step - loss: 5.5425e-04 - val_loss: 3.9020e-04\n",
      "Epoch 66/2000\n",
      "3690/3690 [==============================] - 2s 558us/step - loss: 5.4174e-04 - val_loss: 3.6627e-04\n",
      "Epoch 67/2000\n",
      "3690/3690 [==============================] - 2s 561us/step - loss: 5.1263e-04 - val_loss: 3.7597e-04\n",
      "Epoch 68/2000\n",
      "3690/3690 [==============================] - 2s 558us/step - loss: 5.0597e-04 - val_loss: 4.9600e-04\n",
      "Epoch 69/2000\n",
      "3690/3690 [==============================] - 2s 558us/step - loss: 5.3854e-04 - val_loss: 5.4917e-04\n",
      "Epoch 70/2000\n",
      "3690/3690 [==============================] - 2s 564us/step - loss: 4.9490e-04 - val_loss: 9.6532e-04\n",
      "Epoch 71/2000\n",
      "3690/3690 [==============================] - 2s 558us/step - loss: 5.6851e-04 - val_loss: 0.0012\n",
      "Epoch 72/2000\n",
      "3690/3690 [==============================] - 2s 554us/step - loss: 6.1623e-04 - val_loss: 6.0982e-04\n",
      "Epoch 73/2000\n",
      "3690/3690 [==============================] - 2s 552us/step - loss: 5.0937e-04 - val_loss: 0.0011\n",
      "Epoch 74/2000\n",
      "3690/3690 [==============================] - 2s 555us/step - loss: 5.5657e-04 - val_loss: 4.7827e-04\n",
      "Epoch 75/2000\n",
      "3690/3690 [==============================] - 2s 552us/step - loss: 4.8382e-04 - val_loss: 5.4198e-04\n",
      "Epoch 76/2000\n",
      "3690/3690 [==============================] - 2s 554us/step - loss: 4.9574e-04 - val_loss: 5.8765e-04\n",
      "Epoch 77/2000\n",
      "3690/3690 [==============================] - 2s 561us/step - loss: 5.2985e-04 - val_loss: 5.6339e-04\n",
      "Epoch 78/2000\n",
      "3690/3690 [==============================] - 2s 557us/step - loss: 4.6254e-04 - val_loss: 3.4603e-04\n",
      "Epoch 79/2000\n",
      "3690/3690 [==============================] - 2s 562us/step - loss: 4.7055e-04 - val_loss: 3.3148e-04\n",
      "Epoch 80/2000\n",
      "3690/3690 [==============================] - 2s 557us/step - loss: 4.8205e-04 - val_loss: 3.4193e-04\n",
      "Epoch 81/2000\n",
      "3690/3690 [==============================] - 2s 561us/step - loss: 4.6590e-04 - val_loss: 3.5785e-04\n",
      "Epoch 82/2000\n",
      "3690/3690 [==============================] - 2s 565us/step - loss: 4.6689e-04 - val_loss: 3.2811e-04\n",
      "Epoch 83/2000\n",
      "3690/3690 [==============================] - 2s 553us/step - loss: 4.5657e-04 - val_loss: 4.5037e-04\n",
      "Epoch 84/2000\n",
      "3690/3690 [==============================] - 2s 552us/step - loss: 4.4178e-04 - val_loss: 3.6446e-04\n",
      "Epoch 85/2000\n",
      "3690/3690 [==============================] - 2s 555us/step - loss: 4.4032e-04 - val_loss: 3.8555e-04\n",
      "Epoch 86/2000\n",
      "3690/3690 [==============================] - 2s 555us/step - loss: 4.3769e-04 - val_loss: 3.1245e-04\n",
      "Epoch 87/2000\n",
      "3690/3690 [==============================] - 2s 554us/step - loss: 4.5641e-04 - val_loss: 3.2738e-04\n",
      "Epoch 88/2000\n",
      "3690/3690 [==============================] - 2s 553us/step - loss: 4.3909e-04 - val_loss: 5.8699e-04\n",
      "Epoch 89/2000\n",
      "3690/3690 [==============================] - 2s 554us/step - loss: 4.5092e-04 - val_loss: 3.9382e-04\n",
      "Epoch 90/2000\n",
      "3690/3690 [==============================] - 2s 560us/step - loss: 4.2262e-04 - val_loss: 3.0703e-04\n",
      "Epoch 91/2000\n",
      "3690/3690 [==============================] - 2s 550us/step - loss: 4.3034e-04 - val_loss: 3.0835e-04\n",
      "Epoch 92/2000\n",
      "3690/3690 [==============================] - 2s 552us/step - loss: 4.4903e-04 - val_loss: 3.1302e-04\n",
      "Epoch 93/2000\n",
      "3690/3690 [==============================] - 2s 551us/step - loss: 4.1759e-04 - val_loss: 5.7615e-04\n",
      "Epoch 94/2000\n",
      "3690/3690 [==============================] - 2s 551us/step - loss: 4.4806e-04 - val_loss: 9.7502e-04\n",
      "Epoch 95/2000\n",
      "3690/3690 [==============================] - 2s 557us/step - loss: 5.2483e-04 - val_loss: 6.5970e-04\n",
      "Epoch 96/2000\n",
      "3690/3690 [==============================] - 2s 552us/step - loss: 5.1132e-04 - val_loss: 7.9903e-04\n",
      "Epoch 97/2000\n",
      "3690/3690 [==============================] - 2s 551us/step - loss: 4.9043e-04 - val_loss: 5.6623e-04\n",
      "Epoch 98/2000\n",
      "3690/3690 [==============================] - 2s 555us/step - loss: 4.2718e-04 - val_loss: 4.9629e-04\n",
      "Epoch 99/2000\n",
      "3690/3690 [==============================] - 2s 553us/step - loss: 4.3280e-04 - val_loss: 3.7700e-04\n",
      "Epoch 100/2000\n",
      "3690/3690 [==============================] - 2s 554us/step - loss: 4.2244e-04 - val_loss: 5.7185e-04\n",
      "Epoch 101/2000\n",
      "3690/3690 [==============================] - 2s 581us/step - loss: 4.5990e-04 - val_loss: 6.9350e-04\n",
      "Epoch 102/2000\n",
      "3690/3690 [==============================] - 2s 559us/step - loss: 4.3121e-04 - val_loss: 5.9154e-04\n",
      "Epoch 103/2000\n",
      "3690/3690 [==============================] - 2s 565us/step - loss: 4.3332e-04 - val_loss: 4.9836e-04\n",
      "Epoch 104/2000\n",
      "3690/3690 [==============================] - 2s 564us/step - loss: 3.8062e-04 - val_loss: 4.6726e-04\n",
      "Epoch 105/2000\n",
      "3690/3690 [==============================] - 2s 556us/step - loss: 4.0951e-04 - val_loss: 4.5994e-04\n",
      "Epoch 106/2000\n",
      "3690/3690 [==============================] - 2s 561us/step - loss: 3.9692e-04 - val_loss: 3.2866e-04\n",
      "Epoch 107/2000\n",
      "3690/3690 [==============================] - 2s 562us/step - loss: 3.6595e-04 - val_loss: 4.5177e-04\n",
      "Epoch 108/2000\n",
      "3690/3690 [==============================] - 2s 554us/step - loss: 3.9423e-04 - val_loss: 3.4098e-04\n",
      "Epoch 109/2000\n",
      "3690/3690 [==============================] - 2s 553us/step - loss: 3.8328e-04 - val_loss: 3.0920e-04\n",
      "Epoch 110/2000\n",
      "3690/3690 [==============================] - 2s 554us/step - loss: 3.7962e-04 - val_loss: 3.5186e-04\n",
      "Epoch 111/2000\n",
      "3690/3690 [==============================] - 2s 552us/step - loss: 3.7116e-04 - val_loss: 3.7089e-04\n",
      "Epoch 112/2000\n",
      "3690/3690 [==============================] - 2s 552us/step - loss: 4.1292e-04 - val_loss: 2.7343e-04\n",
      "Epoch 113/2000\n",
      "3690/3690 [==============================] - 2s 552us/step - loss: 3.7269e-04 - val_loss: 3.9236e-04\n",
      "Epoch 114/2000\n",
      "3690/3690 [==============================] - 2s 554us/step - loss: 3.7088e-04 - val_loss: 2.5307e-04\n",
      "Epoch 115/2000\n",
      "3690/3690 [==============================] - 2s 548us/step - loss: 3.7403e-04 - val_loss: 3.0943e-04\n",
      "Epoch 116/2000\n",
      "3690/3690 [==============================] - 2s 552us/step - loss: 4.4523e-04 - val_loss: 3.4016e-04\n",
      "Epoch 117/2000\n",
      "3690/3690 [==============================] - 2s 552us/step - loss: 4.0103e-04 - val_loss: 4.6817e-04\n",
      "Epoch 118/2000\n",
      "3690/3690 [==============================] - 2s 555us/step - loss: 3.9421e-04 - val_loss: 2.5519e-04\n",
      "Epoch 119/2000\n",
      "3690/3690 [==============================] - 2s 552us/step - loss: 3.6075e-04 - val_loss: 3.1510e-04\n",
      "Epoch 120/2000\n",
      "3690/3690 [==============================] - 2s 551us/step - loss: 3.6384e-04 - val_loss: 3.0538e-04\n",
      "Epoch 121/2000\n",
      "3690/3690 [==============================] - 2s 548us/step - loss: 3.5591e-04 - val_loss: 4.5751e-04\n",
      "Epoch 122/2000\n",
      "3690/3690 [==============================] - 2s 550us/step - loss: 3.7941e-04 - val_loss: 3.8766e-04\n",
      "Epoch 123/2000\n",
      "3690/3690 [==============================] - 2s 551us/step - loss: 3.8584e-04 - val_loss: 2.4080e-04\n",
      "Epoch 124/2000\n",
      "3690/3690 [==============================] - 2s 552us/step - loss: 3.9297e-04 - val_loss: 2.7165e-04\n",
      "Epoch 125/2000\n",
      "3690/3690 [==============================] - 2s 552us/step - loss: 3.6756e-04 - val_loss: 3.6823e-04\n",
      "Epoch 126/2000\n",
      "3690/3690 [==============================] - 2s 554us/step - loss: 4.1935e-04 - val_loss: 2.3361e-04\n",
      "Epoch 127/2000\n",
      "3690/3690 [==============================] - 2s 548us/step - loss: 3.6907e-04 - val_loss: 3.8256e-04\n",
      "Epoch 128/2000\n",
      "3690/3690 [==============================] - 2s 563us/step - loss: 3.4280e-04 - val_loss: 3.8033e-04\n",
      "Epoch 129/2000\n",
      "3690/3690 [==============================] - 2s 555us/step - loss: 3.4964e-04 - val_loss: 3.7940e-04\n",
      "Epoch 130/2000\n",
      "3690/3690 [==============================] - 2s 552us/step - loss: 3.3293e-04 - val_loss: 2.5472e-04\n",
      "Epoch 131/2000\n",
      "3690/3690 [==============================] - 2s 552us/step - loss: 3.3245e-04 - val_loss: 2.3521e-04\n",
      "Epoch 132/2000\n",
      "3690/3690 [==============================] - 2s 550us/step - loss: 3.3621e-04 - val_loss: 2.6078e-04\n",
      "Epoch 133/2000\n",
      "3690/3690 [==============================] - 2s 552us/step - loss: 3.4410e-04 - val_loss: 2.2535e-04\n",
      "Epoch 134/2000\n",
      "3690/3690 [==============================] - 2s 552us/step - loss: 3.5254e-04 - val_loss: 3.3610e-04\n",
      "Epoch 135/2000\n",
      "3690/3690 [==============================] - 2s 553us/step - loss: 3.4846e-04 - val_loss: 2.4139e-04\n",
      "Epoch 136/2000\n",
      "3690/3690 [==============================] - 2s 551us/step - loss: 3.6323e-04 - val_loss: 2.2946e-04\n",
      "Epoch 137/2000\n",
      "3690/3690 [==============================] - 2s 549us/step - loss: 3.5585e-04 - val_loss: 3.2902e-04\n",
      "Epoch 138/2000\n",
      "3690/3690 [==============================] - 2s 551us/step - loss: 3.4858e-04 - val_loss: 2.2139e-04\n",
      "Epoch 139/2000\n",
      "3690/3690 [==============================] - 2s 553us/step - loss: 3.0372e-04 - val_loss: 2.8339e-04\n",
      "Epoch 140/2000\n",
      "3690/3690 [==============================] - 2s 552us/step - loss: 3.2402e-04 - val_loss: 2.5972e-04\n",
      "Epoch 141/2000\n",
      "3690/3690 [==============================] - 2s 552us/step - loss: 3.3888e-04 - val_loss: 2.1668e-04\n",
      "Epoch 142/2000\n",
      "3690/3690 [==============================] - 2s 554us/step - loss: 3.1194e-04 - val_loss: 3.0303e-04\n",
      "Epoch 143/2000\n",
      "3690/3690 [==============================] - 2s 551us/step - loss: 3.4759e-04 - val_loss: 4.6044e-04\n",
      "Epoch 144/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3690/3690 [==============================] - 2s 552us/step - loss: 4.3240e-04 - val_loss: 4.6960e-04\n",
      "Epoch 145/2000\n",
      "3690/3690 [==============================] - 2s 551us/step - loss: 3.9534e-04 - val_loss: 7.5252e-04\n",
      "Epoch 146/2000\n",
      "3690/3690 [==============================] - 2s 551us/step - loss: 4.2405e-04 - val_loss: 2.2773e-04\n",
      "Epoch 147/2000\n",
      "3690/3690 [==============================] - 2s 554us/step - loss: 3.4598e-04 - val_loss: 4.5038e-04\n",
      "Epoch 148/2000\n",
      "3690/3690 [==============================] - 2s 550us/step - loss: 4.6250e-04 - val_loss: 3.7434e-04\n",
      "Epoch 149/2000\n",
      "3690/3690 [==============================] - 2s 555us/step - loss: 4.5489e-04 - val_loss: 2.0502e-04\n",
      "Epoch 150/2000\n",
      "3690/3690 [==============================] - 2s 549us/step - loss: 3.4991e-04 - val_loss: 2.9565e-04\n",
      "Epoch 151/2000\n",
      "3690/3690 [==============================] - 2s 550us/step - loss: 3.1886e-04 - val_loss: 2.0799e-04\n",
      "Epoch 152/2000\n",
      "3690/3690 [==============================] - 2s 552us/step - loss: 2.8615e-04 - val_loss: 3.2030e-04\n",
      "Epoch 153/2000\n",
      "3690/3690 [==============================] - 2s 550us/step - loss: 3.2831e-04 - val_loss: 1.9289e-04\n",
      "Epoch 154/2000\n",
      "3690/3690 [==============================] - 2s 551us/step - loss: 3.0661e-04 - val_loss: 2.4584e-04\n",
      "Epoch 155/2000\n",
      "3690/3690 [==============================] - 2s 548us/step - loss: 2.9309e-04 - val_loss: 2.9333e-04\n",
      "Epoch 156/2000\n",
      "3690/3690 [==============================] - 2s 549us/step - loss: 3.4441e-04 - val_loss: 2.0264e-04\n",
      "Epoch 157/2000\n",
      "3690/3690 [==============================] - 2s 555us/step - loss: 3.2580e-04 - val_loss: 2.2411e-04\n",
      "Epoch 158/2000\n",
      "3690/3690 [==============================] - 2s 553us/step - loss: 2.9362e-04 - val_loss: 1.9915e-04\n",
      "Epoch 159/2000\n",
      "3690/3690 [==============================] - 2s 553us/step - loss: 2.9270e-04 - val_loss: 6.0616e-04\n",
      "Epoch 160/2000\n",
      "3690/3690 [==============================] - 2s 554us/step - loss: 3.5668e-04 - val_loss: 2.0328e-04\n",
      "Epoch 161/2000\n",
      "3690/3690 [==============================] - 2s 551us/step - loss: 3.1760e-04 - val_loss: 2.4674e-04\n",
      "Epoch 162/2000\n",
      "3690/3690 [==============================] - 2s 552us/step - loss: 2.8727e-04 - val_loss: 2.9061e-04\n",
      "Epoch 163/2000\n",
      "3690/3690 [==============================] - 2s 550us/step - loss: 2.9045e-04 - val_loss: 1.8439e-04\n",
      "Epoch 164/2000\n",
      "3690/3690 [==============================] - 2s 552us/step - loss: 2.8326e-04 - val_loss: 1.9008e-04\n",
      "Epoch 165/2000\n",
      "3690/3690 [==============================] - 2s 557us/step - loss: 2.7557e-04 - val_loss: 2.9032e-04\n",
      "Epoch 166/2000\n",
      "3690/3690 [==============================] - 2s 549us/step - loss: 2.6585e-04 - val_loss: 2.4633e-04\n",
      "Epoch 167/2000\n",
      "3690/3690 [==============================] - 2s 553us/step - loss: 2.7197e-04 - val_loss: 2.3551e-04\n",
      "Epoch 168/2000\n",
      "3690/3690 [==============================] - 2s 548us/step - loss: 2.6263e-04 - val_loss: 1.9402e-04\n",
      "Epoch 169/2000\n",
      "3690/3690 [==============================] - 2s 551us/step - loss: 2.5849e-04 - val_loss: 2.8082e-04\n",
      "Epoch 170/2000\n",
      "3690/3690 [==============================] - 2s 553us/step - loss: 2.6179e-04 - val_loss: 2.9618e-04\n",
      "Epoch 171/2000\n",
      "3690/3690 [==============================] - 2s 555us/step - loss: 2.8445e-04 - val_loss: 1.7460e-04\n",
      "Epoch 172/2000\n",
      "3690/3690 [==============================] - 2s 552us/step - loss: 2.7523e-04 - val_loss: 1.7165e-04\n",
      "Epoch 173/2000\n",
      "3690/3690 [==============================] - 2s 557us/step - loss: 2.8166e-04 - val_loss: 2.7163e-04\n",
      "Epoch 174/2000\n",
      "3690/3690 [==============================] - 2s 552us/step - loss: 2.6808e-04 - val_loss: 1.6374e-04\n",
      "Epoch 175/2000\n",
      "3690/3690 [==============================] - 2s 551us/step - loss: 2.6774e-04 - val_loss: 3.5922e-04\n",
      "Epoch 176/2000\n",
      "3690/3690 [==============================] - 2s 550us/step - loss: 4.4100e-04 - val_loss: 6.1373e-04\n",
      "Epoch 177/2000\n",
      "3690/3690 [==============================] - 2s 551us/step - loss: 3.4479e-04 - val_loss: 2.3467e-04\n",
      "Epoch 178/2000\n",
      "3690/3690 [==============================] - 2s 553us/step - loss: 2.9239e-04 - val_loss: 2.3137e-04\n",
      "Epoch 179/2000\n",
      "3690/3690 [==============================] - 2s 551us/step - loss: 3.0997e-04 - val_loss: 4.2395e-04\n",
      "Epoch 180/2000\n",
      "3690/3690 [==============================] - 2s 552us/step - loss: 3.1847e-04 - val_loss: 2.2233e-04\n",
      "Epoch 181/2000\n",
      "3690/3690 [==============================] - 2s 550us/step - loss: 3.5683e-04 - val_loss: 1.7013e-04\n",
      "Epoch 182/2000\n",
      "3690/3690 [==============================] - 2s 548us/step - loss: 2.6218e-04 - val_loss: 2.8138e-04\n",
      "Epoch 183/2000\n",
      "3690/3690 [==============================] - 2s 551us/step - loss: 2.4781e-04 - val_loss: 1.8347e-04\n",
      "Epoch 184/2000\n",
      "3690/3690 [==============================] - 2s 554us/step - loss: 2.4454e-04 - val_loss: 1.6259e-04\n",
      "Epoch 185/2000\n",
      "3690/3690 [==============================] - 2s 551us/step - loss: 2.7235e-04 - val_loss: 1.9221e-04\n",
      "Epoch 186/2000\n",
      "3690/3690 [==============================] - 2s 552us/step - loss: 2.9144e-04 - val_loss: 2.9159e-04\n",
      "Epoch 187/2000\n",
      "3690/3690 [==============================] - 2s 552us/step - loss: 2.4093e-04 - val_loss: 2.4696e-04\n",
      "Epoch 188/2000\n",
      "3690/3690 [==============================] - 2s 553us/step - loss: 2.7611e-04 - val_loss: 4.8167e-04\n",
      "Epoch 189/2000\n",
      "3690/3690 [==============================] - 2s 551us/step - loss: 3.4329e-04 - val_loss: 2.1931e-04\n",
      "Epoch 190/2000\n",
      "3690/3690 [==============================] - 2s 553us/step - loss: 2.6449e-04 - val_loss: 4.2731e-04\n",
      "Epoch 191/2000\n",
      "3690/3690 [==============================] - 2s 554us/step - loss: 2.7708e-04 - val_loss: 1.6590e-04\n",
      "Epoch 192/2000\n",
      "3690/3690 [==============================] - 2s 549us/step - loss: 2.2805e-04 - val_loss: 1.6794e-04\n",
      "Epoch 193/2000\n",
      "3690/3690 [==============================] - 2s 551us/step - loss: 2.4521e-04 - val_loss: 3.3808e-04\n",
      "Epoch 194/2000\n",
      "3690/3690 [==============================] - 2s 549us/step - loss: 2.7434e-04 - val_loss: 1.5169e-04\n",
      "Epoch 195/2000\n",
      "3690/3690 [==============================] - 2s 552us/step - loss: 2.5530e-04 - val_loss: 1.4602e-04\n",
      "Epoch 196/2000\n",
      "3690/3690 [==============================] - 2s 556us/step - loss: 2.3521e-04 - val_loss: 2.2111e-04\n",
      "Epoch 197/2000\n",
      "3690/3690 [==============================] - 2s 552us/step - loss: 2.3339e-04 - val_loss: 1.4647e-04\n",
      "Epoch 198/2000\n",
      "3690/3690 [==============================] - 2s 552us/step - loss: 2.2857e-04 - val_loss: 1.3979e-04\n",
      "Epoch 199/2000\n",
      "3690/3690 [==============================] - 2s 551us/step - loss: 2.2444e-04 - val_loss: 1.5422e-04\n",
      "Epoch 200/2000\n",
      "3690/3690 [==============================] - 2s 551us/step - loss: 2.1568e-04 - val_loss: 1.4935e-04\n",
      "Epoch 201/2000\n",
      "3690/3690 [==============================] - 2s 552us/step - loss: 2.3332e-04 - val_loss: 1.3985e-04\n",
      "Epoch 202/2000\n",
      "3690/3690 [==============================] - 2s 549us/step - loss: 2.2753e-04 - val_loss: 2.9342e-04\n",
      "Epoch 203/2000\n",
      "3690/3690 [==============================] - 2s 552us/step - loss: 2.4518e-04 - val_loss: 1.5199e-04\n",
      "Epoch 204/2000\n",
      "3690/3690 [==============================] - 2s 555us/step - loss: 2.1760e-04 - val_loss: 1.8997e-04\n",
      "Epoch 205/2000\n",
      "3690/3690 [==============================] - 2s 550us/step - loss: 2.3329e-04 - val_loss: 1.4545e-04\n",
      "Epoch 206/2000\n",
      "3690/3690 [==============================] - 2s 548us/step - loss: 2.1701e-04 - val_loss: 2.0471e-04\n",
      "Epoch 207/2000\n",
      "3690/3690 [==============================] - 2s 553us/step - loss: 2.1026e-04 - val_loss: 1.2943e-04\n",
      "Epoch 208/2000\n",
      "3690/3690 [==============================] - 2s 551us/step - loss: 2.1365e-04 - val_loss: 1.3605e-04\n",
      "Epoch 209/2000\n",
      "3690/3690 [==============================] - 2s 554us/step - loss: 2.1572e-04 - val_loss: 2.7593e-04\n",
      "Epoch 210/2000\n",
      "3690/3690 [==============================] - 2s 549us/step - loss: 2.2310e-04 - val_loss: 2.0110e-04\n",
      "Epoch 211/2000\n",
      "3690/3690 [==============================] - 2s 563us/step - loss: 2.4462e-04 - val_loss: 4.0345e-04\n",
      "Epoch 212/2000\n",
      "3690/3690 [==============================] - 2s 553us/step - loss: 2.6567e-04 - val_loss: 1.8753e-04\n",
      "Epoch 213/2000\n",
      "3690/3690 [==============================] - 2s 553us/step - loss: 2.2919e-04 - val_loss: 2.0998e-04\n",
      "Epoch 214/2000\n",
      "3690/3690 [==============================] - 2s 550us/step - loss: 2.3423e-04 - val_loss: 2.5854e-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 215/2000\n",
      "3690/3690 [==============================] - 2s 552us/step - loss: 2.6719e-04 - val_loss: 4.4685e-04\n",
      "Epoch 216/2000\n",
      "3690/3690 [==============================] - 2s 550us/step - loss: 2.5530e-04 - val_loss: 1.3953e-04\n",
      "Epoch 217/2000\n",
      "3690/3690 [==============================] - 2s 552us/step - loss: 2.3575e-04 - val_loss: 1.3119e-04\n",
      "Epoch 218/2000\n",
      "3690/3690 [==============================] - 2s 551us/step - loss: 2.0796e-04 - val_loss: 3.2175e-04\n",
      "Epoch 219/2000\n",
      "3690/3690 [==============================] - 2s 559us/step - loss: 2.3739e-04 - val_loss: 1.5252e-04\n",
      "Epoch 220/2000\n",
      "3690/3690 [==============================] - 2s 553us/step - loss: 2.3628e-04 - val_loss: 6.0783e-04\n",
      "Epoch 221/2000\n",
      "3690/3690 [==============================] - 2s 552us/step - loss: 2.3909e-04 - val_loss: 1.4834e-04\n",
      "Epoch 222/2000\n",
      "3690/3690 [==============================] - 2s 553us/step - loss: 2.0424e-04 - val_loss: 1.2146e-04\n",
      "Epoch 223/2000\n",
      "3690/3690 [==============================] - 2s 553us/step - loss: 2.0178e-04 - val_loss: 1.5356e-04\n",
      "Epoch 224/2000\n",
      "3690/3690 [==============================] - 2s 554us/step - loss: 1.8718e-04 - val_loss: 1.2678e-04\n",
      "Epoch 225/2000\n",
      "3690/3690 [==============================] - 2s 564us/step - loss: 1.8929e-04 - val_loss: 1.6048e-04\n",
      "Epoch 226/2000\n",
      "3690/3690 [==============================] - 2s 550us/step - loss: 2.3603e-04 - val_loss: 1.1590e-04\n",
      "Epoch 227/2000\n",
      "3690/3690 [==============================] - 2s 556us/step - loss: 2.1369e-04 - val_loss: 2.6688e-04\n",
      "Epoch 228/2000\n",
      "3690/3690 [==============================] - 2s 548us/step - loss: 2.4984e-04 - val_loss: 4.3661e-04\n",
      "Epoch 229/2000\n",
      "3690/3690 [==============================] - 2s 549us/step - loss: 2.4097e-04 - val_loss: 1.2742e-04\n",
      "Epoch 230/2000\n",
      "3690/3690 [==============================] - 2s 549us/step - loss: 1.9775e-04 - val_loss: 2.2502e-04\n",
      "Epoch 231/2000\n",
      "3690/3690 [==============================] - 2s 551us/step - loss: 1.8739e-04 - val_loss: 1.2338e-04\n",
      "Epoch 232/2000\n",
      "3690/3690 [==============================] - 2s 554us/step - loss: 1.8238e-04 - val_loss: 2.6265e-04\n",
      "Epoch 233/2000\n",
      "3690/3690 [==============================] - 2s 553us/step - loss: 2.2981e-04 - val_loss: 1.3186e-04\n",
      "Epoch 234/2000\n",
      "3690/3690 [==============================] - 2s 554us/step - loss: 1.8339e-04 - val_loss: 1.7756e-04\n",
      "Epoch 235/2000\n",
      "3690/3690 [==============================] - 2s 558us/step - loss: 1.7662e-04 - val_loss: 1.1044e-04\n",
      "Epoch 236/2000\n",
      "3690/3690 [==============================] - 2s 552us/step - loss: 1.8344e-04 - val_loss: 1.1341e-04\n",
      "Epoch 237/2000\n",
      "3690/3690 [==============================] - 2s 552us/step - loss: 1.8538e-04 - val_loss: 1.7787e-04\n",
      "Epoch 238/2000\n",
      "3690/3690 [==============================] - 2s 556us/step - loss: 1.7056e-04 - val_loss: 1.0986e-04\n",
      "Epoch 239/2000\n",
      "3690/3690 [==============================] - 2s 552us/step - loss: 1.7653e-04 - val_loss: 1.0453e-04\n",
      "Epoch 240/2000\n",
      "3690/3690 [==============================] - 2s 552us/step - loss: 1.8660e-04 - val_loss: 2.1225e-04\n",
      "Epoch 241/2000\n",
      "3690/3690 [==============================] - 2s 554us/step - loss: 2.4797e-04 - val_loss: 1.7890e-04\n",
      "Epoch 242/2000\n",
      "3690/3690 [==============================] - 2s 551us/step - loss: 1.9089e-04 - val_loss: 3.0331e-04\n",
      "Epoch 243/2000\n",
      "3690/3690 [==============================] - 2s 556us/step - loss: 2.1046e-04 - val_loss: 2.1433e-04\n",
      "Epoch 244/2000\n",
      "3690/3690 [==============================] - 2s 553us/step - loss: 2.3325e-04 - val_loss: 2.0972e-04\n",
      "Epoch 245/2000\n",
      "3690/3690 [==============================] - 2s 555us/step - loss: 2.5794e-04 - val_loss: 1.6919e-04\n",
      "Epoch 246/2000\n",
      "3690/3690 [==============================] - 2s 553us/step - loss: 1.7575e-04 - val_loss: 1.0174e-04\n",
      "Epoch 247/2000\n",
      "3690/3690 [==============================] - 2s 554us/step - loss: 1.7498e-04 - val_loss: 1.0340e-04\n",
      "Epoch 248/2000\n",
      "3690/3690 [==============================] - 2s 552us/step - loss: 1.6890e-04 - val_loss: 1.7208e-04\n",
      "Epoch 249/2000\n",
      "3690/3690 [==============================] - 2s 552us/step - loss: 1.7428e-04 - val_loss: 1.5382e-04\n",
      "Epoch 250/2000\n",
      "3690/3690 [==============================] - 2s 552us/step - loss: 1.8467e-04 - val_loss: 1.8939e-04\n",
      "Epoch 251/2000\n",
      "3690/3690 [==============================] - 2s 551us/step - loss: 1.7564e-04 - val_loss: 2.4713e-04\n",
      "Epoch 252/2000\n",
      "3690/3690 [==============================] - 2s 551us/step - loss: 1.6582e-04 - val_loss: 1.0281e-04\n",
      "Epoch 253/2000\n",
      "3690/3690 [==============================] - 2s 554us/step - loss: 1.7602e-04 - val_loss: 2.6252e-04\n",
      "Epoch 254/2000\n",
      "3690/3690 [==============================] - 2s 553us/step - loss: 1.6996e-04 - val_loss: 1.0298e-04\n",
      "Epoch 255/2000\n",
      "3690/3690 [==============================] - 2s 552us/step - loss: 1.6560e-04 - val_loss: 3.0415e-04\n",
      "Epoch 256/2000\n",
      "3690/3690 [==============================] - 2s 554us/step - loss: 2.1780e-04 - val_loss: 1.8153e-04\n",
      "Epoch 257/2000\n",
      "3690/3690 [==============================] - 2s 552us/step - loss: 2.0960e-04 - val_loss: 3.3625e-04\n",
      "Epoch 258/2000\n",
      "3690/3690 [==============================] - 2s 554us/step - loss: 2.4013e-04 - val_loss: 1.6957e-04\n",
      "Epoch 259/2000\n",
      "3690/3690 [==============================] - 2s 553us/step - loss: 1.8199e-04 - val_loss: 1.5913e-04\n",
      "Epoch 260/2000\n",
      "3690/3690 [==============================] - 2s 553us/step - loss: 1.8879e-04 - val_loss: 1.7075e-04\n",
      "Epoch 261/2000\n",
      "3690/3690 [==============================] - 2s 552us/step - loss: 2.0790e-04 - val_loss: 1.0136e-04\n",
      "Epoch 262/2000\n",
      "3690/3690 [==============================] - 2s 555us/step - loss: 1.8242e-04 - val_loss: 1.5468e-04\n",
      "Epoch 263/2000\n",
      "3690/3690 [==============================] - 2s 562us/step - loss: 1.4211e-04 - val_loss: 1.2288e-04\n",
      "Epoch 264/2000\n",
      "3690/3690 [==============================] - 2s 550us/step - loss: 1.7555e-04 - val_loss: 1.1322e-04\n",
      "Epoch 265/2000\n",
      "3690/3690 [==============================] - 2s 553us/step - loss: 1.6443e-04 - val_loss: 3.3332e-04\n",
      "Epoch 266/2000\n",
      "3690/3690 [==============================] - 2s 565us/step - loss: 2.0312e-04 - val_loss: 8.9154e-05\n",
      "Epoch 267/2000\n",
      "3690/3690 [==============================] - 2s 560us/step - loss: 1.9459e-04 - val_loss: 1.6075e-04\n",
      "Epoch 268/2000\n",
      "3690/3690 [==============================] - 2s 566us/step - loss: 1.9630e-04 - val_loss: 8.9955e-05\n",
      "Epoch 269/2000\n",
      "3690/3690 [==============================] - 2s 558us/step - loss: 1.5300e-04 - val_loss: 1.2555e-04\n",
      "Epoch 270/2000\n",
      "3690/3690 [==============================] - 2s 561us/step - loss: 1.4715e-04 - val_loss: 8.7843e-05\n",
      "Epoch 271/2000\n",
      "3690/3690 [==============================] - 2s 559us/step - loss: 1.3821e-04 - val_loss: 9.0898e-05\n",
      "Epoch 272/2000\n",
      "3690/3690 [==============================] - 2s 563us/step - loss: 1.5538e-04 - val_loss: 9.8163e-05\n",
      "Epoch 273/2000\n",
      "3690/3690 [==============================] - 2s 564us/step - loss: 1.6182e-04 - val_loss: 3.3825e-04\n",
      "Epoch 274/2000\n",
      "3690/3690 [==============================] - 2s 561us/step - loss: 2.0861e-04 - val_loss: 1.4890e-04\n",
      "Epoch 275/2000\n",
      "3690/3690 [==============================] - 2s 554us/step - loss: 2.0882e-04 - val_loss: 1.7911e-04\n",
      "Epoch 276/2000\n",
      "3690/3690 [==============================] - 2s 557us/step - loss: 1.9550e-04 - val_loss: 1.3081e-04\n",
      "Epoch 277/2000\n",
      "3690/3690 [==============================] - 2s 552us/step - loss: 1.6474e-04 - val_loss: 1.1769e-04\n",
      "Epoch 278/2000\n",
      "3690/3690 [==============================] - 2s 553us/step - loss: 1.6997e-04 - val_loss: 1.8207e-04\n",
      "Epoch 279/2000\n",
      "3690/3690 [==============================] - 2s 554us/step - loss: 1.7043e-04 - val_loss: 1.1932e-04\n",
      "Epoch 280/2000\n",
      "3690/3690 [==============================] - 2s 554us/step - loss: 1.7233e-04 - val_loss: 8.5562e-05\n",
      "Epoch 281/2000\n",
      "3690/3690 [==============================] - 2s 571us/step - loss: 2.3901e-04 - val_loss: 9.3135e-05\n",
      "Epoch 282/2000\n",
      "3690/3690 [==============================] - 2s 565us/step - loss: 1.7362e-04 - val_loss: 1.4573e-04\n",
      "Epoch 283/2000\n",
      "3690/3690 [==============================] - 2s 555us/step - loss: 1.5886e-04 - val_loss: 2.0473e-04\n",
      "Epoch 284/2000\n",
      "3690/3690 [==============================] - 2s 555us/step - loss: 1.6591e-04 - val_loss: 8.5181e-05\n",
      "Epoch 285/2000\n",
      "3690/3690 [==============================] - 2s 559us/step - loss: 1.3936e-04 - val_loss: 1.0273e-04\n",
      "Epoch 286/2000\n",
      "3690/3690 [==============================] - 2s 555us/step - loss: 1.5740e-04 - val_loss: 8.1718e-05\n",
      "Epoch 287/2000\n",
      "3690/3690 [==============================] - 2s 552us/step - loss: 1.5367e-04 - val_loss: 1.6166e-04\n",
      "Epoch 288/2000\n",
      "3690/3690 [==============================] - 2s 554us/step - loss: 1.7824e-04 - val_loss: 1.2730e-04\n",
      "Epoch 289/2000\n",
      "3690/3690 [==============================] - 2s 557us/step - loss: 1.7190e-04 - val_loss: 4.8255e-04\n",
      "Epoch 290/2000\n",
      "3690/3690 [==============================] - 2s 557us/step - loss: 2.5785e-04 - val_loss: 1.5256e-04\n",
      "Epoch 291/2000\n",
      "3690/3690 [==============================] - 2s 555us/step - loss: 1.7798e-04 - val_loss: 1.1752e-04\n",
      "Epoch 292/2000\n",
      "3690/3690 [==============================] - 2s 553us/step - loss: 1.5521e-04 - val_loss: 9.5720e-05\n",
      "Epoch 293/2000\n",
      "3690/3690 [==============================] - 2s 556us/step - loss: 1.6505e-04 - val_loss: 2.8499e-04\n",
      "Epoch 294/2000\n",
      "3690/3690 [==============================] - 2s 556us/step - loss: 1.7388e-04 - val_loss: 8.7172e-05\n",
      "Epoch 295/2000\n",
      "3690/3690 [==============================] - 2s 556us/step - loss: 1.3170e-04 - val_loss: 7.9878e-05\n",
      "Epoch 296/2000\n",
      "3690/3690 [==============================] - 2s 553us/step - loss: 1.2956e-04 - val_loss: 8.2635e-05\n",
      "Epoch 297/2000\n",
      "3690/3690 [==============================] - 2s 557us/step - loss: 1.2758e-04 - val_loss: 2.4042e-04\n",
      "Epoch 298/2000\n",
      "3690/3690 [==============================] - 2s 555us/step - loss: 1.7553e-04 - val_loss: 1.0859e-04\n",
      "Epoch 299/2000\n",
      "3690/3690 [==============================] - 2s 554us/step - loss: 1.5849e-04 - val_loss: 7.9774e-05\n",
      "Epoch 300/2000\n",
      "3690/3690 [==============================] - 2s 555us/step - loss: 1.4926e-04 - val_loss: 8.6528e-05\n",
      "Epoch 301/2000\n",
      "3690/3690 [==============================] - 2s 555us/step - loss: 1.7019e-04 - val_loss: 7.7538e-05\n",
      "Epoch 302/2000\n",
      "3690/3690 [==============================] - 2s 554us/step - loss: 1.3375e-04 - val_loss: 7.7197e-05\n",
      "Epoch 303/2000\n",
      "3690/3690 [==============================] - 2s 569us/step - loss: 1.3133e-04 - val_loss: 1.3095e-04\n",
      "Epoch 304/2000\n",
      "3690/3690 [==============================] - 2s 559us/step - loss: 1.3305e-04 - val_loss: 7.9339e-05\n",
      "Epoch 305/2000\n",
      "3690/3690 [==============================] - 2s 560us/step - loss: 1.3984e-04 - val_loss: 7.5594e-05\n",
      "Epoch 306/2000\n",
      "3690/3690 [==============================] - 2s 568us/step - loss: 1.3669e-04 - val_loss: 8.7544e-05\n",
      "Epoch 307/2000\n",
      "3690/3690 [==============================] - 2s 554us/step - loss: 1.4423e-04 - val_loss: 1.7125e-04\n",
      "Epoch 308/2000\n",
      "3690/3690 [==============================] - 2s 555us/step - loss: 2.0712e-04 - val_loss: 1.0237e-04\n",
      "Epoch 309/2000\n",
      "3690/3690 [==============================] - 2s 554us/step - loss: 2.2428e-04 - val_loss: 7.7306e-05\n",
      "Epoch 310/2000\n",
      "3690/3690 [==============================] - 2s 553us/step - loss: 1.6148e-04 - val_loss: 2.5742e-04\n",
      "Epoch 311/2000\n",
      "3690/3690 [==============================] - 2s 556us/step - loss: 1.8331e-04 - val_loss: 1.1954e-04\n",
      "Epoch 312/2000\n",
      "3690/3690 [==============================] - 2s 555us/step - loss: 1.8691e-04 - val_loss: 1.1678e-04\n",
      "Epoch 313/2000\n",
      "3690/3690 [==============================] - 2s 553us/step - loss: 1.5209e-04 - val_loss: 2.1648e-04\n",
      "Epoch 314/2000\n",
      "3690/3690 [==============================] - 2s 555us/step - loss: 1.5778e-04 - val_loss: 1.9663e-04\n",
      "Epoch 315/2000\n",
      "3690/3690 [==============================] - 2s 558us/step - loss: 1.7560e-04 - val_loss: 1.3433e-04\n",
      "Epoch 316/2000\n",
      "3690/3690 [==============================] - 2s 556us/step - loss: 2.1366e-04 - val_loss: 1.1169e-04\n",
      "Epoch 317/2000\n",
      "3690/3690 [==============================] - 2s 556us/step - loss: 1.5067e-04 - val_loss: 7.6505e-05\n",
      "Epoch 318/2000\n",
      "3690/3690 [==============================] - 2s 556us/step - loss: 1.3705e-04 - val_loss: 7.7207e-05\n",
      "Epoch 319/2000\n",
      "3690/3690 [==============================] - 2s 554us/step - loss: 1.4169e-04 - val_loss: 4.0005e-04\n",
      "Epoch 320/2000\n",
      "3690/3690 [==============================] - 2s 561us/step - loss: 1.8772e-04 - val_loss: 1.1577e-04\n",
      "Epoch 321/2000\n",
      "3690/3690 [==============================] - 2s 557us/step - loss: 1.3755e-04 - val_loss: 1.2242e-04\n",
      "Epoch 322/2000\n",
      "3690/3690 [==============================] - 2s 553us/step - loss: 1.2156e-04 - val_loss: 8.2609e-05\n",
      "Epoch 323/2000\n",
      "3690/3690 [==============================] - 2s 554us/step - loss: 1.2522e-04 - val_loss: 4.3744e-04\n",
      "Epoch 324/2000\n",
      "3690/3690 [==============================] - 2s 555us/step - loss: 1.8959e-04 - val_loss: 7.4496e-05\n",
      "Epoch 325/2000\n",
      "3690/3690 [==============================] - 2s 552us/step - loss: 1.5662e-04 - val_loss: 9.8561e-05\n",
      "Epoch 326/2000\n",
      "3690/3690 [==============================] - 2s 557us/step - loss: 1.3776e-04 - val_loss: 9.4991e-05\n",
      "Epoch 327/2000\n",
      "3690/3690 [==============================] - 2s 552us/step - loss: 2.1749e-04 - val_loss: 5.2785e-04\n",
      "Epoch 328/2000\n",
      "3690/3690 [==============================] - 2s 557us/step - loss: 2.0033e-04 - val_loss: 2.7549e-04\n",
      "Epoch 329/2000\n",
      "3690/3690 [==============================] - 2s 553us/step - loss: 1.6848e-04 - val_loss: 1.7440e-04\n",
      "Epoch 330/2000\n",
      "3690/3690 [==============================] - 2s 552us/step - loss: 1.5711e-04 - val_loss: 8.4687e-05\n",
      "Epoch 331/2000\n",
      "3690/3690 [==============================] - 2s 557us/step - loss: 1.4262e-04 - val_loss: 7.3217e-05\n",
      "Epoch 332/2000\n",
      "3690/3690 [==============================] - 2s 551us/step - loss: 1.4438e-04 - val_loss: 1.1561e-04\n",
      "Epoch 333/2000\n",
      "3690/3690 [==============================] - 2s 558us/step - loss: 1.2655e-04 - val_loss: 7.1975e-05\n",
      "Epoch 334/2000\n",
      "3690/3690 [==============================] - 2s 554us/step - loss: 1.3513e-04 - val_loss: 7.9310e-05\n",
      "Epoch 335/2000\n",
      "3690/3690 [==============================] - 2s 552us/step - loss: 1.4589e-04 - val_loss: 1.1952e-04\n",
      "Epoch 336/2000\n",
      "3690/3690 [==============================] - 2s 559us/step - loss: 1.3697e-04 - val_loss: 1.4018e-04\n",
      "Epoch 337/2000\n",
      "3690/3690 [==============================] - 2s 557us/step - loss: 1.3052e-04 - val_loss: 7.3289e-05\n",
      "Epoch 338/2000\n",
      "3690/3690 [==============================] - 2s 555us/step - loss: 1.1740e-04 - val_loss: 1.5171e-04\n",
      "Epoch 339/2000\n",
      "3690/3690 [==============================] - 2s 557us/step - loss: 1.4758e-04 - val_loss: 7.3054e-05\n",
      "Epoch 340/2000\n",
      "3690/3690 [==============================] - 2s 554us/step - loss: 1.2752e-04 - val_loss: 7.4972e-05\n",
      "Epoch 341/2000\n",
      "3690/3690 [==============================] - 2s 551us/step - loss: 1.2036e-04 - val_loss: 7.3005e-05\n",
      "Epoch 342/2000\n",
      "3690/3690 [==============================] - 2s 552us/step - loss: 1.4984e-04 - val_loss: 1.4617e-04\n",
      "Epoch 343/2000\n",
      "3690/3690 [==============================] - 2s 554us/step - loss: 1.5349e-04 - val_loss: 1.0236e-04\n",
      "Epoch 344/2000\n",
      "3690/3690 [==============================] - 2s 555us/step - loss: 1.5450e-04 - val_loss: 8.0253e-05\n",
      "Epoch 345/2000\n",
      "3690/3690 [==============================] - 2s 553us/step - loss: 1.1725e-04 - val_loss: 8.8391e-05\n",
      "Epoch 346/2000\n",
      "3690/3690 [==============================] - 2s 553us/step - loss: 1.1137e-04 - val_loss: 1.0981e-04\n",
      "Epoch 347/2000\n",
      "3690/3690 [==============================] - 2s 554us/step - loss: 1.2558e-04 - val_loss: 7.3510e-05\n",
      "Epoch 348/2000\n",
      "3690/3690 [==============================] - 2s 553us/step - loss: 1.1578e-04 - val_loss: 1.1851e-04\n",
      "Epoch 349/2000\n",
      "3690/3690 [==============================] - 2s 554us/step - loss: 1.3211e-04 - val_loss: 2.3807e-04\n",
      "Epoch 350/2000\n",
      "3690/3690 [==============================] - 2s 551us/step - loss: 1.7731e-04 - val_loss: 1.1157e-04\n",
      "Epoch 351/2000\n",
      "3690/3690 [==============================] - 2s 559us/step - loss: 1.6598e-04 - val_loss: 7.3029e-05\n",
      "Epoch 352/2000\n",
      "3690/3690 [==============================] - 2s 555us/step - loss: 1.1835e-04 - val_loss: 8.0959e-05\n",
      "Epoch 353/2000\n",
      "3690/3690 [==============================] - 2s 555us/step - loss: 1.2529e-04 - val_loss: 7.3943e-05\n",
      "Epoch 354/2000\n",
      "3690/3690 [==============================] - 2s 551us/step - loss: 1.1255e-04 - val_loss: 9.9128e-05\n",
      "Epoch 355/2000\n",
      "3690/3690 [==============================] - 2s 553us/step - loss: 1.3326e-04 - val_loss: 1.0213e-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 356/2000\n",
      "3690/3690 [==============================] - 2s 555us/step - loss: 1.1345e-04 - val_loss: 1.2769e-04\n",
      "Epoch 357/2000\n",
      "3690/3690 [==============================] - 2s 552us/step - loss: 1.5940e-04 - val_loss: 2.4694e-04\n",
      "Epoch 358/2000\n",
      "3690/3690 [==============================] - 2s 552us/step - loss: 1.2383e-04 - val_loss: 1.7556e-04\n",
      "Epoch 359/2000\n",
      "3690/3690 [==============================] - 2s 555us/step - loss: 1.4747e-04 - val_loss: 1.4392e-04\n",
      "Epoch 360/2000\n",
      "3690/3690 [==============================] - 2s 551us/step - loss: 2.1713e-04 - val_loss: 1.0438e-04\n",
      "Epoch 361/2000\n",
      "3690/3690 [==============================] - 2s 550us/step - loss: 1.3283e-04 - val_loss: 1.3577e-04\n",
      "Epoch 362/2000\n",
      "3690/3690 [==============================] - 2s 553us/step - loss: 1.2255e-04 - val_loss: 3.4175e-04\n",
      "Epoch 363/2000\n",
      "3690/3690 [==============================] - 2s 554us/step - loss: 1.7184e-04 - val_loss: 2.1379e-04\n",
      "Epoch 364/2000\n",
      "3690/3690 [==============================] - 2s 555us/step - loss: 1.5386e-04 - val_loss: 9.5817e-05\n",
      "Epoch 365/2000\n",
      "3690/3690 [==============================] - 2s 552us/step - loss: 1.4763e-04 - val_loss: 1.6341e-04\n",
      "Epoch 366/2000\n",
      "3690/3690 [==============================] - 2s 552us/step - loss: 2.0503e-04 - val_loss: 8.2191e-05\n",
      "Epoch 367/2000\n",
      "3690/3690 [==============================] - 2s 559us/step - loss: 1.9769e-04 - val_loss: 1.0872e-04\n",
      "Epoch 368/2000\n",
      "3690/3690 [==============================] - 2s 551us/step - loss: 1.9133e-04 - val_loss: 3.3191e-04\n",
      "Epoch 369/2000\n",
      "3690/3690 [==============================] - 2s 549us/step - loss: 2.5384e-04 - val_loss: 8.9534e-05\n",
      "Epoch 370/2000\n",
      "3690/3690 [==============================] - 2s 553us/step - loss: 1.3671e-04 - val_loss: 2.3190e-04\n",
      "Epoch 371/2000\n",
      "3690/3690 [==============================] - 2s 553us/step - loss: 1.3923e-04 - val_loss: 7.8216e-05\n",
      "Epoch 372/2000\n",
      "3690/3690 [==============================] - 2s 552us/step - loss: 1.2735e-04 - val_loss: 7.7881e-05\n",
      "Epoch 373/2000\n",
      "3690/3690 [==============================] - 2s 553us/step - loss: 1.4002e-04 - val_loss: 1.0313e-04\n",
      "Epoch 374/2000\n",
      "3690/3690 [==============================] - 2s 552us/step - loss: 1.5034e-04 - val_loss: 8.5121e-05\n",
      "Epoch 375/2000\n",
      "3690/3690 [==============================] - 2s 553us/step - loss: 1.7696e-04 - val_loss: 2.7818e-04\n",
      "Epoch 376/2000\n",
      "3690/3690 [==============================] - 2s 554us/step - loss: 2.6002e-04 - val_loss: 2.7879e-04\n",
      "Epoch 377/2000\n",
      "3690/3690 [==============================] - 2s 555us/step - loss: 1.6718e-04 - val_loss: 1.8501e-04\n",
      "Epoch 378/2000\n",
      "3690/3690 [==============================] - 2s 551us/step - loss: 1.1961e-04 - val_loss: 7.0337e-05\n",
      "Epoch 379/2000\n",
      "3690/3690 [==============================] - 2s 551us/step - loss: 1.1221e-04 - val_loss: 1.0852e-04\n",
      "Epoch 380/2000\n",
      "3690/3690 [==============================] - 2s 552us/step - loss: 1.1742e-04 - val_loss: 8.4677e-05\n",
      "Epoch 381/2000\n",
      "3690/3690 [==============================] - 2s 553us/step - loss: 1.1203e-04 - val_loss: 7.3930e-05\n",
      "Epoch 382/2000\n",
      "3690/3690 [==============================] - 2s 554us/step - loss: 1.2711e-04 - val_loss: 3.5829e-04\n",
      "Epoch 383/2000\n",
      "3690/3690 [==============================] - 2s 557us/step - loss: 1.6451e-04 - val_loss: 8.5749e-05\n",
      "Epoch 384/2000\n",
      "3690/3690 [==============================] - 2s 552us/step - loss: 1.4762e-04 - val_loss: 7.2800e-05\n",
      "Epoch 385/2000\n",
      "3690/3690 [==============================] - 2s 555us/step - loss: 1.2333e-04 - val_loss: 7.5946e-05\n",
      "Epoch 386/2000\n",
      "3690/3690 [==============================] - 2s 552us/step - loss: 1.0308e-04 - val_loss: 9.3030e-05\n",
      "Epoch 387/2000\n",
      "3690/3690 [==============================] - 2s 553us/step - loss: 1.2998e-04 - val_loss: 1.4444e-04\n",
      "Epoch 388/2000\n",
      "3690/3690 [==============================] - 2s 552us/step - loss: 1.4372e-04 - val_loss: 1.8308e-04\n",
      "Epoch 389/2000\n",
      "3690/3690 [==============================] - 2s 554us/step - loss: 1.4470e-04 - val_loss: 1.8936e-04\n",
      "Epoch 390/2000\n",
      "3690/3690 [==============================] - 2s 558us/step - loss: 1.1621e-04 - val_loss: 7.2622e-05\n",
      "Epoch 391/2000\n",
      "3690/3690 [==============================] - 2s 552us/step - loss: 1.1272e-04 - val_loss: 7.5986e-05\n",
      "Epoch 392/2000\n",
      "3690/3690 [==============================] - 2s 553us/step - loss: 1.0983e-04 - val_loss: 7.4933e-05\n",
      "Epoch 393/2000\n",
      "3690/3690 [==============================] - 2s 554us/step - loss: 1.1212e-04 - val_loss: 7.2609e-05\n",
      "Epoch 394/2000\n",
      "3690/3690 [==============================] - 2s 555us/step - loss: 9.9731e-05 - val_loss: 7.4601e-05\n",
      "Epoch 395/2000\n",
      "3690/3690 [==============================] - 2s 552us/step - loss: 1.1847e-04 - val_loss: 2.2874e-04\n",
      "Epoch 396/2000\n",
      "3690/3690 [==============================] - 2s 552us/step - loss: 1.1924e-04 - val_loss: 1.1833e-04\n",
      "Epoch 397/2000\n",
      "3690/3690 [==============================] - 2s 554us/step - loss: 1.2341e-04 - val_loss: 1.5404e-04\n",
      "Epoch 398/2000\n",
      "3690/3690 [==============================] - 2s 556us/step - loss: 1.0948e-04 - val_loss: 8.2172e-05\n",
      "Epoch 399/2000\n",
      "3690/3690 [==============================] - 2s 552us/step - loss: 1.2725e-04 - val_loss: 6.9972e-05\n",
      "Epoch 400/2000\n",
      "3690/3690 [==============================] - 2s 555us/step - loss: 1.6704e-04 - val_loss: 7.0551e-05\n",
      "Epoch 401/2000\n",
      "3690/3690 [==============================] - 2s 554us/step - loss: 2.7520e-04 - val_loss: 1.5197e-04\n",
      "Epoch 402/2000\n",
      "3690/3690 [==============================] - 2s 551us/step - loss: 2.2453e-04 - val_loss: 3.4156e-04\n",
      "Epoch 403/2000\n",
      "3690/3690 [==============================] - 2s 554us/step - loss: 1.4922e-04 - val_loss: 9.3435e-05\n",
      "Epoch 404/2000\n",
      "3690/3690 [==============================] - 2s 556us/step - loss: 1.2464e-04 - val_loss: 7.0889e-05\n",
      "Epoch 405/2000\n",
      "3690/3690 [==============================] - 2s 554us/step - loss: 1.2077e-04 - val_loss: 1.0478e-04\n",
      "Epoch 406/2000\n",
      "3690/3690 [==============================] - 2s 554us/step - loss: 1.2248e-04 - val_loss: 1.0575e-04\n",
      "Epoch 407/2000\n",
      "3690/3690 [==============================] - 2s 552us/step - loss: 1.2458e-04 - val_loss: 6.9434e-05\n",
      "Epoch 408/2000\n",
      "3690/3690 [==============================] - 2s 552us/step - loss: 1.2016e-04 - val_loss: 1.2877e-04\n",
      "Epoch 409/2000\n",
      "3690/3690 [==============================] - 2s 551us/step - loss: 1.2946e-04 - val_loss: 7.7591e-05\n",
      "Epoch 410/2000\n",
      "3690/3690 [==============================] - 2s 551us/step - loss: 1.1081e-04 - val_loss: 6.9227e-05\n",
      "Epoch 411/2000\n",
      "3690/3690 [==============================] - 2s 554us/step - loss: 1.2921e-04 - val_loss: 1.8508e-04\n",
      "Epoch 412/2000\n",
      "3690/3690 [==============================] - 2s 552us/step - loss: 1.2573e-04 - val_loss: 7.7351e-05\n",
      "Epoch 413/2000\n",
      "3690/3690 [==============================] - 2s 552us/step - loss: 1.4109e-04 - val_loss: 7.0973e-05\n",
      "Epoch 414/2000\n",
      "3690/3690 [==============================] - 2s 555us/step - loss: 1.0658e-04 - val_loss: 7.2613e-05\n",
      "Epoch 415/2000\n",
      "3690/3690 [==============================] - 2s 552us/step - loss: 1.1223e-04 - val_loss: 1.1492e-04\n",
      "Epoch 416/2000\n",
      "3690/3690 [==============================] - 2s 554us/step - loss: 1.3148e-04 - val_loss: 7.0836e-05\n",
      "Epoch 417/2000\n",
      "3690/3690 [==============================] - 2s 552us/step - loss: 1.2260e-04 - val_loss: 1.5089e-04\n",
      "Epoch 418/2000\n",
      "3690/3690 [==============================] - 2s 551us/step - loss: 1.1591e-04 - val_loss: 1.1457e-04\n",
      "Epoch 419/2000\n",
      "3690/3690 [==============================] - 2s 553us/step - loss: 1.6322e-04 - val_loss: 1.9162e-04\n",
      "Epoch 420/2000\n",
      "3690/3690 [==============================] - 2s 552us/step - loss: 1.4421e-04 - val_loss: 9.2237e-05\n",
      "Epoch 421/2000\n",
      "3690/3690 [==============================] - 2s 553us/step - loss: 1.0633e-04 - val_loss: 7.8188e-05\n",
      "Epoch 422/2000\n",
      "3690/3690 [==============================] - 2s 558us/step - loss: 1.0914e-04 - val_loss: 1.0929e-04\n",
      "Epoch 423/2000\n",
      "3690/3690 [==============================] - 2s 554us/step - loss: 1.0774e-04 - val_loss: 1.8097e-04\n",
      "Epoch 424/2000\n",
      "3690/3690 [==============================] - 2s 555us/step - loss: 1.2832e-04 - val_loss: 3.2525e-04\n",
      "Epoch 425/2000\n",
      "3690/3690 [==============================] - 2s 554us/step - loss: 2.1541e-04 - val_loss: 3.6101e-04\n",
      "Epoch 426/2000\n",
      "3690/3690 [==============================] - 2s 552us/step - loss: 2.2435e-04 - val_loss: 5.8227e-04\n",
      "Epoch 427/2000\n",
      "3690/3690 [==============================] - 2s 559us/step - loss: 2.4755e-04 - val_loss: 2.9883e-04\n",
      "Epoch 428/2000\n",
      "3690/3690 [==============================] - 2s 556us/step - loss: 1.7045e-04 - val_loss: 1.1848e-04\n",
      "Epoch 429/2000\n",
      "3690/3690 [==============================] - 2s 557us/step - loss: 1.1013e-04 - val_loss: 7.2201e-05\n",
      "Epoch 430/2000\n",
      "3690/3690 [==============================] - 2s 555us/step - loss: 9.8537e-05 - val_loss: 1.0582e-04\n",
      "Epoch 431/2000\n",
      "3690/3690 [==============================] - 2s 553us/step - loss: 1.0301e-04 - val_loss: 7.4920e-05\n",
      "Epoch 432/2000\n",
      "3690/3690 [==============================] - 2s 552us/step - loss: 1.1013e-04 - val_loss: 8.7060e-05\n",
      "Epoch 433/2000\n",
      "3690/3690 [==============================] - 2s 554us/step - loss: 1.0576e-04 - val_loss: 1.3079e-04\n",
      "Epoch 434/2000\n",
      "3690/3690 [==============================] - 2s 557us/step - loss: 1.5018e-04 - val_loss: 2.1882e-04\n",
      "Epoch 435/2000\n",
      "3690/3690 [==============================] - 2s 555us/step - loss: 2.0150e-04 - val_loss: 4.9122e-04\n",
      "Epoch 436/2000\n",
      "3690/3690 [==============================] - 2s 554us/step - loss: 2.3676e-04 - val_loss: 7.8953e-05\n",
      "Epoch 437/2000\n",
      "3690/3690 [==============================] - 2s 554us/step - loss: 1.4546e-04 - val_loss: 1.1327e-04\n",
      "Epoch 438/2000\n",
      "3690/3690 [==============================] - 2s 556us/step - loss: 1.5154e-04 - val_loss: 1.4852e-04\n",
      "Epoch 439/2000\n",
      "3690/3690 [==============================] - 2s 553us/step - loss: 1.2915e-04 - val_loss: 7.4092e-05\n",
      "Epoch 440/2000\n",
      "3690/3690 [==============================] - 2s 558us/step - loss: 1.1692e-04 - val_loss: 7.7698e-05\n",
      "Epoch 441/2000\n",
      "3690/3690 [==============================] - 2s 561us/step - loss: 1.1411e-04 - val_loss: 8.9777e-05\n",
      "Epoch 442/2000\n",
      "3690/3690 [==============================] - 2s 558us/step - loss: 1.1354e-04 - val_loss: 6.9170e-05\n",
      "Epoch 443/2000\n",
      "3690/3690 [==============================] - 2s 562us/step - loss: 1.1583e-04 - val_loss: 1.3911e-04\n",
      "Epoch 444/2000\n",
      "3690/3690 [==============================] - 2s 564us/step - loss: 1.0641e-04 - val_loss: 6.9010e-05\n",
      "Epoch 445/2000\n",
      "3690/3690 [==============================] - 2s 562us/step - loss: 9.4772e-05 - val_loss: 6.8953e-05\n",
      "Epoch 446/2000\n",
      "3690/3690 [==============================] - 2s 564us/step - loss: 1.0658e-04 - val_loss: 8.7864e-05\n",
      "Epoch 447/2000\n",
      "3690/3690 [==============================] - 2s 560us/step - loss: 1.0622e-04 - val_loss: 7.1522e-05\n",
      "Epoch 448/2000\n",
      "3690/3690 [==============================] - 2s 565us/step - loss: 9.8279e-05 - val_loss: 6.9779e-05\n",
      "Epoch 449/2000\n",
      "3690/3690 [==============================] - 2s 558us/step - loss: 1.0056e-04 - val_loss: 1.3879e-04\n",
      "Epoch 450/2000\n",
      "3690/3690 [==============================] - 2s 555us/step - loss: 1.4345e-04 - val_loss: 1.5828e-04\n",
      "Epoch 451/2000\n",
      "3690/3690 [==============================] - 2s 559us/step - loss: 1.4262e-04 - val_loss: 2.7135e-04\n",
      "Epoch 452/2000\n",
      "3690/3690 [==============================] - 2s 564us/step - loss: 1.4819e-04 - val_loss: 3.1099e-04\n",
      "Epoch 453/2000\n",
      "3690/3690 [==============================] - 2s 560us/step - loss: 2.0552e-04 - val_loss: 2.3980e-04\n",
      "Epoch 454/2000\n",
      "3690/3690 [==============================] - 2s 554us/step - loss: 1.4582e-04 - val_loss: 6.7654e-05\n",
      "Epoch 455/2000\n",
      "3690/3690 [==============================] - 2s 553us/step - loss: 1.4319e-04 - val_loss: 6.8210e-05\n",
      "Epoch 456/2000\n",
      "3690/3690 [==============================] - 2s 554us/step - loss: 1.0767e-04 - val_loss: 7.0287e-05\n",
      "Epoch 457/2000\n",
      "3690/3690 [==============================] - 2s 554us/step - loss: 1.0655e-04 - val_loss: 7.7585e-05\n",
      "Epoch 458/2000\n",
      "3690/3690 [==============================] - 2s 553us/step - loss: 1.1375e-04 - val_loss: 1.7001e-04\n",
      "Epoch 459/2000\n",
      "3690/3690 [==============================] - 2s 556us/step - loss: 1.2482e-04 - val_loss: 1.4270e-04\n",
      "Epoch 460/2000\n",
      "3690/3690 [==============================] - 2s 554us/step - loss: 1.0791e-04 - val_loss: 6.8989e-05\n",
      "Epoch 461/2000\n",
      "3690/3690 [==============================] - 2s 555us/step - loss: 1.1533e-04 - val_loss: 8.0808e-05\n",
      "Epoch 462/2000\n",
      "3690/3690 [==============================] - 2s 552us/step - loss: 1.1727e-04 - val_loss: 1.0619e-04\n",
      "Epoch 463/2000\n",
      "3690/3690 [==============================] - 2s 555us/step - loss: 1.6318e-04 - val_loss: 1.2743e-04\n",
      "Epoch 464/2000\n",
      "3690/3690 [==============================] - 2s 555us/step - loss: 1.3835e-04 - val_loss: 8.7525e-05\n",
      "Epoch 465/2000\n",
      "3690/3690 [==============================] - 2s 554us/step - loss: 1.2938e-04 - val_loss: 2.1945e-04\n",
      "Epoch 466/2000\n",
      "3690/3690 [==============================] - 2s 551us/step - loss: 1.7263e-04 - val_loss: 7.4449e-05\n",
      "Epoch 467/2000\n",
      "3690/3690 [==============================] - 2s 553us/step - loss: 1.1608e-04 - val_loss: 6.7189e-05\n",
      "Epoch 468/2000\n",
      "3690/3690 [==============================] - 2s 555us/step - loss: 9.9186e-05 - val_loss: 7.6435e-05\n",
      "Epoch 469/2000\n",
      "3690/3690 [==============================] - 2s 555us/step - loss: 1.1050e-04 - val_loss: 1.8724e-04\n",
      "Epoch 470/2000\n",
      "3690/3690 [==============================] - 2s 566us/step - loss: 1.6018e-04 - val_loss: 2.7508e-04\n",
      "Epoch 471/2000\n",
      "3690/3690 [==============================] - 2s 553us/step - loss: 1.3332e-04 - val_loss: 7.3406e-05\n",
      "Epoch 472/2000\n",
      "3690/3690 [==============================] - 2s 553us/step - loss: 1.2130e-04 - val_loss: 6.9622e-05\n",
      "Epoch 473/2000\n",
      "3690/3690 [==============================] - 2s 551us/step - loss: 1.0510e-04 - val_loss: 8.8893e-05\n",
      "Epoch 474/2000\n",
      "3690/3690 [==============================] - 2s 555us/step - loss: 1.3039e-04 - val_loss: 2.9546e-04\n",
      "Epoch 475/2000\n",
      "3690/3690 [==============================] - 2s 554us/step - loss: 1.5901e-04 - val_loss: 1.1869e-04\n",
      "Epoch 476/2000\n",
      "3690/3690 [==============================] - 2s 558us/step - loss: 1.0973e-04 - val_loss: 6.9217e-05\n",
      "Epoch 477/2000\n",
      "3690/3690 [==============================] - 2s 557us/step - loss: 9.9724e-05 - val_loss: 9.4522e-05\n",
      "Epoch 478/2000\n",
      "3690/3690 [==============================] - 2s 564us/step - loss: 1.1076e-04 - val_loss: 7.0473e-05\n",
      "Epoch 479/2000\n",
      "3690/3690 [==============================] - 2s 562us/step - loss: 1.0249e-04 - val_loss: 7.2639e-05\n",
      "Epoch 480/2000\n",
      "3690/3690 [==============================] - 2s 564us/step - loss: 1.0866e-04 - val_loss: 7.0983e-05\n",
      "Epoch 481/2000\n",
      "3690/3690 [==============================] - 2s 555us/step - loss: 9.3721e-05 - val_loss: 6.8466e-05\n",
      "Epoch 482/2000\n",
      "3690/3690 [==============================] - 2s 557us/step - loss: 9.0795e-05 - val_loss: 6.9051e-05\n",
      "Epoch 483/2000\n",
      "3690/3690 [==============================] - 2s 564us/step - loss: 1.0192e-04 - val_loss: 7.9271e-05\n",
      "Epoch 484/2000\n",
      "3690/3690 [==============================] - 2s 564us/step - loss: 1.1253e-04 - val_loss: 7.1547e-05\n",
      "Epoch 485/2000\n",
      "3690/3690 [==============================] - 2s 554us/step - loss: 2.2085e-04 - val_loss: 1.3540e-04\n",
      "Epoch 486/2000\n",
      "3690/3690 [==============================] - 2s 554us/step - loss: 2.5585e-04 - val_loss: 3.6075e-04\n",
      "Epoch 487/2000\n",
      "3690/3690 [==============================] - 2s 555us/step - loss: 1.8805e-04 - val_loss: 1.0757e-04\n",
      "Epoch 488/2000\n",
      "3690/3690 [==============================] - 2s 553us/step - loss: 1.3959e-04 - val_loss: 1.1540e-04\n",
      "Epoch 489/2000\n",
      "3690/3690 [==============================] - 2s 555us/step - loss: 1.5197e-04 - val_loss: 2.6158e-04\n",
      "Epoch 490/2000\n",
      "3690/3690 [==============================] - 2s 554us/step - loss: 1.6686e-04 - val_loss: 1.1315e-04\n",
      "Epoch 491/2000\n",
      "3690/3690 [==============================] - 2s 555us/step - loss: 1.5920e-04 - val_loss: 7.7856e-05\n",
      "Epoch 492/2000\n",
      "3690/3690 [==============================] - 2s 557us/step - loss: 1.0692e-04 - val_loss: 7.0952e-05\n",
      "Epoch 493/2000\n",
      "3690/3690 [==============================] - 2s 555us/step - loss: 1.0600e-04 - val_loss: 8.1308e-05\n",
      "Epoch 494/2000\n",
      "3690/3690 [==============================] - 2s 584us/step - loss: 1.3634e-04 - val_loss: 9.5201e-05\n",
      "Epoch 495/2000\n",
      "3690/3690 [==============================] - 2s 597us/step - loss: 1.2031e-04 - val_loss: 6.9715e-05\n",
      "Epoch 496/2000\n",
      "3690/3690 [==============================] - 2s 564us/step - loss: 1.1087e-04 - val_loss: 2.8530e-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 497/2000\n",
      "3690/3690 [==============================] - 2s 568us/step - loss: 1.9616e-04 - val_loss: 1.9450e-04\n",
      "Epoch 498/2000\n",
      "3690/3690 [==============================] - 2s 591us/step - loss: 1.2068e-04 - val_loss: 7.6222e-05\n",
      "Epoch 499/2000\n",
      "3690/3690 [==============================] - 2s 571us/step - loss: 1.0662e-04 - val_loss: 1.1497e-04\n",
      "Epoch 500/2000\n",
      "3690/3690 [==============================] - 2s 567us/step - loss: 1.1202e-04 - val_loss: 7.0543e-05\n",
      "Epoch 501/2000\n",
      "3690/3690 [==============================] - 2s 556us/step - loss: 1.1772e-04 - val_loss: 2.0628e-04\n",
      "Epoch 502/2000\n",
      "3690/3690 [==============================] - 2s 556us/step - loss: 1.3991e-04 - val_loss: 1.3106e-04\n",
      "Epoch 503/2000\n",
      "3690/3690 [==============================] - 2s 559us/step - loss: 1.7380e-04 - val_loss: 1.2906e-04\n",
      "Epoch 504/2000\n",
      "3690/3690 [==============================] - 2s 555us/step - loss: 1.8438e-04 - val_loss: 1.1972e-04\n",
      "Epoch 505/2000\n",
      "3690/3690 [==============================] - 2s 558us/step - loss: 1.5057e-04 - val_loss: 7.8788e-05\n",
      "Epoch 506/2000\n",
      "3690/3690 [==============================] - 2s 576us/step - loss: 1.1370e-04 - val_loss: 9.0082e-05\n",
      "Epoch 507/2000\n",
      "3690/3690 [==============================] - 2s 585us/step - loss: 9.5792e-05 - val_loss: 8.8447e-05\n",
      "Epoch 508/2000\n",
      "3690/3690 [==============================] - 2s 578us/step - loss: 1.0371e-04 - val_loss: 9.3555e-05\n",
      "Epoch 509/2000\n",
      "3690/3690 [==============================] - 2s 581us/step - loss: 1.1670e-04 - val_loss: 8.5353e-05\n",
      "Epoch 510/2000\n",
      "3690/3690 [==============================] - 2s 580us/step - loss: 1.0491e-04 - val_loss: 8.3995e-05\n",
      "Epoch 511/2000\n",
      "3690/3690 [==============================] - 2s 561us/step - loss: 1.0623e-04 - val_loss: 1.0121e-04\n",
      "Epoch 512/2000\n",
      "3690/3690 [==============================] - 2s 578us/step - loss: 1.1267e-04 - val_loss: 1.7886e-04\n",
      "Epoch 513/2000\n",
      "3690/3690 [==============================] - 2s 559us/step - loss: 1.4572e-04 - val_loss: 1.4005e-04\n",
      "Epoch 514/2000\n",
      "3690/3690 [==============================] - 2s 576us/step - loss: 9.4537e-05 - val_loss: 7.7234e-05\n",
      "Epoch 515/2000\n",
      "3690/3690 [==============================] - 2s 555us/step - loss: 9.5119e-05 - val_loss: 7.2041e-05\n",
      "Epoch 516/2000\n",
      "3690/3690 [==============================] - 2s 552us/step - loss: 1.0875e-04 - val_loss: 7.1979e-05\n",
      "Epoch 517/2000\n",
      "3690/3690 [==============================] - 2s 556us/step - loss: 9.6602e-05 - val_loss: 1.3058e-04\n",
      "Epoch 518/2000\n",
      "3690/3690 [==============================] - 2s 554us/step - loss: 1.0718e-04 - val_loss: 6.9904e-05\n",
      "Epoch 519/2000\n",
      "3690/3690 [==============================] - 2s 554us/step - loss: 1.0175e-04 - val_loss: 9.7587e-05\n",
      "Epoch 520/2000\n",
      "3690/3690 [==============================] - 2s 552us/step - loss: 9.9197e-05 - val_loss: 7.0163e-05\n",
      "Epoch 521/2000\n",
      "3690/3690 [==============================] - 2s 554us/step - loss: 1.0548e-04 - val_loss: 7.2034e-05\n",
      "Epoch 522/2000\n",
      "3690/3690 [==============================] - 2s 557us/step - loss: 1.5612e-04 - val_loss: 1.8173e-04\n",
      "Epoch 523/2000\n",
      "3690/3690 [==============================] - 2s 554us/step - loss: 1.2803e-04 - val_loss: 9.1308e-05\n",
      "Epoch 524/2000\n",
      "3690/3690 [==============================] - 2s 553us/step - loss: 1.1214e-04 - val_loss: 7.8986e-05\n",
      "Epoch 525/2000\n",
      "3690/3690 [==============================] - 2s 556us/step - loss: 8.9312e-05 - val_loss: 1.2785e-04\n",
      "Epoch 526/2000\n",
      "3690/3690 [==============================] - 2s 554us/step - loss: 1.0080e-04 - val_loss: 1.0287e-04\n",
      "Epoch 527/2000\n",
      "3690/3690 [==============================] - 2s 558us/step - loss: 1.0987e-04 - val_loss: 1.3185e-04\n",
      "Epoch 528/2000\n",
      "3690/3690 [==============================] - 2s 553us/step - loss: 1.2165e-04 - val_loss: 7.2055e-05\n",
      "Epoch 529/2000\n",
      "3690/3690 [==============================] - 2s 554us/step - loss: 1.1332e-04 - val_loss: 7.6095e-05\n",
      "Epoch 530/2000\n",
      "3690/3690 [==============================] - 2s 558us/step - loss: 1.7795e-04 - val_loss: 1.4902e-04\n",
      "Epoch 531/2000\n",
      "3690/3690 [==============================] - 2s 557us/step - loss: 1.7216e-04 - val_loss: 8.2370e-05\n",
      "Epoch 532/2000\n",
      "3690/3690 [==============================] - 2s 554us/step - loss: 1.0429e-04 - val_loss: 7.8287e-05\n",
      "Epoch 533/2000\n",
      "3690/3690 [==============================] - 2s 554us/step - loss: 1.0819e-04 - val_loss: 7.6202e-05\n",
      "Epoch 534/2000\n",
      "3690/3690 [==============================] - 2s 553us/step - loss: 9.0475e-05 - val_loss: 7.6160e-05\n",
      "Epoch 535/2000\n",
      "3690/3690 [==============================] - 2s 553us/step - loss: 8.5640e-05 - val_loss: 7.5915e-05\n",
      "Epoch 536/2000\n",
      "3690/3690 [==============================] - 2s 555us/step - loss: 8.8431e-05 - val_loss: 7.1551e-05\n",
      "Epoch 537/2000\n",
      "3690/3690 [==============================] - 2s 552us/step - loss: 9.0694e-05 - val_loss: 4.7531e-04\n",
      "Epoch 538/2000\n",
      "3690/3690 [==============================] - 2s 555us/step - loss: 2.2400e-04 - val_loss: 7.5944e-05\n",
      "Epoch 539/2000\n",
      "3690/3690 [==============================] - 2s 556us/step - loss: 1.6531e-04 - val_loss: 2.7786e-04\n",
      "Epoch 540/2000\n",
      "3690/3690 [==============================] - 2s 555us/step - loss: 2.4071e-04 - val_loss: 5.4743e-04\n",
      "Epoch 541/2000\n",
      "3690/3690 [==============================] - 2s 558us/step - loss: 2.5230e-04 - val_loss: 2.1520e-04\n",
      "Epoch 542/2000\n",
      "3690/3690 [==============================] - 2s 558us/step - loss: 1.4417e-04 - val_loss: 1.6906e-04\n",
      "Epoch 543/2000\n",
      "3690/3690 [==============================] - 2s 554us/step - loss: 1.3564e-04 - val_loss: 7.5964e-05\n",
      "Epoch 544/2000\n",
      "3690/3690 [==============================] - 2s 556us/step - loss: 1.0415e-04 - val_loss: 7.1832e-05\n",
      "Epoch 545/2000\n",
      "3690/3690 [==============================] - 2s 555us/step - loss: 9.0253e-05 - val_loss: 1.0903e-04\n",
      "Epoch 546/2000\n",
      "3690/3690 [==============================] - 2s 553us/step - loss: 1.0965e-04 - val_loss: 6.9967e-05\n",
      "Epoch 547/2000\n",
      "3690/3690 [==============================] - 2s 555us/step - loss: 8.9024e-05 - val_loss: 7.1803e-05\n",
      "Epoch 548/2000\n",
      "3690/3690 [==============================] - 2s 553us/step - loss: 1.0117e-04 - val_loss: 1.4342e-04\n",
      "Epoch 549/2000\n",
      "3690/3690 [==============================] - 2s 552us/step - loss: 1.1069e-04 - val_loss: 1.7088e-04\n",
      "Epoch 550/2000\n",
      "3690/3690 [==============================] - 2s 555us/step - loss: 1.1781e-04 - val_loss: 2.8448e-04\n",
      "Epoch 551/2000\n",
      "3690/3690 [==============================] - 2s 555us/step - loss: 1.5152e-04 - val_loss: 7.3430e-05\n",
      "Epoch 552/2000\n",
      "3690/3690 [==============================] - 2s 554us/step - loss: 1.0007e-04 - val_loss: 6.8703e-05\n",
      "Epoch 553/2000\n",
      "3690/3690 [==============================] - 2s 559us/step - loss: 1.0181e-04 - val_loss: 2.1316e-04\n",
      "Epoch 554/2000\n",
      "3690/3690 [==============================] - 2s 555us/step - loss: 1.3290e-04 - val_loss: 8.7950e-05\n",
      "Epoch 555/2000\n",
      "3690/3690 [==============================] - 2s 553us/step - loss: 8.9245e-05 - val_loss: 7.3968e-05\n",
      "Epoch 556/2000\n",
      "3690/3690 [==============================] - 2s 555us/step - loss: 9.2192e-05 - val_loss: 9.0158e-05\n",
      "Epoch 557/2000\n",
      "3690/3690 [==============================] - 2s 556us/step - loss: 1.1035e-04 - val_loss: 1.8008e-04\n",
      "Epoch 558/2000\n",
      "3690/3690 [==============================] - 2s 557us/step - loss: 1.2354e-04 - val_loss: 7.0146e-05\n",
      "Epoch 559/2000\n",
      "3690/3690 [==============================] - 2s 552us/step - loss: 1.0367e-04 - val_loss: 9.1365e-05\n",
      "Epoch 560/2000\n",
      "3690/3690 [==============================] - 2s 556us/step - loss: 9.9487e-05 - val_loss: 1.1674e-04\n",
      "Epoch 561/2000\n",
      "3690/3690 [==============================] - 2s 557us/step - loss: 1.0397e-04 - val_loss: 6.7777e-05\n",
      "Epoch 562/2000\n",
      "3690/3690 [==============================] - 2s 554us/step - loss: 2.0135e-04 - val_loss: 2.3850e-04\n",
      "Epoch 563/2000\n",
      "3690/3690 [==============================] - 2s 553us/step - loss: 1.9587e-04 - val_loss: 1.7551e-04\n",
      "Epoch 564/2000\n",
      "3690/3690 [==============================] - 2s 555us/step - loss: 1.7905e-04 - val_loss: 1.5755e-04\n",
      "Epoch 565/2000\n",
      "3690/3690 [==============================] - 2s 554us/step - loss: 1.4787e-04 - val_loss: 3.3766e-04\n",
      "Epoch 566/2000\n",
      "3690/3690 [==============================] - 2s 559us/step - loss: 1.2083e-04 - val_loss: 9.0255e-05\n",
      "Epoch 567/2000\n",
      "3690/3690 [==============================] - 2s 555us/step - loss: 9.4456e-05 - val_loss: 7.2478e-05\n",
      "Epoch 568/2000\n",
      "3690/3690 [==============================] - 2s 556us/step - loss: 8.6592e-05 - val_loss: 6.8543e-05\n",
      "Epoch 569/2000\n",
      "3690/3690 [==============================] - 2s 557us/step - loss: 9.2457e-05 - val_loss: 7.0059e-05\n",
      "Epoch 570/2000\n",
      "3690/3690 [==============================] - 2s 552us/step - loss: 9.4940e-05 - val_loss: 1.6933e-04\n",
      "Epoch 571/2000\n",
      "3690/3690 [==============================] - 2s 554us/step - loss: 1.3727e-04 - val_loss: 9.0403e-05\n",
      "Epoch 572/2000\n",
      "3690/3690 [==============================] - 2s 554us/step - loss: 1.0323e-04 - val_loss: 7.2643e-05\n",
      "Epoch 573/2000\n",
      "3690/3690 [==============================] - 2s 556us/step - loss: 9.7707e-05 - val_loss: 6.6193e-05\n",
      "Epoch 574/2000\n",
      "3690/3690 [==============================] - 2s 555us/step - loss: 8.9393e-05 - val_loss: 9.1300e-05\n",
      "Epoch 575/2000\n",
      "3690/3690 [==============================] - 2s 555us/step - loss: 9.3714e-05 - val_loss: 7.9454e-05\n",
      "Epoch 576/2000\n",
      "3690/3690 [==============================] - 2s 552us/step - loss: 8.8672e-05 - val_loss: 9.9558e-05\n",
      "Epoch 577/2000\n",
      "3690/3690 [==============================] - 2s 559us/step - loss: 1.0297e-04 - val_loss: 7.4659e-05\n",
      "Epoch 578/2000\n",
      "3690/3690 [==============================] - 2s 557us/step - loss: 8.6606e-05 - val_loss: 8.1024e-05\n",
      "Epoch 579/2000\n",
      "3690/3690 [==============================] - 2s 554us/step - loss: 8.6385e-05 - val_loss: 8.1437e-05\n",
      "Epoch 580/2000\n",
      "3690/3690 [==============================] - 2s 553us/step - loss: 9.3867e-05 - val_loss: 6.8717e-05\n",
      "Epoch 581/2000\n",
      "3690/3690 [==============================] - 2s 556us/step - loss: 1.1296e-04 - val_loss: 7.1733e-05\n",
      "Epoch 582/2000\n",
      "3690/3690 [==============================] - 2s 558us/step - loss: 8.5073e-05 - val_loss: 9.0074e-05\n",
      "Epoch 583/2000\n",
      "3690/3690 [==============================] - 2s 551us/step - loss: 8.7531e-05 - val_loss: 9.5103e-05\n",
      "Epoch 584/2000\n",
      "3690/3690 [==============================] - 2s 556us/step - loss: 1.0190e-04 - val_loss: 6.6928e-05\n",
      "Epoch 585/2000\n",
      "3690/3690 [==============================] - 2s 555us/step - loss: 9.8235e-05 - val_loss: 7.2379e-05\n",
      "Epoch 586/2000\n",
      "3690/3690 [==============================] - 2s 554us/step - loss: 9.0518e-05 - val_loss: 1.2100e-04\n",
      "Epoch 587/2000\n",
      "3690/3690 [==============================] - 2s 563us/step - loss: 1.0820e-04 - val_loss: 6.7691e-05\n",
      "Epoch 588/2000\n",
      "3690/3690 [==============================] - 2s 554us/step - loss: 9.6101e-05 - val_loss: 1.0285e-04\n",
      "Epoch 589/2000\n",
      "3690/3690 [==============================] - 2s 555us/step - loss: 1.3386e-04 - val_loss: 8.6842e-05\n",
      "Epoch 590/2000\n",
      "3690/3690 [==============================] - 2s 556us/step - loss: 1.0354e-04 - val_loss: 8.0877e-05\n",
      "Epoch 591/2000\n",
      "3690/3690 [==============================] - 2s 557us/step - loss: 9.7460e-05 - val_loss: 1.1127e-04\n",
      "Epoch 592/2000\n",
      "3690/3690 [==============================] - 2s 557us/step - loss: 1.4797e-04 - val_loss: 7.9247e-05\n",
      "Epoch 593/2000\n",
      "3690/3690 [==============================] - 2s 556us/step - loss: 8.8800e-05 - val_loss: 7.0734e-05\n",
      "Epoch 594/2000\n",
      "3690/3690 [==============================] - 2s 555us/step - loss: 9.5890e-05 - val_loss: 6.9326e-05\n",
      "Epoch 595/2000\n",
      "3690/3690 [==============================] - 2s 555us/step - loss: 9.1418e-05 - val_loss: 1.2685e-04\n",
      "Epoch 596/2000\n",
      "3690/3690 [==============================] - 2s 555us/step - loss: 1.6496e-04 - val_loss: 1.0665e-04\n",
      "Epoch 597/2000\n",
      "3690/3690 [==============================] - 2s 554us/step - loss: 1.2524e-04 - val_loss: 4.4488e-04\n",
      "Epoch 598/2000\n",
      "3690/3690 [==============================] - 2s 554us/step - loss: 1.8672e-04 - val_loss: 2.3704e-04\n",
      "Epoch 599/2000\n",
      "3690/3690 [==============================] - 2s 555us/step - loss: 2.0044e-04 - val_loss: 1.3103e-04\n",
      "Epoch 600/2000\n",
      "3690/3690 [==============================] - 2s 557us/step - loss: 1.9835e-04 - val_loss: 1.0453e-04\n",
      "Epoch 601/2000\n",
      "3690/3690 [==============================] - 2s 553us/step - loss: 1.1929e-04 - val_loss: 6.7572e-05\n",
      "Epoch 602/2000\n",
      "3690/3690 [==============================] - 2s 556us/step - loss: 9.8523e-05 - val_loss: 7.5273e-05\n",
      "Epoch 603/2000\n",
      "3690/3690 [==============================] - 2s 556us/step - loss: 8.8097e-05 - val_loss: 7.4159e-05\n",
      "Epoch 604/2000\n",
      "3690/3690 [==============================] - 2s 554us/step - loss: 8.7285e-05 - val_loss: 7.4819e-05\n",
      "Epoch 605/2000\n",
      "3690/3690 [==============================] - 2s 552us/step - loss: 8.7097e-05 - val_loss: 6.7916e-05\n",
      "Epoch 606/2000\n",
      "3690/3690 [==============================] - 2s 555us/step - loss: 1.0344e-04 - val_loss: 9.5924e-05\n",
      "Epoch 607/2000\n",
      "3690/3690 [==============================] - 2s 555us/step - loss: 1.3352e-04 - val_loss: 1.5838e-04\n",
      "Epoch 608/2000\n",
      "3690/3690 [==============================] - 2s 558us/step - loss: 1.2303e-04 - val_loss: 9.3566e-05\n",
      "Epoch 609/2000\n",
      "3690/3690 [==============================] - 2s 552us/step - loss: 1.0339e-04 - val_loss: 7.2491e-05\n",
      "Epoch 610/2000\n",
      "3690/3690 [==============================] - 2s 551us/step - loss: 1.1292e-04 - val_loss: 8.5411e-05\n",
      "Epoch 611/2000\n",
      "3690/3690 [==============================] - 2s 556us/step - loss: 9.6658e-05 - val_loss: 6.9936e-05\n",
      "Epoch 612/2000\n",
      "3690/3690 [==============================] - 2s 554us/step - loss: 9.4935e-05 - val_loss: 2.1783e-04\n",
      "Epoch 613/2000\n",
      "3690/3690 [==============================] - 2s 552us/step - loss: 1.2966e-04 - val_loss: 9.9316e-05\n",
      "Epoch 614/2000\n",
      "3690/3690 [==============================] - 2s 555us/step - loss: 1.0340e-04 - val_loss: 9.6258e-05\n",
      "Epoch 615/2000\n",
      "3690/3690 [==============================] - 2s 553us/step - loss: 1.0517e-04 - val_loss: 6.6069e-05\n",
      "Epoch 616/2000\n",
      "3690/3690 [==============================] - 2s 558us/step - loss: 9.9759e-05 - val_loss: 9.6657e-05\n",
      "Epoch 617/2000\n",
      "3690/3690 [==============================] - 2s 554us/step - loss: 9.4781e-05 - val_loss: 8.7183e-05\n",
      "Epoch 618/2000\n",
      "3690/3690 [==============================] - 2s 551us/step - loss: 9.4797e-05 - val_loss: 6.7063e-05\n",
      "Epoch 619/2000\n",
      "3690/3690 [==============================] - 2s 555us/step - loss: 1.0319e-04 - val_loss: 6.6164e-05\n",
      "Epoch 620/2000\n",
      "3690/3690 [==============================] - 2s 559us/step - loss: 1.0023e-04 - val_loss: 7.3801e-05\n",
      "Epoch 621/2000\n",
      "3690/3690 [==============================] - 2s 555us/step - loss: 8.6327e-05 - val_loss: 6.8579e-05\n",
      "Epoch 622/2000\n",
      "3690/3690 [==============================] - 2s 558us/step - loss: 8.4923e-05 - val_loss: 7.0633e-05\n",
      "Epoch 623/2000\n",
      "3690/3690 [==============================] - 2s 558us/step - loss: 9.6549e-05 - val_loss: 7.7531e-05\n",
      "Epoch 624/2000\n",
      "3690/3690 [==============================] - 2s 555us/step - loss: 1.1091e-04 - val_loss: 4.1540e-04\n",
      "Epoch 625/2000\n",
      "3690/3690 [==============================] - 2s 555us/step - loss: 2.6685e-04 - val_loss: 2.3460e-04\n",
      "Epoch 626/2000\n",
      "3690/3690 [==============================] - 2s 555us/step - loss: 2.1085e-04 - val_loss: 6.9559e-05\n",
      "Epoch 627/2000\n",
      "3690/3690 [==============================] - 2s 552us/step - loss: 1.1980e-04 - val_loss: 3.0723e-04\n",
      "Epoch 628/2000\n",
      "3690/3690 [==============================] - 2s 555us/step - loss: 1.3790e-04 - val_loss: 1.1125e-04\n",
      "Epoch 629/2000\n",
      "3690/3690 [==============================] - 2s 554us/step - loss: 1.1525e-04 - val_loss: 1.9080e-04\n",
      "Epoch 630/2000\n",
      "3690/3690 [==============================] - 2s 555us/step - loss: 1.4307e-04 - val_loss: 7.5096e-05\n",
      "Epoch 631/2000\n",
      "3690/3690 [==============================] - 2s 560us/step - loss: 1.2430e-04 - val_loss: 1.1548e-04\n",
      "Epoch 632/2000\n",
      "3690/3690 [==============================] - 2s 555us/step - loss: 1.3886e-04 - val_loss: 1.6670e-04\n",
      "Epoch 633/2000\n",
      "3690/3690 [==============================] - 2s 555us/step - loss: 1.1645e-04 - val_loss: 6.8407e-05\n",
      "Epoch 634/2000\n",
      "3690/3690 [==============================] - 2s 555us/step - loss: 1.1054e-04 - val_loss: 8.5465e-05\n",
      "Epoch 635/2000\n",
      "3690/3690 [==============================] - 2s 552us/step - loss: 9.6298e-05 - val_loss: 6.8153e-05\n",
      "Epoch 636/2000\n",
      "3690/3690 [==============================] - 2s 553us/step - loss: 9.4704e-05 - val_loss: 1.1003e-04\n",
      "Epoch 637/2000\n",
      "3690/3690 [==============================] - 2s 556us/step - loss: 1.1837e-04 - val_loss: 1.2236e-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 638/2000\n",
      "3690/3690 [==============================] - 2s 554us/step - loss: 1.0532e-04 - val_loss: 1.3529e-04\n",
      "Epoch 639/2000\n",
      "3690/3690 [==============================] - 2s 557us/step - loss: 1.4921e-04 - val_loss: 1.3840e-04\n",
      "Epoch 640/2000\n",
      "3690/3690 [==============================] - 2s 555us/step - loss: 1.2087e-04 - val_loss: 1.0239e-04\n",
      "Epoch 641/2000\n",
      "3690/3690 [==============================] - 2s 556us/step - loss: 9.4419e-05 - val_loss: 1.9568e-04\n",
      "Epoch 642/2000\n",
      "3690/3690 [==============================] - 2s 553us/step - loss: 9.3849e-05 - val_loss: 6.8972e-05\n",
      "Epoch 643/2000\n",
      "3690/3690 [==============================] - 2s 553us/step - loss: 1.1774e-04 - val_loss: 9.2552e-05\n",
      "Epoch 644/2000\n",
      "3690/3690 [==============================] - 2s 553us/step - loss: 1.1919e-04 - val_loss: 1.1636e-04\n",
      "Epoch 645/2000\n",
      "3690/3690 [==============================] - 2s 554us/step - loss: 1.1892e-04 - val_loss: 2.8855e-04\n",
      "Epoch 646/2000\n",
      "3690/3690 [==============================] - 2s 552us/step - loss: 1.6566e-04 - val_loss: 1.7027e-04\n",
      "Epoch 647/2000\n",
      "3690/3690 [==============================] - 2s 557us/step - loss: 1.1696e-04 - val_loss: 7.7785e-05\n",
      "Epoch 648/2000\n",
      "3690/3690 [==============================] - 2s 562us/step - loss: 1.0593e-04 - val_loss: 3.3007e-04\n",
      "Epoch 649/2000\n",
      "3690/3690 [==============================] - 2s 555us/step - loss: 1.7302e-04 - val_loss: 7.3246e-05\n",
      "Epoch 650/2000\n",
      "3690/3690 [==============================] - 2s 554us/step - loss: 1.1498e-04 - val_loss: 1.7114e-04\n",
      "Epoch 651/2000\n",
      "3690/3690 [==============================] - 2s 555us/step - loss: 1.3607e-04 - val_loss: 2.3267e-04\n",
      "Epoch 652/2000\n",
      "3690/3690 [==============================] - 2s 554us/step - loss: 1.1797e-04 - val_loss: 6.8009e-05\n",
      "Epoch 653/2000\n",
      "3690/3690 [==============================] - 2s 554us/step - loss: 9.3829e-05 - val_loss: 7.5607e-05\n",
      "Epoch 654/2000\n",
      "3690/3690 [==============================] - 2s 554us/step - loss: 8.5533e-05 - val_loss: 1.7580e-04\n",
      "Epoch 655/2000\n",
      "3690/3690 [==============================] - 2s 557us/step - loss: 1.2400e-04 - val_loss: 2.3418e-04\n",
      "Epoch 656/2000\n",
      "3690/3690 [==============================] - 2s 554us/step - loss: 1.1355e-04 - val_loss: 1.9045e-04\n",
      "Epoch 657/2000\n",
      "3690/3690 [==============================] - 2s 556us/step - loss: 9.9594e-05 - val_loss: 7.5684e-05\n",
      "Epoch 658/2000\n",
      "3690/3690 [==============================] - 2s 551us/step - loss: 8.8668e-05 - val_loss: 6.9889e-05\n",
      "Epoch 659/2000\n",
      "3690/3690 [==============================] - 2s 555us/step - loss: 8.8574e-05 - val_loss: 7.7262e-05\n",
      "Epoch 660/2000\n",
      "3690/3690 [==============================] - 2s 551us/step - loss: 8.3213e-05 - val_loss: 1.7701e-04\n",
      "Epoch 661/2000\n",
      "3690/3690 [==============================] - 2s 555us/step - loss: 8.8207e-05 - val_loss: 8.6174e-05\n",
      "Epoch 662/2000\n",
      "3690/3690 [==============================] - 2s 557us/step - loss: 8.3261e-05 - val_loss: 6.9260e-05\n",
      "Epoch 663/2000\n",
      "3690/3690 [==============================] - 2s 555us/step - loss: 9.1055e-05 - val_loss: 6.7876e-05\n",
      "Epoch 664/2000\n",
      "3690/3690 [==============================] - 2s 552us/step - loss: 8.7024e-05 - val_loss: 9.7474e-05\n",
      "Epoch 665/2000\n",
      "3690/3690 [==============================] - 2s 556us/step - loss: 8.6652e-05 - val_loss: 7.6261e-05\n",
      "Epoch 666/2000\n",
      "3690/3690 [==============================] - 2s 553us/step - loss: 1.0458e-04 - val_loss: 7.6950e-05\n",
      "Epoch 667/2000\n",
      "3690/3690 [==============================] - 2s 556us/step - loss: 1.0361e-04 - val_loss: 6.8842e-05\n",
      "Epoch 668/2000\n",
      "3690/3690 [==============================] - 2s 550us/step - loss: 1.0935e-04 - val_loss: 1.2719e-04\n",
      "Epoch 669/2000\n",
      "3690/3690 [==============================] - 2s 552us/step - loss: 1.0001e-04 - val_loss: 9.1912e-05\n",
      "Epoch 670/2000\n",
      "3690/3690 [==============================] - 2s 558us/step - loss: 1.1149e-04 - val_loss: 7.8492e-05\n",
      "Epoch 671/2000\n",
      "3690/3690 [==============================] - 2s 555us/step - loss: 1.0098e-04 - val_loss: 1.9462e-04\n",
      "Epoch 672/2000\n",
      "3690/3690 [==============================] - 2s 556us/step - loss: 1.0442e-04 - val_loss: 7.8516e-05\n",
      "Epoch 673/2000\n",
      "3690/3690 [==============================] - 2s 551us/step - loss: 9.0494e-05 - val_loss: 2.2298e-04\n",
      "Epoch 674/2000\n",
      "3690/3690 [==============================] - 2s 550us/step - loss: 1.3432e-04 - val_loss: 1.0400e-04\n",
      "Epoch 675/2000\n",
      "3690/3690 [==============================] - 2s 551us/step - loss: 1.5800e-04 - val_loss: 1.0657e-04\n",
      "Epoch 676/2000\n",
      "3690/3690 [==============================] - 2s 553us/step - loss: 1.8284e-04 - val_loss: 3.2269e-04\n",
      "Epoch 677/2000\n",
      "3690/3690 [==============================] - 2s 552us/step - loss: 1.9044e-04 - val_loss: 1.0014e-04\n",
      "Epoch 678/2000\n",
      "3690/3690 [==============================] - 2s 553us/step - loss: 1.1995e-04 - val_loss: 3.1591e-04\n",
      "Epoch 679/2000\n",
      "3690/3690 [==============================] - 2s 551us/step - loss: 1.1845e-04 - val_loss: 6.8542e-05\n",
      "Epoch 680/2000\n",
      "3690/3690 [==============================] - 2s 552us/step - loss: 1.0647e-04 - val_loss: 2.2999e-04\n",
      "Epoch 681/2000\n",
      "3690/3690 [==============================] - 2s 554us/step - loss: 2.0769e-04 - val_loss: 1.1983e-04\n",
      "Epoch 682/2000\n",
      "3690/3690 [==============================] - 2s 553us/step - loss: 9.2439e-05 - val_loss: 9.9373e-05\n",
      "Epoch 683/2000\n",
      "3690/3690 [==============================] - 2s 554us/step - loss: 9.1374e-05 - val_loss: 1.1386e-04\n",
      "Epoch 684/2000\n",
      "3690/3690 [==============================] - 2s 558us/step - loss: 1.1850e-04 - val_loss: 1.0468e-04\n",
      "Epoch 685/2000\n",
      "3690/3690 [==============================] - 2s 554us/step - loss: 9.2902e-05 - val_loss: 1.2861e-04\n",
      "Epoch 686/2000\n",
      "3690/3690 [==============================] - 2s 557us/step - loss: 1.0187e-04 - val_loss: 1.6044e-04\n",
      "Epoch 687/2000\n",
      "3690/3690 [==============================] - 2s 553us/step - loss: 8.8787e-05 - val_loss: 1.0886e-04\n",
      "Epoch 688/2000\n",
      "3690/3690 [==============================] - 2s 556us/step - loss: 1.2080e-04 - val_loss: 7.9494e-05\n",
      "Epoch 689/2000\n",
      "3690/3690 [==============================] - 2s 553us/step - loss: 1.1352e-04 - val_loss: 1.8670e-04\n",
      "Epoch 690/2000\n",
      "3690/3690 [==============================] - 2s 552us/step - loss: 1.0394e-04 - val_loss: 7.4623e-05\n",
      "Epoch 691/2000\n",
      "3690/3690 [==============================] - 2s 552us/step - loss: 9.2619e-05 - val_loss: 7.0374e-05\n",
      "Epoch 692/2000\n",
      "3690/3690 [==============================] - 2s 553us/step - loss: 1.0342e-04 - val_loss: 9.4173e-05\n",
      "Epoch 693/2000\n",
      "3690/3690 [==============================] - 2s 554us/step - loss: 9.6008e-05 - val_loss: 8.4079e-05\n",
      "Epoch 694/2000\n",
      "3690/3690 [==============================] - 2s 557us/step - loss: 8.3514e-05 - val_loss: 1.1522e-04\n",
      "Epoch 695/2000\n",
      "3690/3690 [==============================] - 2s 554us/step - loss: 9.1476e-05 - val_loss: 1.0467e-04\n",
      "Epoch 696/2000\n",
      "3690/3690 [==============================] - 2s 553us/step - loss: 7.7528e-05 - val_loss: 7.4197e-05\n",
      "Epoch 697/2000\n",
      "3690/3690 [==============================] - 2s 555us/step - loss: 8.5371e-05 - val_loss: 7.7865e-05\n",
      "Epoch 698/2000\n",
      "3690/3690 [==============================] - 2s 552us/step - loss: 1.1652e-04 - val_loss: 1.4530e-04\n",
      "Epoch 699/2000\n",
      "3690/3690 [==============================] - 2s 553us/step - loss: 1.0315e-04 - val_loss: 6.7702e-05\n",
      "Epoch 700/2000\n",
      "3690/3690 [==============================] - 2s 554us/step - loss: 8.3267e-05 - val_loss: 8.2775e-05\n",
      "Epoch 701/2000\n",
      "3690/3690 [==============================] - 2s 563us/step - loss: 8.2733e-05 - val_loss: 7.5253e-05\n",
      "Epoch 702/2000\n",
      "3690/3690 [==============================] - 2s 558us/step - loss: 7.8709e-05 - val_loss: 1.0155e-04\n",
      "Epoch 703/2000\n",
      "3690/3690 [==============================] - 2s 554us/step - loss: 1.1230e-04 - val_loss: 1.3817e-04\n",
      "Epoch 704/2000\n",
      "3690/3690 [==============================] - 2s 555us/step - loss: 1.2202e-04 - val_loss: 8.5084e-05\n",
      "Epoch 705/2000\n",
      "3690/3690 [==============================] - 2s 552us/step - loss: 9.6613e-05 - val_loss: 9.2367e-05\n",
      "Epoch 706/2000\n",
      "3690/3690 [==============================] - 2s 555us/step - loss: 8.3268e-05 - val_loss: 1.0332e-04\n",
      "Epoch 707/2000\n",
      "3690/3690 [==============================] - 2s 554us/step - loss: 9.2323e-05 - val_loss: 7.8948e-05\n",
      "Epoch 708/2000\n",
      "3690/3690 [==============================] - 2s 559us/step - loss: 8.5268e-05 - val_loss: 9.5119e-05\n",
      "Epoch 709/2000\n",
      "3690/3690 [==============================] - 2s 558us/step - loss: 1.0080e-04 - val_loss: 7.0835e-05\n",
      "Epoch 710/2000\n",
      "3690/3690 [==============================] - 2s 553us/step - loss: 8.8718e-05 - val_loss: 1.7789e-04\n",
      "Epoch 711/2000\n",
      "3690/3690 [==============================] - 2s 554us/step - loss: 1.1193e-04 - val_loss: 3.2033e-04\n",
      "Epoch 712/2000\n",
      "3690/3690 [==============================] - 2s 557us/step - loss: 1.3155e-04 - val_loss: 2.9053e-04\n",
      "Epoch 713/2000\n",
      "3690/3690 [==============================] - 2s 557us/step - loss: 1.1408e-04 - val_loss: 6.8971e-05\n",
      "Epoch 714/2000\n",
      "3690/3690 [==============================] - 2s 554us/step - loss: 1.3243e-04 - val_loss: 7.3562e-05\n",
      "Epoch 715/2000\n",
      "3690/3690 [==============================] - 2s 554us/step - loss: 1.1195e-04 - val_loss: 1.5936e-04\n",
      "Epoch 716/2000\n",
      "3690/3690 [==============================] - 2s 554us/step - loss: 1.3339e-04 - val_loss: 9.3828e-05\n",
      "Epoch 717/2000\n",
      "3690/3690 [==============================] - 2s 557us/step - loss: 1.3527e-04 - val_loss: 1.6400e-04\n",
      "Epoch 718/2000\n",
      "3690/3690 [==============================] - 2s 554us/step - loss: 1.0541e-04 - val_loss: 7.3855e-05\n",
      "Epoch 719/2000\n",
      "3690/3690 [==============================] - 2s 554us/step - loss: 8.1381e-05 - val_loss: 2.5847e-04\n",
      "Epoch 720/2000\n",
      "3690/3690 [==============================] - 2s 554us/step - loss: 1.6017e-04 - val_loss: 3.7106e-04\n",
      "Epoch 721/2000\n",
      "3690/3690 [==============================] - 2s 555us/step - loss: 1.7536e-04 - val_loss: 7.0950e-05\n",
      "Epoch 722/2000\n",
      "3690/3690 [==============================] - 2s 553us/step - loss: 1.1781e-04 - val_loss: 1.3280e-04\n",
      "Epoch 723/2000\n",
      "3690/3690 [==============================] - 2s 553us/step - loss: 9.1256e-05 - val_loss: 1.9049e-04\n",
      "Epoch 724/2000\n",
      "3690/3690 [==============================] - 2s 553us/step - loss: 9.5013e-05 - val_loss: 1.5433e-04\n",
      "Epoch 725/2000\n",
      "3690/3690 [==============================] - 2s 557us/step - loss: 9.3332e-05 - val_loss: 8.0243e-05\n",
      "Epoch 726/2000\n",
      "3690/3690 [==============================] - 2s 554us/step - loss: 8.4619e-05 - val_loss: 9.1345e-05\n",
      "Epoch 727/2000\n",
      "3690/3690 [==============================] - 2s 556us/step - loss: 8.1523e-05 - val_loss: 7.3085e-05\n",
      "Epoch 728/2000\n",
      "3690/3690 [==============================] - 2s 554us/step - loss: 8.3054e-05 - val_loss: 8.4721e-05\n",
      "Epoch 729/2000\n",
      "3690/3690 [==============================] - 2s 557us/step - loss: 8.4205e-05 - val_loss: 1.0688e-04\n",
      "Epoch 730/2000\n",
      "3690/3690 [==============================] - 2s 553us/step - loss: 1.0687e-04 - val_loss: 1.8083e-04\n",
      "Epoch 731/2000\n",
      "3690/3690 [==============================] - 2s 554us/step - loss: 1.0986e-04 - val_loss: 1.1021e-04\n",
      "Epoch 732/2000\n",
      "3690/3690 [==============================] - 2s 553us/step - loss: 1.1483e-04 - val_loss: 1.4213e-04\n",
      "Epoch 733/2000\n",
      "3690/3690 [==============================] - 2s 555us/step - loss: 1.4949e-04 - val_loss: 2.0862e-04\n",
      "Epoch 734/2000\n",
      "3690/3690 [==============================] - 2s 554us/step - loss: 1.5772e-04 - val_loss: 4.3140e-04\n",
      "Epoch 735/2000\n",
      "3690/3690 [==============================] - 2s 554us/step - loss: 1.5591e-04 - val_loss: 1.1418e-04\n",
      "Epoch 736/2000\n",
      "3690/3690 [==============================] - 2s 552us/step - loss: 1.2363e-04 - val_loss: 7.4134e-05\n",
      "Epoch 737/2000\n",
      "3690/3690 [==============================] - 2s 554us/step - loss: 9.7390e-05 - val_loss: 1.2708e-04\n",
      "Epoch 738/2000\n",
      "3690/3690 [==============================] - 2s 553us/step - loss: 9.9605e-05 - val_loss: 6.9896e-05\n",
      "Epoch 739/2000\n",
      "3690/3690 [==============================] - 2s 551us/step - loss: 8.7507e-05 - val_loss: 8.7529e-05\n",
      "Epoch 740/2000\n",
      "3690/3690 [==============================] - 2s 553us/step - loss: 9.7151e-05 - val_loss: 6.9678e-05\n",
      "Epoch 741/2000\n",
      "3690/3690 [==============================] - 2s 556us/step - loss: 8.8307e-05 - val_loss: 1.3105e-04\n",
      "Epoch 742/2000\n",
      "3690/3690 [==============================] - 2s 556us/step - loss: 1.1337e-04 - val_loss: 1.3008e-04\n",
      "Epoch 743/2000\n",
      "3690/3690 [==============================] - 2s 560us/step - loss: 1.1492e-04 - val_loss: 8.3149e-05\n",
      "Epoch 744/2000\n",
      "3690/3690 [==============================] - 2s 559us/step - loss: 1.1150e-04 - val_loss: 1.0475e-04\n",
      "Epoch 745/2000\n",
      "3690/3690 [==============================] - 2s 556us/step - loss: 1.0292e-04 - val_loss: 7.2683e-05\n",
      "Epoch 746/2000\n",
      "3690/3690 [==============================] - 2s 553us/step - loss: 8.1377e-05 - val_loss: 6.9793e-05\n",
      "Epoch 747/2000\n",
      "3690/3690 [==============================] - 2s 553us/step - loss: 9.2836e-05 - val_loss: 1.0841e-04\n",
      "Epoch 748/2000\n",
      "3690/3690 [==============================] - 2s 555us/step - loss: 9.6610e-05 - val_loss: 2.7046e-04\n",
      "Epoch 749/2000\n",
      "3690/3690 [==============================] - 2s 552us/step - loss: 1.4182e-04 - val_loss: 7.0747e-05\n",
      "Epoch 750/2000\n",
      "3690/3690 [==============================] - 2s 554us/step - loss: 8.8016e-05 - val_loss: 7.8303e-05\n",
      "Epoch 751/2000\n",
      "3690/3690 [==============================] - 2s 552us/step - loss: 8.5815e-05 - val_loss: 7.1634e-05\n",
      "Epoch 752/2000\n",
      "3690/3690 [==============================] - 2s 553us/step - loss: 8.3207e-05 - val_loss: 9.0835e-05\n",
      "Epoch 753/2000\n",
      "3690/3690 [==============================] - 2s 553us/step - loss: 8.0666e-05 - val_loss: 7.1665e-05\n",
      "Epoch 754/2000\n",
      "3690/3690 [==============================] - 2s 552us/step - loss: 8.1567e-05 - val_loss: 8.0891e-05\n",
      "Epoch 755/2000\n",
      "3690/3690 [==============================] - 2s 549us/step - loss: 8.9131e-05 - val_loss: 7.7229e-05\n",
      "Epoch 756/2000\n",
      "3690/3690 [==============================] - 2s 555us/step - loss: 9.4112e-05 - val_loss: 7.3764e-05\n",
      "Epoch 757/2000\n",
      "3690/3690 [==============================] - 2s 553us/step - loss: 7.8390e-05 - val_loss: 9.9040e-05\n",
      "Epoch 758/2000\n",
      "3690/3690 [==============================] - 2s 554us/step - loss: 8.1179e-05 - val_loss: 7.4837e-05\n",
      "Epoch 759/2000\n",
      "3690/3690 [==============================] - 2s 551us/step - loss: 9.7637e-05 - val_loss: 1.2605e-04\n",
      "Epoch 760/2000\n",
      "3690/3690 [==============================] - 2s 555us/step - loss: 9.6166e-05 - val_loss: 1.5935e-04\n",
      "Epoch 761/2000\n",
      "3690/3690 [==============================] - 2s 554us/step - loss: 8.3808e-05 - val_loss: 7.7724e-05\n",
      "Epoch 762/2000\n",
      "3690/3690 [==============================] - 2s 554us/step - loss: 8.4645e-05 - val_loss: 7.8131e-05\n",
      "Epoch 763/2000\n",
      "3690/3690 [==============================] - 2s 552us/step - loss: 9.3379e-05 - val_loss: 7.2389e-05\n",
      "Epoch 764/2000\n",
      "3690/3690 [==============================] - 2s 552us/step - loss: 8.1823e-05 - val_loss: 1.0899e-04\n",
      "Epoch 765/2000\n",
      "3690/3690 [==============================] - 2s 555us/step - loss: 7.9860e-05 - val_loss: 1.0280e-04\n",
      "Epoch 766/2000\n",
      "3690/3690 [==============================] - 2s 553us/step - loss: 1.0204e-04 - val_loss: 1.2913e-04\n",
      "Epoch 767/2000\n",
      "3690/3690 [==============================] - 2s 553us/step - loss: 1.0648e-04 - val_loss: 9.1094e-05\n",
      "Epoch 768/2000\n",
      "3690/3690 [==============================] - 2s 551us/step - loss: 8.6529e-05 - val_loss: 7.3368e-05\n",
      "Epoch 769/2000\n",
      "3690/3690 [==============================] - 2s 551us/step - loss: 8.8571e-05 - val_loss: 1.3446e-04\n",
      "Epoch 770/2000\n",
      "3690/3690 [==============================] - 2s 553us/step - loss: 1.0311e-04 - val_loss: 7.8153e-05\n",
      "Epoch 771/2000\n",
      "3690/3690 [==============================] - 2s 552us/step - loss: 8.4407e-05 - val_loss: 7.5714e-05\n",
      "Epoch 772/2000\n",
      "3690/3690 [==============================] - 2s 556us/step - loss: 7.9227e-05 - val_loss: 1.0381e-04\n",
      "Epoch 773/2000\n",
      "3690/3690 [==============================] - 2s 551us/step - loss: 7.7523e-05 - val_loss: 8.2010e-05\n",
      "Epoch 774/2000\n",
      "3690/3690 [==============================] - 2s 555us/step - loss: 7.6296e-05 - val_loss: 1.0561e-04\n",
      "Epoch 775/2000\n",
      "3690/3690 [==============================] - 2s 552us/step - loss: 9.1012e-05 - val_loss: 2.0630e-04\n",
      "Epoch 776/2000\n",
      "3690/3690 [==============================] - 2s 551us/step - loss: 1.1860e-04 - val_loss: 2.5856e-04\n",
      "Epoch 777/2000\n",
      "3690/3690 [==============================] - 2s 551us/step - loss: 1.3935e-04 - val_loss: 7.8933e-05\n",
      "Epoch 778/2000\n",
      "3690/3690 [==============================] - 2s 553us/step - loss: 1.2955e-04 - val_loss: 8.4264e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 779/2000\n",
      "3690/3690 [==============================] - 2s 551us/step - loss: 9.1533e-05 - val_loss: 7.4566e-05\n",
      "Epoch 780/2000\n",
      "3690/3690 [==============================] - 2s 552us/step - loss: 8.2454e-05 - val_loss: 1.4952e-04\n",
      "Epoch 781/2000\n",
      "3690/3690 [==============================] - 2s 551us/step - loss: 8.4146e-05 - val_loss: 1.1868e-04\n",
      "Epoch 782/2000\n",
      "3690/3690 [==============================] - 2s 552us/step - loss: 9.7172e-05 - val_loss: 2.7896e-04\n",
      "Epoch 783/2000\n",
      "3690/3690 [==============================] - 2s 553us/step - loss: 1.0378e-04 - val_loss: 8.6418e-05\n",
      "Epoch 784/2000\n",
      "3690/3690 [==============================] - 2s 551us/step - loss: 8.5998e-05 - val_loss: 1.4281e-04\n",
      "Epoch 785/2000\n",
      "3690/3690 [==============================] - 2s 553us/step - loss: 9.0183e-05 - val_loss: 1.1248e-04\n",
      "Epoch 786/2000\n",
      "3690/3690 [==============================] - 2s 552us/step - loss: 8.1398e-05 - val_loss: 2.5015e-04\n",
      "Epoch 787/2000\n",
      "3690/3690 [==============================] - 2s 554us/step - loss: 1.0713e-04 - val_loss: 8.3866e-05\n",
      "Epoch 788/2000\n",
      "3690/3690 [==============================] - 2s 555us/step - loss: 1.0054e-04 - val_loss: 1.0846e-04\n",
      "Epoch 789/2000\n",
      "3690/3690 [==============================] - 2s 552us/step - loss: 8.9872e-05 - val_loss: 1.4665e-04\n",
      "Epoch 790/2000\n",
      "3690/3690 [==============================] - 2s 553us/step - loss: 9.3205e-05 - val_loss: 7.5102e-05\n",
      "Epoch 791/2000\n",
      "3690/3690 [==============================] - 2s 555us/step - loss: 8.7038e-05 - val_loss: 7.3002e-05\n",
      "Epoch 792/2000\n",
      "3690/3690 [==============================] - 2s 552us/step - loss: 8.5169e-05 - val_loss: 8.2460e-05\n",
      "Epoch 793/2000\n",
      "3690/3690 [==============================] - 2s 555us/step - loss: 9.1943e-05 - val_loss: 8.6075e-05\n",
      "Epoch 794/2000\n",
      "3690/3690 [==============================] - 2s 552us/step - loss: 1.0548e-04 - val_loss: 1.1374e-04\n",
      "Epoch 795/2000\n",
      "3690/3690 [==============================] - 2s 559us/step - loss: 1.0426e-04 - val_loss: 1.9251e-04\n",
      "Epoch 796/2000\n",
      "3690/3690 [==============================] - 2s 554us/step - loss: 1.2787e-04 - val_loss: 1.0959e-04\n",
      "Epoch 797/2000\n",
      "3690/3690 [==============================] - 2s 552us/step - loss: 1.3504e-04 - val_loss: 1.0260e-04\n",
      "Epoch 798/2000\n",
      "3690/3690 [==============================] - 2s 554us/step - loss: 1.4418e-04 - val_loss: 1.1948e-04\n",
      "Epoch 799/2000\n",
      "3690/3690 [==============================] - 2s 554us/step - loss: 9.2127e-05 - val_loss: 7.4974e-05\n",
      "Epoch 800/2000\n",
      "3690/3690 [==============================] - 2s 552us/step - loss: 9.7283e-05 - val_loss: 7.3590e-05\n",
      "Epoch 801/2000\n",
      "3690/3690 [==============================] - 2s 561us/step - loss: 9.0784e-05 - val_loss: 1.3138e-04\n",
      "Epoch 802/2000\n",
      "3690/3690 [==============================] - 2s 553us/step - loss: 1.3643e-04 - val_loss: 4.4374e-04\n",
      "Epoch 803/2000\n",
      "3690/3690 [==============================] - 2s 557us/step - loss: 1.6019e-04 - val_loss: 2.9210e-04\n",
      "Epoch 804/2000\n",
      "3690/3690 [==============================] - 2s 551us/step - loss: 1.1519e-04 - val_loss: 9.1422e-05\n",
      "Epoch 805/2000\n",
      "3690/3690 [==============================] - 2s 555us/step - loss: 9.8055e-05 - val_loss: 7.8617e-05\n",
      "Epoch 806/2000\n",
      "3690/3690 [==============================] - 2s 554us/step - loss: 9.7257e-05 - val_loss: 8.3910e-05\n",
      "Epoch 807/2000\n",
      "3690/3690 [==============================] - 2s 551us/step - loss: 1.1326e-04 - val_loss: 7.1763e-05\n",
      "Epoch 808/2000\n",
      "3690/3690 [==============================] - 2s 550us/step - loss: 1.1372e-04 - val_loss: 1.7466e-04\n",
      "Epoch 809/2000\n",
      "3690/3690 [==============================] - 2s 554us/step - loss: 1.3725e-04 - val_loss: 7.2821e-05\n",
      "Epoch 810/2000\n",
      "3690/3690 [==============================] - 2s 553us/step - loss: 1.2312e-04 - val_loss: 9.1023e-05\n",
      "Epoch 811/2000\n",
      "3690/3690 [==============================] - 2s 555us/step - loss: 8.9695e-05 - val_loss: 7.2864e-05\n",
      "Epoch 812/2000\n",
      "3690/3690 [==============================] - 2s 552us/step - loss: 8.4001e-05 - val_loss: 1.1501e-04\n",
      "Epoch 813/2000\n",
      "3690/3690 [==============================] - 2s 552us/step - loss: 9.6467e-05 - val_loss: 7.7868e-05\n",
      "Epoch 814/2000\n",
      "3690/3690 [==============================] - 2s 553us/step - loss: 9.8484e-05 - val_loss: 7.4768e-05\n",
      "Epoch 815/2000\n",
      "3690/3690 [==============================] - 2s 553us/step - loss: 8.4504e-05 - val_loss: 7.3080e-05\n",
      "Epoch 816/2000\n",
      "3690/3690 [==============================] - 2s 554us/step - loss: 9.1703e-05 - val_loss: 1.5097e-04\n",
      "Epoch 817/2000\n",
      "3690/3690 [==============================] - 2s 552us/step - loss: 1.0859e-04 - val_loss: 2.1467e-04\n",
      "Epoch 818/2000\n",
      "3690/3690 [==============================] - 2s 551us/step - loss: 8.3769e-05 - val_loss: 1.0847e-04\n",
      "Epoch 819/2000\n",
      "3690/3690 [==============================] - 2s 557us/step - loss: 7.4013e-05 - val_loss: 1.5267e-04\n",
      "Epoch 820/2000\n",
      "3690/3690 [==============================] - 2s 554us/step - loss: 8.5878e-05 - val_loss: 1.8577e-04\n",
      "Epoch 821/2000\n",
      "3690/3690 [==============================] - 2s 552us/step - loss: 9.1504e-05 - val_loss: 1.0634e-04\n",
      "Epoch 822/2000\n",
      "3690/3690 [==============================] - 2s 551us/step - loss: 1.1377e-04 - val_loss: 2.9831e-04\n",
      "Epoch 823/2000\n",
      "3690/3690 [==============================] - 2s 552us/step - loss: 9.1630e-05 - val_loss: 9.4534e-05\n",
      "Epoch 824/2000\n",
      "3690/3690 [==============================] - 2s 553us/step - loss: 8.8866e-05 - val_loss: 8.6744e-05\n",
      "Epoch 825/2000\n",
      "3690/3690 [==============================] - 2s 553us/step - loss: 7.8074e-05 - val_loss: 7.5610e-05\n",
      "Epoch 826/2000\n",
      "3690/3690 [==============================] - 2s 556us/step - loss: 7.9649e-05 - val_loss: 1.8086e-04\n",
      "Epoch 827/2000\n",
      "3690/3690 [==============================] - 2s 558us/step - loss: 9.4754e-05 - val_loss: 1.6060e-04\n",
      "Epoch 828/2000\n",
      "3690/3690 [==============================] - 2s 550us/step - loss: 9.5960e-05 - val_loss: 7.4012e-05\n",
      "Epoch 829/2000\n",
      "3690/3690 [==============================] - 2s 559us/step - loss: 8.5646e-05 - val_loss: 1.0075e-04\n",
      "Epoch 830/2000\n",
      "3690/3690 [==============================] - 2s 555us/step - loss: 7.8331e-05 - val_loss: 1.0632e-04\n",
      "Epoch 831/2000\n",
      "3690/3690 [==============================] - 2s 554us/step - loss: 8.7369e-05 - val_loss: 2.3135e-04\n",
      "Epoch 832/2000\n",
      "3690/3690 [==============================] - 2s 552us/step - loss: 9.9252e-05 - val_loss: 1.7194e-04\n",
      "Epoch 833/2000\n",
      "3690/3690 [==============================] - 2s 554us/step - loss: 9.3787e-05 - val_loss: 8.6133e-05\n",
      "Epoch 834/2000\n",
      "3690/3690 [==============================] - 2s 551us/step - loss: 1.0773e-04 - val_loss: 7.4625e-05\n",
      "Epoch 835/2000\n",
      "3690/3690 [==============================] - 2s 555us/step - loss: 1.0059e-04 - val_loss: 7.6959e-05\n",
      "Epoch 836/2000\n",
      "3690/3690 [==============================] - 2s 553us/step - loss: 1.1000e-04 - val_loss: 2.7931e-04\n",
      "Epoch 837/2000\n",
      "3690/3690 [==============================] - 2s 554us/step - loss: 1.5701e-04 - val_loss: 1.9208e-04\n",
      "Epoch 838/2000\n",
      "3690/3690 [==============================] - 2s 555us/step - loss: 1.0637e-04 - val_loss: 9.3257e-05\n",
      "Epoch 839/2000\n",
      "3690/3690 [==============================] - 2s 555us/step - loss: 1.1427e-04 - val_loss: 1.1186e-04\n",
      "Epoch 840/2000\n",
      "3690/3690 [==============================] - 2s 550us/step - loss: 1.1051e-04 - val_loss: 1.6062e-04\n",
      "Epoch 841/2000\n",
      "3690/3690 [==============================] - 2s 554us/step - loss: 1.1986e-04 - val_loss: 1.2490e-04\n",
      "Epoch 842/2000\n",
      "3690/3690 [==============================] - 2s 557us/step - loss: 1.1680e-04 - val_loss: 8.4527e-05\n",
      "Epoch 843/2000\n",
      "3690/3690 [==============================] - 2s 553us/step - loss: 9.1200e-05 - val_loss: 7.2013e-05\n",
      "Epoch 844/2000\n",
      "3690/3690 [==============================] - 2s 554us/step - loss: 8.6031e-05 - val_loss: 1.6216e-04\n",
      "Epoch 845/2000\n",
      "3690/3690 [==============================] - 2s 552us/step - loss: 8.5075e-05 - val_loss: 9.5160e-05\n",
      "Epoch 846/2000\n",
      "3690/3690 [==============================] - 2s 553us/step - loss: 8.8495e-05 - val_loss: 7.4577e-05\n",
      "Epoch 847/2000\n",
      "3690/3690 [==============================] - 2s 555us/step - loss: 8.1629e-05 - val_loss: 7.9861e-05\n",
      "Epoch 848/2000\n",
      "3690/3690 [==============================] - 2s 553us/step - loss: 7.7864e-05 - val_loss: 1.7450e-04\n",
      "Epoch 849/2000\n",
      "3690/3690 [==============================] - 2s 554us/step - loss: 8.4374e-05 - val_loss: 1.4567e-04\n",
      "Epoch 850/2000\n",
      "3690/3690 [==============================] - 2s 558us/step - loss: 8.5597e-05 - val_loss: 9.6917e-05\n",
      "Epoch 851/2000\n",
      "3690/3690 [==============================] - 2s 553us/step - loss: 1.1642e-04 - val_loss: 7.5808e-05\n",
      "Epoch 852/2000\n",
      "3690/3690 [==============================] - 2s 558us/step - loss: 8.0020e-05 - val_loss: 1.5406e-04\n",
      "Epoch 853/2000\n",
      "3690/3690 [==============================] - 2s 552us/step - loss: 8.7890e-05 - val_loss: 1.4365e-04\n",
      "Epoch 854/2000\n",
      "3690/3690 [==============================] - 2s 552us/step - loss: 8.2163e-05 - val_loss: 2.8249e-04\n",
      "Epoch 855/2000\n",
      "3690/3690 [==============================] - 2s 553us/step - loss: 1.2296e-04 - val_loss: 8.0156e-05\n",
      "Epoch 856/2000\n",
      "3690/3690 [==============================] - 2s 551us/step - loss: 1.2276e-04 - val_loss: 1.2771e-04\n",
      "Epoch 857/2000\n",
      "3690/3690 [==============================] - 2s 551us/step - loss: 1.0625e-04 - val_loss: 7.0953e-05\n",
      "Epoch 858/2000\n",
      "3690/3690 [==============================] - 2s 559us/step - loss: 7.9410e-05 - val_loss: 9.9677e-05\n",
      "Epoch 859/2000\n",
      "3690/3690 [==============================] - 2s 554us/step - loss: 7.8877e-05 - val_loss: 1.4770e-04\n",
      "Epoch 860/2000\n",
      "3690/3690 [==============================] - 2s 551us/step - loss: 9.7884e-05 - val_loss: 1.4205e-04\n",
      "Epoch 861/2000\n",
      "3690/3690 [==============================] - 2s 553us/step - loss: 8.3751e-05 - val_loss: 7.8006e-05\n",
      "Epoch 862/2000\n",
      "3690/3690 [==============================] - 2s 554us/step - loss: 9.6464e-05 - val_loss: 8.6514e-05\n",
      "Epoch 863/2000\n",
      "3690/3690 [==============================] - 2s 551us/step - loss: 9.1929e-05 - val_loss: 1.0915e-04\n",
      "Epoch 864/2000\n",
      "3690/3690 [==============================] - 2s 554us/step - loss: 1.0801e-04 - val_loss: 1.7451e-04\n",
      "Epoch 865/2000\n",
      "3690/3690 [==============================] - 2s 552us/step - loss: 9.5279e-05 - val_loss: 1.4906e-04\n",
      "Epoch 866/2000\n",
      "3690/3690 [==============================] - 2s 553us/step - loss: 7.4293e-05 - val_loss: 7.6476e-05\n",
      "Epoch 867/2000\n",
      "3690/3690 [==============================] - 2s 554us/step - loss: 7.7832e-05 - val_loss: 9.5785e-05\n",
      "Epoch 868/2000\n",
      "3690/3690 [==============================] - 2s 555us/step - loss: 8.2594e-05 - val_loss: 7.7079e-05\n",
      "Epoch 869/2000\n",
      "3690/3690 [==============================] - 2s 554us/step - loss: 7.5430e-05 - val_loss: 7.9906e-05\n",
      "Epoch 870/2000\n",
      "3690/3690 [==============================] - 2s 554us/step - loss: 7.5590e-05 - val_loss: 8.4530e-05\n",
      "Epoch 871/2000\n",
      "3690/3690 [==============================] - 2s 556us/step - loss: 7.6355e-05 - val_loss: 1.0996e-04\n",
      "Epoch 872/2000\n",
      "3690/3690 [==============================] - 2s 556us/step - loss: 8.2052e-05 - val_loss: 1.5060e-04\n",
      "Epoch 873/2000\n",
      "3690/3690 [==============================] - 2s 557us/step - loss: 8.3702e-05 - val_loss: 1.0901e-04\n",
      "Epoch 874/2000\n",
      "3690/3690 [==============================] - 2s 555us/step - loss: 9.1617e-05 - val_loss: 7.4706e-05\n",
      "Epoch 875/2000\n",
      "3690/3690 [==============================] - 2s 555us/step - loss: 8.7179e-05 - val_loss: 2.3320e-04\n",
      "Epoch 876/2000\n",
      "3690/3690 [==============================] - 2s 553us/step - loss: 1.1571e-04 - val_loss: 1.5394e-04\n",
      "Epoch 877/2000\n",
      "3690/3690 [==============================] - 2s 554us/step - loss: 1.0080e-04 - val_loss: 2.5614e-04\n",
      "Epoch 878/2000\n",
      "3690/3690 [==============================] - 2s 552us/step - loss: 9.2424e-05 - val_loss: 1.6256e-04\n",
      "Epoch 879/2000\n",
      "3690/3690 [==============================] - 2s 553us/step - loss: 8.1481e-05 - val_loss: 7.9597e-05\n",
      "Epoch 880/2000\n",
      "3690/3690 [==============================] - 2s 551us/step - loss: 8.7423e-05 - val_loss: 7.2358e-05\n",
      "Epoch 881/2000\n",
      "3690/3690 [==============================] - 2s 554us/step - loss: 8.4146e-05 - val_loss: 1.1347e-04\n",
      "Epoch 882/2000\n",
      "3690/3690 [==============================] - 2s 552us/step - loss: 8.6003e-05 - val_loss: 1.2318e-04\n",
      "Epoch 883/2000\n",
      "3690/3690 [==============================] - 2s 550us/step - loss: 1.0204e-04 - val_loss: 1.0734e-04\n",
      "Epoch 884/2000\n",
      "3690/3690 [==============================] - 2s 556us/step - loss: 9.7901e-05 - val_loss: 7.5589e-05\n",
      "Epoch 885/2000\n",
      "3690/3690 [==============================] - 2s 551us/step - loss: 9.8792e-05 - val_loss: 8.1062e-05\n",
      "Epoch 886/2000\n",
      "3690/3690 [==============================] - 2s 553us/step - loss: 8.9503e-05 - val_loss: 7.2725e-05\n",
      "Epoch 887/2000\n",
      "3690/3690 [==============================] - 2s 553us/step - loss: 7.9887e-05 - val_loss: 8.0575e-05\n",
      "Epoch 888/2000\n",
      "3690/3690 [==============================] - 2s 553us/step - loss: 7.3608e-05 - val_loss: 1.1434e-04\n",
      "Epoch 889/2000\n",
      "3690/3690 [==============================] - 2s 557us/step - loss: 7.7076e-05 - val_loss: 1.2158e-04\n",
      "Epoch 890/2000\n",
      "3690/3690 [==============================] - 2s 554us/step - loss: 8.9170e-05 - val_loss: 1.6924e-04\n",
      "Epoch 891/2000\n",
      "3690/3690 [==============================] - 2s 555us/step - loss: 1.1620e-04 - val_loss: 8.0027e-05\n",
      "Epoch 892/2000\n",
      "3690/3690 [==============================] - 2s 553us/step - loss: 8.7424e-05 - val_loss: 1.5510e-04\n",
      "Epoch 893/2000\n",
      "3690/3690 [==============================] - 2s 552us/step - loss: 9.5070e-05 - val_loss: 1.4222e-04\n",
      "Epoch 894/2000\n",
      "3690/3690 [==============================] - 2s 553us/step - loss: 8.9841e-05 - val_loss: 7.7163e-05\n",
      "Epoch 895/2000\n",
      "3690/3690 [==============================] - 2s 552us/step - loss: 7.6744e-05 - val_loss: 1.1361e-04\n",
      "Epoch 896/2000\n",
      "3690/3690 [==============================] - 2s 553us/step - loss: 9.0188e-05 - val_loss: 7.6505e-05\n",
      "Epoch 897/2000\n",
      "3690/3690 [==============================] - 2s 553us/step - loss: 8.3621e-05 - val_loss: 9.9461e-05\n",
      "Epoch 898/2000\n",
      "3690/3690 [==============================] - 2s 553us/step - loss: 7.7013e-05 - val_loss: 1.0582e-04\n",
      "Epoch 899/2000\n",
      "3690/3690 [==============================] - 2s 553us/step - loss: 1.2810e-04 - val_loss: 8.2536e-05\n",
      "Epoch 900/2000\n",
      "3690/3690 [==============================] - 2s 553us/step - loss: 9.8900e-05 - val_loss: 1.9352e-04\n",
      "Epoch 901/2000\n",
      "3690/3690 [==============================] - 2s 554us/step - loss: 1.0014e-04 - val_loss: 2.1309e-04\n",
      "Epoch 902/2000\n",
      "3690/3690 [==============================] - 2s 550us/step - loss: 9.4536e-05 - val_loss: 1.3499e-04\n",
      "Epoch 903/2000\n",
      "3690/3690 [==============================] - 2s 551us/step - loss: 8.7172e-05 - val_loss: 8.0081e-05\n",
      "Epoch 904/2000\n",
      "3690/3690 [==============================] - 2s 551us/step - loss: 9.0022e-05 - val_loss: 7.7039e-05\n",
      "Epoch 905/2000\n",
      "3690/3690 [==============================] - 2s 552us/step - loss: 7.4466e-05 - val_loss: 7.9422e-05\n",
      "Epoch 906/2000\n",
      "3690/3690 [==============================] - 2s 552us/step - loss: 8.7053e-05 - val_loss: 8.0764e-05\n",
      "Epoch 907/2000\n",
      "3690/3690 [==============================] - 2s 555us/step - loss: 9.1570e-05 - val_loss: 8.1390e-05\n",
      "Epoch 908/2000\n",
      "3690/3690 [==============================] - 2s 553us/step - loss: 8.1755e-05 - val_loss: 9.2164e-05\n",
      "Epoch 909/2000\n",
      "3690/3690 [==============================] - 2s 554us/step - loss: 9.2397e-05 - val_loss: 1.3107e-04\n",
      "Epoch 910/2000\n",
      "3690/3690 [==============================] - 2s 555us/step - loss: 8.1556e-05 - val_loss: 8.1239e-05\n",
      "Epoch 911/2000\n",
      "3690/3690 [==============================] - 2s 556us/step - loss: 9.8083e-05 - val_loss: 2.2384e-04\n",
      "Epoch 912/2000\n",
      "3690/3690 [==============================] - 2s 554us/step - loss: 8.5740e-05 - val_loss: 1.3915e-04\n",
      "Epoch 913/2000\n",
      "3690/3690 [==============================] - 2s 556us/step - loss: 7.4925e-05 - val_loss: 8.2580e-05\n",
      "Epoch 914/2000\n",
      "3690/3690 [==============================] - 2s 555us/step - loss: 8.7184e-05 - val_loss: 1.4093e-04\n",
      "Epoch 915/2000\n",
      "3690/3690 [==============================] - 2s 552us/step - loss: 9.2767e-05 - val_loss: 8.8404e-05\n",
      "Epoch 916/2000\n",
      "3690/3690 [==============================] - 2s 552us/step - loss: 9.2890e-05 - val_loss: 7.7710e-05\n",
      "Epoch 917/2000\n",
      "3690/3690 [==============================] - 2s 551us/step - loss: 1.1374e-04 - val_loss: 3.2588e-04\n",
      "Epoch 918/2000\n",
      "3690/3690 [==============================] - 2s 559us/step - loss: 1.4279e-04 - val_loss: 1.3177e-04\n",
      "Epoch 919/2000\n",
      "3690/3690 [==============================] - 2s 556us/step - loss: 1.2782e-04 - val_loss: 4.4219e-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 920/2000\n",
      "3690/3690 [==============================] - 2s 557us/step - loss: 1.6778e-04 - val_loss: 1.1252e-04\n",
      "Epoch 921/2000\n",
      "3690/3690 [==============================] - 2s 552us/step - loss: 1.4225e-04 - val_loss: 1.6727e-04\n",
      "Epoch 922/2000\n",
      "3690/3690 [==============================] - 2s 553us/step - loss: 1.1224e-04 - val_loss: 7.3270e-05\n",
      "Epoch 923/2000\n",
      "3690/3690 [==============================] - 2s 552us/step - loss: 1.1105e-04 - val_loss: 8.0425e-05\n",
      "Epoch 924/2000\n",
      "3690/3690 [==============================] - 2s 555us/step - loss: 8.4594e-05 - val_loss: 1.7612e-04\n",
      "Epoch 925/2000\n",
      "3690/3690 [==============================] - 2s 553us/step - loss: 8.4176e-05 - val_loss: 7.5059e-05\n",
      "Epoch 926/2000\n",
      "3690/3690 [==============================] - 2s 549us/step - loss: 8.7955e-05 - val_loss: 1.2518e-04\n",
      "Epoch 927/2000\n",
      "3690/3690 [==============================] - 2s 552us/step - loss: 9.5214e-05 - val_loss: 2.3573e-04\n",
      "Epoch 928/2000\n",
      "3690/3690 [==============================] - 2s 556us/step - loss: 9.3664e-05 - val_loss: 1.2578e-04\n",
      "Epoch 929/2000\n",
      "3690/3690 [==============================] - 2s 554us/step - loss: 8.3713e-05 - val_loss: 7.9265e-05\n",
      "Epoch 930/2000\n",
      "3690/3690 [==============================] - 2s 552us/step - loss: 8.7174e-05 - val_loss: 7.8552e-05\n",
      "Epoch 931/2000\n",
      "3690/3690 [==============================] - 2s 554us/step - loss: 7.0909e-05 - val_loss: 1.1708e-04\n",
      "Epoch 932/2000\n",
      "3690/3690 [==============================] - 2s 554us/step - loss: 1.2701e-04 - val_loss: 3.2448e-04\n",
      "Epoch 933/2000\n",
      "3690/3690 [==============================] - 2s 554us/step - loss: 1.6591e-04 - val_loss: 7.3903e-05\n",
      "Epoch 934/2000\n",
      "3690/3690 [==============================] - 2s 552us/step - loss: 8.1397e-05 - val_loss: 1.9531e-04\n",
      "Epoch 935/2000\n",
      "3690/3690 [==============================] - 2s 554us/step - loss: 1.0296e-04 - val_loss: 1.5067e-04\n",
      "Epoch 936/2000\n",
      "3690/3690 [==============================] - 2s 555us/step - loss: 9.4614e-05 - val_loss: 1.3121e-04\n",
      "Epoch 937/2000\n",
      "3690/3690 [==============================] - 2s 552us/step - loss: 9.6118e-05 - val_loss: 7.2033e-05\n",
      "Epoch 938/2000\n",
      "3690/3690 [==============================] - 2s 554us/step - loss: 1.2394e-04 - val_loss: 1.3121e-04\n",
      "Epoch 939/2000\n",
      "3690/3690 [==============================] - 2s 552us/step - loss: 1.0460e-04 - val_loss: 2.6713e-04\n",
      "Epoch 940/2000\n",
      "3690/3690 [==============================] - 2s 555us/step - loss: 1.8039e-04 - val_loss: 8.1664e-05\n",
      "Epoch 941/2000\n",
      "3690/3690 [==============================] - 2s 554us/step - loss: 1.3084e-04 - val_loss: 9.3791e-05\n",
      "Epoch 942/2000\n",
      "3690/3690 [==============================] - 2s 554us/step - loss: 1.2084e-04 - val_loss: 9.2761e-05\n",
      "Epoch 943/2000\n",
      "3690/3690 [==============================] - 2s 553us/step - loss: 1.6495e-04 - val_loss: 7.9477e-05\n",
      "Epoch 944/2000\n",
      "3690/3690 [==============================] - 2s 555us/step - loss: 1.2881e-04 - val_loss: 1.1521e-04\n",
      "Epoch 945/2000\n",
      "3690/3690 [==============================] - 2s 549us/step - loss: 9.9234e-05 - val_loss: 7.9361e-05\n",
      "Epoch 946/2000\n",
      "3690/3690 [==============================] - 2s 556us/step - loss: 7.6658e-05 - val_loss: 1.0996e-04\n",
      "Epoch 947/2000\n",
      "3690/3690 [==============================] - 2s 553us/step - loss: 7.3409e-05 - val_loss: 1.0117e-04\n",
      "Epoch 948/2000\n",
      "3690/3690 [==============================] - 2s 556us/step - loss: 7.9220e-05 - val_loss: 1.8708e-04\n",
      "Epoch 949/2000\n",
      "3690/3690 [==============================] - 2s 552us/step - loss: 9.5700e-05 - val_loss: 1.6495e-04\n",
      "Epoch 950/2000\n",
      "3690/3690 [==============================] - 2s 552us/step - loss: 1.1055e-04 - val_loss: 1.1429e-04\n",
      "Epoch 951/2000\n",
      "3690/3690 [==============================] - 2s 570us/step - loss: 1.0246e-04 - val_loss: 1.2737e-04\n",
      "Epoch 952/2000\n",
      "3690/3690 [==============================] - 2s 552us/step - loss: 1.2218e-04 - val_loss: 2.6013e-04\n",
      "Epoch 953/2000\n",
      "3690/3690 [==============================] - 2s 545us/step - loss: 1.0278e-04 - val_loss: 1.0005e-04\n",
      "Epoch 954/2000\n",
      "3690/3690 [==============================] - 2s 550us/step - loss: 7.9943e-05 - val_loss: 9.5683e-05\n",
      "Epoch 955/2000\n",
      "3690/3690 [==============================] - 2s 547us/step - loss: 9.7173e-05 - val_loss: 1.9529e-04\n",
      "Epoch 956/2000\n",
      "3690/3690 [==============================] - 2s 545us/step - loss: 8.7992e-05 - val_loss: 7.3286e-05\n",
      "Epoch 957/2000\n",
      "3690/3690 [==============================] - 2s 548us/step - loss: 7.2525e-05 - val_loss: 1.3744e-04\n",
      "Epoch 958/2000\n",
      "3690/3690 [==============================] - 2s 548us/step - loss: 8.5146e-05 - val_loss: 1.1014e-04\n",
      "Epoch 959/2000\n",
      "3690/3690 [==============================] - 2s 548us/step - loss: 7.6638e-05 - val_loss: 9.3284e-05\n",
      "Epoch 960/2000\n",
      "3690/3690 [==============================] - 2s 551us/step - loss: 8.7263e-05 - val_loss: 1.2633e-04\n",
      "Epoch 961/2000\n",
      "3690/3690 [==============================] - 2s 549us/step - loss: 8.6159e-05 - val_loss: 1.9001e-04\n",
      "Epoch 962/2000\n",
      "3690/3690 [==============================] - 2s 548us/step - loss: 1.5137e-04 - val_loss: 8.5499e-05\n",
      "Epoch 963/2000\n",
      "3690/3690 [==============================] - 2s 548us/step - loss: 9.0562e-05 - val_loss: 8.1517e-05\n",
      "Epoch 964/2000\n",
      "3690/3690 [==============================] - 2s 550us/step - loss: 7.4763e-05 - val_loss: 1.2605e-04\n",
      "Epoch 965/2000\n",
      "3690/3690 [==============================] - 2s 551us/step - loss: 7.4033e-05 - val_loss: 1.2649e-04\n",
      "Epoch 966/2000\n",
      "3690/3690 [==============================] - 2s 556us/step - loss: 7.1360e-05 - val_loss: 7.8993e-05\n",
      "Epoch 967/2000\n",
      "3690/3690 [==============================] - 2s 551us/step - loss: 8.3968e-05 - val_loss: 1.0013e-04\n",
      "Epoch 968/2000\n",
      "3690/3690 [==============================] - 2s 549us/step - loss: 8.5308e-05 - val_loss: 7.3350e-05\n",
      "Epoch 969/2000\n",
      "3690/3690 [==============================] - 2s 549us/step - loss: 8.3591e-05 - val_loss: 7.8402e-05\n",
      "Epoch 970/2000\n",
      "3690/3690 [==============================] - 2s 547us/step - loss: 1.0278e-04 - val_loss: 1.3649e-04\n",
      "Epoch 971/2000\n",
      "3690/3690 [==============================] - 2s 547us/step - loss: 7.9246e-05 - val_loss: 1.3908e-04\n",
      "Epoch 972/2000\n",
      "3690/3690 [==============================] - 2s 550us/step - loss: 7.6577e-05 - val_loss: 9.3345e-05\n",
      "Epoch 973/2000\n",
      "3690/3690 [==============================] - 2s 548us/step - loss: 7.8774e-05 - val_loss: 9.0612e-05\n",
      "Epoch 974/2000\n",
      "3690/3690 [==============================] - 2s 550us/step - loss: 7.8013e-05 - val_loss: 9.7095e-05\n",
      "Epoch 975/2000\n",
      "3690/3690 [==============================] - 2s 554us/step - loss: 7.7850e-05 - val_loss: 1.0982e-04\n",
      "Epoch 976/2000\n",
      "3690/3690 [==============================] - 2s 550us/step - loss: 7.2361e-05 - val_loss: 4.9127e-04\n",
      "Epoch 977/2000\n",
      "3690/3690 [==============================] - 2s 554us/step - loss: 1.2326e-04 - val_loss: 7.8613e-05\n",
      "Epoch 978/2000\n",
      "3690/3690 [==============================] - 2s 551us/step - loss: 8.5777e-05 - val_loss: 1.0097e-04\n",
      "Epoch 979/2000\n",
      "3690/3690 [==============================] - 2s 549us/step - loss: 7.4859e-05 - val_loss: 7.5598e-05\n",
      "Epoch 980/2000\n",
      "3690/3690 [==============================] - 2s 547us/step - loss: 8.3083e-05 - val_loss: 1.3944e-04\n",
      "Epoch 981/2000\n",
      "3690/3690 [==============================] - 2s 553us/step - loss: 8.0894e-05 - val_loss: 7.8208e-05\n",
      "Epoch 982/2000\n",
      "3690/3690 [==============================] - 2s 549us/step - loss: 8.5403e-05 - val_loss: 7.2358e-05\n",
      "Epoch 983/2000\n",
      "3690/3690 [==============================] - 2s 552us/step - loss: 7.9840e-05 - val_loss: 1.1536e-04\n",
      "Epoch 984/2000\n",
      "3690/3690 [==============================] - 2s 548us/step - loss: 7.3031e-05 - val_loss: 9.0673e-05\n",
      "Epoch 985/2000\n",
      "3690/3690 [==============================] - 2s 549us/step - loss: 8.0885e-05 - val_loss: 7.7359e-05\n",
      "Epoch 986/2000\n",
      "3690/3690 [==============================] - 2s 547us/step - loss: 7.8229e-05 - val_loss: 8.3603e-05\n",
      "Epoch 987/2000\n",
      "3690/3690 [==============================] - 2s 548us/step - loss: 8.8215e-05 - val_loss: 1.1265e-04\n",
      "Epoch 988/2000\n",
      "3690/3690 [==============================] - 2s 549us/step - loss: 7.7327e-05 - val_loss: 7.6969e-05\n",
      "Epoch 989/2000\n",
      "3690/3690 [==============================] - 2s 550us/step - loss: 8.1931e-05 - val_loss: 3.2984e-04\n",
      "Epoch 990/2000\n",
      "3690/3690 [==============================] - 2s 554us/step - loss: 1.3473e-04 - val_loss: 8.0095e-05\n",
      "Epoch 991/2000\n",
      "3690/3690 [==============================] - 2s 551us/step - loss: 8.2590e-05 - val_loss: 1.0010e-04\n",
      "Epoch 992/2000\n",
      "3690/3690 [==============================] - 2s 550us/step - loss: 7.2488e-05 - val_loss: 1.1453e-04\n",
      "Epoch 993/2000\n",
      "3690/3690 [==============================] - 2s 545us/step - loss: 8.1769e-05 - val_loss: 7.8078e-05\n",
      "Epoch 994/2000\n",
      "3690/3690 [==============================] - 2s 551us/step - loss: 6.8613e-05 - val_loss: 8.6110e-05\n",
      "Epoch 995/2000\n",
      "3690/3690 [==============================] - 2s 549us/step - loss: 7.0773e-05 - val_loss: 1.3844e-04\n",
      "Epoch 996/2000\n",
      "3690/3690 [==============================] - 2s 551us/step - loss: 7.7642e-05 - val_loss: 9.9055e-05\n",
      "Epoch 997/2000\n",
      "3690/3690 [==============================] - 2s 548us/step - loss: 9.0269e-05 - val_loss: 9.7473e-05\n",
      "Epoch 998/2000\n",
      "3690/3690 [==============================] - 2s 548us/step - loss: 1.0661e-04 - val_loss: 1.3920e-04\n",
      "Epoch 999/2000\n",
      "3690/3690 [==============================] - 2s 549us/step - loss: 9.5014e-05 - val_loss: 2.0017e-04\n",
      "Epoch 1000/2000\n",
      "3690/3690 [==============================] - 2s 549us/step - loss: 8.5245e-05 - val_loss: 8.0721e-05\n",
      "Epoch 1001/2000\n",
      "3690/3690 [==============================] - 2s 551us/step - loss: 7.9145e-05 - val_loss: 1.9706e-04\n",
      "Epoch 1002/2000\n",
      "3690/3690 [==============================] - 2s 549us/step - loss: 1.0259e-04 - val_loss: 7.8063e-05\n",
      "Epoch 1003/2000\n",
      "3690/3690 [==============================] - 2s 548us/step - loss: 9.6597e-05 - val_loss: 8.9601e-05\n",
      "Epoch 1004/2000\n",
      "3690/3690 [==============================] - 2s 548us/step - loss: 9.5634e-05 - val_loss: 3.9581e-04\n",
      "Epoch 1005/2000\n",
      "3690/3690 [==============================] - 2s 548us/step - loss: 1.2299e-04 - val_loss: 9.2122e-05\n",
      "Epoch 1006/2000\n",
      "3690/3690 [==============================] - 2s 550us/step - loss: 9.8772e-05 - val_loss: 9.2600e-05\n",
      "Epoch 1007/2000\n",
      "3690/3690 [==============================] - 2s 552us/step - loss: 7.3912e-05 - val_loss: 1.2106e-04\n",
      "Epoch 1008/2000\n",
      "3690/3690 [==============================] - 2s 551us/step - loss: 9.0502e-05 - val_loss: 8.3861e-05\n",
      "Epoch 1009/2000\n",
      "3690/3690 [==============================] - 2s 549us/step - loss: 8.1094e-05 - val_loss: 1.8243e-04\n",
      "Epoch 1010/2000\n",
      "3690/3690 [==============================] - 2s 543us/step - loss: 8.4061e-05 - val_loss: 8.3536e-05\n",
      "Epoch 1011/2000\n",
      "3690/3690 [==============================] - 2s 546us/step - loss: 7.4048e-05 - val_loss: 7.7953e-05\n",
      "Epoch 1012/2000\n",
      "3690/3690 [==============================] - 2s 549us/step - loss: 7.2238e-05 - val_loss: 1.1674e-04\n",
      "Epoch 1013/2000\n",
      "3690/3690 [==============================] - 2s 550us/step - loss: 7.8059e-05 - val_loss: 8.4347e-05\n",
      "Epoch 1014/2000\n",
      "3690/3690 [==============================] - 2s 548us/step - loss: 8.6331e-05 - val_loss: 1.5979e-04\n",
      "Epoch 1015/2000\n",
      "3690/3690 [==============================] - 2s 552us/step - loss: 8.4079e-05 - val_loss: 8.5793e-05\n",
      "Epoch 1016/2000\n",
      "3690/3690 [==============================] - 2s 550us/step - loss: 7.5806e-05 - val_loss: 8.3383e-05\n",
      "Epoch 1017/2000\n",
      "3690/3690 [==============================] - 2s 549us/step - loss: 9.8611e-05 - val_loss: 1.1870e-04\n",
      "Epoch 1018/2000\n",
      "3690/3690 [==============================] - 2s 548us/step - loss: 8.9641e-05 - val_loss: 1.4663e-04\n",
      "Epoch 1019/2000\n",
      "3690/3690 [==============================] - 2s 548us/step - loss: 1.0855e-04 - val_loss: 9.2634e-05\n",
      "Epoch 1020/2000\n",
      "3690/3690 [==============================] - 2s 545us/step - loss: 9.0417e-05 - val_loss: 1.8201e-04\n",
      "Epoch 1021/2000\n",
      "3690/3690 [==============================] - 2s 549us/step - loss: 8.3259e-05 - val_loss: 9.8026e-05\n",
      "Epoch 1022/2000\n",
      "3690/3690 [==============================] - 2s 552us/step - loss: 7.5472e-05 - val_loss: 1.0928e-04\n",
      "Epoch 1023/2000\n",
      "3690/3690 [==============================] - 2s 554us/step - loss: 7.2780e-05 - val_loss: 8.9017e-05\n",
      "Epoch 1024/2000\n",
      "3690/3690 [==============================] - 2s 549us/step - loss: 7.6947e-05 - val_loss: 1.8705e-04\n",
      "Epoch 1025/2000\n",
      "3690/3690 [==============================] - 2s 552us/step - loss: 8.0485e-05 - val_loss: 7.6398e-05\n",
      "Epoch 1026/2000\n",
      "3690/3690 [==============================] - 2s 549us/step - loss: 7.4438e-05 - val_loss: 8.6732e-05\n",
      "Epoch 1027/2000\n",
      "3690/3690 [==============================] - 2s 549us/step - loss: 8.1965e-05 - val_loss: 7.5212e-05\n",
      "Epoch 1028/2000\n",
      "3690/3690 [==============================] - 2s 550us/step - loss: 7.5113e-05 - val_loss: 1.1024e-04\n",
      "Epoch 1029/2000\n",
      "3690/3690 [==============================] - 2s 550us/step - loss: 8.3458e-05 - val_loss: 8.9482e-05\n",
      "Epoch 1030/2000\n",
      "3690/3690 [==============================] - 2s 551us/step - loss: 1.1086e-04 - val_loss: 8.4980e-05\n",
      "Epoch 1031/2000\n",
      "3690/3690 [==============================] - 2s 551us/step - loss: 1.0691e-04 - val_loss: 2.1815e-04\n",
      "Epoch 1032/2000\n",
      "3690/3690 [==============================] - 2s 547us/step - loss: 1.1806e-04 - val_loss: 9.9931e-05\n",
      "Epoch 1033/2000\n",
      "3690/3690 [==============================] - 2s 550us/step - loss: 9.7767e-05 - val_loss: 1.1539e-04\n",
      "Epoch 1034/2000\n",
      "3690/3690 [==============================] - 2s 551us/step - loss: 9.1742e-05 - val_loss: 8.7170e-05\n",
      "Epoch 1035/2000\n",
      "3690/3690 [==============================] - 2s 548us/step - loss: 8.8524e-05 - val_loss: 9.2245e-05\n",
      "Epoch 1036/2000\n",
      "3690/3690 [==============================] - 2s 551us/step - loss: 9.3896e-05 - val_loss: 1.2704e-04\n",
      "Epoch 1037/2000\n",
      "3690/3690 [==============================] - 2s 550us/step - loss: 8.8811e-05 - val_loss: 8.2910e-05\n",
      "Epoch 1038/2000\n",
      "3690/3690 [==============================] - 2s 553us/step - loss: 8.0609e-05 - val_loss: 8.2002e-05\n",
      "Epoch 1039/2000\n",
      "3690/3690 [==============================] - 2s 549us/step - loss: 7.2928e-05 - val_loss: 1.3378e-04\n",
      "Epoch 1040/2000\n",
      "3690/3690 [==============================] - 2s 548us/step - loss: 8.6999e-05 - val_loss: 1.1700e-04\n",
      "Epoch 1041/2000\n",
      "3690/3690 [==============================] - 2s 549us/step - loss: 8.3033e-05 - val_loss: 7.5592e-05\n",
      "Epoch 1042/2000\n",
      "3690/3690 [==============================] - 2s 548us/step - loss: 7.6723e-05 - val_loss: 7.9831e-05\n",
      "Epoch 1043/2000\n",
      "3690/3690 [==============================] - 2s 548us/step - loss: 8.0251e-05 - val_loss: 1.1697e-04\n",
      "Epoch 1044/2000\n",
      "3690/3690 [==============================] - 2s 547us/step - loss: 7.8858e-05 - val_loss: 8.3281e-05\n",
      "Epoch 1045/2000\n",
      "3690/3690 [==============================] - 2s 548us/step - loss: 7.4982e-05 - val_loss: 1.3709e-04\n",
      "Epoch 1046/2000\n",
      "3690/3690 [==============================] - 2s 553us/step - loss: 7.1278e-05 - val_loss: 7.9136e-05\n",
      "Epoch 1047/2000\n",
      "3690/3690 [==============================] - 2s 547us/step - loss: 8.4865e-05 - val_loss: 1.0593e-04\n",
      "Epoch 1048/2000\n",
      "3690/3690 [==============================] - 2s 550us/step - loss: 7.8221e-05 - val_loss: 7.5350e-05\n",
      "Epoch 1049/2000\n",
      "3690/3690 [==============================] - 2s 546us/step - loss: 7.7444e-05 - val_loss: 1.0899e-04\n",
      "Epoch 1050/2000\n",
      "3690/3690 [==============================] - 2s 549us/step - loss: 8.5874e-05 - val_loss: 8.3375e-05\n",
      "Epoch 1051/2000\n",
      "3690/3690 [==============================] - 2s 548us/step - loss: 7.5569e-05 - val_loss: 1.0397e-04\n",
      "Epoch 1052/2000\n",
      "3690/3690 [==============================] - 2s 547us/step - loss: 9.0954e-05 - val_loss: 7.2885e-05\n",
      "Epoch 1053/2000\n",
      "3690/3690 [==============================] - 2s 551us/step - loss: 7.8216e-05 - val_loss: 8.5201e-05\n",
      "Epoch 1054/2000\n",
      "3690/3690 [==============================] - 2s 551us/step - loss: 7.2578e-05 - val_loss: 7.6788e-05\n",
      "Epoch 1055/2000\n",
      "3690/3690 [==============================] - 2s 549us/step - loss: 7.3510e-05 - val_loss: 8.0142e-05\n",
      "Epoch 1056/2000\n",
      "3690/3690 [==============================] - 2s 546us/step - loss: 7.3987e-05 - val_loss: 7.7786e-05\n",
      "Epoch 1057/2000\n",
      "3690/3690 [==============================] - 2s 549us/step - loss: 7.3482e-05 - val_loss: 2.3568e-04\n",
      "Epoch 1058/2000\n",
      "3690/3690 [==============================] - 2s 547us/step - loss: 8.7148e-05 - val_loss: 1.3571e-04\n",
      "Epoch 1059/2000\n",
      "3690/3690 [==============================] - 2s 546us/step - loss: 7.6530e-05 - val_loss: 1.1275e-04\n",
      "Epoch 1060/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3690/3690 [==============================] - 2s 548us/step - loss: 8.8753e-05 - val_loss: 1.4953e-04\n",
      "Epoch 1061/2000\n",
      "3690/3690 [==============================] - 2s 548us/step - loss: 1.0493e-04 - val_loss: 1.3995e-04\n",
      "Epoch 1062/2000\n",
      "3690/3690 [==============================] - 2s 553us/step - loss: 9.4128e-05 - val_loss: 1.3859e-04\n",
      "Epoch 1063/2000\n",
      "3690/3690 [==============================] - 2s 549us/step - loss: 1.1885e-04 - val_loss: 9.2374e-05\n",
      "Epoch 1064/2000\n",
      "3690/3690 [==============================] - 2s 552us/step - loss: 9.2695e-05 - val_loss: 9.2757e-05\n",
      "Epoch 1065/2000\n",
      "3690/3690 [==============================] - 2s 551us/step - loss: 7.2512e-05 - val_loss: 8.1703e-05\n",
      "Epoch 1066/2000\n",
      "3690/3690 [==============================] - 2s 548us/step - loss: 7.4674e-05 - val_loss: 1.7003e-04\n",
      "Epoch 1067/2000\n",
      "3690/3690 [==============================] - 2s 547us/step - loss: 9.0953e-05 - val_loss: 7.7709e-05\n",
      "Epoch 1068/2000\n",
      "3690/3690 [==============================] - 2s 552us/step - loss: 1.0007e-04 - val_loss: 1.2209e-04\n",
      "Epoch 1069/2000\n",
      "3690/3690 [==============================] - 2s 550us/step - loss: 1.3252e-04 - val_loss: 2.8877e-04\n",
      "Epoch 1070/2000\n",
      "3690/3690 [==============================] - 2s 552us/step - loss: 9.9283e-05 - val_loss: 8.5590e-05\n",
      "Epoch 1071/2000\n",
      "3690/3690 [==============================] - 2s 552us/step - loss: 8.8345e-05 - val_loss: 7.7370e-05\n",
      "Epoch 1072/2000\n",
      "3690/3690 [==============================] - 2s 550us/step - loss: 7.9625e-05 - val_loss: 1.2066e-04\n",
      "Epoch 1073/2000\n",
      "3690/3690 [==============================] - 2s 549us/step - loss: 8.0316e-05 - val_loss: 8.4844e-05\n",
      "Epoch 1074/2000\n",
      "3690/3690 [==============================] - 2s 550us/step - loss: 8.8608e-05 - val_loss: 9.9862e-05\n",
      "Epoch 1075/2000\n",
      "3690/3690 [==============================] - 2s 549us/step - loss: 8.1774e-05 - val_loss: 1.3888e-04\n",
      "Epoch 1076/2000\n",
      "3690/3690 [==============================] - 2s 550us/step - loss: 7.4258e-05 - val_loss: 2.1277e-04\n",
      "Epoch 1077/2000\n",
      "3690/3690 [==============================] - 2s 548us/step - loss: 7.4216e-05 - val_loss: 8.3733e-05\n",
      "Epoch 1078/2000\n",
      "3690/3690 [==============================] - 2s 554us/step - loss: 7.0476e-05 - val_loss: 8.1769e-05\n",
      "Epoch 1079/2000\n",
      "3690/3690 [==============================] - 2s 546us/step - loss: 7.1046e-05 - val_loss: 8.0701e-05\n",
      "Epoch 1080/2000\n",
      "3690/3690 [==============================] - 2s 551us/step - loss: 8.4802e-05 - val_loss: 8.5886e-05\n",
      "Epoch 1081/2000\n",
      "3690/3690 [==============================] - 2s 549us/step - loss: 7.5628e-05 - val_loss: 7.7200e-05\n",
      "Epoch 1082/2000\n",
      "3690/3690 [==============================] - 2s 548us/step - loss: 8.1879e-05 - val_loss: 7.5063e-05\n",
      "Epoch 1083/2000\n",
      "3690/3690 [==============================] - 2s 549us/step - loss: 7.1006e-05 - val_loss: 1.0678e-04\n",
      "Epoch 1084/2000\n",
      "3690/3690 [==============================] - 2s 545us/step - loss: 7.6849e-05 - val_loss: 9.0501e-05\n",
      "Epoch 1085/2000\n",
      "3690/3690 [==============================] - 2s 547us/step - loss: 8.1819e-05 - val_loss: 1.2825e-04\n",
      "Epoch 1086/2000\n",
      "3690/3690 [==============================] - 2s 552us/step - loss: 8.1802e-05 - val_loss: 8.2741e-05\n",
      "Epoch 1087/2000\n",
      "3690/3690 [==============================] - 2s 548us/step - loss: 9.0983e-05 - val_loss: 1.2919e-04\n",
      "Epoch 1088/2000\n",
      "3690/3690 [==============================] - 2s 547us/step - loss: 9.3836e-05 - val_loss: 8.4335e-05\n",
      "Epoch 1089/2000\n",
      "3690/3690 [==============================] - 2s 554us/step - loss: 7.6096e-05 - val_loss: 1.7814e-04\n",
      "Epoch 1090/2000\n",
      "3690/3690 [==============================] - 2s 546us/step - loss: 8.6950e-05 - val_loss: 1.0912e-04\n",
      "Epoch 1091/2000\n",
      "3690/3690 [==============================] - 2s 548us/step - loss: 6.7611e-05 - val_loss: 9.4937e-05\n",
      "Epoch 1092/2000\n",
      "3690/3690 [==============================] - 2s 548us/step - loss: 7.3064e-05 - val_loss: 8.8710e-05\n",
      "Epoch 1093/2000\n",
      "3690/3690 [==============================] - 2s 548us/step - loss: 7.3983e-05 - val_loss: 1.0291e-04\n",
      "Epoch 1094/2000\n",
      "3690/3690 [==============================] - 2s 549us/step - loss: 9.6919e-05 - val_loss: 1.1213e-04\n",
      "Epoch 1095/2000\n",
      "3690/3690 [==============================] - 2s 549us/step - loss: 1.3639e-04 - val_loss: 1.5293e-04\n",
      "Epoch 1096/2000\n",
      "3690/3690 [==============================] - 2s 550us/step - loss: 9.3271e-05 - val_loss: 1.2171e-04\n",
      "Epoch 1097/2000\n",
      "3690/3690 [==============================] - 2s 548us/step - loss: 9.6363e-05 - val_loss: 1.8028e-04\n",
      "Epoch 1098/2000\n",
      "3690/3690 [==============================] - 2s 544us/step - loss: 7.9764e-05 - val_loss: 7.9026e-05\n",
      "Epoch 1099/2000\n",
      "3690/3690 [==============================] - 2s 547us/step - loss: 8.4986e-05 - val_loss: 1.1947e-04\n",
      "Epoch 1100/2000\n",
      "3690/3690 [==============================] - 2s 549us/step - loss: 7.6219e-05 - val_loss: 1.1542e-04\n",
      "Epoch 1101/2000\n",
      "3690/3690 [==============================] - 2s 549us/step - loss: 9.0041e-05 - val_loss: 9.7469e-05\n",
      "Epoch 1102/2000\n",
      "3690/3690 [==============================] - 2s 551us/step - loss: 7.7726e-05 - val_loss: 1.0311e-04\n",
      "Epoch 1103/2000\n",
      "3690/3690 [==============================] - 2s 548us/step - loss: 7.2755e-05 - val_loss: 1.0130e-04\n",
      "Epoch 1104/2000\n",
      "3690/3690 [==============================] - 2s 547us/step - loss: 7.8744e-05 - val_loss: 1.1970e-04\n",
      "Epoch 1105/2000\n",
      "3690/3690 [==============================] - 2s 548us/step - loss: 7.2684e-05 - val_loss: 1.3912e-04\n",
      "Epoch 1106/2000\n",
      "3690/3690 [==============================] - 2s 547us/step - loss: 8.3949e-05 - val_loss: 2.4325e-04\n",
      "Epoch 1107/2000\n",
      "3690/3690 [==============================] - 2s 549us/step - loss: 1.2163e-04 - val_loss: 7.7110e-05\n",
      "Epoch 1108/2000\n",
      "3690/3690 [==============================] - 2s 549us/step - loss: 9.2614e-05 - val_loss: 8.1872e-05\n",
      "Epoch 1109/2000\n",
      "3690/3690 [==============================] - 2s 551us/step - loss: 8.3684e-05 - val_loss: 9.3249e-05\n",
      "Epoch 1110/2000\n",
      "3690/3690 [==============================] - 2s 552us/step - loss: 7.3909e-05 - val_loss: 7.2334e-05\n",
      "Epoch 1111/2000\n",
      "3690/3690 [==============================] - 2s 548us/step - loss: 7.0352e-05 - val_loss: 7.5618e-05\n",
      "Epoch 1112/2000\n",
      "3690/3690 [==============================] - 2s 549us/step - loss: 7.1200e-05 - val_loss: 7.5625e-05\n",
      "Epoch 1113/2000\n",
      "3690/3690 [==============================] - 2s 550us/step - loss: 6.8977e-05 - val_loss: 1.5927e-04\n",
      "Epoch 1114/2000\n",
      "3690/3690 [==============================] - 2s 550us/step - loss: 7.5010e-05 - val_loss: 1.3975e-04\n",
      "Epoch 1115/2000\n",
      "3690/3690 [==============================] - 2s 546us/step - loss: 6.7707e-05 - val_loss: 8.3925e-05\n",
      "Epoch 1116/2000\n",
      "3690/3690 [==============================] - 2s 551us/step - loss: 6.4623e-05 - val_loss: 7.7801e-05\n",
      "Epoch 1117/2000\n",
      "3690/3690 [==============================] - 2s 551us/step - loss: 6.7176e-05 - val_loss: 8.0813e-05\n",
      "Epoch 1118/2000\n",
      "3690/3690 [==============================] - 2s 549us/step - loss: 6.8022e-05 - val_loss: 1.5492e-04\n",
      "Epoch 1119/2000\n",
      "3690/3690 [==============================] - 2s 550us/step - loss: 8.7244e-05 - val_loss: 1.2069e-04\n",
      "Epoch 1120/2000\n",
      "3690/3690 [==============================] - 2s 546us/step - loss: 9.5822e-05 - val_loss: 1.0240e-04\n",
      "Epoch 1121/2000\n",
      "3690/3690 [==============================] - 2s 545us/step - loss: 7.9261e-05 - val_loss: 1.2426e-04\n",
      "Epoch 1122/2000\n",
      "3690/3690 [==============================] - 2s 549us/step - loss: 7.0968e-05 - val_loss: 7.9088e-05\n",
      "Epoch 1123/2000\n",
      "3690/3690 [==============================] - 2s 546us/step - loss: 7.7491e-05 - val_loss: 1.7742e-04\n",
      "Epoch 1124/2000\n",
      "3690/3690 [==============================] - 2s 546us/step - loss: 9.4194e-05 - val_loss: 1.0124e-04\n",
      "Epoch 1125/2000\n",
      "3690/3690 [==============================] - 2s 554us/step - loss: 7.8171e-05 - val_loss: 1.0017e-04\n",
      "Epoch 1126/2000\n",
      "3690/3690 [==============================] - 2s 548us/step - loss: 1.0068e-04 - val_loss: 1.8591e-04\n",
      "Epoch 1127/2000\n",
      "3690/3690 [==============================] - 2s 548us/step - loss: 9.7452e-05 - val_loss: 3.0770e-04\n",
      "Epoch 1128/2000\n",
      "3690/3690 [==============================] - 2s 549us/step - loss: 8.4369e-05 - val_loss: 7.9859e-05\n",
      "Epoch 1129/2000\n",
      "3690/3690 [==============================] - 2s 550us/step - loss: 7.3828e-05 - val_loss: 7.9151e-05\n",
      "Epoch 1130/2000\n",
      "3690/3690 [==============================] - 2s 549us/step - loss: 8.0922e-05 - val_loss: 2.2136e-04\n",
      "Epoch 1131/2000\n",
      "3690/3690 [==============================] - 2s 550us/step - loss: 9.1355e-05 - val_loss: 2.3350e-04\n",
      "Epoch 1132/2000\n",
      "3690/3690 [==============================] - 2s 547us/step - loss: 1.3968e-04 - val_loss: 9.5450e-05\n",
      "Epoch 1133/2000\n",
      "3690/3690 [==============================] - 2s 552us/step - loss: 9.3871e-05 - val_loss: 9.1858e-05\n",
      "Epoch 1134/2000\n",
      "3690/3690 [==============================] - 2s 551us/step - loss: 8.0509e-05 - val_loss: 7.7801e-05\n",
      "Epoch 1135/2000\n",
      "3690/3690 [==============================] - 2s 549us/step - loss: 7.6334e-05 - val_loss: 2.0835e-04\n",
      "Epoch 1136/2000\n",
      "3690/3690 [==============================] - 2s 548us/step - loss: 7.9276e-05 - val_loss: 1.0697e-04\n",
      "Epoch 1137/2000\n",
      "3690/3690 [==============================] - 2s 550us/step - loss: 7.8642e-05 - val_loss: 1.0989e-04\n",
      "Epoch 1138/2000\n",
      "3690/3690 [==============================] - 2s 544us/step - loss: 7.8944e-05 - val_loss: 9.2302e-05\n",
      "Epoch 1139/2000\n",
      "3690/3690 [==============================] - 2s 548us/step - loss: 7.6462e-05 - val_loss: 7.9390e-05\n",
      "Epoch 1140/2000\n",
      "3690/3690 [==============================] - 2s 548us/step - loss: 8.0972e-05 - val_loss: 2.1132e-04\n",
      "Epoch 1141/2000\n",
      "3690/3690 [==============================] - 2s 552us/step - loss: 8.4530e-05 - val_loss: 8.5513e-05\n",
      "Epoch 1142/2000\n",
      "3690/3690 [==============================] - 2s 549us/step - loss: 7.6779e-05 - val_loss: 8.7670e-05\n",
      "Epoch 1143/2000\n",
      "3690/3690 [==============================] - 2s 550us/step - loss: 7.5454e-05 - val_loss: 1.3061e-04\n",
      "Epoch 1144/2000\n",
      "3690/3690 [==============================] - 2s 557us/step - loss: 7.1750e-05 - val_loss: 9.1449e-05\n",
      "Epoch 1145/2000\n",
      "3690/3690 [==============================] - 2s 548us/step - loss: 6.9543e-05 - val_loss: 1.1970e-04\n",
      "Epoch 1146/2000\n",
      "3690/3690 [==============================] - 2s 549us/step - loss: 8.0554e-05 - val_loss: 7.8104e-05\n",
      "Epoch 1147/2000\n",
      "3690/3690 [==============================] - 2s 548us/step - loss: 9.3806e-05 - val_loss: 1.3341e-04\n",
      "Epoch 1148/2000\n",
      "3690/3690 [==============================] - 2s 551us/step - loss: 7.7029e-05 - val_loss: 1.2605e-04\n",
      "Epoch 1149/2000\n",
      "3690/3690 [==============================] - 2s 553us/step - loss: 6.9350e-05 - val_loss: 9.5573e-05\n",
      "Epoch 1150/2000\n",
      "3690/3690 [==============================] - 2s 546us/step - loss: 7.5728e-05 - val_loss: 8.2637e-05\n",
      "Epoch 1151/2000\n",
      "3690/3690 [==============================] - 2s 548us/step - loss: 7.9172e-05 - val_loss: 1.3452e-04\n",
      "Epoch 1152/2000\n",
      "3690/3690 [==============================] - 2s 550us/step - loss: 8.0136e-05 - val_loss: 1.9496e-04\n",
      "Epoch 1153/2000\n",
      "3690/3690 [==============================] - 2s 549us/step - loss: 7.9193e-05 - val_loss: 1.2021e-04\n",
      "Epoch 1154/2000\n",
      "3690/3690 [==============================] - 2s 551us/step - loss: 8.7075e-05 - val_loss: 8.8126e-05\n",
      "Epoch 1155/2000\n",
      "3690/3690 [==============================] - 2s 551us/step - loss: 8.4129e-05 - val_loss: 1.0758e-04\n",
      "Epoch 1156/2000\n",
      "3690/3690 [==============================] - 2s 551us/step - loss: 8.5451e-05 - val_loss: 9.6436e-05\n",
      "Epoch 1157/2000\n",
      "3690/3690 [==============================] - 2s 552us/step - loss: 6.9044e-05 - val_loss: 1.0271e-04\n",
      "Epoch 1158/2000\n",
      "3690/3690 [==============================] - 2s 549us/step - loss: 6.9765e-05 - val_loss: 2.2558e-04\n",
      "Epoch 1159/2000\n",
      "3690/3690 [==============================] - 2s 550us/step - loss: 7.3417e-05 - val_loss: 9.2342e-05\n",
      "Epoch 1160/2000\n",
      "3690/3690 [==============================] - 2s 551us/step - loss: 8.3440e-05 - val_loss: 8.9077e-05\n",
      "Epoch 1161/2000\n",
      "3690/3690 [==============================] - 2s 553us/step - loss: 7.9912e-05 - val_loss: 1.5737e-04\n",
      "Epoch 1162/2000\n",
      "3690/3690 [==============================] - 2s 548us/step - loss: 9.7414e-05 - val_loss: 1.6364e-04\n",
      "Epoch 1163/2000\n",
      "3690/3690 [==============================] - 2s 550us/step - loss: 8.1908e-05 - val_loss: 8.2235e-05\n",
      "Epoch 1164/2000\n",
      "3690/3690 [==============================] - 2s 552us/step - loss: 7.7343e-05 - val_loss: 2.0387e-04\n",
      "Epoch 1165/2000\n",
      "3690/3690 [==============================] - 2s 551us/step - loss: 8.6521e-05 - val_loss: 1.0482e-04\n",
      "Epoch 1166/2000\n",
      "3690/3690 [==============================] - 2s 551us/step - loss: 6.9875e-05 - val_loss: 1.2449e-04\n",
      "Epoch 1167/2000\n",
      "3690/3690 [==============================] - 2s 552us/step - loss: 1.0087e-04 - val_loss: 1.7836e-04\n",
      "Epoch 1168/2000\n",
      "3690/3690 [==============================] - 2s 549us/step - loss: 9.8481e-05 - val_loss: 1.5726e-04\n",
      "Epoch 1169/2000\n",
      "3690/3690 [==============================] - 2s 552us/step - loss: 1.0153e-04 - val_loss: 3.5414e-04\n",
      "Epoch 1170/2000\n",
      "3690/3690 [==============================] - 2s 549us/step - loss: 1.0166e-04 - val_loss: 8.3586e-05\n",
      "Epoch 1171/2000\n",
      "3690/3690 [==============================] - 2s 551us/step - loss: 7.5356e-05 - val_loss: 8.2314e-05\n",
      "Epoch 1172/2000\n",
      "3690/3690 [==============================] - 2s 555us/step - loss: 7.2221e-05 - val_loss: 1.2003e-04\n",
      "Epoch 1173/2000\n",
      "3690/3690 [==============================] - 2s 550us/step - loss: 8.1965e-05 - val_loss: 9.7014e-05\n",
      "Epoch 1174/2000\n",
      "3690/3690 [==============================] - 2s 552us/step - loss: 9.5341e-05 - val_loss: 1.5381e-04\n",
      "Epoch 1175/2000\n",
      "3690/3690 [==============================] - 2s 550us/step - loss: 8.9321e-05 - val_loss: 7.6641e-05\n",
      "Epoch 1176/2000\n",
      "3690/3690 [==============================] - 2s 549us/step - loss: 7.1759e-05 - val_loss: 8.1271e-05\n",
      "Epoch 1177/2000\n",
      "3690/3690 [==============================] - 2s 548us/step - loss: 7.1127e-05 - val_loss: 1.9044e-04\n",
      "Epoch 1178/2000\n",
      "3690/3690 [==============================] - 2s 550us/step - loss: 7.7620e-05 - val_loss: 7.1406e-05\n",
      "Epoch 1179/2000\n",
      "3690/3690 [==============================] - 2s 549us/step - loss: 7.6508e-05 - val_loss: 1.8009e-04\n",
      "Epoch 1180/2000\n",
      "3690/3690 [==============================] - 2s 556us/step - loss: 9.8459e-05 - val_loss: 9.9137e-05\n",
      "Epoch 1181/2000\n",
      "3690/3690 [==============================] - 2s 550us/step - loss: 8.3183e-05 - val_loss: 1.1401e-04\n",
      "Epoch 1182/2000\n",
      "3690/3690 [==============================] - 2s 550us/step - loss: 7.0410e-05 - val_loss: 8.7537e-05\n",
      "Epoch 1183/2000\n",
      "3690/3690 [==============================] - 2s 545us/step - loss: 7.4901e-05 - val_loss: 7.7916e-05\n",
      "Epoch 1184/2000\n",
      "3690/3690 [==============================] - 2s 550us/step - loss: 7.4066e-05 - val_loss: 8.2229e-05\n",
      "Epoch 1185/2000\n",
      "3690/3690 [==============================] - 2s 547us/step - loss: 7.9628e-05 - val_loss: 7.9036e-05\n",
      "Epoch 1186/2000\n",
      "3690/3690 [==============================] - 2s 548us/step - loss: 7.3797e-05 - val_loss: 7.8125e-05\n",
      "Epoch 1187/2000\n",
      "3690/3690 [==============================] - 2s 666us/step - loss: 6.6185e-05 - val_loss: 9.6945e-05\n",
      "Epoch 1188/2000\n",
      "3690/3690 [==============================] - 2s 569us/step - loss: 6.7724e-05 - val_loss: 8.6283e-05\n",
      "Epoch 1189/2000\n",
      "3690/3690 [==============================] - 2s 568us/step - loss: 6.7491e-05 - val_loss: 2.4596e-04\n",
      "Epoch 1190/2000\n",
      "3690/3690 [==============================] - 2s 562us/step - loss: 9.0991e-05 - val_loss: 1.3046e-04\n",
      "Epoch 1191/2000\n",
      "3690/3690 [==============================] - 2s 561us/step - loss: 7.3914e-05 - val_loss: 1.5772e-04\n",
      "Epoch 1192/2000\n",
      "3690/3690 [==============================] - 2s 557us/step - loss: 9.0811e-05 - val_loss: 9.7735e-05\n",
      "Epoch 1193/2000\n",
      "3690/3690 [==============================] - 2s 563us/step - loss: 8.0735e-05 - val_loss: 1.2569e-04\n",
      "Epoch 1194/2000\n",
      "3690/3690 [==============================] - 2s 554us/step - loss: 6.6324e-05 - val_loss: 8.8173e-05\n",
      "Epoch 1195/2000\n",
      "3690/3690 [==============================] - 2s 550us/step - loss: 6.6221e-05 - val_loss: 9.3388e-05\n",
      "Epoch 1196/2000\n",
      "3690/3690 [==============================] - 2s 561us/step - loss: 7.3488e-05 - val_loss: 1.1931e-04\n",
      "Epoch 1197/2000\n",
      "3690/3690 [==============================] - 2s 564us/step - loss: 7.2088e-05 - val_loss: 1.0380e-04\n",
      "Epoch 1198/2000\n",
      "3690/3690 [==============================] - 2s 563us/step - loss: 6.8528e-05 - val_loss: 9.8713e-05\n",
      "Epoch 1199/2000\n",
      "3690/3690 [==============================] - 2s 562us/step - loss: 8.0979e-05 - val_loss: 1.6679e-04\n",
      "Epoch 1200/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3690/3690 [==============================] - 2s 556us/step - loss: 7.1373e-05 - val_loss: 1.4379e-04\n",
      "Epoch 1201/2000\n",
      "3690/3690 [==============================] - 2s 554us/step - loss: 6.7825e-05 - val_loss: 8.6851e-05\n",
      "Epoch 1202/2000\n",
      "3690/3690 [==============================] - 2s 559us/step - loss: 7.8399e-05 - val_loss: 9.7539e-05\n",
      "Epoch 1203/2000\n",
      "3690/3690 [==============================] - 2s 564us/step - loss: 8.0311e-05 - val_loss: 1.4151e-04\n",
      "Epoch 1204/2000\n",
      "3690/3690 [==============================] - 2s 557us/step - loss: 8.2640e-05 - val_loss: 1.7538e-04\n",
      "Epoch 1205/2000\n",
      "3690/3690 [==============================] - 2s 557us/step - loss: 6.7111e-05 - val_loss: 8.6110e-05\n",
      "Epoch 1206/2000\n",
      "3690/3690 [==============================] - 2s 561us/step - loss: 7.1414e-05 - val_loss: 1.2771e-04\n",
      "Epoch 1207/2000\n",
      "3690/3690 [==============================] - 2s 551us/step - loss: 7.3167e-05 - val_loss: 9.9679e-05\n",
      "Epoch 1208/2000\n",
      "3690/3690 [==============================] - 2s 559us/step - loss: 7.9669e-05 - val_loss: 1.5762e-04\n",
      "Epoch 1209/2000\n",
      "3690/3690 [==============================] - 2s 558us/step - loss: 9.8969e-05 - val_loss: 1.5579e-04\n",
      "Epoch 1210/2000\n",
      "3690/3690 [==============================] - 2s 559us/step - loss: 8.5774e-05 - val_loss: 2.2922e-04\n",
      "Epoch 1211/2000\n",
      "3690/3690 [==============================] - 2s 561us/step - loss: 8.2260e-05 - val_loss: 8.7971e-05\n",
      "Epoch 1212/2000\n",
      "3690/3690 [==============================] - 2s 561us/step - loss: 7.3542e-05 - val_loss: 1.1461e-04\n",
      "Epoch 1213/2000\n",
      "3690/3690 [==============================] - 2s 556us/step - loss: 6.7361e-05 - val_loss: 1.0587e-04\n",
      "Epoch 1214/2000\n",
      "3690/3690 [==============================] - 2s 554us/step - loss: 7.6856e-05 - val_loss: 1.0183e-04\n",
      "Epoch 1215/2000\n",
      "3690/3690 [==============================] - 2s 556us/step - loss: 7.7913e-05 - val_loss: 1.0311e-04\n",
      "Epoch 1216/2000\n",
      "3690/3690 [==============================] - 2s 553us/step - loss: 7.6117e-05 - val_loss: 1.0612e-04\n",
      "Epoch 1217/2000\n",
      "3690/3690 [==============================] - 2s 560us/step - loss: 6.5801e-05 - val_loss: 8.8961e-05\n",
      "Epoch 1218/2000\n",
      "3690/3690 [==============================] - 2s 554us/step - loss: 6.8636e-05 - val_loss: 8.4973e-05\n",
      "Epoch 1219/2000\n",
      "3690/3690 [==============================] - 2s 564us/step - loss: 9.9593e-05 - val_loss: 1.8027e-04\n",
      "Epoch 1220/2000\n",
      "3690/3690 [==============================] - 2s 552us/step - loss: 9.4518e-05 - val_loss: 1.5620e-04\n",
      "Epoch 1221/2000\n",
      "3690/3690 [==============================] - 2s 555us/step - loss: 1.0005e-04 - val_loss: 1.2160e-04\n",
      "Epoch 1222/2000\n",
      "3690/3690 [==============================] - 2s 555us/step - loss: 8.2638e-05 - val_loss: 1.3949e-04\n",
      "Epoch 1223/2000\n",
      "3690/3690 [==============================] - 2s 552us/step - loss: 8.4387e-05 - val_loss: 1.5256e-04\n",
      "Epoch 1224/2000\n",
      "3690/3690 [==============================] - 2s 552us/step - loss: 7.7150e-05 - val_loss: 8.8347e-05\n",
      "Epoch 1225/2000\n",
      "3690/3690 [==============================] - 2s 551us/step - loss: 7.0129e-05 - val_loss: 8.0024e-05\n",
      "Epoch 1226/2000\n",
      "3690/3690 [==============================] - 2s 554us/step - loss: 6.4901e-05 - val_loss: 9.5225e-05\n",
      "Epoch 1227/2000\n",
      "3690/3690 [==============================] - 2s 558us/step - loss: 6.8125e-05 - val_loss: 9.5950e-05\n",
      "Epoch 1228/2000\n",
      "3690/3690 [==============================] - 2s 553us/step - loss: 8.1855e-05 - val_loss: 1.5560e-04\n",
      "Epoch 1229/2000\n",
      "3690/3690 [==============================] - 2s 554us/step - loss: 9.3344e-05 - val_loss: 1.8744e-04\n",
      "Epoch 1230/2000\n",
      "3690/3690 [==============================] - 2s 563us/step - loss: 8.4114e-05 - val_loss: 1.1013e-04\n",
      "Epoch 1231/2000\n",
      "3690/3690 [==============================] - 2s 553us/step - loss: 8.9284e-05 - val_loss: 1.9808e-04\n",
      "Epoch 1232/2000\n",
      "3690/3690 [==============================] - 2s 549us/step - loss: 8.0204e-05 - val_loss: 1.8067e-04\n",
      "Epoch 1233/2000\n",
      "3690/3690 [==============================] - 2s 554us/step - loss: 8.0178e-05 - val_loss: 2.1597e-04\n",
      "Epoch 1234/2000\n",
      "3690/3690 [==============================] - 2s 556us/step - loss: 8.8851e-05 - val_loss: 8.5640e-05\n",
      "Epoch 1235/2000\n",
      "3690/3690 [==============================] - 2s 555us/step - loss: 8.5276e-05 - val_loss: 1.5007e-04\n",
      "Epoch 1236/2000\n",
      "3690/3690 [==============================] - 2s 554us/step - loss: 6.8846e-05 - val_loss: 8.0546e-05\n",
      "Epoch 1237/2000\n",
      "3690/3690 [==============================] - 2s 554us/step - loss: 6.7310e-05 - val_loss: 9.5050e-05\n",
      "Epoch 1238/2000\n",
      "3690/3690 [==============================] - 2s 554us/step - loss: 8.4817e-05 - val_loss: 9.0261e-05\n",
      "Epoch 1239/2000\n",
      "3690/3690 [==============================] - 2s 555us/step - loss: 8.8124e-05 - val_loss: 1.1952e-04\n",
      "Epoch 1240/2000\n",
      "3690/3690 [==============================] - 2s 554us/step - loss: 8.7932e-05 - val_loss: 7.6345e-05\n",
      "Epoch 1241/2000\n",
      "3690/3690 [==============================] - 2s 555us/step - loss: 8.8506e-05 - val_loss: 1.0274e-04\n",
      "Epoch 1242/2000\n",
      "3690/3690 [==============================] - 2s 558us/step - loss: 8.7122e-05 - val_loss: 1.8273e-04\n",
      "Epoch 1243/2000\n",
      "3690/3690 [==============================] - 2s 551us/step - loss: 7.5743e-05 - val_loss: 9.6485e-05\n",
      "Epoch 1244/2000\n",
      "3690/3690 [==============================] - 2s 558us/step - loss: 7.4029e-05 - val_loss: 1.3697e-04\n",
      "Epoch 1245/2000\n",
      "3690/3690 [==============================] - 2s 550us/step - loss: 8.1168e-05 - val_loss: 1.3575e-04\n",
      "Epoch 1246/2000\n",
      "3690/3690 [==============================] - 2s 556us/step - loss: 1.1435e-04 - val_loss: 9.5778e-05\n",
      "Epoch 1247/2000\n",
      "3690/3690 [==============================] - 2s 553us/step - loss: 9.5454e-05 - val_loss: 3.2826e-04\n",
      "Epoch 1248/2000\n",
      "3690/3690 [==============================] - 2s 553us/step - loss: 9.9709e-05 - val_loss: 1.0656e-04\n",
      "Epoch 1249/2000\n",
      "3690/3690 [==============================] - 2s 554us/step - loss: 8.9931e-05 - val_loss: 7.7794e-05\n",
      "Epoch 1250/2000\n",
      "3690/3690 [==============================] - 2s 556us/step - loss: 1.0204e-04 - val_loss: 7.7722e-05\n",
      "Epoch 1251/2000\n",
      "3690/3690 [==============================] - 2s 553us/step - loss: 9.2504e-05 - val_loss: 1.7959e-04\n",
      "Epoch 1252/2000\n",
      "3690/3690 [==============================] - 2s 553us/step - loss: 9.3652e-05 - val_loss: 9.2782e-05\n",
      "Epoch 1253/2000\n",
      "3690/3690 [==============================] - 2s 551us/step - loss: 8.5577e-05 - val_loss: 9.3619e-05\n",
      "Epoch 1254/2000\n",
      "3690/3690 [==============================] - 2s 552us/step - loss: 8.2967e-05 - val_loss: 1.1028e-04\n",
      "Epoch 1255/2000\n",
      "3690/3690 [==============================] - 2s 553us/step - loss: 6.8690e-05 - val_loss: 8.7686e-05\n",
      "Epoch 1256/2000\n",
      "3690/3690 [==============================] - 2s 558us/step - loss: 7.9835e-05 - val_loss: 9.4056e-05\n",
      "Epoch 1257/2000\n",
      "3690/3690 [==============================] - 2s 554us/step - loss: 8.5154e-05 - val_loss: 9.8041e-05\n",
      "Epoch 1258/2000\n",
      "3690/3690 [==============================] - 2s 555us/step - loss: 8.4633e-05 - val_loss: 1.3235e-04\n",
      "Epoch 1259/2000\n",
      "3690/3690 [==============================] - 2s 557us/step - loss: 8.5078e-05 - val_loss: 1.2837e-04\n",
      "Epoch 1260/2000\n",
      "3690/3690 [==============================] - 2s 555us/step - loss: 1.1529e-04 - val_loss: 1.1945e-04\n",
      "Epoch 1261/2000\n",
      "3690/3690 [==============================] - 2s 556us/step - loss: 1.0652e-04 - val_loss: 2.4546e-04\n",
      "Epoch 1262/2000\n",
      "3690/3690 [==============================] - 2s 555us/step - loss: 8.5108e-05 - val_loss: 8.1927e-05\n",
      "Epoch 1263/2000\n",
      "3690/3690 [==============================] - 2s 554us/step - loss: 6.9480e-05 - val_loss: 8.7314e-05\n",
      "Epoch 1264/2000\n",
      "3690/3690 [==============================] - 2s 553us/step - loss: 6.9639e-05 - val_loss: 8.0803e-05\n",
      "Epoch 1265/2000\n",
      "3690/3690 [==============================] - 2s 551us/step - loss: 7.5425e-05 - val_loss: 9.7908e-05\n",
      "Epoch 1266/2000\n",
      "3690/3690 [==============================] - 2s 559us/step - loss: 7.9785e-05 - val_loss: 1.8162e-04\n",
      "Epoch 1267/2000\n",
      "3690/3690 [==============================] - 2s 553us/step - loss: 8.7137e-05 - val_loss: 1.2754e-04\n",
      "Epoch 1268/2000\n",
      "3690/3690 [==============================] - 2s 555us/step - loss: 6.5442e-05 - val_loss: 1.0791e-04\n",
      "Epoch 1269/2000\n",
      "3690/3690 [==============================] - 2s 555us/step - loss: 6.5126e-05 - val_loss: 8.9404e-05\n",
      "Epoch 1270/2000\n",
      "3690/3690 [==============================] - 2s 553us/step - loss: 6.5489e-05 - val_loss: 1.5787e-04\n",
      "Epoch 1271/2000\n",
      "3690/3690 [==============================] - 2s 555us/step - loss: 6.8813e-05 - val_loss: 1.0733e-04\n",
      "Epoch 1272/2000\n",
      "3690/3690 [==============================] - 2s 552us/step - loss: 6.4107e-05 - val_loss: 1.4625e-04\n",
      "Epoch 1273/2000\n",
      "3690/3690 [==============================] - 2s 551us/step - loss: 7.0626e-05 - val_loss: 7.8477e-05\n",
      "Epoch 1274/2000\n",
      "3690/3690 [==============================] - 2s 556us/step - loss: 6.4029e-05 - val_loss: 7.6970e-05\n",
      "Epoch 1275/2000\n",
      "3690/3690 [==============================] - 2s 557us/step - loss: 6.6555e-05 - val_loss: 2.4364e-04\n",
      "Epoch 1276/2000\n",
      "3690/3690 [==============================] - 2s 554us/step - loss: 8.0215e-05 - val_loss: 1.2407e-04\n",
      "Epoch 1277/2000\n",
      "3690/3690 [==============================] - 2s 552us/step - loss: 6.8683e-05 - val_loss: 8.2712e-05\n",
      "Epoch 1278/2000\n",
      "3690/3690 [==============================] - 2s 554us/step - loss: 6.6414e-05 - val_loss: 1.1160e-04\n",
      "Epoch 1279/2000\n",
      "3690/3690 [==============================] - 2s 555us/step - loss: 6.7468e-05 - val_loss: 1.0615e-04\n",
      "Epoch 1280/2000\n",
      "3690/3690 [==============================] - 2s 555us/step - loss: 6.4979e-05 - val_loss: 8.6039e-05\n",
      "Epoch 1281/2000\n",
      "3690/3690 [==============================] - 2s 557us/step - loss: 7.0678e-05 - val_loss: 1.9989e-04\n",
      "Epoch 1282/2000\n",
      "3690/3690 [==============================] - 2s 553us/step - loss: 8.3126e-05 - val_loss: 1.3621e-04\n",
      "Epoch 1283/2000\n",
      "3690/3690 [==============================] - 2s 552us/step - loss: 8.6244e-05 - val_loss: 9.6771e-05\n",
      "Epoch 1284/2000\n",
      "3690/3690 [==============================] - 2s 552us/step - loss: 7.1490e-05 - val_loss: 2.3252e-04\n",
      "Epoch 1285/2000\n",
      "3690/3690 [==============================] - 2s 554us/step - loss: 9.1861e-05 - val_loss: 9.3389e-05\n",
      "Epoch 1286/2000\n",
      "3690/3690 [==============================] - 2s 554us/step - loss: 8.1938e-05 - val_loss: 9.4135e-05\n",
      "Epoch 1287/2000\n",
      "3690/3690 [==============================] - 2s 553us/step - loss: 7.0958e-05 - val_loss: 1.4556e-04\n",
      "Epoch 1288/2000\n",
      "3690/3690 [==============================] - 2s 553us/step - loss: 7.0137e-05 - val_loss: 9.2650e-05\n",
      "Epoch 1289/2000\n",
      "3690/3690 [==============================] - 2s 557us/step - loss: 7.1433e-05 - val_loss: 1.0284e-04\n",
      "Epoch 1290/2000\n",
      "3690/3690 [==============================] - 2s 555us/step - loss: 7.1626e-05 - val_loss: 1.3808e-04\n",
      "Epoch 1291/2000\n",
      "3690/3690 [==============================] - 2s 556us/step - loss: 7.5487e-05 - val_loss: 1.1032e-04\n",
      "Epoch 1292/2000\n",
      "3690/3690 [==============================] - 2s 555us/step - loss: 7.8776e-05 - val_loss: 7.9147e-05\n",
      "Epoch 1293/2000\n",
      "3690/3690 [==============================] - 2s 554us/step - loss: 7.3981e-05 - val_loss: 8.6608e-05\n",
      "Epoch 1294/2000\n",
      "3690/3690 [==============================] - 2s 553us/step - loss: 6.6300e-05 - val_loss: 2.5210e-04\n",
      "Epoch 1295/2000\n",
      "3690/3690 [==============================] - 2s 552us/step - loss: 9.9131e-05 - val_loss: 9.2439e-05\n",
      "Epoch 1296/2000\n",
      "3690/3690 [==============================] - 2s 553us/step - loss: 7.6399e-05 - val_loss: 8.6882e-05\n",
      "Epoch 1297/2000\n",
      "3690/3690 [==============================] - 2s 557us/step - loss: 6.8486e-05 - val_loss: 2.1891e-04\n",
      "Epoch 1298/2000\n",
      "3690/3690 [==============================] - 2s 552us/step - loss: 6.6712e-05 - val_loss: 1.3358e-04\n",
      "Epoch 1299/2000\n",
      "3690/3690 [==============================] - 2s 559us/step - loss: 6.7051e-05 - val_loss: 1.1351e-04\n",
      "Epoch 1300/2000\n",
      "3690/3690 [==============================] - 2s 589us/step - loss: 7.5777e-05 - val_loss: 1.0549e-04\n",
      "Epoch 1301/2000\n",
      "3690/3690 [==============================] - 2s 557us/step - loss: 6.8472e-05 - val_loss: 1.0317e-04\n",
      "Epoch 1302/2000\n",
      "3690/3690 [==============================] - 2s 565us/step - loss: 6.4864e-05 - val_loss: 1.5981e-04\n",
      "Epoch 1303/2000\n",
      "3690/3690 [==============================] - 2s 563us/step - loss: 6.8354e-05 - val_loss: 9.5346e-05\n",
      "Epoch 1304/2000\n",
      "3690/3690 [==============================] - 2s 583us/step - loss: 7.5750e-05 - val_loss: 8.7025e-05\n",
      "Epoch 1305/2000\n",
      "3690/3690 [==============================] - 2s 558us/step - loss: 7.4395e-05 - val_loss: 1.9829e-04\n",
      "Epoch 1306/2000\n",
      "3690/3690 [==============================] - 2s 556us/step - loss: 1.1889e-04 - val_loss: 1.2510e-04\n",
      "Epoch 1307/2000\n",
      "3690/3690 [==============================] - 2s 556us/step - loss: 8.6666e-05 - val_loss: 1.4021e-04\n",
      "Epoch 1308/2000\n",
      "3690/3690 [==============================] - 2s 554us/step - loss: 9.8689e-05 - val_loss: 8.6056e-05\n",
      "Epoch 1309/2000\n",
      "3690/3690 [==============================] - 2s 551us/step - loss: 8.8366e-05 - val_loss: 3.6662e-04\n",
      "Epoch 1310/2000\n",
      "3690/3690 [==============================] - 2s 554us/step - loss: 9.1977e-05 - val_loss: 7.9622e-05\n",
      "Epoch 1311/2000\n",
      "3690/3690 [==============================] - 2s 554us/step - loss: 8.0430e-05 - val_loss: 1.2947e-04\n",
      "Epoch 1312/2000\n",
      "3690/3690 [==============================] - 2s 555us/step - loss: 6.7090e-05 - val_loss: 1.0431e-04\n",
      "Epoch 1313/2000\n",
      "3690/3690 [==============================] - 2s 553us/step - loss: 7.4461e-05 - val_loss: 1.6125e-04\n",
      "Epoch 1314/2000\n",
      "3690/3690 [==============================] - 2s 554us/step - loss: 6.8215e-05 - val_loss: 9.2272e-05\n",
      "Epoch 1315/2000\n",
      "3690/3690 [==============================] - 2s 554us/step - loss: 7.0278e-05 - val_loss: 3.2934e-04\n",
      "Epoch 1316/2000\n",
      "3690/3690 [==============================] - 2s 550us/step - loss: 1.0948e-04 - val_loss: 1.3681e-04\n",
      "Epoch 1317/2000\n",
      "3690/3690 [==============================] - 2s 554us/step - loss: 1.1592e-04 - val_loss: 8.9343e-05\n",
      "Epoch 1318/2000\n",
      "3690/3690 [==============================] - 2s 555us/step - loss: 8.3715e-05 - val_loss: 1.2174e-04\n",
      "Epoch 1319/2000\n",
      "3690/3690 [==============================] - 2s 554us/step - loss: 8.0738e-05 - val_loss: 1.6560e-04\n",
      "Epoch 1320/2000\n",
      "3690/3690 [==============================] - 2s 556us/step - loss: 8.2441e-05 - val_loss: 8.7768e-05\n",
      "Epoch 1321/2000\n",
      "3690/3690 [==============================] - 2s 553us/step - loss: 7.7394e-05 - val_loss: 1.6436e-04\n",
      "Epoch 1322/2000\n",
      "3690/3690 [==============================] - 2s 551us/step - loss: 9.8592e-05 - val_loss: 1.1408e-04\n",
      "Epoch 1323/2000\n",
      "3690/3690 [==============================] - 2s 551us/step - loss: 1.1117e-04 - val_loss: 8.3073e-05\n",
      "Epoch 1324/2000\n",
      "3690/3690 [==============================] - 2s 571us/step - loss: 8.0687e-05 - val_loss: 1.7554e-04\n",
      "Epoch 1325/2000\n",
      "3690/3690 [==============================] - 2s 557us/step - loss: 7.1697e-05 - val_loss: 9.5480e-05\n",
      "Epoch 1326/2000\n",
      "3690/3690 [==============================] - 2s 554us/step - loss: 7.1073e-05 - val_loss: 9.5137e-05\n",
      "Epoch 1327/2000\n",
      "3690/3690 [==============================] - 2s 551us/step - loss: 6.4287e-05 - val_loss: 1.4872e-04\n",
      "Epoch 1328/2000\n",
      "3690/3690 [==============================] - 2s 560us/step - loss: 6.2251e-05 - val_loss: 8.0831e-05\n",
      "Epoch 1329/2000\n",
      "3690/3690 [==============================] - 2s 554us/step - loss: 6.8759e-05 - val_loss: 1.5422e-04\n",
      "Epoch 1330/2000\n",
      "3690/3690 [==============================] - 2s 555us/step - loss: 6.7256e-05 - val_loss: 1.3898e-04\n",
      "Epoch 1331/2000\n",
      "3690/3690 [==============================] - 2s 555us/step - loss: 6.7219e-05 - val_loss: 8.7355e-05\n",
      "Epoch 1332/2000\n",
      "3690/3690 [==============================] - 2s 555us/step - loss: 6.7897e-05 - val_loss: 1.8581e-04\n",
      "Epoch 1333/2000\n",
      "3690/3690 [==============================] - 2s 555us/step - loss: 8.5759e-05 - val_loss: 9.8599e-05\n",
      "Epoch 1334/2000\n",
      "3690/3690 [==============================] - 2s 554us/step - loss: 8.2317e-05 - val_loss: 1.3243e-04\n",
      "Epoch 1335/2000\n",
      "3690/3690 [==============================] - 2s 555us/step - loss: 6.9129e-05 - val_loss: 1.2302e-04\n",
      "Epoch 1336/2000\n",
      "3690/3690 [==============================] - 2s 554us/step - loss: 6.5108e-05 - val_loss: 1.1301e-04\n",
      "Epoch 1337/2000\n",
      "3690/3690 [==============================] - 2s 559us/step - loss: 9.3515e-05 - val_loss: 2.1500e-04\n",
      "Epoch 1338/2000\n",
      "3690/3690 [==============================] - 2s 552us/step - loss: 7.3443e-05 - val_loss: 7.8373e-05\n",
      "Epoch 1339/2000\n",
      "3690/3690 [==============================] - 2s 557us/step - loss: 6.4068e-05 - val_loss: 1.6123e-04\n",
      "Epoch 1340/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3690/3690 [==============================] - 2s 555us/step - loss: 6.9645e-05 - val_loss: 9.0946e-05\n",
      "Epoch 1341/2000\n",
      "3690/3690 [==============================] - 2s 559us/step - loss: 6.6406e-05 - val_loss: 1.1540e-04\n",
      "Epoch 1342/2000\n",
      "3690/3690 [==============================] - 2s 577us/step - loss: 6.1496e-05 - val_loss: 9.6194e-05\n",
      "Epoch 1343/2000\n",
      "3690/3690 [==============================] - 2s 565us/step - loss: 6.8558e-05 - val_loss: 9.9975e-05\n",
      "Epoch 1344/2000\n",
      "3690/3690 [==============================] - 2s 564us/step - loss: 6.8220e-05 - val_loss: 1.0467e-04\n",
      "Epoch 1345/2000\n",
      "3690/3690 [==============================] - 2s 554us/step - loss: 6.2252e-05 - val_loss: 3.0295e-04\n",
      "Epoch 1346/2000\n",
      "3690/3690 [==============================] - 2s 554us/step - loss: 7.8615e-05 - val_loss: 9.6933e-05\n",
      "Epoch 1347/2000\n",
      "3690/3690 [==============================] - 2s 554us/step - loss: 7.9573e-05 - val_loss: 1.7492e-04\n",
      "Epoch 1348/2000\n",
      "3690/3690 [==============================] - 2s 568us/step - loss: 1.0012e-04 - val_loss: 1.3694e-04\n",
      "Epoch 1349/2000\n",
      "3690/3690 [==============================] - 2s 560us/step - loss: 9.3401e-05 - val_loss: 1.0330e-04\n",
      "Epoch 1350/2000\n",
      "3690/3690 [==============================] - 2s 561us/step - loss: 6.9742e-05 - val_loss: 8.0461e-05\n",
      "Epoch 1351/2000\n",
      "3690/3690 [==============================] - 2s 575us/step - loss: 6.4395e-05 - val_loss: 7.5743e-05\n",
      "Epoch 1352/2000\n",
      "3690/3690 [==============================] - 2s 585us/step - loss: 6.6038e-05 - val_loss: 8.3609e-05\n",
      "Epoch 1353/2000\n",
      "3690/3690 [==============================] - 2s 574us/step - loss: 6.3769e-05 - val_loss: 1.4749e-04\n",
      "Epoch 1354/2000\n",
      "3690/3690 [==============================] - 3s 775us/step - loss: 6.7902e-05 - val_loss: 9.8056e-05\n",
      "Epoch 1355/2000\n",
      "3690/3690 [==============================] - 3s 820us/step - loss: 7.8056e-05 - val_loss: 1.0409e-04\n",
      "Epoch 1356/2000\n",
      "3690/3690 [==============================] - 2s 583us/step - loss: 7.0199e-05 - val_loss: 1.2877e-04\n",
      "Epoch 1357/2000\n",
      "3690/3690 [==============================] - 2s 589us/step - loss: 6.6132e-05 - val_loss: 9.0433e-05\n",
      "Epoch 1358/2000\n",
      "3690/3690 [==============================] - 2s 584us/step - loss: 6.9286e-05 - val_loss: 1.4298e-04\n",
      "Epoch 1359/2000\n",
      "3690/3690 [==============================] - 2s 579us/step - loss: 6.3225e-05 - val_loss: 1.0387e-04\n",
      "Epoch 1360/2000\n",
      "3690/3690 [==============================] - 2s 562us/step - loss: 6.1514e-05 - val_loss: 9.2146e-05\n",
      "Epoch 1361/2000\n",
      "3690/3690 [==============================] - 2s 565us/step - loss: 6.5553e-05 - val_loss: 1.0594e-04\n",
      "Epoch 1362/2000\n",
      "3690/3690 [==============================] - 2s 563us/step - loss: 8.2886e-05 - val_loss: 1.1706e-04\n",
      "Epoch 1363/2000\n",
      "3690/3690 [==============================] - 2s 564us/step - loss: 7.7623e-05 - val_loss: 1.2455e-04\n",
      "Epoch 1364/2000\n",
      "3690/3690 [==============================] - 2s 566us/step - loss: 7.4197e-05 - val_loss: 8.7770e-05\n",
      "Epoch 1365/2000\n",
      "3690/3690 [==============================] - 2s 576us/step - loss: 7.0937e-05 - val_loss: 1.2975e-04\n",
      "Epoch 1366/2000\n",
      "3690/3690 [==============================] - 2s 605us/step - loss: 6.5181e-05 - val_loss: 1.2910e-04\n",
      "Epoch 1367/2000\n",
      "3690/3690 [==============================] - 2s 574us/step - loss: 7.0470e-05 - val_loss: 8.3653e-05\n",
      "Epoch 1368/2000\n",
      "3690/3690 [==============================] - 2s 620us/step - loss: 7.6976e-05 - val_loss: 1.1377e-04\n",
      "Epoch 1369/2000\n",
      "3690/3690 [==============================] - 2s 592us/step - loss: 1.0362e-04 - val_loss: 2.1772e-04\n",
      "Epoch 1370/2000\n",
      "3690/3690 [==============================] - 2s 580us/step - loss: 7.8279e-05 - val_loss: 8.5551e-05\n",
      "Epoch 1371/2000\n",
      "3690/3690 [==============================] - 2s 577us/step - loss: 7.2287e-05 - val_loss: 9.6245e-05\n",
      "Epoch 1372/2000\n",
      "3690/3690 [==============================] - 2s 583us/step - loss: 7.0503e-05 - val_loss: 9.3043e-05\n",
      "Epoch 1373/2000\n",
      "3690/3690 [==============================] - 2s 581us/step - loss: 7.0788e-05 - val_loss: 1.7810e-04\n",
      "Epoch 1374/2000\n",
      "3690/3690 [==============================] - 2s 573us/step - loss: 6.3947e-05 - val_loss: 8.6856e-05\n",
      "Epoch 1375/2000\n",
      "3690/3690 [==============================] - 2s 591us/step - loss: 6.5533e-05 - val_loss: 1.0296e-04\n",
      "Epoch 1376/2000\n",
      "3690/3690 [==============================] - 2s 576us/step - loss: 6.8759e-05 - val_loss: 1.9690e-04\n",
      "Epoch 1377/2000\n",
      "3690/3690 [==============================] - 2s 558us/step - loss: 7.3762e-05 - val_loss: 1.0199e-04\n",
      "Epoch 1378/2000\n",
      "3690/3690 [==============================] - 2s 565us/step - loss: 7.4888e-05 - val_loss: 1.1746e-04\n",
      "Epoch 1379/2000\n",
      "3690/3690 [==============================] - 2s 574us/step - loss: 6.7229e-05 - val_loss: 3.9323e-04\n",
      "Epoch 1380/2000\n",
      "3690/3690 [==============================] - 2s 563us/step - loss: 7.7097e-05 - val_loss: 9.1482e-05\n",
      "Epoch 1381/2000\n",
      "3690/3690 [==============================] - 2s 577us/step - loss: 6.4493e-05 - val_loss: 9.7748e-05\n",
      "Epoch 1382/2000\n",
      "3690/3690 [==============================] - 2s 559us/step - loss: 6.5028e-05 - val_loss: 1.6384e-04\n",
      "Epoch 1383/2000\n",
      "3690/3690 [==============================] - 2s 556us/step - loss: 7.4541e-05 - val_loss: 9.1712e-05\n",
      "Epoch 1384/2000\n",
      "3690/3690 [==============================] - 2s 558us/step - loss: 6.8035e-05 - val_loss: 1.1775e-04\n",
      "Epoch 1385/2000\n",
      "3690/3690 [==============================] - 2s 558us/step - loss: 7.4477e-05 - val_loss: 1.7022e-04\n",
      "Epoch 1386/2000\n",
      "3690/3690 [==============================] - 2s 553us/step - loss: 7.2032e-05 - val_loss: 3.3844e-04\n",
      "Epoch 1387/2000\n",
      "3690/3690 [==============================] - 2s 554us/step - loss: 7.3572e-05 - val_loss: 9.7063e-05\n",
      "Epoch 1388/2000\n",
      "3690/3690 [==============================] - 2s 558us/step - loss: 8.7549e-05 - val_loss: 9.5789e-05\n",
      "Epoch 1389/2000\n",
      "3690/3690 [==============================] - 2s 556us/step - loss: 7.0734e-05 - val_loss: 1.1131e-04\n",
      "Epoch 1390/2000\n",
      "3690/3690 [==============================] - 2s 587us/step - loss: 6.8336e-05 - val_loss: 1.3166e-04\n",
      "Epoch 1391/2000\n",
      "3690/3690 [==============================] - 2s 567us/step - loss: 6.2530e-05 - val_loss: 1.5656e-04\n",
      "Epoch 1392/2000\n",
      "3690/3690 [==============================] - 2s 564us/step - loss: 6.4992e-05 - val_loss: 1.1564e-04\n",
      "Epoch 1393/2000\n",
      "3690/3690 [==============================] - 2s 552us/step - loss: 7.0033e-05 - val_loss: 1.6061e-04\n",
      "Epoch 1394/2000\n",
      "3690/3690 [==============================] - 2s 557us/step - loss: 6.7523e-05 - val_loss: 1.2152e-04\n",
      "Epoch 1395/2000\n",
      "3690/3690 [==============================] - 2s 557us/step - loss: 7.3468e-05 - val_loss: 1.6515e-04\n",
      "Epoch 1396/2000\n",
      "3690/3690 [==============================] - 2s 559us/step - loss: 7.0850e-05 - val_loss: 8.5303e-05\n",
      "Epoch 1397/2000\n",
      "3690/3690 [==============================] - 2s 565us/step - loss: 6.8927e-05 - val_loss: 1.1101e-04\n",
      "Epoch 1398/2000\n",
      "3690/3690 [==============================] - 2s 565us/step - loss: 7.4152e-05 - val_loss: 1.3836e-04\n",
      "Epoch 1399/2000\n",
      "3690/3690 [==============================] - 2s 567us/step - loss: 7.0308e-05 - val_loss: 1.2738e-04\n",
      "Epoch 1400/2000\n",
      "3690/3690 [==============================] - 2s 565us/step - loss: 7.1012e-05 - val_loss: 2.0051e-04\n",
      "Epoch 1401/2000\n",
      "3690/3690 [==============================] - 2s 559us/step - loss: 6.9066e-05 - val_loss: 2.1393e-04\n",
      "Epoch 1402/2000\n",
      "3690/3690 [==============================] - 2s 555us/step - loss: 7.7194e-05 - val_loss: 9.3070e-05\n",
      "Epoch 1403/2000\n",
      "3690/3690 [==============================] - 2s 554us/step - loss: 7.8081e-05 - val_loss: 1.0538e-04\n",
      "Epoch 1404/2000\n",
      "3690/3690 [==============================] - 2s 559us/step - loss: 7.4171e-05 - val_loss: 4.1244e-04\n",
      "Epoch 1405/2000\n",
      "3690/3690 [==============================] - 2s 560us/step - loss: 8.2758e-05 - val_loss: 9.4077e-05\n",
      "Epoch 1406/2000\n",
      "3690/3690 [==============================] - 2s 570us/step - loss: 7.8853e-05 - val_loss: 1.0681e-04\n",
      "Epoch 1407/2000\n",
      "3690/3690 [==============================] - 2s 572us/step - loss: 6.5860e-05 - val_loss: 1.1652e-04\n",
      "Epoch 1408/2000\n",
      "3690/3690 [==============================] - 2s 600us/step - loss: 6.6073e-05 - val_loss: 1.2247e-04\n",
      "Epoch 1409/2000\n",
      "3690/3690 [==============================] - 2s 640us/step - loss: 6.7710e-05 - val_loss: 1.0633e-04\n",
      "Epoch 1410/2000\n",
      "3690/3690 [==============================] - 2s 641us/step - loss: 6.5532e-05 - val_loss: 1.6011e-04\n",
      "Epoch 1411/2000\n",
      "3690/3690 [==============================] - 2s 604us/step - loss: 7.4968e-05 - val_loss: 1.5463e-04\n",
      "Epoch 1412/2000\n",
      "3690/3690 [==============================] - 2s 607us/step - loss: 7.5868e-05 - val_loss: 1.5138e-04\n",
      "Epoch 1413/2000\n",
      "3690/3690 [==============================] - 2s 593us/step - loss: 8.5954e-05 - val_loss: 1.0862e-04\n",
      "Epoch 1414/2000\n",
      "3690/3690 [==============================] - 2s 594us/step - loss: 8.8972e-05 - val_loss: 1.6384e-04\n",
      "Epoch 1415/2000\n",
      "3690/3690 [==============================] - 2s 588us/step - loss: 7.1575e-05 - val_loss: 9.2679e-05\n",
      "Epoch 1416/2000\n",
      "3690/3690 [==============================] - 2s 585us/step - loss: 6.3450e-05 - val_loss: 2.0523e-04\n",
      "Epoch 1417/2000\n",
      "3690/3690 [==============================] - 2s 604us/step - loss: 8.7649e-05 - val_loss: 1.3259e-04\n",
      "Epoch 1418/2000\n",
      "3690/3690 [==============================] - 2s 587us/step - loss: 1.1577e-04 - val_loss: 1.1139e-04\n",
      "Epoch 1419/2000\n",
      "3690/3690 [==============================] - 2s 567us/step - loss: 1.0236e-04 - val_loss: 3.2048e-04\n",
      "Epoch 1420/2000\n",
      "3690/3690 [==============================] - 2s 587us/step - loss: 8.2732e-05 - val_loss: 1.3967e-04\n",
      "Epoch 1421/2000\n",
      "3690/3690 [==============================] - 2s 571us/step - loss: 9.5735e-05 - val_loss: 1.7321e-04\n",
      "Epoch 1422/2000\n",
      "3690/3690 [==============================] - 2s 585us/step - loss: 8.1221e-05 - val_loss: 1.0427e-04\n",
      "Epoch 1423/2000\n",
      "3690/3690 [==============================] - 2s 614us/step - loss: 7.9779e-05 - val_loss: 2.1761e-04\n",
      "Epoch 1424/2000\n",
      "3690/3690 [==============================] - 2s 601us/step - loss: 7.2491e-05 - val_loss: 1.0878e-04\n",
      "Epoch 1425/2000\n",
      "3690/3690 [==============================] - 2s 598us/step - loss: 7.7501e-05 - val_loss: 9.7496e-05\n",
      "Epoch 1426/2000\n",
      "3690/3690 [==============================] - 2s 606us/step - loss: 7.2305e-05 - val_loss: 8.8251e-05\n",
      "Epoch 1427/2000\n",
      "3690/3690 [==============================] - 2s 616us/step - loss: 7.3451e-05 - val_loss: 1.1904e-04\n",
      "Epoch 1428/2000\n",
      "3690/3690 [==============================] - 2s 580us/step - loss: 6.4088e-05 - val_loss: 1.5923e-04\n",
      "Epoch 1429/2000\n",
      "3690/3690 [==============================] - 2s 574us/step - loss: 6.7841e-05 - val_loss: 1.0079e-04\n",
      "Epoch 1430/2000\n",
      "3690/3690 [==============================] - 2s 559us/step - loss: 6.5885e-05 - val_loss: 1.5997e-04\n",
      "Epoch 1431/2000\n",
      "3690/3690 [==============================] - 2s 600us/step - loss: 6.8691e-05 - val_loss: 1.5176e-04\n",
      "Epoch 1432/2000\n",
      "3690/3690 [==============================] - 2s 598us/step - loss: 7.2565e-05 - val_loss: 1.5686e-04\n",
      "Epoch 1433/2000\n",
      "3690/3690 [==============================] - 2s 598us/step - loss: 9.4362e-05 - val_loss: 2.3255e-04\n",
      "Epoch 1434/2000\n",
      "3690/3690 [==============================] - 2s 596us/step - loss: 7.7338e-05 - val_loss: 1.5302e-04\n",
      "Epoch 1435/2000\n",
      "3690/3690 [==============================] - 2s 610us/step - loss: 7.0189e-05 - val_loss: 1.0351e-04\n",
      "Epoch 1436/2000\n",
      "3690/3690 [==============================] - 2s 675us/step - loss: 6.8278e-05 - val_loss: 1.5837e-04\n",
      "Epoch 1437/2000\n",
      "3690/3690 [==============================] - 2s 605us/step - loss: 6.3084e-05 - val_loss: 1.0129e-04\n",
      "Epoch 1438/2000\n",
      "3690/3690 [==============================] - 2s 593us/step - loss: 6.7874e-05 - val_loss: 2.2680e-04\n",
      "Epoch 1439/2000\n",
      "3690/3690 [==============================] - 2s 585us/step - loss: 7.4190e-05 - val_loss: 1.0308e-04\n",
      "Epoch 1440/2000\n",
      "3690/3690 [==============================] - 2s 581us/step - loss: 1.0027e-04 - val_loss: 1.1676e-04\n",
      "Epoch 1441/2000\n",
      "3690/3690 [==============================] - 2s 584us/step - loss: 1.0903e-04 - val_loss: 1.6247e-04\n",
      "Epoch 1442/2000\n",
      "3690/3690 [==============================] - 2s 586us/step - loss: 1.1233e-04 - val_loss: 1.2330e-04\n",
      "Epoch 1443/2000\n",
      "3690/3690 [==============================] - 2s 585us/step - loss: 8.2742e-05 - val_loss: 2.5884e-04\n",
      "Epoch 1444/2000\n",
      "3690/3690 [==============================] - 2s 587us/step - loss: 7.5320e-05 - val_loss: 1.8034e-04\n",
      "Epoch 1445/2000\n",
      "3690/3690 [==============================] - 2s 583us/step - loss: 9.3374e-05 - val_loss: 2.4859e-04\n",
      "Epoch 1446/2000\n",
      "3690/3690 [==============================] - 2s 576us/step - loss: 6.4347e-05 - val_loss: 1.3243e-04\n",
      "Epoch 1447/2000\n",
      "3690/3690 [==============================] - 2s 589us/step - loss: 6.9228e-05 - val_loss: 9.6600e-05\n",
      "Epoch 1448/2000\n",
      "3690/3690 [==============================] - 2s 616us/step - loss: 7.2269e-05 - val_loss: 1.0669e-04\n",
      "Epoch 1449/2000\n",
      "3690/3690 [==============================] - 2s 606us/step - loss: 8.5231e-05 - val_loss: 1.9307e-04\n",
      "Epoch 1450/2000\n",
      "3690/3690 [==============================] - 2s 626us/step - loss: 1.4120e-04 - val_loss: 1.3040e-04\n",
      "Epoch 1451/2000\n",
      "3690/3690 [==============================] - 2s 578us/step - loss: 1.2963e-04 - val_loss: 1.2022e-04\n",
      "Epoch 1452/2000\n",
      "3690/3690 [==============================] - 2s 582us/step - loss: 6.8114e-05 - val_loss: 1.4750e-04\n",
      "Epoch 1453/2000\n",
      "3690/3690 [==============================] - 2s 584us/step - loss: 6.7163e-05 - val_loss: 1.1774e-04\n",
      "Epoch 1454/2000\n",
      "3690/3690 [==============================] - 2s 570us/step - loss: 6.0939e-05 - val_loss: 1.6150e-04\n",
      "Epoch 1455/2000\n",
      "3690/3690 [==============================] - 2s 572us/step - loss: 6.4172e-05 - val_loss: 1.0170e-04\n",
      "Epoch 1456/2000\n",
      "3690/3690 [==============================] - 2s 568us/step - loss: 7.8208e-05 - val_loss: 2.4865e-04\n",
      "Epoch 1457/2000\n",
      "3690/3690 [==============================] - 2s 567us/step - loss: 8.7427e-05 - val_loss: 1.1809e-04\n",
      "Epoch 1458/2000\n",
      "3690/3690 [==============================] - 2s 568us/step - loss: 7.0213e-05 - val_loss: 1.8130e-04\n",
      "Epoch 1459/2000\n",
      "3690/3690 [==============================] - 2s 566us/step - loss: 6.2443e-05 - val_loss: 1.0775e-04\n",
      "Epoch 1460/2000\n",
      "3690/3690 [==============================] - 2s 561us/step - loss: 6.6763e-05 - val_loss: 9.7561e-05\n",
      "Epoch 1461/2000\n",
      "3690/3690 [==============================] - 2s 564us/step - loss: 6.5548e-05 - val_loss: 3.6859e-04\n",
      "Epoch 1462/2000\n",
      "3690/3690 [==============================] - 2s 574us/step - loss: 9.7887e-05 - val_loss: 1.2430e-04\n",
      "Epoch 1463/2000\n",
      "3690/3690 [==============================] - 2s 583us/step - loss: 8.6285e-05 - val_loss: 1.8327e-04\n",
      "Epoch 1464/2000\n",
      "3690/3690 [==============================] - 2s 615us/step - loss: 6.8839e-05 - val_loss: 1.1917e-04\n",
      "Epoch 1465/2000\n",
      "3690/3690 [==============================] - 2s 622us/step - loss: 7.4908e-05 - val_loss: 1.7902e-04\n",
      "Epoch 1466/2000\n",
      "3690/3690 [==============================] - 2s 565us/step - loss: 7.2861e-05 - val_loss: 1.5682e-04\n",
      "Epoch 1467/2000\n",
      "3690/3690 [==============================] - 2s 565us/step - loss: 6.4987e-05 - val_loss: 1.9625e-04\n",
      "Epoch 1468/2000\n",
      "3690/3690 [==============================] - 2s 571us/step - loss: 6.0464e-05 - val_loss: 1.2672e-04\n",
      "Epoch 1469/2000\n",
      "3690/3690 [==============================] - 2s 568us/step - loss: 5.8208e-05 - val_loss: 1.2724e-04\n",
      "Epoch 1470/2000\n",
      "3690/3690 [==============================] - 2s 570us/step - loss: 6.0201e-05 - val_loss: 1.2406e-04\n",
      "Epoch 1471/2000\n",
      "3690/3690 [==============================] - 2s 565us/step - loss: 6.4171e-05 - val_loss: 1.1000e-04\n",
      "Epoch 1472/2000\n",
      "3690/3690 [==============================] - 2s 566us/step - loss: 7.7639e-05 - val_loss: 2.5582e-04\n",
      "Epoch 1473/2000\n",
      "3690/3690 [==============================] - 2s 567us/step - loss: 8.1789e-05 - val_loss: 1.6195e-04\n",
      "Epoch 1474/2000\n",
      "3690/3690 [==============================] - 2s 572us/step - loss: 7.5785e-05 - val_loss: 2.1753e-04\n",
      "Epoch 1475/2000\n",
      "3690/3690 [==============================] - 2s 562us/step - loss: 6.7512e-05 - val_loss: 8.9643e-05\n",
      "Epoch 1476/2000\n",
      "3690/3690 [==============================] - 2s 566us/step - loss: 6.6176e-05 - val_loss: 1.3552e-04\n",
      "Epoch 1477/2000\n",
      "3690/3690 [==============================] - 2s 580us/step - loss: 5.9881e-05 - val_loss: 1.9034e-04\n",
      "Epoch 1478/2000\n",
      "3690/3690 [==============================] - 2s 577us/step - loss: 6.5155e-05 - val_loss: 9.3962e-05\n",
      "Epoch 1479/2000\n",
      "3690/3690 [==============================] - 2s 613us/step - loss: 7.4864e-05 - val_loss: 1.1579e-04\n",
      "Epoch 1480/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3690/3690 [==============================] - 2s 617us/step - loss: 6.7426e-05 - val_loss: 3.5438e-04\n",
      "Epoch 1481/2000\n",
      "3690/3690 [==============================] - 2s 579us/step - loss: 7.1577e-05 - val_loss: 1.1685e-04\n",
      "Epoch 1482/2000\n",
      "3690/3690 [==============================] - 2s 579us/step - loss: 6.7513e-05 - val_loss: 1.4165e-04\n",
      "Epoch 1483/2000\n",
      "3690/3690 [==============================] - 2s 584us/step - loss: 6.4105e-05 - val_loss: 2.1865e-04\n",
      "Epoch 1484/2000\n",
      "3690/3690 [==============================] - 2s 584us/step - loss: 6.2532e-05 - val_loss: 9.5172e-05\n",
      "Epoch 1485/2000\n",
      "3690/3690 [==============================] - 2s 586us/step - loss: 6.6311e-05 - val_loss: 1.1086e-04\n",
      "Epoch 1486/2000\n",
      "3690/3690 [==============================] - 2s 579us/step - loss: 6.5565e-05 - val_loss: 1.2651e-04\n",
      "Epoch 1487/2000\n",
      "3690/3690 [==============================] - 2s 576us/step - loss: 6.8848e-05 - val_loss: 2.2842e-04\n",
      "Epoch 1488/2000\n",
      "3690/3690 [==============================] - 2s 576us/step - loss: 7.2811e-05 - val_loss: 1.4372e-04\n",
      "Epoch 1489/2000\n",
      "3690/3690 [==============================] - 2s 577us/step - loss: 8.3377e-05 - val_loss: 3.1979e-04\n",
      "Epoch 1490/2000\n",
      "3690/3690 [==============================] - 2s 575us/step - loss: 6.9855e-05 - val_loss: 1.1209e-04\n",
      "Epoch 1491/2000\n",
      "3690/3690 [==============================] - 2s 574us/step - loss: 7.0475e-05 - val_loss: 3.5202e-04\n",
      "Epoch 1492/2000\n",
      "3690/3690 [==============================] - 2s 585us/step - loss: 7.1279e-05 - val_loss: 9.1698e-05\n",
      "Epoch 1493/2000\n",
      "3690/3690 [==============================] - 2s 600us/step - loss: 6.4308e-05 - val_loss: 1.7348e-04\n",
      "Epoch 1494/2000\n",
      "3690/3690 [==============================] - 2s 584us/step - loss: 7.6762e-05 - val_loss: 1.8322e-04\n",
      "Epoch 1495/2000\n",
      "3690/3690 [==============================] - 2s 577us/step - loss: 7.9396e-05 - val_loss: 1.2092e-04\n",
      "Epoch 1496/2000\n",
      "3690/3690 [==============================] - 2s 565us/step - loss: 7.6021e-05 - val_loss: 1.0626e-04\n",
      "Epoch 1497/2000\n",
      "3690/3690 [==============================] - 2s 574us/step - loss: 7.4308e-05 - val_loss: 1.4528e-04\n",
      "Epoch 1498/2000\n",
      "3690/3690 [==============================] - 2s 565us/step - loss: 6.4272e-05 - val_loss: 1.4578e-04\n",
      "Epoch 1499/2000\n",
      "3690/3690 [==============================] - 2s 568us/step - loss: 6.7854e-05 - val_loss: 1.2100e-04\n",
      "Epoch 1500/2000\n",
      "3690/3690 [==============================] - 2s 572us/step - loss: 7.1156e-05 - val_loss: 2.5276e-04\n",
      "Epoch 1501/2000\n",
      "3690/3690 [==============================] - 2s 559us/step - loss: 7.6623e-05 - val_loss: 1.5212e-04\n",
      "Epoch 1502/2000\n",
      "3690/3690 [==============================] - 2s 564us/step - loss: 6.3648e-05 - val_loss: 1.5421e-04\n",
      "Epoch 1503/2000\n",
      "3690/3690 [==============================] - 2s 563us/step - loss: 6.5484e-05 - val_loss: 1.1863e-04\n",
      "Epoch 1504/2000\n",
      "3690/3690 [==============================] - 2s 566us/step - loss: 8.9571e-05 - val_loss: 3.3554e-04\n",
      "Epoch 1505/2000\n",
      "3690/3690 [==============================] - 2s 562us/step - loss: 8.8730e-05 - val_loss: 9.8061e-05\n",
      "Epoch 1506/2000\n",
      "3690/3690 [==============================] - 2s 560us/step - loss: 7.4991e-05 - val_loss: 4.0632e-04\n",
      "Epoch 1507/2000\n",
      "3690/3690 [==============================] - 2s 606us/step - loss: 1.1730e-04 - val_loss: 1.3051e-04\n",
      "Epoch 1508/2000\n",
      "3690/3690 [==============================] - 2s 612us/step - loss: 1.2008e-04 - val_loss: 4.3117e-04\n",
      "Epoch 1509/2000\n",
      "3690/3690 [==============================] - 2s 592us/step - loss: 8.7466e-05 - val_loss: 1.4173e-04\n",
      "Epoch 1510/2000\n",
      "3690/3690 [==============================] - 2s 591us/step - loss: 8.6176e-05 - val_loss: 1.7382e-04\n",
      "Epoch 1511/2000\n",
      "3690/3690 [==============================] - 2s 579us/step - loss: 6.9679e-05 - val_loss: 3.5341e-04\n",
      "Epoch 1512/2000\n",
      "3690/3690 [==============================] - 2s 584us/step - loss: 9.7363e-05 - val_loss: 1.3515e-04\n",
      "Epoch 1513/2000\n",
      "3690/3690 [==============================] - 2s 574us/step - loss: 8.2105e-05 - val_loss: 1.3858e-04\n",
      "Epoch 1514/2000\n",
      "3690/3690 [==============================] - 2s 574us/step - loss: 6.8578e-05 - val_loss: 1.0786e-04\n",
      "Epoch 1515/2000\n",
      "3690/3690 [==============================] - 2s 574us/step - loss: 6.8376e-05 - val_loss: 1.1193e-04\n",
      "Epoch 1516/2000\n",
      "3690/3690 [==============================] - 2s 579us/step - loss: 6.5930e-05 - val_loss: 1.3025e-04\n",
      "Epoch 1517/2000\n",
      "3690/3690 [==============================] - 2s 573us/step - loss: 6.4985e-05 - val_loss: 1.1430e-04\n",
      "Epoch 1518/2000\n",
      "3690/3690 [==============================] - 2s 577us/step - loss: 6.8442e-05 - val_loss: 2.4182e-04\n",
      "Epoch 1519/2000\n",
      "3690/3690 [==============================] - 2s 584us/step - loss: 6.9942e-05 - val_loss: 1.5563e-04\n",
      "Epoch 1520/2000\n",
      "3690/3690 [==============================] - 2s 585us/step - loss: 6.4224e-05 - val_loss: 1.3070e-04\n",
      "Epoch 1521/2000\n",
      "3690/3690 [==============================] - 2s 607us/step - loss: 6.1852e-05 - val_loss: 1.4452e-04\n",
      "Epoch 1522/2000\n",
      "3690/3690 [==============================] - 2s 616us/step - loss: 6.4812e-05 - val_loss: 1.5312e-04\n",
      "Epoch 1523/2000\n",
      "3690/3690 [==============================] - 2s 575us/step - loss: 6.6740e-05 - val_loss: 4.9311e-04\n",
      "Epoch 1524/2000\n",
      "3690/3690 [==============================] - 2s 572us/step - loss: 7.7944e-05 - val_loss: 1.1475e-04\n",
      "Epoch 1525/2000\n",
      "3690/3690 [==============================] - 2s 569us/step - loss: 7.6448e-05 - val_loss: 1.1982e-04\n",
      "Epoch 1526/2000\n",
      "3690/3690 [==============================] - 2s 571us/step - loss: 6.2772e-05 - val_loss: 1.3970e-04\n",
      "Epoch 1527/2000\n",
      "3690/3690 [==============================] - 2s 572us/step - loss: 6.8170e-05 - val_loss: 1.6684e-04\n",
      "Epoch 1528/2000\n",
      "3690/3690 [==============================] - 2s 566us/step - loss: 6.2756e-05 - val_loss: 1.4840e-04\n",
      "Epoch 1529/2000\n",
      "3690/3690 [==============================] - 2s 567us/step - loss: 6.1762e-05 - val_loss: 1.4603e-04\n",
      "Epoch 1530/2000\n",
      "3690/3690 [==============================] - 2s 568us/step - loss: 6.3468e-05 - val_loss: 1.8331e-04\n",
      "Epoch 1531/2000\n",
      "3690/3690 [==============================] - 2s 568us/step - loss: 6.4524e-05 - val_loss: 1.3056e-04\n",
      "Epoch 1532/2000\n",
      "3690/3690 [==============================] - 2s 568us/step - loss: 6.1186e-05 - val_loss: 1.2858e-04\n",
      "Epoch 1533/2000\n",
      "3690/3690 [==============================] - 2s 579us/step - loss: 7.4752e-05 - val_loss: 1.5116e-04\n",
      "Epoch 1534/2000\n",
      "3690/3690 [==============================] - 2s 566us/step - loss: 6.1228e-05 - val_loss: 2.1456e-04\n",
      "Epoch 1535/2000\n",
      "3690/3690 [==============================] - 2s 568us/step - loss: 6.8049e-05 - val_loss: 1.0553e-04\n",
      "Epoch 1536/2000\n",
      "3690/3690 [==============================] - 2s 627us/step - loss: 6.4429e-05 - val_loss: 2.7732e-04\n",
      "Epoch 1537/2000\n",
      "3690/3690 [==============================] - 2s 581us/step - loss: 6.0682e-05 - val_loss: 1.1589e-04\n",
      "Epoch 1538/2000\n",
      "3690/3690 [==============================] - 2s 578us/step - loss: 7.0087e-05 - val_loss: 2.7210e-04\n",
      "Epoch 1539/2000\n",
      "3690/3690 [==============================] - 2s 578us/step - loss: 7.5473e-05 - val_loss: 1.5121e-04\n",
      "Epoch 1540/2000\n",
      "3690/3690 [==============================] - 2s 576us/step - loss: 7.0190e-05 - val_loss: 3.5523e-04\n",
      "Epoch 1541/2000\n",
      "3690/3690 [==============================] - 2s 575us/step - loss: 7.1356e-05 - val_loss: 1.7616e-04\n",
      "Epoch 1542/2000\n",
      "3690/3690 [==============================] - 2s 574us/step - loss: 6.0250e-05 - val_loss: 1.2566e-04\n",
      "Epoch 1543/2000\n",
      "3690/3690 [==============================] - 2s 576us/step - loss: 6.2523e-05 - val_loss: 1.1557e-04\n",
      "Epoch 1544/2000\n",
      "3690/3690 [==============================] - 2s 574us/step - loss: 6.5800e-05 - val_loss: 9.2050e-05\n",
      "Epoch 1545/2000\n",
      "3690/3690 [==============================] - 2s 573us/step - loss: 7.2856e-05 - val_loss: 1.0024e-04\n",
      "Epoch 1546/2000\n",
      "3690/3690 [==============================] - 2s 574us/step - loss: 8.2821e-05 - val_loss: 2.9809e-04\n",
      "Epoch 1547/2000\n",
      "3690/3690 [==============================] - 2s 582us/step - loss: 7.5104e-05 - val_loss: 1.3005e-04\n",
      "Epoch 1548/2000\n",
      "3690/3690 [==============================] - 2s 566us/step - loss: 7.5494e-05 - val_loss: 1.7330e-04\n",
      "Epoch 1549/2000\n",
      "3690/3690 [==============================] - 2s 571us/step - loss: 7.7357e-05 - val_loss: 1.5197e-04\n",
      "Epoch 1550/2000\n",
      "3690/3690 [==============================] - 2s 606us/step - loss: 9.4030e-05 - val_loss: 4.5891e-04\n",
      "Epoch 1551/2000\n",
      "3690/3690 [==============================] - 2s 574us/step - loss: 8.4519e-05 - val_loss: 1.0652e-04\n",
      "Epoch 1552/2000\n",
      "3690/3690 [==============================] - 2s 577us/step - loss: 7.5432e-05 - val_loss: 1.1870e-04\n",
      "Epoch 1553/2000\n",
      "3690/3690 [==============================] - 2s 569us/step - loss: 6.5624e-05 - val_loss: 1.0958e-04\n",
      "Epoch 1554/2000\n",
      "3690/3690 [==============================] - 2s 579us/step - loss: 6.7656e-05 - val_loss: 3.4727e-04\n",
      "Epoch 1555/2000\n",
      "3690/3690 [==============================] - 2s 574us/step - loss: 7.0458e-05 - val_loss: 1.1643e-04\n",
      "Epoch 1556/2000\n",
      "3690/3690 [==============================] - 2s 590us/step - loss: 9.3343e-05 - val_loss: 1.5547e-04\n",
      "Epoch 1557/2000\n",
      "3690/3690 [==============================] - 2s 571us/step - loss: 7.0370e-05 - val_loss: 1.0098e-04\n",
      "Epoch 1558/2000\n",
      "3690/3690 [==============================] - 2s 578us/step - loss: 7.1226e-05 - val_loss: 4.4527e-04\n",
      "Epoch 1559/2000\n",
      "3690/3690 [==============================] - 2s 563us/step - loss: 9.4540e-05 - val_loss: 1.1003e-04\n",
      "Epoch 1560/2000\n",
      "3690/3690 [==============================] - 2s 572us/step - loss: 7.8560e-05 - val_loss: 1.7406e-04\n",
      "Epoch 1561/2000\n",
      "3690/3690 [==============================] - 2s 581us/step - loss: 6.4194e-05 - val_loss: 1.1317e-04\n",
      "Epoch 1562/2000\n",
      "3690/3690 [==============================] - 2s 564us/step - loss: 9.1689e-05 - val_loss: 2.5768e-04\n",
      "Epoch 1563/2000\n",
      "3690/3690 [==============================] - 2s 581us/step - loss: 7.2586e-05 - val_loss: 1.0499e-04\n",
      "Epoch 1564/2000\n",
      "3690/3690 [==============================] - 2s 584us/step - loss: 6.9352e-05 - val_loss: 1.8871e-04\n",
      "Epoch 1565/2000\n",
      "3690/3690 [==============================] - 2s 598us/step - loss: 6.8820e-05 - val_loss: 2.7032e-04\n",
      "Epoch 1566/2000\n",
      "3690/3690 [==============================] - 2s 573us/step - loss: 7.2180e-05 - val_loss: 1.0258e-04\n",
      "Epoch 1567/2000\n",
      "3690/3690 [==============================] - 2s 637us/step - loss: 6.5716e-05 - val_loss: 1.5184e-04\n",
      "Epoch 1568/2000\n",
      "3690/3690 [==============================] - 2s 596us/step - loss: 6.9099e-05 - val_loss: 1.2730e-04\n",
      "Epoch 1569/2000\n",
      "3690/3690 [==============================] - 2s 563us/step - loss: 9.7734e-05 - val_loss: 4.5801e-04\n",
      "Epoch 1570/2000\n",
      "3690/3690 [==============================] - 2s 552us/step - loss: 9.2973e-05 - val_loss: 1.0730e-04\n",
      "Epoch 1571/2000\n",
      "3690/3690 [==============================] - 2s 554us/step - loss: 7.7309e-05 - val_loss: 2.1950e-04\n",
      "Epoch 1572/2000\n",
      "3690/3690 [==============================] - 2s 557us/step - loss: 7.1737e-05 - val_loss: 1.1880e-04\n",
      "Epoch 1573/2000\n",
      "3690/3690 [==============================] - 2s 615us/step - loss: 7.1621e-05 - val_loss: 9.7745e-05\n",
      "Epoch 1574/2000\n",
      "3690/3690 [==============================] - 2s 581us/step - loss: 7.0086e-05 - val_loss: 2.4931e-04\n",
      "Epoch 1575/2000\n",
      "3690/3690 [==============================] - 2s 580us/step - loss: 7.3430e-05 - val_loss: 3.0347e-04\n",
      "Epoch 1576/2000\n",
      "3690/3690 [==============================] - 2s 592us/step - loss: 7.6454e-05 - val_loss: 1.8849e-04\n",
      "Epoch 1577/2000\n",
      "3690/3690 [==============================] - 2s 587us/step - loss: 8.3625e-05 - val_loss: 9.4859e-05\n",
      "Epoch 1578/2000\n",
      "3690/3690 [==============================] - 2s 593us/step - loss: 8.1015e-05 - val_loss: 2.9061e-04\n",
      "Epoch 1579/2000\n",
      "3690/3690 [==============================] - 2s 567us/step - loss: 8.2153e-05 - val_loss: 1.2996e-04\n",
      "Epoch 1580/2000\n",
      "3690/3690 [==============================] - 2s 559us/step - loss: 6.2671e-05 - val_loss: 1.0738e-04\n",
      "Epoch 1581/2000\n",
      "3690/3690 [==============================] - 2s 562us/step - loss: 6.1500e-05 - val_loss: 1.5256e-04\n",
      "Epoch 1582/2000\n",
      "3690/3690 [==============================] - 2s 568us/step - loss: 6.2981e-05 - val_loss: 1.0511e-04\n",
      "Epoch 1583/2000\n",
      "3690/3690 [==============================] - 2s 572us/step - loss: 5.7981e-05 - val_loss: 9.3763e-05\n",
      "Epoch 1584/2000\n",
      "3690/3690 [==============================] - 2s 569us/step - loss: 6.8907e-05 - val_loss: 1.0304e-04\n",
      "Epoch 1585/2000\n",
      "3690/3690 [==============================] - 2s 602us/step - loss: 7.4807e-05 - val_loss: 2.2729e-04\n",
      "Epoch 1586/2000\n",
      "3690/3690 [==============================] - 2s 565us/step - loss: 6.5769e-05 - val_loss: 1.2287e-04\n",
      "Epoch 1587/2000\n",
      "3690/3690 [==============================] - 2s 557us/step - loss: 6.0935e-05 - val_loss: 1.0696e-04\n",
      "Epoch 1588/2000\n",
      "3690/3690 [==============================] - 2s 566us/step - loss: 6.6575e-05 - val_loss: 9.2024e-05\n",
      "Epoch 1589/2000\n",
      "3690/3690 [==============================] - 2s 561us/step - loss: 6.4638e-05 - val_loss: 1.2376e-04\n",
      "Epoch 1590/2000\n",
      "3690/3690 [==============================] - 2s 561us/step - loss: 6.0347e-05 - val_loss: 1.3216e-04\n",
      "Epoch 1591/2000\n",
      "3690/3690 [==============================] - 2s 558us/step - loss: 6.0831e-05 - val_loss: 1.3592e-04\n",
      "Epoch 1592/2000\n",
      "3690/3690 [==============================] - 2s 555us/step - loss: 6.2045e-05 - val_loss: 1.5260e-04\n",
      "Epoch 1593/2000\n",
      "3690/3690 [==============================] - 2s 566us/step - loss: 6.1956e-05 - val_loss: 1.0487e-04\n",
      "Epoch 1594/2000\n",
      "3690/3690 [==============================] - 2s 558us/step - loss: 6.2901e-05 - val_loss: 1.8146e-04\n",
      "Epoch 1595/2000\n",
      "3690/3690 [==============================] - 2s 555us/step - loss: 6.3526e-05 - val_loss: 1.9877e-04\n",
      "Epoch 1596/2000\n",
      "3690/3690 [==============================] - 2s 557us/step - loss: 7.1701e-05 - val_loss: 1.3822e-04\n",
      "Epoch 1597/2000\n",
      "3690/3690 [==============================] - 2s 585us/step - loss: 6.5731e-05 - val_loss: 1.0773e-04\n",
      "Epoch 1598/2000\n",
      "3690/3690 [==============================] - 2s 561us/step - loss: 8.5002e-05 - val_loss: 1.6560e-04\n",
      "Epoch 1599/2000\n",
      "3690/3690 [==============================] - 2s 556us/step - loss: 7.1586e-05 - val_loss: 1.2153e-04\n",
      "Epoch 1600/2000\n",
      "3690/3690 [==============================] - 2s 556us/step - loss: 7.0610e-05 - val_loss: 1.6836e-04\n",
      "Epoch 1601/2000\n",
      "3690/3690 [==============================] - 2s 553us/step - loss: 7.4004e-05 - val_loss: 2.6400e-04\n",
      "Epoch 1602/2000\n",
      "3690/3690 [==============================] - 2s 558us/step - loss: 8.7609e-05 - val_loss: 9.7159e-05\n",
      "Epoch 1603/2000\n",
      "3690/3690 [==============================] - 2s 572us/step - loss: 6.9113e-05 - val_loss: 1.8585e-04\n",
      "Epoch 1604/2000\n",
      "3690/3690 [==============================] - 2s 559us/step - loss: 6.7852e-05 - val_loss: 1.4521e-04\n",
      "Epoch 1605/2000\n",
      "3690/3690 [==============================] - 2s 570us/step - loss: 8.7501e-05 - val_loss: 2.8730e-04\n",
      "Epoch 1606/2000\n",
      "3690/3690 [==============================] - 2s 561us/step - loss: 8.7002e-05 - val_loss: 1.3808e-04\n",
      "Epoch 1607/2000\n",
      "3690/3690 [==============================] - 2s 562us/step - loss: 6.8472e-05 - val_loss: 4.8429e-04\n",
      "Epoch 1608/2000\n",
      "3690/3690 [==============================] - 2s 557us/step - loss: 7.6567e-05 - val_loss: 1.2676e-04\n",
      "Epoch 1609/2000\n",
      "3690/3690 [==============================] - 2s 553us/step - loss: 8.4824e-05 - val_loss: 2.0609e-04\n",
      "Epoch 1610/2000\n",
      "3690/3690 [==============================] - 2s 555us/step - loss: 7.5725e-05 - val_loss: 1.0668e-04\n",
      "Epoch 1611/2000\n",
      "3690/3690 [==============================] - 2s 557us/step - loss: 8.3468e-05 - val_loss: 1.3929e-04\n",
      "Epoch 1612/2000\n",
      "3690/3690 [==============================] - 2s 554us/step - loss: 8.5347e-05 - val_loss: 9.9452e-05\n",
      "Epoch 1613/2000\n",
      "3690/3690 [==============================] - 2s 555us/step - loss: 7.4825e-05 - val_loss: 3.1321e-04\n",
      "Epoch 1614/2000\n",
      "3690/3690 [==============================] - 2s 562us/step - loss: 8.0863e-05 - val_loss: 1.0568e-04\n",
      "Epoch 1615/2000\n",
      "3690/3690 [==============================] - 2s 555us/step - loss: 6.7660e-05 - val_loss: 1.3004e-04\n",
      "Epoch 1616/2000\n",
      "3690/3690 [==============================] - 2s 556us/step - loss: 5.8771e-05 - val_loss: 1.1812e-04\n",
      "Epoch 1617/2000\n",
      "3690/3690 [==============================] - 2s 565us/step - loss: 6.6184e-05 - val_loss: 2.4041e-04\n",
      "Epoch 1618/2000\n",
      "3690/3690 [==============================] - 2s 557us/step - loss: 7.1060e-05 - val_loss: 1.0408e-04\n",
      "Epoch 1619/2000\n",
      "3690/3690 [==============================] - 2s 554us/step - loss: 6.6577e-05 - val_loss: 1.0939e-04\n",
      "Epoch 1620/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3690/3690 [==============================] - 2s 555us/step - loss: 7.0969e-05 - val_loss: 1.0652e-04\n",
      "Epoch 1621/2000\n",
      "3690/3690 [==============================] - 2s 553us/step - loss: 5.8236e-05 - val_loss: 1.3435e-04\n",
      "Epoch 1622/2000\n",
      "3690/3690 [==============================] - 2s 554us/step - loss: 6.6101e-05 - val_loss: 1.0816e-04\n",
      "Epoch 1623/2000\n",
      "3690/3690 [==============================] - 2s 554us/step - loss: 6.5637e-05 - val_loss: 2.0144e-04\n",
      "Epoch 1624/2000\n",
      "3690/3690 [==============================] - 2s 554us/step - loss: 6.4455e-05 - val_loss: 1.2902e-04\n",
      "Epoch 1625/2000\n",
      "3690/3690 [==============================] - 2s 559us/step - loss: 6.7545e-05 - val_loss: 1.1238e-04\n",
      "Epoch 1626/2000\n",
      "3690/3690 [==============================] - 2s 557us/step - loss: 6.4183e-05 - val_loss: 4.3876e-04\n",
      "Epoch 1627/2000\n",
      "3690/3690 [==============================] - 2s 557us/step - loss: 8.1994e-05 - val_loss: 1.3472e-04\n",
      "Epoch 1628/2000\n",
      "3690/3690 [==============================] - 2s 561us/step - loss: 8.2376e-05 - val_loss: 1.1062e-04\n",
      "Epoch 1629/2000\n",
      "3690/3690 [==============================] - 2s 562us/step - loss: 8.9530e-05 - val_loss: 4.3000e-04\n",
      "Epoch 1630/2000\n",
      "3690/3690 [==============================] - 2s 561us/step - loss: 9.0857e-05 - val_loss: 1.3586e-04\n",
      "Epoch 1631/2000\n",
      "3690/3690 [==============================] - 2s 556us/step - loss: 8.3324e-05 - val_loss: 1.0217e-04\n",
      "Epoch 1632/2000\n",
      "3690/3690 [==============================] - 2s 561us/step - loss: 7.6737e-05 - val_loss: 3.4236e-04\n",
      "Epoch 1633/2000\n",
      "3690/3690 [==============================] - 2s 555us/step - loss: 7.4400e-05 - val_loss: 1.3457e-04\n",
      "Epoch 1634/2000\n",
      "3690/3690 [==============================] - 2s 553us/step - loss: 6.6880e-05 - val_loss: 1.4430e-04\n",
      "Epoch 1635/2000\n",
      "3690/3690 [==============================] - 2s 557us/step - loss: 7.6728e-05 - val_loss: 1.8320e-04\n",
      "Epoch 1636/2000\n",
      "3690/3690 [==============================] - 2s 554us/step - loss: 7.1876e-05 - val_loss: 3.3544e-04\n",
      "Epoch 1637/2000\n",
      "3690/3690 [==============================] - 2s 559us/step - loss: 7.7744e-05 - val_loss: 1.7269e-04\n",
      "Epoch 1638/2000\n",
      "3690/3690 [==============================] - 2s 560us/step - loss: 7.8016e-05 - val_loss: 2.2473e-04\n",
      "Epoch 1639/2000\n",
      "3690/3690 [==============================] - 2s 569us/step - loss: 6.7153e-05 - val_loss: 1.8765e-04\n",
      "Epoch 1640/2000\n",
      "3690/3690 [==============================] - 2s 558us/step - loss: 7.2610e-05 - val_loss: 1.0817e-04\n",
      "Epoch 1641/2000\n",
      "3690/3690 [==============================] - 2s 561us/step - loss: 6.2051e-05 - val_loss: 1.2718e-04\n",
      "Epoch 1642/2000\n",
      "3690/3690 [==============================] - 2s 564us/step - loss: 6.0727e-05 - val_loss: 1.3568e-04\n",
      "Epoch 1643/2000\n",
      "3690/3690 [==============================] - 2s 560us/step - loss: 6.3901e-05 - val_loss: 2.0781e-04\n",
      "Epoch 1644/2000\n",
      "3690/3690 [==============================] - 2s 557us/step - loss: 6.3221e-05 - val_loss: 1.5747e-04\n",
      "Epoch 1645/2000\n",
      "3690/3690 [==============================] - 2s 554us/step - loss: 6.1466e-05 - val_loss: 2.9524e-04\n",
      "Epoch 1646/2000\n",
      "3690/3690 [==============================] - 2s 562us/step - loss: 6.8629e-05 - val_loss: 1.1055e-04\n",
      "Epoch 1647/2000\n",
      "3690/3690 [==============================] - 2s 567us/step - loss: 8.0075e-05 - val_loss: 1.3240e-04\n",
      "Epoch 1648/2000\n",
      "3690/3690 [==============================] - 2s 555us/step - loss: 8.2478e-05 - val_loss: 2.6679e-04\n",
      "Epoch 1649/2000\n",
      "3690/3690 [==============================] - 2s 555us/step - loss: 6.6866e-05 - val_loss: 1.5649e-04\n",
      "Epoch 1650/2000\n",
      "3690/3690 [==============================] - 2s 552us/step - loss: 7.8280e-05 - val_loss: 9.9913e-05\n",
      "Epoch 1651/2000\n",
      "3690/3690 [==============================] - 2s 555us/step - loss: 8.5801e-05 - val_loss: 2.7083e-04\n",
      "Epoch 1652/2000\n",
      "3690/3690 [==============================] - 2s 556us/step - loss: 8.1256e-05 - val_loss: 1.6360e-04\n",
      "Epoch 1653/2000\n",
      "3690/3690 [==============================] - 2s 555us/step - loss: 8.9248e-05 - val_loss: 1.9681e-04\n",
      "Epoch 1654/2000\n",
      "3690/3690 [==============================] - 2s 557us/step - loss: 7.4105e-05 - val_loss: 1.4153e-04\n",
      "Epoch 1655/2000\n",
      "3690/3690 [==============================] - 2s 557us/step - loss: 6.0308e-05 - val_loss: 1.9447e-04\n",
      "Epoch 1656/2000\n",
      "3690/3690 [==============================] - 2s 552us/step - loss: 6.4296e-05 - val_loss: 1.1652e-04\n",
      "Epoch 1657/2000\n",
      "3690/3690 [==============================] - 2s 555us/step - loss: 6.7533e-05 - val_loss: 2.3369e-04\n",
      "Epoch 1658/2000\n",
      "3690/3690 [==============================] - 2s 555us/step - loss: 6.2733e-05 - val_loss: 1.9096e-04\n",
      "Epoch 1659/2000\n",
      "3690/3690 [==============================] - 2s 559us/step - loss: 6.4057e-05 - val_loss: 1.4966e-04\n",
      "Epoch 1660/2000\n",
      "3690/3690 [==============================] - 2s 558us/step - loss: 6.0866e-05 - val_loss: 1.0150e-04\n",
      "Epoch 1661/2000\n",
      "3690/3690 [==============================] - 2s 555us/step - loss: 6.6987e-05 - val_loss: 1.4155e-04\n",
      "Epoch 1662/2000\n",
      "3690/3690 [==============================] - 2s 552us/step - loss: 7.6822e-05 - val_loss: 1.1540e-04\n",
      "Epoch 1663/2000\n",
      "3690/3690 [==============================] - 2s 553us/step - loss: 6.6764e-05 - val_loss: 1.1144e-04\n",
      "Epoch 1664/2000\n",
      "3690/3690 [==============================] - 2s 555us/step - loss: 6.7244e-05 - val_loss: 2.4061e-04\n",
      "Epoch 1665/2000\n",
      "3690/3690 [==============================] - 2s 557us/step - loss: 6.0890e-05 - val_loss: 2.1854e-04\n",
      "Epoch 1666/2000\n",
      "3690/3690 [==============================] - 2s 554us/step - loss: 6.4955e-05 - val_loss: 1.1529e-04\n",
      "Epoch 1667/2000\n",
      "3690/3690 [==============================] - 2s 555us/step - loss: 6.5463e-05 - val_loss: 2.5580e-04\n",
      "Epoch 1668/2000\n",
      "3690/3690 [==============================] - 2s 555us/step - loss: 6.4935e-05 - val_loss: 1.3386e-04\n",
      "Epoch 1669/2000\n",
      "3690/3690 [==============================] - 2s 556us/step - loss: 7.2075e-05 - val_loss: 1.3080e-04\n",
      "Epoch 1670/2000\n",
      "3690/3690 [==============================] - 2s 555us/step - loss: 6.3724e-05 - val_loss: 2.1016e-04\n",
      "Epoch 1671/2000\n",
      "3690/3690 [==============================] - 2s 552us/step - loss: 6.2712e-05 - val_loss: 1.0407e-04\n",
      "Epoch 1672/2000\n",
      "3690/3690 [==============================] - 2s 554us/step - loss: 5.9632e-05 - val_loss: 2.0954e-04\n",
      "Epoch 1673/2000\n",
      "3690/3690 [==============================] - 2s 552us/step - loss: 6.1624e-05 - val_loss: 1.3859e-04\n",
      "Epoch 1674/2000\n",
      "3690/3690 [==============================] - 2s 555us/step - loss: 6.5259e-05 - val_loss: 1.7896e-04\n",
      "Epoch 1675/2000\n",
      "3690/3690 [==============================] - 2s 597us/step - loss: 6.6013e-05 - val_loss: 1.5611e-04\n",
      "Epoch 1676/2000\n",
      "3690/3690 [==============================] - 2s 556us/step - loss: 7.4584e-05 - val_loss: 1.3588e-04\n",
      "Epoch 1677/2000\n",
      "3690/3690 [==============================] - 2s 566us/step - loss: 7.7614e-05 - val_loss: 1.9401e-04\n",
      "Epoch 1678/2000\n",
      "3690/3690 [==============================] - 2s 558us/step - loss: 6.2768e-05 - val_loss: 2.2368e-04\n",
      "Epoch 1679/2000\n",
      "3690/3690 [==============================] - 2s 554us/step - loss: 6.6861e-05 - val_loss: 2.5183e-04\n",
      "Epoch 1680/2000\n",
      "3690/3690 [==============================] - 2s 554us/step - loss: 6.9117e-05 - val_loss: 1.4406e-04\n",
      "Epoch 1681/2000\n",
      "3690/3690 [==============================] - 2s 554us/step - loss: 6.6232e-05 - val_loss: 2.7910e-04\n",
      "Epoch 1682/2000\n",
      "3690/3690 [==============================] - 2s 555us/step - loss: 6.4114e-05 - val_loss: 1.6599e-04\n",
      "Epoch 1683/2000\n",
      "3690/3690 [==============================] - 2s 559us/step - loss: 6.1625e-05 - val_loss: 2.1177e-04\n",
      "Epoch 1684/2000\n",
      "3690/3690 [==============================] - 2s 555us/step - loss: 5.8911e-05 - val_loss: 2.0308e-04\n",
      "Epoch 1685/2000\n",
      "3690/3690 [==============================] - 2s 564us/step - loss: 6.1527e-05 - val_loss: 1.2602e-04\n",
      "Epoch 1686/2000\n",
      "3690/3690 [==============================] - 2s 567us/step - loss: 6.4785e-05 - val_loss: 2.1161e-04\n",
      "Epoch 1687/2000\n",
      "3690/3690 [==============================] - 2s 556us/step - loss: 6.7059e-05 - val_loss: 1.3142e-04\n",
      "Epoch 1688/2000\n",
      "3690/3690 [==============================] - 2s 555us/step - loss: 6.3418e-05 - val_loss: 2.6821e-04\n",
      "Epoch 1689/2000\n",
      "3690/3690 [==============================] - 2s 554us/step - loss: 6.1733e-05 - val_loss: 1.4070e-04\n",
      "Epoch 1690/2000\n",
      "3690/3690 [==============================] - 2s 560us/step - loss: 6.6306e-05 - val_loss: 2.7471e-04\n",
      "Epoch 1691/2000\n",
      "3690/3690 [==============================] - 2s 563us/step - loss: 6.6360e-05 - val_loss: 1.3207e-04\n",
      "Epoch 1692/2000\n",
      "3690/3690 [==============================] - 2s 564us/step - loss: 8.1460e-05 - val_loss: 2.1865e-04\n",
      "Epoch 1693/2000\n",
      "3690/3690 [==============================] - 2s 562us/step - loss: 8.5472e-05 - val_loss: 2.6910e-04\n",
      "Epoch 1694/2000\n",
      "3690/3690 [==============================] - 2s 562us/step - loss: 6.1512e-05 - val_loss: 9.8684e-05\n",
      "Epoch 1695/2000\n",
      "3690/3690 [==============================] - 2s 555us/step - loss: 6.8996e-05 - val_loss: 1.1523e-04\n",
      "Epoch 1696/2000\n",
      "3690/3690 [==============================] - 2s 563us/step - loss: 6.8593e-05 - val_loss: 2.8270e-04\n",
      "Epoch 1697/2000\n",
      "3690/3690 [==============================] - 2s 559us/step - loss: 7.4506e-05 - val_loss: 1.3304e-04\n",
      "Epoch 1698/2000\n",
      "3690/3690 [==============================] - 2s 558us/step - loss: 6.6690e-05 - val_loss: 2.3575e-04\n",
      "Epoch 1699/2000\n",
      "3690/3690 [==============================] - 2s 554us/step - loss: 6.9121e-05 - val_loss: 2.2712e-04\n",
      "Epoch 1700/2000\n",
      "3690/3690 [==============================] - 2s 555us/step - loss: 7.2254e-05 - val_loss: 1.1549e-04\n",
      "Epoch 1701/2000\n",
      "3690/3690 [==============================] - 2s 559us/step - loss: 7.7258e-05 - val_loss: 2.6034e-04\n",
      "Epoch 1702/2000\n",
      "3690/3690 [==============================] - 2s 557us/step - loss: 6.6237e-05 - val_loss: 4.8164e-04\n",
      "Epoch 1703/2000\n",
      "3690/3690 [==============================] - 2s 560us/step - loss: 7.4407e-05 - val_loss: 1.1670e-04\n",
      "Epoch 1704/2000\n",
      "3690/3690 [==============================] - 2s 550us/step - loss: 6.5079e-05 - val_loss: 1.9397e-04\n",
      "Epoch 1705/2000\n",
      "3690/3690 [==============================] - 2s 554us/step - loss: 6.2303e-05 - val_loss: 1.8036e-04\n",
      "Epoch 1706/2000\n",
      "3690/3690 [==============================] - 2s 556us/step - loss: 6.2796e-05 - val_loss: 1.0621e-04\n",
      "Epoch 1707/2000\n",
      "3690/3690 [==============================] - 2s 554us/step - loss: 6.3700e-05 - val_loss: 2.2121e-04\n",
      "Epoch 1708/2000\n",
      "3690/3690 [==============================] - 2s 555us/step - loss: 6.3988e-05 - val_loss: 1.3428e-04\n",
      "Epoch 1709/2000\n",
      "3690/3690 [==============================] - 2s 552us/step - loss: 6.8249e-05 - val_loss: 1.4101e-04\n",
      "Epoch 1710/2000\n",
      "3690/3690 [==============================] - 2s 555us/step - loss: 6.9098e-05 - val_loss: 1.5437e-04\n",
      "Epoch 1711/2000\n",
      "3690/3690 [==============================] - 2s 553us/step - loss: 7.4629e-05 - val_loss: 1.5231e-04\n",
      "Epoch 1712/2000\n",
      "3690/3690 [==============================] - 2s 559us/step - loss: 7.3754e-05 - val_loss: 1.3215e-04\n",
      "Epoch 1713/2000\n",
      "3690/3690 [==============================] - 2s 554us/step - loss: 7.4274e-05 - val_loss: 1.3753e-04\n",
      "Epoch 1714/2000\n",
      "3690/3690 [==============================] - 2s 555us/step - loss: 6.7321e-05 - val_loss: 2.9482e-04\n",
      "Epoch 1715/2000\n",
      "3690/3690 [==============================] - 2s 556us/step - loss: 6.4637e-05 - val_loss: 1.2172e-04\n",
      "Epoch 1716/2000\n",
      "3690/3690 [==============================] - 2s 557us/step - loss: 5.9318e-05 - val_loss: 1.6558e-04\n",
      "Epoch 1717/2000\n",
      "3690/3690 [==============================] - 2s 558us/step - loss: 5.8423e-05 - val_loss: 1.2218e-04\n",
      "Epoch 1718/2000\n",
      "3690/3690 [==============================] - 2s 554us/step - loss: 5.9116e-05 - val_loss: 1.2910e-04\n",
      "Epoch 1719/2000\n",
      "3690/3690 [==============================] - 2s 557us/step - loss: 6.4986e-05 - val_loss: 2.6331e-04\n",
      "Epoch 1720/2000\n",
      "3690/3690 [==============================] - 2s 557us/step - loss: 6.3619e-05 - val_loss: 1.2674e-04\n",
      "Epoch 1721/2000\n",
      "3690/3690 [==============================] - 2s 555us/step - loss: 6.6663e-05 - val_loss: 1.5789e-04\n",
      "Epoch 1722/2000\n",
      "3690/3690 [==============================] - 2s 555us/step - loss: 6.1123e-05 - val_loss: 1.6819e-04\n",
      "Epoch 1723/2000\n",
      "3690/3690 [==============================] - 2s 553us/step - loss: 6.4754e-05 - val_loss: 2.5944e-04\n",
      "Epoch 1724/2000\n",
      "3690/3690 [==============================] - 2s 556us/step - loss: 6.1332e-05 - val_loss: 1.6017e-04\n",
      "Epoch 1725/2000\n",
      "3690/3690 [==============================] - 2s 549us/step - loss: 5.9512e-05 - val_loss: 1.0463e-04\n",
      "Epoch 1726/2000\n",
      "3690/3690 [==============================] - 2s 552us/step - loss: 5.5434e-05 - val_loss: 1.1146e-04\n",
      "Epoch 1727/2000\n",
      "3690/3690 [==============================] - 2s 568us/step - loss: 8.1316e-05 - val_loss: 3.5240e-04\n",
      "Epoch 1728/2000\n",
      "3690/3690 [==============================] - 2s 610us/step - loss: 7.5814e-05 - val_loss: 1.9509e-04\n",
      "Epoch 1729/2000\n",
      "3690/3690 [==============================] - 2s 571us/step - loss: 7.5655e-05 - val_loss: 1.9446e-04\n",
      "Epoch 1730/2000\n",
      "3690/3690 [==============================] - 2s 589us/step - loss: 7.7905e-05 - val_loss: 1.3228e-04\n",
      "Epoch 1731/2000\n",
      "3690/3690 [==============================] - 2s 562us/step - loss: 6.5556e-05 - val_loss: 1.1749e-04\n",
      "Epoch 1732/2000\n",
      "3690/3690 [==============================] - 2s 558us/step - loss: 6.5149e-05 - val_loss: 1.1364e-04\n",
      "Epoch 1733/2000\n",
      "3690/3690 [==============================] - 2s 557us/step - loss: 5.7712e-05 - val_loss: 2.1528e-04\n",
      "Epoch 1734/2000\n",
      "3690/3690 [==============================] - 2s 567us/step - loss: 6.5672e-05 - val_loss: 1.9390e-04\n",
      "Epoch 1735/2000\n",
      "3690/3690 [==============================] - 2s 556us/step - loss: 6.2275e-05 - val_loss: 8.9526e-05\n",
      "Epoch 1736/2000\n",
      "3690/3690 [==============================] - 2s 556us/step - loss: 5.9858e-05 - val_loss: 1.4419e-04\n",
      "Epoch 1737/2000\n",
      "3690/3690 [==============================] - 2s 557us/step - loss: 6.0194e-05 - val_loss: 2.0418e-04\n",
      "Epoch 1738/2000\n",
      "3690/3690 [==============================] - 2s 553us/step - loss: 6.2244e-05 - val_loss: 2.9938e-04\n",
      "Epoch 1739/2000\n",
      "3690/3690 [==============================] - 2s 552us/step - loss: 8.2564e-05 - val_loss: 1.4450e-04\n",
      "Epoch 1740/2000\n",
      "3690/3690 [==============================] - 2s 556us/step - loss: 6.2406e-05 - val_loss: 1.6021e-04\n",
      "Epoch 1741/2000\n",
      "3690/3690 [==============================] - 2s 554us/step - loss: 7.1092e-05 - val_loss: 3.0160e-04\n",
      "Epoch 1742/2000\n",
      "3690/3690 [==============================] - 2s 555us/step - loss: 7.3923e-05 - val_loss: 2.2156e-04\n",
      "Epoch 1743/2000\n",
      "3690/3690 [==============================] - 2s 555us/step - loss: 8.1635e-05 - val_loss: 1.6843e-04\n",
      "Epoch 1744/2000\n",
      "3690/3690 [==============================] - 2s 557us/step - loss: 1.1076e-04 - val_loss: 2.9222e-04\n",
      "Epoch 1745/2000\n",
      "3690/3690 [==============================] - 2s 558us/step - loss: 6.8310e-05 - val_loss: 1.5186e-04\n",
      "Epoch 1746/2000\n",
      "3690/3690 [==============================] - 2s 555us/step - loss: 5.9241e-05 - val_loss: 1.1034e-04\n",
      "Epoch 1747/2000\n",
      "3690/3690 [==============================] - 2s 552us/step - loss: 6.6788e-05 - val_loss: 8.7409e-05\n",
      "Epoch 1748/2000\n",
      "3690/3690 [==============================] - 2s 553us/step - loss: 7.0002e-05 - val_loss: 1.9199e-04\n",
      "Epoch 1749/2000\n",
      "3690/3690 [==============================] - 2s 555us/step - loss: 7.1225e-05 - val_loss: 1.5067e-04\n",
      "Epoch 1750/2000\n",
      "3690/3690 [==============================] - 2s 556us/step - loss: 6.1455e-05 - val_loss: 2.0232e-04\n",
      "Epoch 1751/2000\n",
      "3690/3690 [==============================] - 2s 551us/step - loss: 6.1918e-05 - val_loss: 1.1448e-04\n",
      "Epoch 1752/2000\n",
      "3690/3690 [==============================] - 2s 551us/step - loss: 6.4361e-05 - val_loss: 1.6220e-04\n",
      "Epoch 1753/2000\n",
      "3690/3690 [==============================] - 2s 555us/step - loss: 5.9238e-05 - val_loss: 1.0771e-04\n",
      "Epoch 1754/2000\n",
      "3690/3690 [==============================] - 2s 554us/step - loss: 7.0089e-05 - val_loss: 1.7067e-04\n",
      "Epoch 1755/2000\n",
      "3690/3690 [==============================] - 2s 554us/step - loss: 7.6965e-05 - val_loss: 1.3331e-04\n",
      "Epoch 1756/2000\n",
      "3690/3690 [==============================] - 2s 557us/step - loss: 6.2565e-05 - val_loss: 1.0580e-04\n",
      "Epoch 1757/2000\n",
      "3690/3690 [==============================] - 2s 550us/step - loss: 6.3570e-05 - val_loss: 1.1079e-04\n",
      "Epoch 1758/2000\n",
      "3690/3690 [==============================] - 2s 554us/step - loss: 6.5579e-05 - val_loss: 2.2532e-04\n",
      "Epoch 1759/2000\n",
      "3690/3690 [==============================] - 2s 554us/step - loss: 6.6991e-05 - val_loss: 2.2590e-04\n",
      "Epoch 1760/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3690/3690 [==============================] - 2s 554us/step - loss: 7.2706e-05 - val_loss: 1.0090e-04\n",
      "Epoch 1761/2000\n",
      "3690/3690 [==============================] - 2s 554us/step - loss: 6.7273e-05 - val_loss: 1.0334e-04\n",
      "Epoch 1762/2000\n",
      "3690/3690 [==============================] - 2s 552us/step - loss: 6.3276e-05 - val_loss: 3.0741e-04\n",
      "Epoch 1763/2000\n",
      "3690/3690 [==============================] - 2s 554us/step - loss: 6.1159e-05 - val_loss: 2.9543e-04\n",
      "Epoch 1764/2000\n",
      "3690/3690 [==============================] - 2s 554us/step - loss: 7.2050e-05 - val_loss: 1.5420e-04\n",
      "Epoch 1765/2000\n",
      "3690/3690 [==============================] - 2s 551us/step - loss: 6.4829e-05 - val_loss: 2.6610e-04\n",
      "Epoch 1766/2000\n",
      "3690/3690 [==============================] - 2s 553us/step - loss: 6.8050e-05 - val_loss: 1.8015e-04\n",
      "Epoch 1767/2000\n",
      "3690/3690 [==============================] - 2s 555us/step - loss: 5.7523e-05 - val_loss: 1.3378e-04\n",
      "Epoch 1768/2000\n",
      "3690/3690 [==============================] - 2s 554us/step - loss: 5.7630e-05 - val_loss: 1.2158e-04\n",
      "Epoch 1769/2000\n",
      "3690/3690 [==============================] - 2s 554us/step - loss: 6.7769e-05 - val_loss: 2.7736e-04\n",
      "Epoch 1770/2000\n",
      "3690/3690 [==============================] - 2s 553us/step - loss: 6.7988e-05 - val_loss: 2.5116e-04\n",
      "Epoch 1771/2000\n",
      "3690/3690 [==============================] - 2s 552us/step - loss: 6.1341e-05 - val_loss: 1.1282e-04\n",
      "Epoch 1772/2000\n",
      "3690/3690 [==============================] - 2s 553us/step - loss: 5.9911e-05 - val_loss: 3.1353e-04\n",
      "Epoch 1773/2000\n",
      "3690/3690 [==============================] - 2s 554us/step - loss: 6.6676e-05 - val_loss: 1.2585e-04\n",
      "Epoch 1774/2000\n",
      "3690/3690 [==============================] - 2s 555us/step - loss: 6.8290e-05 - val_loss: 2.9559e-04\n",
      "Epoch 1775/2000\n",
      "3690/3690 [==============================] - 2s 554us/step - loss: 8.1243e-05 - val_loss: 1.3082e-04\n",
      "Epoch 1776/2000\n",
      "3690/3690 [==============================] - 2s 556us/step - loss: 7.7010e-05 - val_loss: 3.7222e-04\n",
      "Epoch 1777/2000\n",
      "3690/3690 [==============================] - 2s 553us/step - loss: 6.8018e-05 - val_loss: 1.2929e-04\n",
      "Epoch 1778/2000\n",
      "3690/3690 [==============================] - 2s 553us/step - loss: 7.1258e-05 - val_loss: 2.0042e-04\n",
      "Epoch 1779/2000\n",
      "3690/3690 [==============================] - 2s 554us/step - loss: 5.8402e-05 - val_loss: 5.5167e-04\n",
      "Epoch 1780/2000\n",
      "3690/3690 [==============================] - 2s 552us/step - loss: 7.1282e-05 - val_loss: 1.6334e-04\n",
      "Epoch 1781/2000\n",
      "3690/3690 [==============================] - 2s 551us/step - loss: 7.4258e-05 - val_loss: 1.0417e-04\n",
      "Epoch 1782/2000\n",
      "3690/3690 [==============================] - 2s 553us/step - loss: 6.1367e-05 - val_loss: 2.9267e-04\n",
      "Epoch 1783/2000\n",
      "3690/3690 [==============================] - 2s 554us/step - loss: 6.9458e-05 - val_loss: 2.5263e-04\n",
      "Epoch 1784/2000\n",
      "3690/3690 [==============================] - 2s 553us/step - loss: 6.1559e-05 - val_loss: 4.8955e-04\n",
      "Epoch 1785/2000\n",
      "3690/3690 [==============================] - 2s 556us/step - loss: 7.6115e-05 - val_loss: 1.4782e-04\n",
      "Epoch 1786/2000\n",
      "3690/3690 [==============================] - 2s 555us/step - loss: 7.4227e-05 - val_loss: 2.0364e-04\n",
      "Epoch 1787/2000\n",
      "3690/3690 [==============================] - 2s 553us/step - loss: 7.2611e-05 - val_loss: 1.6499e-04\n",
      "Epoch 1788/2000\n",
      "3690/3690 [==============================] - 2s 555us/step - loss: 8.9416e-05 - val_loss: 3.8634e-04\n",
      "Epoch 1789/2000\n",
      "3690/3690 [==============================] - 2s 554us/step - loss: 8.1032e-05 - val_loss: 1.8471e-04\n",
      "Epoch 1790/2000\n",
      "3690/3690 [==============================] - 2s 554us/step - loss: 7.9931e-05 - val_loss: 1.3643e-04\n",
      "Epoch 1791/2000\n",
      "3690/3690 [==============================] - 2s 554us/step - loss: 6.5031e-05 - val_loss: 3.0182e-04\n",
      "Epoch 1792/2000\n",
      "3690/3690 [==============================] - 2s 557us/step - loss: 5.7318e-05 - val_loss: 1.5032e-04\n",
      "Epoch 1793/2000\n",
      "3690/3690 [==============================] - 2s 554us/step - loss: 6.1137e-05 - val_loss: 1.8930e-04\n",
      "Epoch 1794/2000\n",
      "3690/3690 [==============================] - 2s 553us/step - loss: 5.7602e-05 - val_loss: 1.7860e-04\n",
      "Epoch 1795/2000\n",
      "3690/3690 [==============================] - 2s 555us/step - loss: 6.3195e-05 - val_loss: 1.2721e-04\n",
      "Epoch 1796/2000\n",
      "3690/3690 [==============================] - 2s 555us/step - loss: 6.4559e-05 - val_loss: 1.6259e-04\n",
      "Epoch 1797/2000\n",
      "3690/3690 [==============================] - 2s 554us/step - loss: 5.9115e-05 - val_loss: 1.1832e-04\n",
      "Epoch 1798/2000\n",
      "3690/3690 [==============================] - 2s 551us/step - loss: 6.2900e-05 - val_loss: 3.2189e-04\n",
      "Epoch 1799/2000\n",
      "3690/3690 [==============================] - 2s 557us/step - loss: 6.8891e-05 - val_loss: 1.2080e-04\n",
      "Epoch 1800/2000\n",
      "3690/3690 [==============================] - 2s 552us/step - loss: 5.4744e-05 - val_loss: 1.6388e-04\n",
      "Epoch 1801/2000\n",
      "3690/3690 [==============================] - 2s 556us/step - loss: 5.7553e-05 - val_loss: 2.0978e-04\n",
      "Epoch 1802/2000\n",
      "3690/3690 [==============================] - 2s 555us/step - loss: 6.5950e-05 - val_loss: 1.2421e-04\n",
      "Epoch 1803/2000\n",
      "3690/3690 [==============================] - 2s 558us/step - loss: 8.2687e-05 - val_loss: 2.3790e-04\n",
      "Epoch 1804/2000\n",
      "3690/3690 [==============================] - 2s 559us/step - loss: 7.9445e-05 - val_loss: 1.2983e-04\n",
      "Epoch 1805/2000\n",
      "3690/3690 [==============================] - 2s 554us/step - loss: 6.4995e-05 - val_loss: 3.6428e-04\n",
      "Epoch 1806/2000\n",
      "3690/3690 [==============================] - 2s 554us/step - loss: 6.6904e-05 - val_loss: 1.0149e-04\n",
      "Epoch 1807/2000\n",
      "3690/3690 [==============================] - 2s 555us/step - loss: 6.6102e-05 - val_loss: 1.5614e-04\n",
      "Epoch 1808/2000\n",
      "3690/3690 [==============================] - 2s 551us/step - loss: 6.5070e-05 - val_loss: 2.4346e-04\n",
      "Epoch 1809/2000\n",
      "3690/3690 [==============================] - 2s 551us/step - loss: 5.7233e-05 - val_loss: 2.0575e-04\n",
      "Epoch 1810/2000\n",
      "3690/3690 [==============================] - 2s 551us/step - loss: 6.0847e-05 - val_loss: 1.2269e-04\n",
      "Epoch 1811/2000\n",
      "3690/3690 [==============================] - 2s 551us/step - loss: 6.6438e-05 - val_loss: 1.5888e-04\n",
      "Epoch 1812/2000\n",
      "3690/3690 [==============================] - 2s 555us/step - loss: 6.4963e-05 - val_loss: 1.1565e-04\n",
      "Epoch 1813/2000\n",
      "3690/3690 [==============================] - 2s 553us/step - loss: 5.7259e-05 - val_loss: 1.3905e-04\n",
      "Epoch 1814/2000\n",
      "3690/3690 [==============================] - 2s 551us/step - loss: 6.1860e-05 - val_loss: 3.4860e-04\n",
      "Epoch 1815/2000\n",
      "3690/3690 [==============================] - 2s 558us/step - loss: 6.4325e-05 - val_loss: 1.5437e-04\n",
      "Epoch 1816/2000\n",
      "3690/3690 [==============================] - 2s 551us/step - loss: 6.2655e-05 - val_loss: 2.5219e-04\n",
      "Epoch 1817/2000\n",
      "3690/3690 [==============================] - 2s 554us/step - loss: 6.0830e-05 - val_loss: 1.4680e-04\n",
      "Epoch 1818/2000\n",
      "3690/3690 [==============================] - 2s 556us/step - loss: 6.4537e-05 - val_loss: 3.4106e-04\n",
      "Epoch 1819/2000\n",
      "3690/3690 [==============================] - 2s 555us/step - loss: 7.0725e-05 - val_loss: 1.3291e-04\n",
      "Epoch 1820/2000\n",
      "3690/3690 [==============================] - 2s 557us/step - loss: 6.1937e-05 - val_loss: 1.6805e-04\n",
      "Epoch 1821/2000\n",
      "3690/3690 [==============================] - 2s 555us/step - loss: 5.9965e-05 - val_loss: 2.0570e-04\n",
      "Epoch 1822/2000\n",
      "3690/3690 [==============================] - 2s 555us/step - loss: 5.7014e-05 - val_loss: 1.6140e-04\n",
      "Epoch 1823/2000\n",
      "3690/3690 [==============================] - 2s 557us/step - loss: 5.6228e-05 - val_loss: 2.2518e-04\n",
      "Epoch 1824/2000\n",
      "3690/3690 [==============================] - 2s 557us/step - loss: 5.6266e-05 - val_loss: 1.5195e-04\n",
      "Epoch 1825/2000\n",
      "3690/3690 [==============================] - 2s 552us/step - loss: 5.5794e-05 - val_loss: 1.3915e-04\n",
      "Epoch 1826/2000\n",
      "3690/3690 [==============================] - 2s 554us/step - loss: 5.7671e-05 - val_loss: 1.5026e-04\n",
      "Epoch 1827/2000\n",
      "3690/3690 [==============================] - 2s 558us/step - loss: 6.6870e-05 - val_loss: 2.5203e-04\n",
      "Epoch 1828/2000\n",
      "3690/3690 [==============================] - 2s 553us/step - loss: 6.9706e-05 - val_loss: 1.3804e-04\n",
      "Epoch 1829/2000\n",
      "3690/3690 [==============================] - 2s 553us/step - loss: 7.1159e-05 - val_loss: 3.3021e-04\n",
      "Epoch 1830/2000\n",
      "3690/3690 [==============================] - 2s 555us/step - loss: 7.2620e-05 - val_loss: 1.4966e-04\n",
      "Epoch 1831/2000\n",
      "3690/3690 [==============================] - 2s 557us/step - loss: 8.1745e-05 - val_loss: 2.0538e-04\n",
      "Epoch 1832/2000\n",
      "3690/3690 [==============================] - 2s 555us/step - loss: 8.1436e-05 - val_loss: 1.7806e-04\n",
      "Epoch 1833/2000\n",
      "3690/3690 [==============================] - 2s 552us/step - loss: 7.4987e-05 - val_loss: 2.5670e-04\n",
      "Epoch 1834/2000\n",
      "3690/3690 [==============================] - 2s 558us/step - loss: 6.4797e-05 - val_loss: 2.4003e-04\n",
      "Epoch 1835/2000\n",
      "3690/3690 [==============================] - 2s 554us/step - loss: 6.7984e-05 - val_loss: 1.5367e-04\n",
      "Epoch 1836/2000\n",
      "3690/3690 [==============================] - 2s 553us/step - loss: 6.4236e-05 - val_loss: 1.4932e-04\n",
      "Epoch 1837/2000\n",
      "3690/3690 [==============================] - 2s 552us/step - loss: 6.9263e-05 - val_loss: 1.8342e-04\n",
      "Epoch 1838/2000\n",
      "3690/3690 [==============================] - 2s 555us/step - loss: 6.2577e-05 - val_loss: 2.1620e-04\n",
      "Epoch 1839/2000\n",
      "3690/3690 [==============================] - 2s 552us/step - loss: 7.5664e-05 - val_loss: 1.2434e-04\n",
      "Epoch 1840/2000\n",
      "3690/3690 [==============================] - 2s 551us/step - loss: 7.7028e-05 - val_loss: 4.2746e-04\n",
      "Epoch 1841/2000\n",
      "3690/3690 [==============================] - 2s 552us/step - loss: 6.8035e-05 - val_loss: 9.5957e-05\n",
      "Epoch 1842/2000\n",
      "3690/3690 [==============================] - 2s 556us/step - loss: 6.2464e-05 - val_loss: 1.4584e-04\n",
      "Epoch 1843/2000\n",
      "3690/3690 [==============================] - 2s 556us/step - loss: 6.0814e-05 - val_loss: 2.1660e-04\n",
      "Epoch 1844/2000\n",
      "3690/3690 [==============================] - 2s 555us/step - loss: 6.2607e-05 - val_loss: 2.3133e-04\n",
      "Epoch 1845/2000\n",
      "3690/3690 [==============================] - 2s 556us/step - loss: 6.2478e-05 - val_loss: 1.1856e-04\n",
      "Epoch 1846/2000\n",
      "3690/3690 [==============================] - 2s 558us/step - loss: 6.5404e-05 - val_loss: 2.5152e-04\n",
      "Epoch 1847/2000\n",
      "3690/3690 [==============================] - 2s 552us/step - loss: 7.0770e-05 - val_loss: 1.2078e-04\n",
      "Epoch 1848/2000\n",
      "3690/3690 [==============================] - 2s 553us/step - loss: 6.1837e-05 - val_loss: 2.9269e-04\n",
      "Epoch 1849/2000\n",
      "3690/3690 [==============================] - 2s 553us/step - loss: 5.8110e-05 - val_loss: 3.5449e-04\n",
      "Epoch 1850/2000\n",
      "3690/3690 [==============================] - 2s 553us/step - loss: 6.4900e-05 - val_loss: 1.1975e-04\n",
      "Epoch 1851/2000\n",
      "3690/3690 [==============================] - 2s 554us/step - loss: 6.5288e-05 - val_loss: 2.5706e-04\n",
      "Epoch 1852/2000\n",
      "3690/3690 [==============================] - 2s 554us/step - loss: 6.8173e-05 - val_loss: 1.8371e-04\n",
      "Epoch 1853/2000\n",
      "3690/3690 [==============================] - 2s 553us/step - loss: 6.7174e-05 - val_loss: 2.6817e-04\n",
      "Epoch 1854/2000\n",
      "3690/3690 [==============================] - 2s 565us/step - loss: 5.9124e-05 - val_loss: 1.5252e-04\n",
      "Epoch 1855/2000\n",
      "3690/3690 [==============================] - 2s 554us/step - loss: 5.6236e-05 - val_loss: 1.8495e-04\n",
      "Epoch 1856/2000\n",
      "3690/3690 [==============================] - 2s 554us/step - loss: 6.7873e-05 - val_loss: 2.3462e-04\n",
      "Epoch 1857/2000\n",
      "3690/3690 [==============================] - 2s 553us/step - loss: 6.3856e-05 - val_loss: 2.4310e-04\n",
      "Epoch 1858/2000\n",
      "3690/3690 [==============================] - 2s 553us/step - loss: 5.7302e-05 - val_loss: 3.2161e-04\n",
      "Epoch 1859/2000\n",
      "3690/3690 [==============================] - 2s 556us/step - loss: 5.7147e-05 - val_loss: 1.5055e-04\n",
      "Epoch 1860/2000\n",
      "3690/3690 [==============================] - 2s 552us/step - loss: 6.5727e-05 - val_loss: 7.2543e-04\n",
      "Epoch 1861/2000\n",
      "3690/3690 [==============================] - 2s 552us/step - loss: 1.2963e-04 - val_loss: 2.3835e-04\n",
      "Epoch 1862/2000\n",
      "3690/3690 [==============================] - 2s 554us/step - loss: 8.7162e-05 - val_loss: 3.7640e-04\n",
      "Epoch 1863/2000\n",
      "3690/3690 [==============================] - 2s 553us/step - loss: 6.8127e-05 - val_loss: 2.0155e-04\n",
      "Epoch 1864/2000\n",
      "3690/3690 [==============================] - 2s 554us/step - loss: 6.1260e-05 - val_loss: 1.7395e-04\n",
      "Epoch 1865/2000\n",
      "3690/3690 [==============================] - 2s 558us/step - loss: 6.1959e-05 - val_loss: 1.6886e-04\n",
      "Epoch 1866/2000\n",
      "3690/3690 [==============================] - 2s 555us/step - loss: 6.1127e-05 - val_loss: 4.2676e-04\n",
      "Epoch 1867/2000\n",
      "3690/3690 [==============================] - 2s 556us/step - loss: 6.0604e-05 - val_loss: 1.7924e-04\n",
      "Epoch 1868/2000\n",
      "3690/3690 [==============================] - 2s 555us/step - loss: 6.1258e-05 - val_loss: 1.2012e-04\n",
      "Epoch 1869/2000\n",
      "3690/3690 [==============================] - 2s 555us/step - loss: 6.8396e-05 - val_loss: 2.5768e-04\n",
      "Epoch 1870/2000\n",
      "3690/3690 [==============================] - 2s 558us/step - loss: 6.8348e-05 - val_loss: 1.9527e-04\n",
      "Epoch 1871/2000\n",
      "3690/3690 [==============================] - 2s 552us/step - loss: 6.6366e-05 - val_loss: 1.4627e-04\n",
      "Epoch 1872/2000\n",
      "3690/3690 [==============================] - 2s 553us/step - loss: 5.8147e-05 - val_loss: 3.1556e-04\n",
      "Epoch 1873/2000\n",
      "3690/3690 [==============================] - 2s 555us/step - loss: 6.0048e-05 - val_loss: 1.7634e-04\n",
      "Epoch 1874/2000\n",
      "3690/3690 [==============================] - 2s 553us/step - loss: 5.8484e-05 - val_loss: 1.7541e-04\n",
      "Epoch 1875/2000\n",
      "3690/3690 [==============================] - 2s 554us/step - loss: 6.0022e-05 - val_loss: 2.9621e-04\n",
      "Epoch 1876/2000\n",
      "3690/3690 [==============================] - 2s 552us/step - loss: 6.6092e-05 - val_loss: 1.9865e-04\n",
      "Epoch 1877/2000\n",
      "3690/3690 [==============================] - 2s 552us/step - loss: 5.9448e-05 - val_loss: 3.1767e-04\n",
      "Epoch 1878/2000\n",
      "3690/3690 [==============================] - 2s 556us/step - loss: 5.7941e-05 - val_loss: 1.8646e-04\n",
      "Epoch 1879/2000\n",
      "3690/3690 [==============================] - 2s 553us/step - loss: 5.7267e-05 - val_loss: 1.1660e-04\n",
      "Epoch 1880/2000\n",
      "3690/3690 [==============================] - 2s 554us/step - loss: 6.4542e-05 - val_loss: 2.0629e-04\n",
      "Epoch 1881/2000\n",
      "3690/3690 [==============================] - 2s 553us/step - loss: 5.3266e-05 - val_loss: 2.1934e-04\n",
      "Epoch 1882/2000\n",
      "3690/3690 [==============================] - 2s 552us/step - loss: 5.2671e-05 - val_loss: 2.9879e-04\n",
      "Epoch 1883/2000\n",
      "3690/3690 [==============================] - 2s 554us/step - loss: 6.0703e-05 - val_loss: 1.7439e-04\n",
      "Epoch 1884/2000\n",
      "3690/3690 [==============================] - 2s 555us/step - loss: 5.8741e-05 - val_loss: 1.2936e-04\n",
      "Epoch 1885/2000\n",
      "3690/3690 [==============================] - 2s 556us/step - loss: 5.8403e-05 - val_loss: 2.8129e-04\n",
      "Epoch 1886/2000\n",
      "3690/3690 [==============================] - 2s 551us/step - loss: 5.5827e-05 - val_loss: 2.0995e-04\n",
      "Epoch 1887/2000\n",
      "3690/3690 [==============================] - 2s 553us/step - loss: 5.7151e-05 - val_loss: 1.8339e-04\n",
      "Epoch 1888/2000\n",
      "3690/3690 [==============================] - 2s 551us/step - loss: 5.5665e-05 - val_loss: 1.9557e-04\n",
      "Epoch 1889/2000\n",
      "3690/3690 [==============================] - 2s 553us/step - loss: 5.8436e-05 - val_loss: 2.7884e-04\n",
      "Epoch 1890/2000\n",
      "3690/3690 [==============================] - 2s 556us/step - loss: 6.0638e-05 - val_loss: 2.4465e-04\n",
      "Epoch 1891/2000\n",
      "3690/3690 [==============================] - 2s 553us/step - loss: 6.1525e-05 - val_loss: 1.3441e-04\n",
      "Epoch 1892/2000\n",
      "3690/3690 [==============================] - 2s 550us/step - loss: 6.3816e-05 - val_loss: 1.5152e-04\n",
      "Epoch 1893/2000\n",
      "3690/3690 [==============================] - 2s 556us/step - loss: 5.5273e-05 - val_loss: 1.6573e-04\n",
      "Epoch 1894/2000\n",
      "3690/3690 [==============================] - 2s 553us/step - loss: 5.7233e-05 - val_loss: 2.0728e-04\n",
      "Epoch 1895/2000\n",
      "3690/3690 [==============================] - 2s 552us/step - loss: 5.4495e-05 - val_loss: 3.5132e-04\n",
      "Epoch 1896/2000\n",
      "3690/3690 [==============================] - 2s 553us/step - loss: 5.7574e-05 - val_loss: 2.4572e-04\n",
      "Epoch 1897/2000\n",
      "3690/3690 [==============================] - 2s 555us/step - loss: 5.7778e-05 - val_loss: 2.2450e-04\n",
      "Epoch 1898/2000\n",
      "3690/3690 [==============================] - 2s 555us/step - loss: 5.8418e-05 - val_loss: 1.6282e-04\n",
      "Epoch 1899/2000\n",
      "3690/3690 [==============================] - 2s 553us/step - loss: 5.8713e-05 - val_loss: 3.5603e-04\n",
      "Epoch 1900/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3690/3690 [==============================] - 2s 554us/step - loss: 6.8251e-05 - val_loss: 1.3510e-04\n",
      "Epoch 1901/2000\n",
      "3690/3690 [==============================] - 2s 555us/step - loss: 7.0945e-05 - val_loss: 2.1235e-04\n",
      "Epoch 1902/2000\n",
      "3690/3690 [==============================] - 2s 557us/step - loss: 6.4663e-05 - val_loss: 2.8609e-04\n",
      "Epoch 1903/2000\n",
      "3690/3690 [==============================] - 2s 554us/step - loss: 6.2409e-05 - val_loss: 1.7804e-04\n",
      "Epoch 1904/2000\n",
      "3690/3690 [==============================] - 2s 553us/step - loss: 6.5012e-05 - val_loss: 4.0434e-04\n",
      "Epoch 1905/2000\n",
      "3690/3690 [==============================] - 2s 554us/step - loss: 6.6018e-05 - val_loss: 1.4593e-04\n",
      "Epoch 1906/2000\n",
      "3690/3690 [==============================] - 2s 555us/step - loss: 6.1242e-05 - val_loss: 3.2989e-04\n",
      "Epoch 1907/2000\n",
      "3690/3690 [==============================] - 2s 555us/step - loss: 5.9151e-05 - val_loss: 1.4271e-04\n",
      "Epoch 1908/2000\n",
      "3690/3690 [==============================] - 2s 552us/step - loss: 7.3664e-05 - val_loss: 1.5981e-04\n",
      "Epoch 1909/2000\n",
      "3690/3690 [==============================] - 2s 554us/step - loss: 6.1589e-05 - val_loss: 3.0762e-04\n",
      "Epoch 1910/2000\n",
      "3690/3690 [==============================] - 2s 554us/step - loss: 5.7045e-05 - val_loss: 1.2364e-04\n",
      "Epoch 1911/2000\n",
      "3690/3690 [==============================] - 2s 555us/step - loss: 6.1859e-05 - val_loss: 2.2671e-04\n",
      "Epoch 1912/2000\n",
      "3690/3690 [==============================] - 2s 554us/step - loss: 6.3565e-05 - val_loss: 2.0746e-04\n",
      "Epoch 1913/2000\n",
      "3690/3690 [==============================] - 2s 552us/step - loss: 6.7540e-05 - val_loss: 3.1254e-04\n",
      "Epoch 1914/2000\n",
      "3690/3690 [==============================] - 2s 553us/step - loss: 6.3635e-05 - val_loss: 1.5216e-04\n",
      "Epoch 1915/2000\n",
      "3690/3690 [==============================] - 2s 553us/step - loss: 7.4314e-05 - val_loss: 3.7932e-04\n",
      "Epoch 1916/2000\n",
      "3690/3690 [==============================] - 2s 552us/step - loss: 5.8647e-05 - val_loss: 2.7596e-04\n",
      "Epoch 1917/2000\n",
      "3690/3690 [==============================] - 2s 558us/step - loss: 5.4398e-05 - val_loss: 4.6829e-04\n",
      "Epoch 1918/2000\n",
      "3690/3690 [==============================] - 2s 553us/step - loss: 7.5066e-05 - val_loss: 1.6923e-04\n",
      "Epoch 1919/2000\n",
      "3690/3690 [==============================] - 2s 552us/step - loss: 6.0554e-05 - val_loss: 4.5857e-04\n",
      "Epoch 1920/2000\n",
      "3690/3690 [==============================] - 2s 556us/step - loss: 6.3038e-05 - val_loss: 1.2215e-04\n",
      "Epoch 1921/2000\n",
      "3690/3690 [==============================] - 2s 554us/step - loss: 6.2726e-05 - val_loss: 3.4719e-04\n",
      "Epoch 1922/2000\n",
      "3690/3690 [==============================] - 2s 554us/step - loss: 7.6867e-05 - val_loss: 1.5269e-04\n",
      "Epoch 1923/2000\n",
      "3690/3690 [==============================] - 2s 555us/step - loss: 6.6873e-05 - val_loss: 2.7984e-04\n",
      "Epoch 1924/2000\n",
      "3690/3690 [==============================] - 2s 560us/step - loss: 5.6198e-05 - val_loss: 1.4159e-04\n",
      "Epoch 1925/2000\n",
      "3690/3690 [==============================] - 2s 557us/step - loss: 6.3653e-05 - val_loss: 3.2505e-04\n",
      "Epoch 1926/2000\n",
      "3690/3690 [==============================] - 2s 555us/step - loss: 6.1138e-05 - val_loss: 3.1765e-04\n",
      "Epoch 1927/2000\n",
      "3690/3690 [==============================] - 2s 554us/step - loss: 6.8106e-05 - val_loss: 2.3863e-04\n",
      "Epoch 1928/2000\n",
      "3690/3690 [==============================] - 2s 552us/step - loss: 6.2146e-05 - val_loss: 2.0157e-04\n",
      "Epoch 1929/2000\n",
      "3690/3690 [==============================] - 2s 553us/step - loss: 7.0323e-05 - val_loss: 4.0691e-04\n",
      "Epoch 1930/2000\n",
      "3690/3690 [==============================] - 2s 554us/step - loss: 7.0492e-05 - val_loss: 1.4899e-04\n",
      "Epoch 1931/2000\n",
      "3690/3690 [==============================] - 2s 553us/step - loss: 6.0685e-05 - val_loss: 1.8753e-04\n",
      "Epoch 1932/2000\n",
      "3690/3690 [==============================] - 2s 558us/step - loss: 5.5970e-05 - val_loss: 2.8436e-04\n",
      "Epoch 1933/2000\n",
      "3690/3690 [==============================] - 2s 551us/step - loss: 6.5220e-05 - val_loss: 2.2326e-04\n",
      "Epoch 1934/2000\n",
      "3690/3690 [==============================] - 2s 554us/step - loss: 5.8103e-05 - val_loss: 1.7965e-04\n",
      "Epoch 1935/2000\n",
      "3690/3690 [==============================] - 2s 555us/step - loss: 5.9696e-05 - val_loss: 4.0055e-04\n",
      "Epoch 1936/2000\n",
      "3690/3690 [==============================] - 2s 554us/step - loss: 6.5216e-05 - val_loss: 1.3836e-04\n",
      "Epoch 1937/2000\n",
      "3690/3690 [==============================] - 2s 554us/step - loss: 5.7743e-05 - val_loss: 1.9388e-04\n",
      "Epoch 1938/2000\n",
      "3690/3690 [==============================] - 2s 557us/step - loss: 5.6690e-05 - val_loss: 1.3697e-04\n",
      "Epoch 1939/2000\n",
      "3690/3690 [==============================] - 2s 553us/step - loss: 6.0419e-05 - val_loss: 2.2605e-04\n",
      "Epoch 1940/2000\n",
      "3690/3690 [==============================] - 2s 558us/step - loss: 5.4267e-05 - val_loss: 2.2789e-04\n",
      "Epoch 1941/2000\n",
      "3690/3690 [==============================] - 2s 556us/step - loss: 6.2263e-05 - val_loss: 3.4972e-04\n",
      "Epoch 1942/2000\n",
      "3690/3690 [==============================] - 2s 555us/step - loss: 6.1487e-05 - val_loss: 2.1799e-04\n",
      "Epoch 1943/2000\n",
      "3690/3690 [==============================] - 2s 554us/step - loss: 5.9511e-05 - val_loss: 5.5007e-04\n",
      "Epoch 1944/2000\n",
      "3690/3690 [==============================] - 2s 554us/step - loss: 7.6315e-05 - val_loss: 2.3997e-04\n",
      "Epoch 1945/2000\n",
      "3690/3690 [==============================] - 2s 553us/step - loss: 6.0087e-05 - val_loss: 2.4568e-04\n",
      "Epoch 1946/2000\n",
      "3690/3690 [==============================] - 2s 555us/step - loss: 6.8199e-05 - val_loss: 3.3829e-04\n",
      "Epoch 1947/2000\n",
      "3690/3690 [==============================] - 2s 554us/step - loss: 5.6159e-05 - val_loss: 3.0253e-04\n",
      "Epoch 1948/2000\n",
      "3690/3690 [==============================] - 2s 555us/step - loss: 6.0233e-05 - val_loss: 1.4564e-04\n",
      "Epoch 1949/2000\n",
      "3690/3690 [==============================] - 2s 553us/step - loss: 6.9629e-05 - val_loss: 2.2765e-04\n",
      "Epoch 1950/2000\n",
      "3690/3690 [==============================] - 2s 556us/step - loss: 5.9928e-05 - val_loss: 1.4298e-04\n",
      "Epoch 1951/2000\n",
      "3690/3690 [==============================] - 2s 556us/step - loss: 5.6458e-05 - val_loss: 6.1577e-04\n",
      "Epoch 1952/2000\n",
      "3690/3690 [==============================] - 2s 554us/step - loss: 7.6745e-05 - val_loss: 3.7028e-04\n",
      "Epoch 1953/2000\n",
      "3690/3690 [==============================] - 2s 553us/step - loss: 5.8996e-05 - val_loss: 2.2262e-04\n",
      "Epoch 1954/2000\n",
      "3690/3690 [==============================] - 2s 552us/step - loss: 5.4438e-05 - val_loss: 2.8960e-04\n",
      "Epoch 1955/2000\n",
      "3690/3690 [==============================] - 2s 557us/step - loss: 5.4146e-05 - val_loss: 3.4700e-04\n",
      "Epoch 1956/2000\n",
      "3690/3690 [==============================] - 2s 558us/step - loss: 5.9799e-05 - val_loss: 1.8563e-04\n",
      "Epoch 1957/2000\n",
      "3690/3690 [==============================] - 2s 553us/step - loss: 5.7460e-05 - val_loss: 2.1654e-04\n",
      "Epoch 1958/2000\n",
      "3690/3690 [==============================] - 2s 555us/step - loss: 6.1862e-05 - val_loss: 3.6638e-04\n",
      "Epoch 1959/2000\n",
      "3690/3690 [==============================] - 2s 556us/step - loss: 6.0076e-05 - val_loss: 2.0448e-04\n",
      "Epoch 1960/2000\n",
      "3690/3690 [==============================] - 2s 563us/step - loss: 5.8664e-05 - val_loss: 2.4898e-04\n",
      "Epoch 1961/2000\n",
      "3690/3690 [==============================] - 2s 563us/step - loss: 5.7411e-05 - val_loss: 3.5984e-04\n",
      "Epoch 1962/2000\n",
      "3690/3690 [==============================] - 2s 554us/step - loss: 5.5535e-05 - val_loss: 2.6487e-04\n",
      "Epoch 1963/2000\n",
      "3690/3690 [==============================] - 2s 569us/step - loss: 5.4752e-05 - val_loss: 3.6943e-04\n",
      "Epoch 1964/2000\n",
      "3690/3690 [==============================] - 2s 562us/step - loss: 5.6086e-05 - val_loss: 2.7379e-04\n",
      "Epoch 1965/2000\n",
      "3690/3690 [==============================] - 2s 553us/step - loss: 5.7763e-05 - val_loss: 5.3901e-04\n",
      "Epoch 1966/2000\n",
      "3690/3690 [==============================] - 2s 554us/step - loss: 6.3996e-05 - val_loss: 1.5443e-04\n",
      "Epoch 1967/2000\n",
      "3690/3690 [==============================] - 2s 555us/step - loss: 5.9474e-05 - val_loss: 1.8496e-04\n",
      "Epoch 1968/2000\n",
      "3690/3690 [==============================] - 2s 555us/step - loss: 5.7543e-05 - val_loss: 2.3448e-04\n",
      "Epoch 1969/2000\n",
      "3690/3690 [==============================] - 2s 554us/step - loss: 5.8607e-05 - val_loss: 3.2107e-04\n",
      "Epoch 1970/2000\n",
      "3690/3690 [==============================] - 2s 554us/step - loss: 5.7481e-05 - val_loss: 2.1779e-04\n",
      "Epoch 1971/2000\n",
      "3690/3690 [==============================] - 2s 557us/step - loss: 5.7722e-05 - val_loss: 3.8812e-04\n",
      "Epoch 1972/2000\n",
      "3690/3690 [==============================] - 2s 553us/step - loss: 5.4016e-05 - val_loss: 2.1038e-04\n",
      "Epoch 1973/2000\n",
      "3690/3690 [==============================] - 2s 553us/step - loss: 5.4980e-05 - val_loss: 1.6435e-04\n",
      "Epoch 1974/2000\n",
      "3690/3690 [==============================] - 2s 555us/step - loss: 5.7087e-05 - val_loss: 3.6819e-04\n",
      "Epoch 1975/2000\n",
      "3690/3690 [==============================] - 2s 553us/step - loss: 5.5345e-05 - val_loss: 2.4161e-04\n",
      "Epoch 1976/2000\n",
      "3690/3690 [==============================] - 2s 553us/step - loss: 6.4430e-05 - val_loss: 1.5756e-04\n",
      "Epoch 1977/2000\n",
      "3690/3690 [==============================] - 2s 553us/step - loss: 7.5446e-05 - val_loss: 4.5684e-04\n",
      "Epoch 1978/2000\n",
      "3690/3690 [==============================] - 2s 554us/step - loss: 6.8836e-05 - val_loss: 1.9570e-04\n",
      "Epoch 1979/2000\n",
      "3690/3690 [==============================] - 2s 557us/step - loss: 6.3090e-05 - val_loss: 2.3358e-04\n",
      "Epoch 1980/2000\n",
      "3690/3690 [==============================] - 2s 553us/step - loss: 5.5861e-05 - val_loss: 2.0142e-04\n",
      "Epoch 1981/2000\n",
      "3690/3690 [==============================] - 2s 553us/step - loss: 5.6956e-05 - val_loss: 1.4354e-04\n",
      "Epoch 1982/2000\n",
      "3690/3690 [==============================] - 2s 553us/step - loss: 5.7606e-05 - val_loss: 2.2305e-04\n",
      "Epoch 1983/2000\n",
      "3690/3690 [==============================] - 2s 553us/step - loss: 5.9024e-05 - val_loss: 2.4123e-04\n",
      "Epoch 1984/2000\n",
      "3690/3690 [==============================] - 2s 557us/step - loss: 5.3092e-05 - val_loss: 3.7646e-04\n",
      "Epoch 1985/2000\n",
      "3690/3690 [==============================] - 2s 553us/step - loss: 5.6115e-05 - val_loss: 3.4432e-04\n",
      "Epoch 1986/2000\n",
      "3690/3690 [==============================] - 2s 552us/step - loss: 5.6824e-05 - val_loss: 1.9135e-04\n",
      "Epoch 1987/2000\n",
      "3690/3690 [==============================] - 2s 553us/step - loss: 6.1397e-05 - val_loss: 2.4777e-04\n",
      "Epoch 1988/2000\n",
      "3690/3690 [==============================] - 2s 554us/step - loss: 5.6213e-05 - val_loss: 2.7940e-04\n",
      "Epoch 1989/2000\n",
      "3690/3690 [==============================] - 2s 554us/step - loss: 6.2174e-05 - val_loss: 1.6615e-04\n",
      "Epoch 1990/2000\n",
      "3690/3690 [==============================] - 2s 555us/step - loss: 9.1067e-05 - val_loss: 4.1152e-04\n",
      "Epoch 1991/2000\n",
      "3690/3690 [==============================] - 2s 559us/step - loss: 7.1701e-05 - val_loss: 1.6498e-04\n",
      "Epoch 1992/2000\n",
      "3690/3690 [==============================] - 2s 552us/step - loss: 7.4568e-05 - val_loss: 2.6060e-04\n",
      "Epoch 1993/2000\n",
      "3690/3690 [==============================] - 2s 554us/step - loss: 5.7870e-05 - val_loss: 4.6242e-04\n",
      "Epoch 1994/2000\n",
      "3690/3690 [==============================] - 2s 551us/step - loss: 5.9121e-05 - val_loss: 2.9297e-04\n",
      "Epoch 1995/2000\n",
      "3690/3690 [==============================] - 2s 558us/step - loss: 6.4634e-05 - val_loss: 2.7799e-04\n",
      "Epoch 1996/2000\n",
      "3690/3690 [==============================] - 2s 556us/step - loss: 6.4303e-05 - val_loss: 1.4654e-04\n",
      "Epoch 1997/2000\n",
      "3690/3690 [==============================] - 2s 555us/step - loss: 6.5723e-05 - val_loss: 3.0840e-04\n",
      "Epoch 1998/2000\n",
      "3690/3690 [==============================] - 2s 557us/step - loss: 6.4816e-05 - val_loss: 1.8103e-04\n",
      "Epoch 1999/2000\n",
      "3690/3690 [==============================] - 2s 555us/step - loss: 6.4878e-05 - val_loss: 1.6865e-04\n",
      "Epoch 2000/2000\n",
      "3690/3690 [==============================] - 2s 553us/step - loss: 5.7982e-05 - val_loss: 2.7680e-04\n"
     ]
    }
   ],
   "source": [
    "final_model = build_lstm(**params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'optimizer': 'adam',\n",
       " 'shuffle': True,\n",
       " 'dropout': 0.1,\n",
       " 'full_density': True,\n",
       " 'twice': True,\n",
       " 'density': 230,\n",
       " 'activation': 'softplus',\n",
       " 'lstmsize': 150,\n",
       " 'callbacks': [<keras.callbacks.callbacks.ModelCheckpoint at 0x189c7cf7708>]}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_model[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_182\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_362 (LSTM)              (None, 92, 150)           93600     \n",
      "_________________________________________________________________\n",
      "dropout_362 (Dropout)        (None, 92, 150)           0         \n",
      "_________________________________________________________________\n",
      "lstm_363 (LSTM)              (None, 150)               180600    \n",
      "_________________________________________________________________\n",
      "dropout_363 (Dropout)        (None, 150)               0         \n",
      "_________________________________________________________________\n",
      "dense_931 (Dense)            (None, 230)               34730     \n",
      "_________________________________________________________________\n",
      "dense_932 (Dense)            (None, 115)               26565     \n",
      "_________________________________________________________________\n",
      "dense_933 (Dense)            (None, 57)                6612      \n",
      "_________________________________________________________________\n",
      "dense_934 (Dense)            (None, 28)                1624      \n",
      "_________________________________________________________________\n",
      "dense_935 (Dense)            (None, 14)                406       \n",
      "_________________________________________________________________\n",
      "dense_936 (Dense)            (None, 1)                 15        \n",
      "=================================================================\n",
      "Total params: 344,152\n",
      "Trainable params: 344,152\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "final_model[0].summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_model[0].load_weights('./checkpoints/IBM_MSE/IBM.(actual_best).hdf5')\n",
    "y_predicted = final_model[0].predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_y_test = y_normaliser.inverse_transform(y_predicted)\n",
    "true_y_test = y_normaliser.inverse_transform(Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE is 2.21\n",
      "Medium error is 0.88\n"
     ]
    }
   ],
   "source": [
    "loss = np.mean((true_y_test - predicted_y_test)**2)\n",
    "medium_error = np.mean(abs(true_y_test - predicted_y_test))\n",
    "print(f'MSE is {loss:.2f}\\nMedium error is {medium_error:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "up_down_test = []\n",
    "up_down_predicted = []\n",
    "\n",
    "for y,x in zip(Y_test,X_test):\n",
    "    if y[0] > x[-1][0]: up_down_test.append(1)\n",
    "    elif y[0] < x[-1][0]: up_down_test.append(-1)\n",
    "    else: up_down_test.append(0)\n",
    "        \n",
    "for y,x in zip(y_predicted,X_test):\n",
    "    if y[0] > x[-1][0]: up_down_predicted.append(1)\n",
    "    elif y[0] < x[-1][0]: up_down_predicted.append(-1)\n",
    "    else: up_down_predicted.append(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trend accuracy is: 80.02%\n"
     ]
    }
   ],
   "source": [
    "print(f'Trend accuracy is: {(sum(1 for r,p in zip(up_down_test,up_down_predicted) if r == p)/len(up_down_test))*100:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for upward trend is: 76.67%\n",
      "Accuracy for downward trend is: 84.31%\n"
     ]
    }
   ],
   "source": [
    "print(f'Accuracy for upward trend is: {(sum(1 for r,p in zip(up_down_test,up_down_predicted) if r == 1 and p == 1)/sum(1 for v in up_down_test if v == 1))*100:.2f}%')\n",
    "print(f'Accuracy for downward trend is: {(sum(1 for r,p in zip(up_down_test,up_down_predicted) if r == -1 and p == -1)/sum(1 for v in up_down_test if v == -1))*100:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Predictions over the last 92 days in test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 3200x2400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from matplotlib.pyplot import figure\n",
    "figure(num=None, figsize=(40, 30), dpi=80, facecolor='w', edgecolor='k')\n",
    "\n",
    "plt.plot(range(92), true_y_test[-92:])\n",
    "plt.plot(range(92), predicted_y_test[-92:])\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
