{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "C:\\ProgramData\\Anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\ProgramData\\Anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\ProgramData\\Anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\ProgramData\\Anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:529: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\ProgramData\\Anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:530: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\ProgramData\\Anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:535: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import preprocessing\n",
    "from joblib import load\n",
    "\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, LSTM\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from tensorflow import set_random_seed\n",
    "set_random_seed(12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('./datasets/IBM_daily.csv').sort_values(by='date')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>1. open</th>\n",
       "      <th>2. high</th>\n",
       "      <th>3. low</th>\n",
       "      <th>4. close</th>\n",
       "      <th>5. volume</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5217</th>\n",
       "      <td>1999-11-01</td>\n",
       "      <td>98.50</td>\n",
       "      <td>98.8100</td>\n",
       "      <td>96.37</td>\n",
       "      <td>96.75</td>\n",
       "      <td>9551800.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5216</th>\n",
       "      <td>1999-11-02</td>\n",
       "      <td>96.75</td>\n",
       "      <td>96.8100</td>\n",
       "      <td>93.69</td>\n",
       "      <td>94.81</td>\n",
       "      <td>11105400.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5215</th>\n",
       "      <td>1999-11-03</td>\n",
       "      <td>95.87</td>\n",
       "      <td>95.9400</td>\n",
       "      <td>93.50</td>\n",
       "      <td>94.37</td>\n",
       "      <td>10369100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5214</th>\n",
       "      <td>1999-11-04</td>\n",
       "      <td>94.44</td>\n",
       "      <td>94.4400</td>\n",
       "      <td>90.00</td>\n",
       "      <td>91.56</td>\n",
       "      <td>16697600.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5213</th>\n",
       "      <td>1999-11-05</td>\n",
       "      <td>92.75</td>\n",
       "      <td>92.9400</td>\n",
       "      <td>90.19</td>\n",
       "      <td>90.25</td>\n",
       "      <td>13737600.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2020-07-22</td>\n",
       "      <td>125.90</td>\n",
       "      <td>129.4700</td>\n",
       "      <td>125.80</td>\n",
       "      <td>128.67</td>\n",
       "      <td>8195366.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2020-07-23</td>\n",
       "      <td>129.10</td>\n",
       "      <td>129.3700</td>\n",
       "      <td>127.15</td>\n",
       "      <td>127.33</td>\n",
       "      <td>4220136.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2020-07-24</td>\n",
       "      <td>126.48</td>\n",
       "      <td>127.6459</td>\n",
       "      <td>125.50</td>\n",
       "      <td>125.79</td>\n",
       "      <td>3531076.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2020-07-27</td>\n",
       "      <td>124.86</td>\n",
       "      <td>126.3200</td>\n",
       "      <td>124.71</td>\n",
       "      <td>126.21</td>\n",
       "      <td>3733547.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2020-07-28</td>\n",
       "      <td>125.82</td>\n",
       "      <td>126.3400</td>\n",
       "      <td>124.15</td>\n",
       "      <td>124.47</td>\n",
       "      <td>4157538.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5218 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            date  1. open   2. high  3. low  4. close   5. volume\n",
       "5217  1999-11-01    98.50   98.8100   96.37     96.75   9551800.0\n",
       "5216  1999-11-02    96.75   96.8100   93.69     94.81  11105400.0\n",
       "5215  1999-11-03    95.87   95.9400   93.50     94.37  10369100.0\n",
       "5214  1999-11-04    94.44   94.4400   90.00     91.56  16697600.0\n",
       "5213  1999-11-05    92.75   92.9400   90.19     90.25  13737600.0\n",
       "...          ...      ...       ...     ...       ...         ...\n",
       "4     2020-07-22   125.90  129.4700  125.80    128.67   8195366.0\n",
       "3     2020-07-23   129.10  129.3700  127.15    127.33   4220136.0\n",
       "2     2020-07-24   126.48  127.6459  125.50    125.79   3531076.0\n",
       "1     2020-07-27   124.86  126.3200  124.71    126.21   3733547.0\n",
       "0     2020-07-28   125.82  126.3400  124.15    124.47   4157538.0\n",
       "\n",
       "[5218 rows x 6 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1. open</th>\n",
       "      <th>2. high</th>\n",
       "      <th>3. low</th>\n",
       "      <th>4. close</th>\n",
       "      <th>5. volume</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5217</th>\n",
       "      <td>98.50</td>\n",
       "      <td>98.8100</td>\n",
       "      <td>96.37</td>\n",
       "      <td>96.75</td>\n",
       "      <td>9551800.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5216</th>\n",
       "      <td>96.75</td>\n",
       "      <td>96.8100</td>\n",
       "      <td>93.69</td>\n",
       "      <td>94.81</td>\n",
       "      <td>11105400.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5215</th>\n",
       "      <td>95.87</td>\n",
       "      <td>95.9400</td>\n",
       "      <td>93.50</td>\n",
       "      <td>94.37</td>\n",
       "      <td>10369100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5214</th>\n",
       "      <td>94.44</td>\n",
       "      <td>94.4400</td>\n",
       "      <td>90.00</td>\n",
       "      <td>91.56</td>\n",
       "      <td>16697600.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5213</th>\n",
       "      <td>92.75</td>\n",
       "      <td>92.9400</td>\n",
       "      <td>90.19</td>\n",
       "      <td>90.25</td>\n",
       "      <td>13737600.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>125.90</td>\n",
       "      <td>129.4700</td>\n",
       "      <td>125.80</td>\n",
       "      <td>128.67</td>\n",
       "      <td>8195366.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>129.10</td>\n",
       "      <td>129.3700</td>\n",
       "      <td>127.15</td>\n",
       "      <td>127.33</td>\n",
       "      <td>4220136.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>126.48</td>\n",
       "      <td>127.6459</td>\n",
       "      <td>125.50</td>\n",
       "      <td>125.79</td>\n",
       "      <td>3531076.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>124.86</td>\n",
       "      <td>126.3200</td>\n",
       "      <td>124.71</td>\n",
       "      <td>126.21</td>\n",
       "      <td>3733547.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>125.82</td>\n",
       "      <td>126.3400</td>\n",
       "      <td>124.15</td>\n",
       "      <td>124.47</td>\n",
       "      <td>4157538.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5218 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      1. open   2. high  3. low  4. close   5. volume\n",
       "5217    98.50   98.8100   96.37     96.75   9551800.0\n",
       "5216    96.75   96.8100   93.69     94.81  11105400.0\n",
       "5215    95.87   95.9400   93.50     94.37  10369100.0\n",
       "5214    94.44   94.4400   90.00     91.56  16697600.0\n",
       "5213    92.75   92.9400   90.19     90.25  13737600.0\n",
       "...       ...       ...     ...       ...         ...\n",
       "4      125.90  129.4700  125.80    128.67   8195366.0\n",
       "3      129.10  129.3700  127.15    127.33   4220136.0\n",
       "2      126.48  127.6459  125.50    125.79   3531076.0\n",
       "1      124.86  126.3200  124.71    126.21   3733547.0\n",
       "0      125.82  126.3400  124.15    124.47   4157538.0\n",
       "\n",
       "[5218 rows x 5 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = data.drop(columns='date')\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sequence for the LSTM network\n",
    "backlook = 92\n",
    "\n",
    "# Days to predict\n",
    "days = 2\n",
    "\n",
    "# Size of data split for testing\n",
    "train_size = 0.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train-Test split\n",
    "split = int(historical_sequences_norm.shape[0] * train_size)\n",
    "\n",
    "train = data.to_numpy()[:split]\n",
    "test = data.to_numpy()[split:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load normalisers\n",
    "normaliser = load('./normalisers/x_normaliser.joblib')\n",
    "y_normaliser = load('./normalisers/y_normaliser.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalise data\n",
    "train_norm = normaliser.transform(train)\n",
    "test_norm = normaliser.transform(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Now I get indexes for chunks from 2 in 2 days (history doubles the backlook)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = []\n",
    "\n",
    "for i in range(days):\n",
    "    index.append(np.arange(i,len(train),days))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "ordered_index = []\n",
    "\n",
    "for i in range(len(index[0]) - backlook):\n",
    "    for e in range(days):\n",
    "        try: ordered_index.append(index[e][i:i+backlook])\n",
    "        except: pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalised train chunks\n",
    "X_train = np.array([train[ix].copy() for ix in ordered_index])\n",
    "Y_train = np.array([train[ordered_index[i+days][-1],0].copy() for i in range(len(ordered_index) - days)])\n",
    "Y_train = np.expand_dims(Y_train, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete those sequences that doesn't have a -th day in the results\n",
    "X_train = X_train[:Y_train.shape[0]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Same for test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = []\n",
    "\n",
    "for i in range(days):\n",
    "    index.append(np.arange(i,len(test),days))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ordered_index = []\n",
    "\n",
    "for i in range(len(index[0]) - backlook):\n",
    "    for e in range(days):\n",
    "        try: ordered_index.append(index[e][i:i+backlook])\n",
    "        except: pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalised train chunks\n",
    "X_test = np.array([test[ix].copy() for ix in ordered_index])\n",
    "Y_test = np.array([test[ordered_index[i+days][-1],0].copy() for i in range(len(ordered_index) - days)])\n",
    "Y_test = np.expand_dims(Y_test, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete those sequences that doesn't have a -th day in the results\n",
    "X_test = X_test[:Y_test.shape[0]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Model constructor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_lstm(x, y, batch_size=512, epochs=24, **params):\n",
    "    \n",
    "    # List of parameters\n",
    "    if 'lstmsize' not in params: params['lstmsize'] = x.shape[1]\n",
    "    if 'density' not in params: params['density'] = int((params['lstmsize']//1.5)*2)\n",
    "    if 'activation' not in params: params['activation'] = 'relu'\n",
    "    if 'twice' not in params: params['twice'] = False\n",
    "    if 'optimizer' not in params: params['optimizer'] = 'adam'\n",
    "    if 'shuffle' not in params: params['shuffle'] = False\n",
    "    \n",
    "    # Model definition\n",
    "    model = Sequential()\n",
    "    \n",
    "    model.add(LSTM(params['lstmsize'], input_shape=x.shape[1:], return_sequences=params['twice']))\n",
    "    \n",
    "    if 'dropout' in params:\n",
    "        model.add(Dropout(params['dropout']))\n",
    "    \n",
    "    if params['twice']:\n",
    "        model.add(LSTM(params['lstmsize']))\n",
    "        \n",
    "        if 'dropout' in params:\n",
    "            model.add(Dropout(params['dropout']))\n",
    "            \n",
    "    model.add(Dense(params['density'], activation=params['activation']))\n",
    "    \n",
    "    if 'full_density' in params and params['full_density']:\n",
    "        density = params['density']//2\n",
    "        while density >= 12:\n",
    "            model.add(Dense(density, activation=params['activation']))\n",
    "            density //= 2\n",
    "            \n",
    "    model.add(Dense(1, activation='linear'))\n",
    "    \n",
    "    model.compile(loss='mse', optimizer=params['optimizer'])\n",
    "    \n",
    "    if 'callbacks' in params:\n",
    "        callback = model.fit(x=x, y=y, validation_split=0.1, batch_size=batch_size, epochs=epochs, shuffle=params['shuffle'],\n",
    "                            callbacks=params['callbacks'])\n",
    "    else:\n",
    "        callback = model.fit(x=x, y=y, validation_split=0.1, batch_size=batch_size, epochs=epochs, shuffle=params['shuffle'])\n",
    "    \n",
    "    return [model, callback, params]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Evolutive algorith to search for the most optimal model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_genes(x, y, population_size, population=[]):\n",
    "    if population_size-len(population) < 0:\n",
    "        print('population size must be greater or equal than actual population')\n",
    "        return\n",
    "    \n",
    "    if len(population) > 0:\n",
    "        if len(population) < 3:\n",
    "            print('population should be empty or 3 at least')\n",
    "            return\n",
    "        \n",
    "        population[2] = breed_genes(x, y, population[0][2].copy())\n",
    "        population[1] = breed_genes(x, y, combine=(population[0][2], population[1][2]))\n",
    "        population = population[:3]\n",
    "    \n",
    "    for _ in range(population_size-len(population)):\n",
    "        subject = breed_genes(x, y)\n",
    "        population.append(subject)\n",
    "        \n",
    "    return population\n",
    "\n",
    "def breed_genes(x, y, genes={}, combine=None):\n",
    "    genes['x'] = x\n",
    "    genes['y'] = y\n",
    "    \n",
    "    if type(combine) is list or type(combine) is tuple:\n",
    "        genes['lstmsize'] = combine[np.random.randint(0,2)]['lstmsize']\n",
    "        genes['density'] = combine[np.random.randint(0,2)]['density']\n",
    "        genes['activation'] = combine[np.random.randint(0,2)]['activation']\n",
    "        genes['twice'] = combine[np.random.randint(0,2)]['twice']\n",
    "        genes['optimizer'] = combine[np.random.randint(0,2)]['optimizer']\n",
    "        genes['shuffle'] = combine[np.random.randint(0,2)]['shuffle']\n",
    "        \n",
    "        genes['dropout'] = combine[np.random.randint(0,2)].get('dropout')\n",
    "        if genes['dropout'] is None: del genes['dropout']\n",
    "        genes['full_density'] = combine[np.random.randint(0,2)].get('full_density')\n",
    "        if genes['full_density'] is None: del genes['full_density']\n",
    "            \n",
    "    else:\n",
    "        if np.random.randint(0,3) == 1:\n",
    "            genes['lstmsize'] = int((np.random.randint(x.shape[1],x.shape[1]*2)//2)*2)\n",
    "\n",
    "        if np.random.randint(0,3) == 1:\n",
    "            genes['density'] = int((np.random.randint(x.shape[1]//2,x.shape[1]*2.66)//2)*2)\n",
    "\n",
    "        if np.random.randint(0,3) == 1:\n",
    "            activation = [\n",
    "                'relu',\n",
    "                'sigmoid',\n",
    "                'softplus',\n",
    "                'softsign',\n",
    "                'tanh',\n",
    "                'selu',\n",
    "                'elu',\n",
    "                'exponential'\n",
    "            ]\n",
    "            genes['activation'] = activation[np.random.randint(0,8)]\n",
    "\n",
    "        if np.random.randint(0,3) == 1:\n",
    "            genes['twice'] = True\n",
    "\n",
    "        if np.random.randint(0,3) == 1:\n",
    "            optimizer = [\n",
    "                'sgd',\n",
    "                'rmsprop',\n",
    "                'adam',\n",
    "                'adadelta',\n",
    "                'adagrad',\n",
    "                'adamax',\n",
    "                'nadam'\n",
    "            ]\n",
    "            genes['optimizer'] = optimizer[np.random.randint(0,7)]\n",
    "\n",
    "        if np.random.randint(0,3) == 1:\n",
    "            genes['shuffle'] = True\n",
    "\n",
    "        if np.random.randint(0,3) == 1:\n",
    "            genes['dropout'] = np.random.randint(1,4)/10\n",
    "\n",
    "        if np.random.randint(0,3) == 1:\n",
    "            genes['full_density'] = True\n",
    "            \n",
    "    new_model = build_lstm(**genes)\n",
    "        \n",
    "    return new_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def breed_population(x, y, generations, population_size, population=[]):\n",
    "    \n",
    "    if generations > 1 and population_size < 3:\n",
    "        print('population size should be of a minimum of 3 for more than one generation')\n",
    "        return\n",
    "    \n",
    "    for g in range(generations):\n",
    "        print(f'\\nGENERATION {g}\\n')\n",
    "        population = set_genes(x, y, population_size, population)\n",
    "        population = sorted(population, key=lambda x: x[1].history['val_loss'][-1]+x[1].history['loss'][-1])\n",
    "        \n",
    "    return population"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "GENERATION 0\n",
      "\n",
      "WARNING:tensorflow:From C:\\ProgramData\\Anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\tensorflow\\python\\ops\\resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From C:\\ProgramData\\Anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Train on 3622 samples, validate on 403 samples\n",
      "Epoch 1/24\n",
      "3622/3622 [==============================] - 2s 444us/step - loss: 0.0355 - val_loss: 0.0303\n",
      "Epoch 2/24\n",
      "3622/3622 [==============================] - 1s 184us/step - loss: 0.0238 - val_loss: 0.0036\n",
      "Epoch 3/24\n",
      "3622/3622 [==============================] - 1s 185us/step - loss: 0.0033 - val_loss: 7.7264e-04\n",
      "Epoch 4/24\n",
      "3622/3622 [==============================] - ETA: 0s - loss: 0.007 - 1s 185us/step - loss: 0.0071 - val_loss: 0.0033\n",
      "Epoch 5/24\n",
      "3622/3622 [==============================] - 1s 190us/step - loss: 0.0107 - val_loss: 0.0061\n",
      "Epoch 6/24\n",
      "3622/3622 [==============================] - 1s 185us/step - loss: 0.0321 - val_loss: 0.0082\n",
      "Epoch 7/24\n",
      "3622/3622 [==============================] - 1s 186us/step - loss: 0.0109 - val_loss: 0.0025\n",
      "Epoch 8/24\n",
      "3622/3622 [==============================] - 1s 188us/step - loss: 0.0156 - val_loss: 0.0115\n",
      "Epoch 9/24\n",
      "3622/3622 [==============================] - 1s 194us/step - loss: 0.0011 - val_loss: 0.0029\n",
      "Epoch 10/24\n",
      "3622/3622 [==============================] - 1s 199us/step - loss: 0.0030 - val_loss: 6.3096e-04\n",
      "Epoch 11/24\n",
      "3622/3622 [==============================] - 1s 218us/step - loss: 0.0032 - val_loss: 5.5171e-04\n",
      "Epoch 12/24\n",
      "3622/3622 [==============================] - 1s 198us/step - loss: 0.0027 - val_loss: 6.2164e-04\n",
      "Epoch 13/24\n",
      "3622/3622 [==============================] - 1s 186us/step - loss: 0.0053 - val_loss: 9.6897e-04\n",
      "Epoch 14/24\n",
      "3622/3622 [==============================] - 1s 185us/step - loss: 0.0027 - val_loss: 0.0011\n",
      "Epoch 15/24\n",
      "3622/3622 [==============================] - 1s 190us/step - loss: 0.0044 - val_loss: 0.0015466\n",
      "Epoch 16/24\n",
      "3622/3622 [==============================] - 1s 186us/step - loss: 0.0014 - val_loss: 0.0013\n",
      "Epoch 17/24\n",
      "3622/3622 [==============================] - 1s 189us/step - loss: 0.0020 - val_loss: 7.8831e-04\n",
      "Epoch 18/24\n",
      "3622/3622 [==============================] - 1s 189us/step - loss: 7.1764e-04 - val_loss: 8.3266e-04\n",
      "Epoch 19/24\n",
      "3622/3622 [==============================] - 1s 190us/step - loss: 0.0010 - val_loss: 5.3197e-04\n",
      "Epoch 20/24\n",
      "3622/3622 [==============================] - 1s 190us/step - loss: 5.7390e-04 - val_loss: 6.2343e-04\n",
      "Epoch 21/24\n",
      "3622/3622 [==============================] - 1s 185us/step - loss: 8.4049e-04 - val_loss: 5.0231e-04\n",
      "Epoch 22/24\n",
      "3622/3622 [==============================] - 1s 188us/step - loss: 5.6537e-04 - val_loss: 5.6086e-04\n",
      "Epoch 23/24\n",
      "3622/3622 [==============================] - 1s 187us/step - loss: 9.2175e-04 - val_loss: 4.9596e-04\n",
      "Epoch 24/24\n",
      "3622/3622 [==============================] - 1s 189us/step - loss: 6.4775e-04 - val_loss: 5.3137e-04\n",
      "Train on 3622 samples, validate on 403 samples\n",
      "Epoch 1/24\n",
      "3622/3622 [==============================] - 1s 318us/step - loss: 0.0235 - val_loss: 0.0045\n",
      "Epoch 2/24\n",
      "3622/3622 [==============================] - 1s 190us/step - loss: 0.0107 - val_loss: 0.0131\n",
      "Epoch 3/24\n",
      "3622/3622 [==============================] - 1s 187us/step - loss: 0.0110 - val_loss: 0.0178\n",
      "Epoch 4/24\n",
      "3622/3622 [==============================] - 1s 190us/step - loss: 0.0327 - val_loss: 0.0016\n",
      "Epoch 5/24\n",
      "3622/3622 [==============================] - 1s 188us/step - loss: 0.0266 - val_loss: 0.0034\n",
      "Epoch 6/24\n",
      "3622/3622 [==============================] - 1s 186us/step - loss: 0.0543 - val_loss: 0.0164\n",
      "Epoch 7/24\n",
      "3622/3622 [==============================] - 1s 197us/step - loss: 0.0045 - val_loss: 0.0047\n",
      "Epoch 8/24\n",
      "3622/3622 [==============================] - 1s 190us/step - loss: 0.0138 - val_loss: 5.9620e-04\n",
      "Epoch 9/24\n",
      "3622/3622 [==============================] - 1s 185us/step - loss: 0.0084 - val_loss: 0.0012\n",
      "Epoch 10/24\n",
      "3622/3622 [==============================] - 1s 187us/step - loss: 0.0118 - val_loss: 0.0012\n",
      "Epoch 11/24\n",
      "3622/3622 [==============================] - 1s 188us/step - loss: 0.0049 - val_loss: 0.0052\n",
      "Epoch 12/24\n",
      "3622/3622 [==============================] - 1s 189us/step - loss: 0.0049 - val_loss: 0.0015\n",
      "Epoch 13/24\n",
      "3622/3622 [==============================] - 1s 187us/step - loss: 7.3470e-04 - val_loss: 0.0027\n",
      "Epoch 14/24\n",
      "3622/3622 [==============================] - 1s 188us/step - loss: 0.0011 - val_loss: 7.4137e-04\n",
      "Epoch 15/24\n",
      "3622/3622 [==============================] - 1s 186us/step - loss: 0.0011 - val_loss: 7.1745e-04\n",
      "Epoch 16/24\n",
      "3622/3622 [==============================] - 1s 191us/step - loss: 7.5574e-04 - val_loss: 0.0013\n",
      "Epoch 17/24\n",
      "3622/3622 [==============================] - 1s 192us/step - loss: 9.5169e-04 - val_loss: 6.4239e-04\n",
      "Epoch 18/24\n",
      "3622/3622 [==============================] - 1s 189us/step - loss: 6.6130e-04 - val_loss: 0.0011\n",
      "Epoch 19/24\n",
      "3622/3622 [==============================] - 1s 188us/step - loss: 7.6453e-04 - val_loss: 7.1099e-04\n",
      "Epoch 20/24\n",
      "3622/3622 [==============================] - 1s 188us/step - loss: 6.3777e-04 - val_loss: 8.5905e-04\n",
      "Epoch 21/24\n",
      "3622/3622 [==============================] - 1s 189us/step - loss: 6.6635e-04 - val_loss: 7.3509e-04\n",
      "Epoch 22/24\n",
      "3622/3622 [==============================] - 1s 187us/step - loss: 6.1043e-04 - val_loss: 7.6626e-04\n",
      "Epoch 23/24\n",
      "3622/3622 [==============================] - 1s 193us/step - loss: 6.2168e-04 - val_loss: 7.1104e-04\n",
      "Epoch 24/24\n",
      "3622/3622 [==============================] - 1s 186us/step - loss: 5.7796e-04 - val_loss: 7.0638e-04\n",
      "Train on 3622 samples, validate on 403 samples\n",
      "Epoch 1/24\n",
      "3622/3622 [==============================] - 1s 385us/step - loss: 1.1413 - val_loss: 0.0103\n",
      "Epoch 2/24\n",
      "3622/3622 [==============================] - 1s 260us/step - loss: 0.0112 - val_loss: 0.0021\n",
      "Epoch 3/24\n",
      "3622/3622 [==============================] - 1s 261us/step - loss: 0.0089 - val_loss: 0.0014\n",
      "Epoch 4/24\n",
      "3622/3622 [==============================] - 1s 265us/step - loss: 0.0055 - val_loss: 0.0010\n",
      "Epoch 5/24\n",
      "3622/3622 [==============================] - 1s 260us/step - loss: 0.0037 - val_loss: 8.5711e-04\n",
      "Epoch 6/24\n",
      "3622/3622 [==============================] - 1s 264us/step - loss: 0.0026 - val_loss: 7.6784e-04\n",
      "Epoch 7/24\n",
      "3622/3622 [==============================] - 1s 264us/step - loss: 0.0021 - val_loss: 7.2253e-04\n",
      "Epoch 8/24\n",
      "3622/3622 [==============================] - 1s 255us/step - loss: 0.0017 - val_loss: 6.9762e-04\n",
      "Epoch 9/24\n",
      "3622/3622 [==============================] - 1s 255us/step - loss: 0.0016 - val_loss: 6.8214e-04\n",
      "Epoch 10/24\n",
      "3622/3622 [==============================] - 1s 253us/step - loss: 0.0014 - val_loss: 6.7108e-04\n",
      "Epoch 11/24\n",
      "3622/3622 [==============================] - 1s 255us/step - loss: 0.0014 - val_loss: 6.6220e-04\n",
      "Epoch 12/24\n",
      "3622/3622 [==============================] - 1s 253us/step - loss: 0.0013 - val_loss: 6.5451e-04\n",
      "Epoch 13/24\n",
      "3622/3622 [==============================] - 1s 253us/step - loss: 0.0012 - val_loss: 6.4751e-04\n",
      "Epoch 14/24\n",
      "3622/3622 [==============================] - 1s 253us/step - loss: 0.0012 - val_loss: 6.4101e-04\n",
      "Epoch 15/24\n",
      "3622/3622 [==============================] - 1s 253us/step - loss: 0.0012 - val_loss: 6.3488e-04\n",
      "Epoch 16/24\n",
      "3622/3622 [==============================] - 1s 258us/step - loss: 0.0012 - val_loss: 6.2907e-04\n",
      "Epoch 17/24\n",
      "3622/3622 [==============================] - 1s 262us/step - loss: 0.0011 - val_loss: 6.2353e-04\n",
      "Epoch 18/24\n",
      "3622/3622 [==============================] - 1s 274us/step - loss: 0.0011 - val_loss: 6.1824e-04\n",
      "Epoch 19/24\n",
      "3622/3622 [==============================] - 1s 262us/step - loss: 0.0011 - val_loss: 6.1318e-04\n",
      "Epoch 20/24\n",
      "3622/3622 [==============================] - 1s 267us/step - loss: 0.0011 - val_loss: 6.0832e-04\n",
      "Epoch 21/24\n",
      "3622/3622 [==============================] - 1s 262us/step - loss: 0.0011 - val_loss: 6.0366e-04\n",
      "Epoch 22/24\n",
      "3622/3622 [==============================] - 1s 248us/step - loss: 0.0010 - val_loss: 5.9918e-04\n",
      "Epoch 23/24\n",
      "3622/3622 [==============================] - 1s 248us/step - loss: 0.0010 - val_loss: 5.9487e-04\n",
      "Epoch 24/24\n",
      "3622/3622 [==============================] - 1s 263us/step - loss: 0.0010 - val_loss: 5.9072e-04\n",
      "Train on 3622 samples, validate on 403 samples\n",
      "Epoch 1/24\n",
      "3622/3622 [==============================] - 1s 353us/step - loss: 0.3913 - val_loss: 9.6086e-04\n",
      "Epoch 2/24\n",
      "3622/3622 [==============================] - 1s 222us/step - loss: 0.0011 - val_loss: 0.0023\n",
      "Epoch 3/24\n",
      "3622/3622 [==============================] - 1s 222us/step - loss: 0.0039 - val_loss: 0.0923\n",
      "Epoch 4/24\n",
      "3622/3622 [==============================] - 1s 224us/step - loss: 0.0233 - val_loss: 0.0037\n",
      "Epoch 5/24\n",
      "3622/3622 [==============================] - 1s 225us/step - loss: 0.0058 - val_loss: 0.0400\n",
      "Epoch 6/24\n",
      "3622/3622 [==============================] - 1s 227us/step - loss: 0.0179 - val_loss: 0.0078\n",
      "Epoch 7/24\n",
      "3622/3622 [==============================] - 1s 223us/step - loss: 0.0076 - val_loss: 0.0274\n",
      "Epoch 8/24\n",
      "3622/3622 [==============================] - 1s 225us/step - loss: 0.0095 - val_loss: 0.0261\n",
      "Epoch 9/24\n",
      "3622/3622 [==============================] - 1s 226us/step - loss: 0.0090 - val_loss: 0.0254\n",
      "Epoch 10/24\n",
      "3622/3622 [==============================] - 1s 222us/step - loss: 0.0118 - val_loss: 0.0081\n",
      "Epoch 11/24\n",
      "3622/3622 [==============================] - 1s 223us/step - loss: 0.0030 - val_loss: 0.0317\n",
      "Epoch 12/24\n",
      "3622/3622 [==============================] - 1s 223us/step - loss: 0.0102 - val_loss: 0.0139\n",
      "Epoch 13/24\n",
      "3622/3622 [==============================] - 1s 222us/step - loss: 0.0072 - val_loss: 0.0278\n",
      "Epoch 14/24\n",
      "3622/3622 [==============================] - 1s 222us/step - loss: 0.0083 - val_loss: 0.0010\n",
      "Epoch 15/24\n",
      "3622/3622 [==============================] - 1s 222us/step - loss: 5.8418e-04 - val_loss: 0.0035\n",
      "Epoch 16/24\n",
      "3622/3622 [==============================] - 1s 223us/step - loss: 0.0138 - val_loss: 0.0035\n",
      "Epoch 17/24\n",
      "3622/3622 [==============================] - 1s 222us/step - loss: 0.0011 - val_loss: 0.0015\n",
      "Epoch 18/24\n",
      "3622/3622 [==============================] - 1s 222us/step - loss: 0.0053 - val_loss: 0.0142\n",
      "Epoch 19/24\n",
      "3622/3622 [==============================] - 1s 222us/step - loss: 0.0034 - val_loss: 0.0085\n",
      "Epoch 20/24\n",
      "3622/3622 [==============================] - 1s 223us/step - loss: 0.0052 - val_loss: 0.0095\n",
      "Epoch 21/24\n",
      "3622/3622 [==============================] - 1s 239us/step - loss: 0.0053 - val_loss: 0.0038\n",
      "Epoch 22/24\n",
      "3622/3622 [==============================] - 1s 222us/step - loss: 0.0027 - val_loss: 0.0072\n",
      "Epoch 23/24\n",
      "3622/3622 [==============================] - 1s 227us/step - loss: 0.0068 - val_loss: 0.0030\n",
      "Epoch 24/24\n",
      "3622/3622 [==============================] - 1s 225us/step - loss: 0.0013 - val_loss: 0.0075\n",
      "Train on 3622 samples, validate on 403 samples\n",
      "Epoch 1/24\n",
      "3622/3622 [==============================] - 1s 360us/step - loss: 0.0526 - val_loss: 0.0016\n",
      "Epoch 2/24\n",
      "3622/3622 [==============================] - 1s 204us/step - loss: 0.0025 - val_loss: 0.0019\n",
      "Epoch 3/24\n",
      "3622/3622 [==============================] - 1s 199us/step - loss: 0.0019 - val_loss: 0.0011\n",
      "Epoch 4/24\n",
      "3622/3622 [==============================] - 1s 195us/step - loss: 0.0032 - val_loss: 0.0206\n",
      "Epoch 5/24\n",
      "3622/3622 [==============================] - 1s 205us/step - loss: 0.0153 - val_loss: 0.0159\n",
      "Epoch 6/24\n",
      "3622/3622 [==============================] - 1s 201us/step - loss: 0.0059 - val_loss: 0.0030\n",
      "Epoch 7/24\n",
      "3622/3622 [==============================] - 1s 194us/step - loss: 0.0019 - val_loss: 0.0052\n",
      "Epoch 8/24\n",
      "3622/3622 [==============================] - 1s 197us/step - loss: 0.0029 - val_loss: 0.0081\n",
      "Epoch 9/24\n",
      "3622/3622 [==============================] - 1s 194us/step - loss: 0.0076 - val_loss: 0.0033\n",
      "Epoch 10/24\n",
      "3622/3622 [==============================] - 1s 195us/step - loss: 0.0019 - val_loss: 0.0019\n",
      "Epoch 11/24\n",
      "3622/3622 [==============================] - 1s 193us/step - loss: 0.0025 - val_loss: 0.0086\n",
      "Epoch 12/24\n",
      "3622/3622 [==============================] - 1s 194us/step - loss: 0.0088 - val_loss: 0.0176\n",
      "Epoch 13/24\n",
      "3622/3622 [==============================] - 1s 195us/step - loss: 0.0066 - val_loss: 0.0055\n",
      "Epoch 14/24\n",
      "3622/3622 [==============================] - 1s 194us/step - loss: 0.0022 - val_loss: 0.0037\n",
      "Epoch 15/24\n",
      "3622/3622 [==============================] - 1s 197us/step - loss: 0.0020 - val_loss: 0.0027\n",
      "Epoch 16/24\n",
      "3622/3622 [==============================] - 1s 203us/step - loss: 0.0021 - val_loss: 0.0045\n",
      "Epoch 17/24\n",
      "3622/3622 [==============================] - 1s 194us/step - loss: 0.0050 - val_loss: 0.0162\n",
      "Epoch 18/24\n",
      "3622/3622 [==============================] - 1s 196us/step - loss: 0.0073 - val_loss: 0.0038\n",
      "Epoch 19/24\n",
      "3622/3622 [==============================] - 1s 196us/step - loss: 0.0017 - val_loss: 0.0028\n",
      "Epoch 20/24\n",
      "3622/3622 [==============================] - 1s 194us/step - loss: 0.0018 - val_loss: 0.0025\n",
      "Epoch 21/24\n",
      "3622/3622 [==============================] - 1s 196us/step - loss: 0.0020 - val_loss: 0.0201\n",
      "Epoch 22/24\n",
      "3622/3622 [==============================] - 1s 195us/step - loss: 0.0082 - val_loss: 0.0149\n",
      "Epoch 23/24\n",
      "3622/3622 [==============================] - 1s 196us/step - loss: 0.0045 - val_loss: 0.0015\n",
      "Epoch 24/24\n",
      "3622/3622 [==============================] - 1s 196us/step - loss: 0.0011 - val_loss: 0.0018\n",
      "Train on 3622 samples, validate on 403 samples\n",
      "Epoch 1/24\n",
      "3622/3622 [==============================] - 3s 848us/step - loss: 0.0734 - val_loss: 0.0026\n",
      "Epoch 2/24\n",
      "3622/3622 [==============================] - 2s 586us/step - loss: 0.0021 - val_loss: 0.0024\n",
      "Epoch 3/24\n",
      "3622/3622 [==============================] - 2s 584us/step - loss: 0.0018 - val_loss: 0.0020\n",
      "Epoch 4/24\n",
      "3622/3622 [==============================] - 2s 581us/step - loss: 0.0018 - val_loss: 0.0027\n",
      "Epoch 5/24\n",
      "3622/3622 [==============================] - 2s 585us/step - loss: 0.0041 - val_loss: 0.0286\n",
      "Epoch 6/24\n",
      "3622/3622 [==============================] - 2s 588us/step - loss: 0.0137 - val_loss: 0.0305\n",
      "Epoch 7/24\n",
      "3622/3622 [==============================] - 2s 587us/step - loss: 0.0094 - val_loss: 0.0057\n",
      "Epoch 8/24\n",
      "3622/3622 [==============================] - 2s 593us/step - loss: 0.0047 - val_loss: 0.0032\n",
      "Epoch 9/24\n",
      "3622/3622 [==============================] - 2s 590us/step - loss: 0.0037 - val_loss: 0.0138\n",
      "Epoch 10/24\n",
      "3622/3622 [==============================] - 2s 586us/step - loss: 0.0083 - val_loss: 0.0092\n",
      "Epoch 11/24\n",
      "3622/3622 [==============================] - 2s 583us/step - loss: 0.0075 - val_loss: 0.0091\n",
      "Epoch 12/24\n",
      "3622/3622 [==============================] - 2s 586us/step - loss: 0.0084 - val_loss: 0.0174\n",
      "Epoch 13/24\n",
      "3622/3622 [==============================] - 2s 586us/step - loss: 0.0085 - val_loss: 0.0082\n",
      "Epoch 14/24\n",
      "3622/3622 [==============================] - 2s 588us/step - loss: 0.0045 - val_loss: 0.0175\n",
      "Epoch 15/24\n",
      "3622/3622 [==============================] - 2s 589us/step - loss: 0.0075 - val_loss: 0.0133\n",
      "Epoch 16/24\n",
      "3622/3622 [==============================] - 2s 585us/step - loss: 0.0060 - val_loss: 0.0044\n",
      "Epoch 17/24\n",
      "3622/3622 [==============================] - 2s 583us/step - loss: 0.0033 - val_loss: 0.0176\n",
      "Epoch 18/24\n",
      "3622/3622 [==============================] - 2s 583us/step - loss: 0.0070 - val_loss: 0.0025\n",
      "Epoch 19/24\n",
      "3622/3622 [==============================] - 2s 586us/step - loss: 0.0017 - val_loss: 0.0028\n",
      "Epoch 20/24\n",
      "3622/3622 [==============================] - 2s 588us/step - loss: 0.0025 - val_loss: 0.0082\n",
      "Epoch 21/24\n",
      "3622/3622 [==============================] - 2s 583us/step - loss: 0.0079 - val_loss: 0.0097\n",
      "Epoch 22/24\n",
      "3622/3622 [==============================] - 2s 588us/step - loss: 0.0060 - val_loss: 0.0292\n",
      "Epoch 23/24\n",
      "3622/3622 [==============================] - 2s 583us/step - loss: 0.0086 - val_loss: 0.0029\n",
      "Epoch 24/24\n",
      "3622/3622 [==============================] - 2s 585us/step - loss: 0.0016 - val_loss: 0.0041\n",
      "Train on 3622 samples, validate on 403 samples\n",
      "Epoch 1/24\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3622/3622 [==============================] - 3s 869us/step - loss: 0.1115 - val_loss: 0.0014\n",
      "Epoch 2/24\n",
      "3622/3622 [==============================] - 2s 585us/step - loss: 0.0036 - val_loss: 0.0014\n",
      "Epoch 3/24\n",
      "3622/3622 [==============================] - 2s 585us/step - loss: 0.0030 - val_loss: 0.0033\n",
      "Epoch 4/24\n",
      "3622/3622 [==============================] - 2s 585us/step - loss: 0.0025 - val_loss: 0.0024\n",
      "Epoch 5/24\n",
      "3622/3622 [==============================] - 2s 585us/step - loss: 0.0032 - val_loss: 0.0114\n",
      "Epoch 6/24\n",
      "3622/3622 [==============================] - 2s 586us/step - loss: 0.0074 - val_loss: 0.0033\n",
      "Epoch 7/24\n",
      "3622/3622 [==============================] - 2s 586us/step - loss: 0.0030 - val_loss: 0.0100\n",
      "Epoch 8/24\n",
      "3622/3622 [==============================] - 2s 582us/step - loss: 0.0075 - val_loss: 0.0190\n",
      "Epoch 9/24\n",
      "3622/3622 [==============================] - 2s 588us/step - loss: 0.0107 - val_loss: 0.0204\n",
      "Epoch 10/24\n",
      "3622/3622 [==============================] - 2s 585us/step - loss: 0.0089 - val_loss: 0.0050\n",
      "Epoch 11/24\n",
      "3622/3622 [==============================] - 2s 585us/step - loss: 0.0043 - val_loss: 0.0123\n",
      "Epoch 12/24\n",
      "3622/3622 [==============================] - 2s 585us/step - loss: 0.0080 - val_loss: 0.0058\n",
      "Epoch 13/24\n",
      "3622/3622 [==============================] - 2s 585us/step - loss: 0.0051 - val_loss: 0.0134\n",
      "Epoch 14/24\n",
      "3622/3622 [==============================] - 2s 584us/step - loss: 0.0107 - val_loss: 0.0146\n",
      "Epoch 15/24\n",
      "3622/3622 [==============================] - 2s 585us/step - loss: 0.0085 - val_loss: 0.0081\n",
      "Epoch 16/24\n",
      "3622/3622 [==============================] - 2s 591us/step - loss: 0.0053 - val_loss: 0.0200\n",
      "Epoch 17/24\n",
      "3622/3622 [==============================] - 2s 583us/step - loss: 0.0097 - val_loss: 0.0237\n",
      "Epoch 18/24\n",
      "3622/3622 [==============================] - 2s 585us/step - loss: 0.0088 - val_loss: 0.0011\n",
      "Epoch 19/24\n",
      "3622/3622 [==============================] - 2s 591us/step - loss: 0.0017 - val_loss: 0.0016\n",
      "Epoch 20/24\n",
      "3622/3622 [==============================] - 2s 584us/step - loss: 0.0016 - val_loss: 0.0056\n",
      "Epoch 21/24\n",
      "3622/3622 [==============================] - 2s 585us/step - loss: 0.0068 - val_loss: 0.0035\n",
      "Epoch 22/24\n",
      "3622/3622 [==============================] - 2s 586us/step - loss: 0.0042 - val_loss: 0.0154\n",
      "Epoch 23/24\n",
      "3622/3622 [==============================] - 2s 590us/step - loss: 0.0104 - val_loss: 0.0070\n",
      "Epoch 24/24\n",
      "3622/3622 [==============================] - 2s 585us/step - loss: 0.0061 - val_loss: 0.0043\n",
      "Train on 3622 samples, validate on 403 samples\n",
      "Epoch 1/24\n",
      "3622/3622 [==============================] - 3s 886us/step - loss: 0.0697 - val_loss: 0.0246\n",
      "Epoch 2/24\n",
      "3622/3622 [==============================] - 2s 586us/step - loss: 0.0162 - val_loss: 0.0155\n",
      "Epoch 3/24\n",
      "3622/3622 [==============================] - 2s 584us/step - loss: 0.0083 - val_loss: 0.0222\n",
      "Epoch 4/24\n",
      "3622/3622 [==============================] - 2s 586us/step - loss: 0.0097 - val_loss: 0.0036\n",
      "Epoch 5/24\n",
      "3622/3622 [==============================] - 2s 586us/step - loss: 0.0024 - val_loss: 0.0028\n",
      "Epoch 6/24\n",
      "3622/3622 [==============================] - 2s 583us/step - loss: 0.0030 - val_loss: 0.0130\n",
      "Epoch 7/24\n",
      "3622/3622 [==============================] - 2s 585us/step - loss: 0.0093 - val_loss: 0.0109\n",
      "Epoch 8/24\n",
      "3622/3622 [==============================] - 2s 594us/step - loss: 0.0060 - val_loss: 0.0017\n",
      "Epoch 9/24\n",
      "3622/3622 [==============================] - 2s 590us/step - loss: 0.0023 - val_loss: 0.0046\n",
      "Epoch 10/24\n",
      "3622/3622 [==============================] - 2s 587us/step - loss: 0.0092 - val_loss: 0.0103\n",
      "Epoch 11/24\n",
      "3622/3622 [==============================] - 2s 587us/step - loss: 0.0090 - val_loss: 0.0031\n",
      "Epoch 12/24\n",
      "3622/3622 [==============================] - 2s 585us/step - loss: 0.0056 - val_loss: 0.0139\n",
      "Epoch 13/24\n",
      "3622/3622 [==============================] - 2s 585us/step - loss: 0.0076 - val_loss: 0.0037\n",
      "Epoch 14/24\n",
      "3622/3622 [==============================] - 2s 585us/step - loss: 0.0067 - val_loss: 0.0059\n",
      "Epoch 15/24\n",
      "3622/3622 [==============================] - 2s 585us/step - loss: 0.0065 - val_loss: 0.0020\n",
      "Epoch 16/24\n",
      "3622/3622 [==============================] - 2s 587us/step - loss: 0.0047 - val_loss: 0.0104\n",
      "Epoch 17/24\n",
      "3622/3622 [==============================] - 2s 587us/step - loss: 0.0070 - val_loss: 0.0021\n",
      "Epoch 18/24\n",
      "3622/3622 [==============================] - 2s 584us/step - loss: 0.0047 - val_loss: 0.0030\n",
      "Epoch 19/24\n",
      "3622/3622 [==============================] - 2s 585us/step - loss: 0.0058 - val_loss: 0.0047\n",
      "Epoch 20/24\n",
      "3622/3622 [==============================] - 2s 585us/step - loss: 0.0069 - val_loss: 0.0020\n",
      "Epoch 21/24\n",
      "3622/3622 [==============================] - 2s 587us/step - loss: 0.0038 - val_loss: 0.0011\n",
      "Epoch 22/24\n",
      "3622/3622 [==============================] - 2s 587us/step - loss: 0.0036 - val_loss: 0.0026\n",
      "Epoch 23/24\n",
      "3622/3622 [==============================] - 2s 589us/step - loss: 0.0050 - val_loss: 0.0036\n",
      "Epoch 24/24\n",
      "3622/3622 [==============================] - 2s 587us/step - loss: 0.0048 - val_loss: 0.0047\n",
      "Train on 3622 samples, validate on 403 samples\n",
      "Epoch 1/24\n",
      "3622/3622 [==============================] - 3s 893us/step - loss: 0.3178 - val_loss: 0.0065\n",
      "Epoch 2/24\n",
      "3622/3622 [==============================] - 2s 585us/step - loss: 0.0057 - val_loss: 0.0071\n",
      "Epoch 3/24\n",
      "3622/3622 [==============================] - 2s 585us/step - loss: 0.0120 - val_loss: 0.0125\n",
      "Epoch 4/24\n",
      "3622/3622 [==============================] - 2s 585us/step - loss: 0.0107 - val_loss: 0.0120\n",
      "Epoch 5/24\n",
      "3622/3622 [==============================] - 2s 587us/step - loss: 0.0129 - val_loss: 0.0274\n",
      "Epoch 6/24\n",
      "3622/3622 [==============================] - 2s 588us/step - loss: 0.0118 - val_loss: 0.0175\n",
      "Epoch 7/24\n",
      "3622/3622 [==============================] - 2s 586us/step - loss: 0.0144 - val_loss: 0.0217\n",
      "Epoch 8/24\n",
      "3622/3622 [==============================] - 2s 584us/step - loss: 0.0161 - val_loss: 0.0383\n",
      "Epoch 9/24\n",
      "3622/3622 [==============================] - 2s 584us/step - loss: 0.0113 - val_loss: 0.0073\n",
      "Epoch 10/24\n",
      "3622/3622 [==============================] - 2s 586us/step - loss: 0.0113 - val_loss: 0.0206\n",
      "Epoch 11/24\n",
      "3622/3622 [==============================] - 2s 587us/step - loss: 0.0174 - val_loss: 0.0076\n",
      "Epoch 12/24\n",
      "3622/3622 [==============================] - 2s 585us/step - loss: 0.0046 - val_loss: 0.0096\n",
      "Epoch 13/24\n",
      "3622/3622 [==============================] - 2s 590us/step - loss: 0.0124 - val_loss: 0.0477\n",
      "Epoch 14/24\n",
      "3622/3622 [==============================] - 2s 588us/step - loss: 0.0114 - val_loss: 0.0113\n",
      "Epoch 15/24\n",
      "3622/3622 [==============================] - 2s 585us/step - loss: 0.0063 - val_loss: 0.0379\n",
      "Epoch 16/24\n",
      "3622/3622 [==============================] - 2s 588us/step - loss: 0.0141 - val_loss: 0.0030\n",
      "Epoch 17/24\n",
      "3622/3622 [==============================] - 2s 584us/step - loss: 0.0017 - val_loss: 7.6105e-04\n",
      "Epoch 18/24\n",
      "3622/3622 [==============================] - 2s 586us/step - loss: 0.0150 - val_loss: 0.0047\n",
      "Epoch 19/24\n",
      "3622/3622 [==============================] - 2s 584us/step - loss: 0.0050 - val_loss: 0.0052\n",
      "Epoch 20/24\n",
      "3622/3622 [==============================] - 2s 587us/step - loss: 0.0070 - val_loss: 0.0103\n",
      "Epoch 21/24\n",
      "3622/3622 [==============================] - 2s 585us/step - loss: 0.0084 - val_loss: 0.0044\n",
      "Epoch 22/24\n",
      "3622/3622 [==============================] - 2s 587us/step - loss: 0.0040 - val_loss: 0.0065\n",
      "Epoch 23/24\n",
      "3622/3622 [==============================] - 2s 585us/step - loss: 0.0098 - val_loss: 0.0028\n",
      "Epoch 24/24\n",
      "3622/3622 [==============================] - 2s 586us/step - loss: 0.0037 - val_loss: 0.0051\n",
      "Train on 3622 samples, validate on 403 samples\n",
      "Epoch 1/24\n",
      "3622/3622 [==============================] - 3s 830us/step - loss: 0.1344 - val_loss: 0.0017\n",
      "Epoch 2/24\n",
      "3622/3622 [==============================] - 2s 529us/step - loss: 0.0036 - val_loss: 0.0012\n",
      "Epoch 3/24\n",
      "3622/3622 [==============================] - 2s 538us/step - loss: 0.0097 - val_loss: 0.0022\n",
      "Epoch 4/24\n",
      "3622/3622 [==============================] - 2s 527us/step - loss: 0.0038 - val_loss: 0.0254\n",
      "Epoch 5/24\n",
      "3622/3622 [==============================] - 2s 526us/step - loss: 0.0112 - val_loss: 0.0141\n",
      "Epoch 6/24\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3622/3622 [==============================] - 2s 526us/step - loss: 0.0103 - val_loss: 0.0048\n",
      "Epoch 7/24\n",
      "3622/3622 [==============================] - 2s 530us/step - loss: 0.0098 - val_loss: 0.0035\n",
      "Epoch 8/24\n",
      "3622/3622 [==============================] - 2s 529us/step - loss: 0.0078 - val_loss: 0.0176\n",
      "Epoch 9/24\n",
      "3622/3622 [==============================] - 2s 528us/step - loss: 0.0129 - val_loss: 0.0049\n",
      "Epoch 10/24\n",
      "3622/3622 [==============================] - 2s 525us/step - loss: 0.0024 - val_loss: 0.0201\n",
      "Epoch 11/24\n",
      "3622/3622 [==============================] - 2s 532us/step - loss: 0.0124 - val_loss: 0.0128\n",
      "Epoch 12/24\n",
      "3622/3622 [==============================] - 2s 529us/step - loss: 0.0032 - val_loss: 0.0059\n",
      "Epoch 13/24\n",
      "3622/3622 [==============================] - 2s 531us/step - loss: 0.0079 - val_loss: 0.0571\n",
      "Epoch 14/24\n",
      "3622/3622 [==============================] - 2s 527us/step - loss: 0.0086 - val_loss: 0.0043\n",
      "Epoch 15/24\n",
      "3622/3622 [==============================] - 2s 529us/step - loss: 0.0016 - val_loss: 0.0161\n",
      "Epoch 16/24\n",
      "3622/3622 [==============================] - 2s 530us/step - loss: 0.0114 - val_loss: 0.0039\n",
      "Epoch 17/24\n",
      "3622/3622 [==============================] - 2s 530us/step - loss: 0.0016 - val_loss: 0.0174\n",
      "Epoch 18/24\n",
      "3622/3622 [==============================] - 2s 530us/step - loss: 0.0083 - val_loss: 0.0045\n",
      "Epoch 19/24\n",
      "3622/3622 [==============================] - 2s 534us/step - loss: 0.0020 - val_loss: 0.0241\n",
      "Epoch 20/24\n",
      "3622/3622 [==============================] - 2s 530us/step - loss: 0.0074 - val_loss: 0.0066\n",
      "Epoch 21/24\n",
      "3622/3622 [==============================] - 2s 532us/step - loss: 0.0036 - val_loss: 0.0054\n",
      "Epoch 22/24\n",
      "3622/3622 [==============================] - 2s 526us/step - loss: 0.0035 - val_loss: 0.0244\n",
      "Epoch 23/24\n",
      "3622/3622 [==============================] - 2s 528us/step - loss: 0.0059 - val_loss: 0.0079\n",
      "Epoch 24/24\n",
      "3622/3622 [==============================] - 2s 531us/step - loss: 0.0024 - val_loss: 0.0161\n",
      "\n",
      "GENERATION 1\n",
      "\n",
      "Train on 3622 samples, validate on 403 samples\n",
      "Epoch 1/24\n",
      "3622/3622 [==============================] - 2s 433us/step - loss: nan - val_loss: nan\n",
      "Epoch 2/24\n",
      "3622/3622 [==============================] - 1s 181us/step - loss: nan - val_loss: nan\n",
      "Epoch 3/24\n",
      "3622/3622 [==============================] - 1s 182us/step - loss: nan - val_loss: nan\n",
      "Epoch 4/24\n",
      "3622/3622 [==============================] - 1s 180us/step - loss: nan - val_loss: nan\n",
      "Epoch 5/24\n",
      "3622/3622 [==============================] - 1s 181us/step - loss: nan - val_loss: nan\n",
      "Epoch 6/24\n",
      "3622/3622 [==============================] - 1s 185us/step - loss: nan - val_loss: nan\n",
      "Epoch 7/24\n",
      "3622/3622 [==============================] - 1s 184us/step - loss: nan - val_loss: nan\n",
      "Epoch 8/24\n",
      "3622/3622 [==============================] - 1s 181us/step - loss: nan - val_loss: nan\n",
      "Epoch 9/24\n",
      "3622/3622 [==============================] - 1s 180us/step - loss: nan - val_loss: nan\n",
      "Epoch 10/24\n",
      "3622/3622 [==============================] - 1s 181us/step - loss: nan - val_loss: nan\n",
      "Epoch 11/24\n",
      "3622/3622 [==============================] - 1s 181us/step - loss: nan - val_loss: nan\n",
      "Epoch 12/24\n",
      "3622/3622 [==============================] - 1s 181us/step - loss: nan - val_loss: nan\n",
      "Epoch 13/24\n",
      "3622/3622 [==============================] - 1s 181us/step - loss: nan - val_loss: nan\n",
      "Epoch 14/24\n",
      "3622/3622 [==============================] - 1s 182us/step - loss: nan - val_loss: nan\n",
      "Epoch 15/24\n",
      "3622/3622 [==============================] - 1s 181us/step - loss: nan - val_loss: nan\n",
      "Epoch 16/24\n",
      "3622/3622 [==============================] - 1s 181us/step - loss: nan - val_loss: nan\n",
      "Epoch 17/24\n",
      "3622/3622 [==============================] - 1s 182us/step - loss: nan - val_loss: nan\n",
      "Epoch 18/24\n",
      "3622/3622 [==============================] - 1s 182us/step - loss: nan - val_loss: nan\n",
      "Epoch 19/24\n",
      "3622/3622 [==============================] - 1s 181us/step - loss: nan - val_loss: nan\n",
      "Epoch 20/24\n",
      "3622/3622 [==============================] - 1s 182us/step - loss: nan - val_loss: nan\n",
      "Epoch 21/24\n",
      "3622/3622 [==============================] - 1s 183us/step - loss: nan - val_loss: nan\n",
      "Epoch 22/24\n",
      "3622/3622 [==============================] - 1s 183us/step - loss: nan - val_loss: nan\n",
      "Epoch 23/24\n",
      "3622/3622 [==============================] - 1s 181us/step - loss: nan - val_loss: nan\n",
      "Epoch 24/24\n",
      "3622/3622 [==============================] - 1s 185us/step - loss: nan - val_loss: nan\n",
      "Train on 3622 samples, validate on 403 samples\n",
      "Epoch 1/24\n",
      "3622/3622 [==============================] - 2s 457us/step - loss: 0.0160 - val_loss: 0.0058\n",
      "Epoch 2/24\n",
      "3622/3622 [==============================] - 1s 184us/step - loss: 0.0333 - val_loss: 0.0128\n",
      "Epoch 3/24\n",
      "3622/3622 [==============================] - 1s 185us/step - loss: 0.0237 - val_loss: 0.0016\n",
      "Epoch 4/24\n",
      "3622/3622 [==============================] - 1s 182us/step - loss: 0.0026 - val_loss: 0.0026\n",
      "Epoch 5/24\n",
      "3622/3622 [==============================] - 1s 182us/step - loss: 0.0013 - val_loss: 0.0054\n",
      "Epoch 6/24\n",
      "3622/3622 [==============================] - 1s 181us/step - loss: 0.0021 - val_loss: 6.8913e-04\n",
      "Epoch 7/24\n",
      "3622/3622 [==============================] - 1s 188us/step - loss: 0.0080 - val_loss: 0.0131\n",
      "Epoch 8/24\n",
      "3622/3622 [==============================] - 1s 182us/step - loss: 0.0091 - val_loss: 0.0089\n",
      "Epoch 9/24\n",
      "3622/3622 [==============================] - 1s 184us/step - loss: 0.0160 - val_loss: 0.0015\n",
      "Epoch 10/24\n",
      "3622/3622 [==============================] - 1s 182us/step - loss: 0.0218 - val_loss: 0.0040\n",
      "Epoch 11/24\n",
      "3622/3622 [==============================] - 1s 182us/step - loss: 0.0460 - val_loss: 0.0287\n",
      "Epoch 12/24\n",
      "3622/3622 [==============================] - 1s 183us/step - loss: 0.0021 - val_loss: 0.0061\n",
      "Epoch 13/24\n",
      "3622/3622 [==============================] - 1s 183us/step - loss: 0.0076 - val_loss: 6.0796e-04\n",
      "Epoch 14/24\n",
      "3622/3622 [==============================] - 1s 182us/step - loss: 0.0017 - val_loss: 5.3907e-04\n",
      "Epoch 15/24\n",
      "3622/3622 [==============================] - 1s 182us/step - loss: 0.0020 - val_loss: 0.0014\n",
      "Epoch 16/24\n",
      "3622/3622 [==============================] - 1s 183us/step - loss: 0.0022 - val_loss: 7.8048e-04\n",
      "Epoch 17/24\n",
      "3622/3622 [==============================] - 1s 181us/step - loss: 0.0080 - val_loss: 5.6341e-04\n",
      "Epoch 18/24\n",
      "3622/3622 [==============================] - 1s 183us/step - loss: 0.0078 - val_loss: 5.4392e-04\n",
      "Epoch 19/24\n",
      "3622/3622 [==============================] - 1s 190us/step - loss: 0.0168 - val_loss: 0.0061\n",
      "Epoch 20/24\n",
      "3622/3622 [==============================] - 1s 186us/step - loss: 0.0038 - val_loss: 0.0038\n",
      "Epoch 21/24\n",
      "3622/3622 [==============================] - 1s 184us/step - loss: 0.0043 - val_loss: 0.0019\n",
      "Epoch 22/24\n",
      "3622/3622 [==============================] - 1s 200us/step - loss: 5.8427e-04 - val_loss: 0.0014-\n",
      "Epoch 23/24\n",
      "3622/3622 [==============================] - 1s 183us/step - loss: 6.6788e-04 - val_loss: 5.0744e-04\n",
      "Epoch 24/24\n",
      "3622/3622 [==============================] - 1s 189us/step - loss: 7.3013e-04 - val_loss: 6.9939e-04\n",
      "Train on 3622 samples, validate on 403 samples\n",
      "Epoch 1/24\n",
      "3622/3622 [==============================] - 2s 475us/step - loss: 0.0199 - val_loss: 0.0049\n",
      "Epoch 2/24\n",
      "3622/3622 [==============================] - 1s 183us/step - loss: 0.0259 - val_loss: 0.0025\n",
      "Epoch 3/24\n",
      "3622/3622 [==============================] - 1s 182us/step - loss: 0.0238 - val_loss: 0.0125\n",
      "Epoch 4/24\n",
      "3622/3622 [==============================] - 1s 184us/step - loss: 0.0332 - val_loss: 0.0179\n",
      "Epoch 5/24\n",
      "3622/3622 [==============================] - 1s 182us/step - loss: 0.0089 - val_loss: 6.9184e-04\n",
      "Epoch 6/24\n",
      "3622/3622 [==============================] - 1s 181us/step - loss: 0.0162 - val_loss: 0.0108\n",
      "Epoch 7/24\n",
      "3622/3622 [==============================] - 1s 181us/step - loss: 0.0034 - val_loss: 0.0011\n",
      "Epoch 8/24\n",
      "3622/3622 [==============================] - 1s 183us/step - loss: 0.0047 - val_loss: 0.0022\n",
      "Epoch 9/24\n",
      "3622/3622 [==============================] - 1s 183us/step - loss: 0.0022 - val_loss: 9.8610e-04\n",
      "Epoch 10/24\n",
      "3622/3622 [==============================] - 1s 183us/step - loss: 0.0024 - val_loss: 0.0013\n",
      "Epoch 11/24\n",
      "3622/3622 [==============================] - 1s 188us/step - loss: 0.0022 - val_loss: 0.0023\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12/24\n",
      "3622/3622 [==============================] - 1s 188us/step - loss: 0.0050 - val_loss: 0.0015\n",
      "Epoch 13/24\n",
      "3622/3622 [==============================] - 1s 184us/step - loss: 0.0054 - val_loss: 0.0026\n",
      "Epoch 14/24\n",
      "3622/3622 [==============================] - 1s 181us/step - loss: 0.0130 - val_loss: 0.0057\n",
      "Epoch 15/24\n",
      "3622/3622 [==============================] - 1s 183us/step - loss: 0.0071 - val_loss: 6.9919e-04\n",
      "Epoch 16/24\n",
      "3622/3622 [==============================] - 1s 181us/step - loss: 0.0117 - val_loss: 0.0070\n",
      "Epoch 17/24\n",
      "3622/3622 [==============================] - 1s 182us/step - loss: 0.0046 - val_loss: 6.3538e-04\n",
      "Epoch 18/24\n",
      "3622/3622 [==============================] - 1s 181us/step - loss: 0.0076 - val_loss: 0.0041\n",
      "Epoch 19/24\n",
      "3622/3622 [==============================] - 1s 183us/step - loss: 0.0036 - val_loss: 5.1420e-04\n",
      "Epoch 20/24\n",
      "3622/3622 [==============================] - 1s 182us/step - loss: 0.0046 - val_loss: 0.0026\n",
      "Epoch 21/24\n",
      "3622/3622 [==============================] - 1s 183us/step - loss: 0.0024 - val_loss: 5.3569e-04\n",
      "Epoch 22/24\n",
      "3622/3622 [==============================] - 1s 185us/step - loss: 0.0030 - val_loss: 0.0020\n",
      "Epoch 23/24\n",
      "3622/3622 [==============================] - 1s 182us/step - loss: 0.0018 - val_loss: 6.8527e-04\n",
      "Epoch 24/24\n",
      "3622/3622 [==============================] - 1s 181us/step - loss: 0.0022 - val_loss: 0.0016\n",
      "Train on 3622 samples, validate on 403 samples\n",
      "Epoch 1/24\n",
      "3622/3622 [==============================] - 2s 486us/step - loss: 0.1862 - val_loss: 0.0464\n",
      "Epoch 2/24\n",
      "3622/3622 [==============================] - 1s 164us/step - loss: 0.0260 - val_loss: 0.0055\n",
      "Epoch 3/24\n",
      "3622/3622 [==============================] - 1s 164us/step - loss: 0.0136 - val_loss: 0.0053\n",
      "Epoch 4/24\n",
      "3622/3622 [==============================] - 1s 168us/step - loss: 0.0071 - val_loss: 0.0013\n",
      "Epoch 5/24\n",
      "3622/3622 [==============================] - 1s 170us/step - loss: 0.0044 - val_loss: 0.0028\n",
      "Epoch 6/24\n",
      "3622/3622 [==============================] - 1s 165us/step - loss: 0.0032 - val_loss: 7.4030e-04\n",
      "Epoch 7/24\n",
      "3622/3622 [==============================] - 1s 166us/step - loss: 0.0025 - val_loss: 0.0010\n",
      "Epoch 8/24\n",
      "3622/3622 [==============================] - 1s 174us/step - loss: 0.0025 - val_loss: 0.0013\n",
      "Epoch 9/24\n",
      "3622/3622 [==============================] - 1s 171us/step - loss: 0.0021 - val_loss: 0.0012\n",
      "Epoch 10/24\n",
      "3622/3622 [==============================] - 1s 164us/step - loss: 0.0019 - val_loss: 0.0014\n",
      "Epoch 11/24\n",
      "3622/3622 [==============================] - 1s 164us/step - loss: 0.0019 - val_loss: 0.0016\n",
      "Epoch 12/24\n",
      "3622/3622 [==============================] - 1s 164us/step - loss: 0.0017 - val_loss: 0.0014\n",
      "Epoch 13/24\n",
      "3622/3622 [==============================] - 1s 174us/step - loss: 0.0017 - val_loss: 0.0016\n",
      "Epoch 14/24\n",
      "3622/3622 [==============================] - 1s 175us/step - loss: 0.0017 - val_loss: 0.0028\n",
      "Epoch 15/24\n",
      "3622/3622 [==============================] - 1s 173us/step - loss: 0.0016 - val_loss: 0.0019\n",
      "Epoch 16/24\n",
      "3622/3622 [==============================] - 1s 167us/step - loss: 0.0016 - val_loss: 0.0015\n",
      "Epoch 17/24\n",
      "3622/3622 [==============================] - 1s 165us/step - loss: 0.0016 - val_loss: 9.6951e-04\n",
      "Epoch 18/24\n",
      "3622/3622 [==============================] - 1s 166us/step - loss: 0.0016 - val_loss: 0.0016\n",
      "Epoch 19/24\n",
      "3622/3622 [==============================] - 1s 164us/step - loss: 0.0015 - val_loss: 0.0025\n",
      "Epoch 20/24\n",
      "3622/3622 [==============================] - 1s 162us/step - loss: 0.0015 - val_loss: 0.0040\n",
      "Epoch 21/24\n",
      "3622/3622 [==============================] - 1s 161us/step - loss: 0.0015 - val_loss: 0.0019\n",
      "Epoch 22/24\n",
      "3622/3622 [==============================] - 1s 162us/step - loss: 0.0014 - val_loss: 0.0018\n",
      "Epoch 23/24\n",
      "3622/3622 [==============================] - 1s 162us/step - loss: 0.0014 - val_loss: 0.0018\n",
      "Epoch 24/24\n",
      "3622/3622 [==============================] - 1s 162us/step - loss: 0.0012 - val_loss: 0.0021\n",
      "Train on 3622 samples, validate on 403 samples\n",
      "Epoch 1/24\n",
      "3622/3622 [==============================] - 2s 487us/step - loss: 0.1820 - val_loss: 0.0152\n",
      "Epoch 2/24\n",
      "3622/3622 [==============================] - 1s 167us/step - loss: 0.0244 - val_loss: 0.0124\n",
      "Epoch 3/24\n",
      "3622/3622 [==============================] - 1s 163us/step - loss: 0.0143 - val_loss: 0.0019\n",
      "Epoch 4/24\n",
      "3622/3622 [==============================] - 1s 161us/step - loss: 0.0062 - val_loss: 8.4525e-04\n",
      "Epoch 5/24\n",
      "3622/3622 [==============================] - 1s 163us/step - loss: 0.0037 - val_loss: 0.0017\n",
      "Epoch 6/24\n",
      "3622/3622 [==============================] - 1s 163us/step - loss: 0.0027 - val_loss: 9.5452e-04\n",
      "Epoch 7/24\n",
      "3622/3622 [==============================] - 1s 161us/step - loss: 0.0026 - val_loss: 7.3611e-04\n",
      "Epoch 8/24\n",
      "3622/3622 [==============================] - 1s 161us/step - loss: 0.0026 - val_loss: 0.0021\n",
      "Epoch 9/24\n",
      "3622/3622 [==============================] - 1s 162us/step - loss: 0.0021 - val_loss: 7.1404e-04\n",
      "Epoch 10/24\n",
      "3622/3622 [==============================] - 1s 160us/step - loss: 0.0022 - val_loss: 0.0019\n",
      "Epoch 11/24\n",
      "3622/3622 [==============================] - 1s 161us/step - loss: 0.0020 - val_loss: 0.0021\n",
      "Epoch 12/24\n",
      "3622/3622 [==============================] - 1s 161us/step - loss: 0.0019 - val_loss: 0.0012\n",
      "Epoch 13/24\n",
      "3622/3622 [==============================] - 1s 164us/step - loss: 0.0018 - val_loss: 0.0025\n",
      "Epoch 14/24\n",
      "3622/3622 [==============================] - 1s 161us/step - loss: 0.0016 - val_loss: 0.0030\n",
      "Epoch 15/24\n",
      "3622/3622 [==============================] - 1s 164us/step - loss: 0.0015 - val_loss: 0.0020\n",
      "Epoch 16/24\n",
      "3622/3622 [==============================] - 1s 164us/step - loss: 0.0015 - val_loss: 0.0029\n",
      "Epoch 17/24\n",
      "3622/3622 [==============================] - 1s 162us/step - loss: 0.0015 - val_loss: 0.0033\n",
      "Epoch 18/24\n",
      "3622/3622 [==============================] - 1s 162us/step - loss: 0.0014 - val_loss: 0.0037\n",
      "Epoch 19/24\n",
      "3622/3622 [==============================] - 1s 162us/step - loss: 0.0014 - val_loss: 0.0037\n",
      "Epoch 20/24\n",
      "3622/3622 [==============================] - 1s 161us/step - loss: 0.0013 - val_loss: 0.0034\n",
      "Epoch 21/24\n",
      "3622/3622 [==============================] - 1s 162us/step - loss: 0.0012 - val_loss: 0.0044\n",
      "Epoch 22/24\n",
      "3622/3622 [==============================] - 1s 161us/step - loss: 0.0013 - val_loss: 0.0054\n",
      "Epoch 23/24\n",
      "3622/3622 [==============================] - 1s 162us/step - loss: 0.0011 - val_loss: 0.0056\n",
      "Epoch 24/24\n",
      "3622/3622 [==============================] - 1s 162us/step - loss: 0.0011 - val_loss: 0.0053\n",
      "Train on 3622 samples, validate on 403 samples\n",
      "Epoch 1/24\n",
      "3622/3622 [==============================] - 2s 498us/step - loss: 0.0713 - val_loss: 0.0012\n",
      "Epoch 2/24\n",
      "3622/3622 [==============================] - 1s 173us/step - loss: 0.0067 - val_loss: 0.0041\n",
      "Epoch 3/24\n",
      "3622/3622 [==============================] - 1s 175us/step - loss: 0.0089 - val_loss: 0.0048\n",
      "Epoch 4/24\n",
      "3622/3622 [==============================] - 1s 174us/step - loss: 0.0048 - val_loss: 0.0091\n",
      "Epoch 5/24\n",
      "3622/3622 [==============================] - 1s 174us/step - loss: 0.0068 - val_loss: 0.0069\n",
      "Epoch 6/24\n",
      "3622/3622 [==============================] - 1s 176us/step - loss: 0.0055 - val_loss: 0.0056\n",
      "Epoch 7/24\n",
      "3622/3622 [==============================] - 1s 176us/step - loss: 0.0063 - val_loss: 0.0097\n",
      "Epoch 8/24\n",
      "3622/3622 [==============================] - 1s 176us/step - loss: 0.0040 - val_loss: 0.0033\n",
      "Epoch 9/24\n",
      "3622/3622 [==============================] - 1s 175us/step - loss: 0.0053 - val_loss: 0.0065\n",
      "Epoch 10/24\n",
      "3622/3622 [==============================] - 1s 174us/step - loss: 0.0046 - val_loss: 0.0063\n",
      "Epoch 11/24\n",
      "3622/3622 [==============================] - 1s 175us/step - loss: 0.0039 - val_loss: 0.0068\n",
      "Epoch 12/24\n",
      "3622/3622 [==============================] - 1s 180us/step - loss: 0.0040 - val_loss: 0.0011\n",
      "Epoch 13/24\n",
      "3622/3622 [==============================] - 1s 179us/step - loss: 0.0049 - val_loss: 0.0110\n",
      "Epoch 14/24\n",
      "3622/3622 [==============================] - 1s 175us/step - loss: 0.0030 - val_loss: 0.0049\n",
      "Epoch 15/24\n",
      "3622/3622 [==============================] - 1s 174us/step - loss: 0.0039 - val_loss: 0.0051\n",
      "Epoch 16/24\n",
      "3622/3622 [==============================] - 1s 176us/step - loss: 0.0034 - val_loss: 0.0059\n",
      "Epoch 17/24\n",
      "3622/3622 [==============================] - 1s 175us/step - loss: 0.0035 - val_loss: 0.0053\n",
      "Epoch 18/24\n",
      "3622/3622 [==============================] - 1s 174us/step - loss: 0.0032 - val_loss: 0.0080\n",
      "Epoch 19/24\n",
      "3622/3622 [==============================] - 1s 175us/step - loss: 0.0036 - val_loss: 0.0011\n",
      "Epoch 20/24\n",
      "3622/3622 [==============================] - 1s 175us/step - loss: 0.0030 - val_loss: 0.0061\n",
      "Epoch 21/24\n",
      "3622/3622 [==============================] - 1s 176us/step - loss: 0.0033 - val_loss: 0.0033\n",
      "Epoch 22/24\n",
      "3622/3622 [==============================] - 1s 180us/step - loss: 0.0025 - val_loss: 0.0099\n",
      "Epoch 23/24\n",
      "3622/3622 [==============================] - 1s 175us/step - loss: 0.0032 - val_loss: 0.0021\n",
      "Epoch 24/24\n",
      "3622/3622 [==============================] - 1s 174us/step - loss: 0.0036 - val_loss: 0.0018\n",
      "Train on 3622 samples, validate on 403 samples\n",
      "Epoch 1/24\n",
      "3622/3622 [==============================] - 2s 507us/step - loss: 0.1172 - val_loss: 0.1338\n",
      "Epoch 2/24\n",
      "3622/3622 [==============================] - 1s 175us/step - loss: 0.0233 - val_loss: 0.0012\n",
      "Epoch 3/24\n",
      "3622/3622 [==============================] - 1s 175us/step - loss: 0.0027 - val_loss: 0.0054\n",
      "Epoch 4/24\n",
      "3622/3622 [==============================] - 1s 176us/step - loss: 0.0061 - val_loss: 0.0017\n",
      "Epoch 5/24\n",
      "3622/3622 [==============================] - 1s 177us/step - loss: 0.0073 - val_loss: 0.0050\n",
      "Epoch 6/24\n",
      "3622/3622 [==============================] - 1s 176us/step - loss: 0.0061 - val_loss: 0.0133\n",
      "Epoch 7/24\n",
      "3622/3622 [==============================] - 1s 175us/step - loss: 0.0073 - val_loss: 0.0094\n",
      "Epoch 8/24\n",
      "3622/3622 [==============================] - 1s 179us/step - loss: 0.0065 - val_loss: 0.0010\n",
      "Epoch 9/24\n",
      "3622/3622 [==============================] - 1s 177us/step - loss: 0.0053 - val_loss: 0.0027\n",
      "Epoch 10/24\n",
      "3622/3622 [==============================] - 1s 175us/step - loss: 0.0050 - val_loss: 0.0195\n",
      "Epoch 11/24\n",
      "3622/3622 [==============================] - 1s 176us/step - loss: 0.0051 - val_loss: 0.0015\n",
      "Epoch 12/24\n",
      "3622/3622 [==============================] - 1s 177us/step - loss: 0.0048 - val_loss: 0.0051\n",
      "Epoch 13/24\n",
      "3622/3622 [==============================] - 1s 174us/step - loss: 0.0050 - val_loss: 0.0052\n",
      "Epoch 14/24\n",
      "3622/3622 [==============================] - 1s 174us/step - loss: 0.0042 - val_loss: 0.0018\n",
      "Epoch 15/24\n",
      "3622/3622 [==============================] - 1s 178us/step - loss: 0.0048 - val_loss: 0.0047\n",
      "Epoch 16/24\n",
      "3622/3622 [==============================] - 1s 174us/step - loss: 0.0031 - val_loss: 0.0044\n",
      "Epoch 17/24\n",
      "3622/3622 [==============================] - 1s 175us/step - loss: 0.0053 - val_loss: 0.0026\n",
      "Epoch 18/24\n",
      "3622/3622 [==============================] - 1s 176us/step - loss: 0.0022 - val_loss: 0.0075\n",
      "Epoch 19/24\n",
      "3622/3622 [==============================] - 1s 175us/step - loss: 0.0048 - val_loss: 8.7397e-04\n",
      "Epoch 20/24\n",
      "3622/3622 [==============================] - 1s 175us/step - loss: 0.0031 - val_loss: 0.0157\n",
      "Epoch 21/24\n",
      "3622/3622 [==============================] - 1s 175us/step - loss: 0.0034 - val_loss: 0.0020\n",
      "Epoch 22/24\n",
      "3622/3622 [==============================] - 1s 175us/step - loss: 0.0034 - val_loss: 0.0027\n",
      "Epoch 23/24\n",
      "3622/3622 [==============================] - 1s 176us/step - loss: 0.0035 - val_loss: 0.0033\n",
      "Epoch 24/24\n",
      "3622/3622 [==============================] - 1s 176us/step - loss: 0.0027 - val_loss: 0.0057\n",
      "Train on 3622 samples, validate on 403 samples\n",
      "Epoch 1/24\n",
      "3622/3622 [==============================] - 2s 545us/step - loss: 0.1138 - val_loss: 0.0015\n",
      "Epoch 2/24\n",
      "3622/3622 [==============================] - 1s 194us/step - loss: 0.0026 - val_loss: 7.1941e-04\n",
      "Epoch 3/24\n",
      "3622/3622 [==============================] - 1s 194us/step - loss: 0.0020 - val_loss: 0.0081\n",
      "Epoch 4/24\n",
      "3622/3622 [==============================] - 1s 197us/step - loss: 0.0126 - val_loss: 0.0079\n",
      "Epoch 5/24\n",
      "3622/3622 [==============================] - 1s 193us/step - loss: 0.0099 - val_loss: 0.0021\n",
      "Epoch 6/24\n",
      "3622/3622 [==============================] - 1s 194us/step - loss: 0.0085 - val_loss: 0.0113\n",
      "Epoch 7/24\n",
      "3622/3622 [==============================] - 1s 192us/step - loss: 0.0074 - val_loss: 0.0069\n",
      "Epoch 8/24\n",
      "3622/3622 [==============================] - 1s 192us/step - loss: 0.0073 - val_loss: 0.0083\n",
      "Epoch 9/24\n",
      "3622/3622 [==============================] - 1s 199us/step - loss: 0.0078 - val_loss: 0.0021\n",
      "Epoch 10/24\n",
      "3622/3622 [==============================] - 1s 195us/step - loss: 0.0062 - val_loss: 0.0101\n",
      "Epoch 11/24\n",
      "3622/3622 [==============================] - 1s 193us/step - loss: 0.0086 - val_loss: 0.0017\n",
      "Epoch 12/24\n",
      "3622/3622 [==============================] - 1s 193us/step - loss: 0.0046 - val_loss: 0.0028\n",
      "Epoch 13/24\n",
      "3622/3622 [==============================] - 1s 192us/step - loss: 0.0075 - val_loss: 0.0083\n",
      "Epoch 14/24\n",
      "3622/3622 [==============================] - 1s 193us/step - loss: 0.0044 - val_loss: 6.9836e-04\n",
      "Epoch 15/24\n",
      "3622/3622 [==============================] - 1s 192us/step - loss: 0.0059 - val_loss: 0.0086\n",
      "Epoch 16/24\n",
      "3622/3622 [==============================] - 1s 193us/step - loss: 0.0054 - val_loss: 0.0041\n",
      "Epoch 17/24\n",
      "3622/3622 [==============================] - 1s 192us/step - loss: 0.0051 - val_loss: 0.0028\n",
      "Epoch 18/24\n",
      "3622/3622 [==============================] - 1s 193us/step - loss: 0.0038 - val_loss: 0.0040\n",
      "Epoch 19/24\n",
      "3622/3622 [==============================] - 1s 194us/step - loss: 0.0060 - val_loss: 0.0110\n",
      "Epoch 20/24\n",
      "3622/3622 [==============================] - 1s 193us/step - loss: 0.0033 - val_loss: 0.0011\n",
      "Epoch 21/24\n",
      "3622/3622 [==============================] - 1s 194us/step - loss: 0.0048 - val_loss: 0.0081\n",
      "Epoch 22/24\n",
      "3622/3622 [==============================] - 1s 193us/step - loss: 0.0037 - val_loss: 0.0017\n",
      "Epoch 23/24\n",
      "3622/3622 [==============================] - 1s 194us/step - loss: 0.0043 - val_loss: 0.0129\n",
      "Epoch 24/24\n",
      "3622/3622 [==============================] - 1s 194us/step - loss: 0.0050 - val_loss: 4.8712e-04\n",
      "Train on 3622 samples, validate on 403 samples\n",
      "Epoch 1/24\n",
      "3622/3622 [==============================] - 2s 547us/step - loss: 0.0613 - val_loss: 0.0094\n",
      "Epoch 2/24\n",
      "3622/3622 [==============================] - 1s 193us/step - loss: 0.0109 - val_loss: 0.0026\n",
      "Epoch 3/24\n",
      "3622/3622 [==============================] - 1s 192us/step - loss: 0.0102 - val_loss: 0.0044\n",
      "Epoch 4/24\n",
      "3622/3622 [==============================] - 1s 194us/step - loss: 0.0061 - val_loss: 0.0022\n",
      "Epoch 5/24\n",
      "3622/3622 [==============================] - 1s 192us/step - loss: 0.0057 - val_loss: 0.0122\n",
      "Epoch 6/24\n",
      "3622/3622 [==============================] - 1s 193us/step - loss: 0.0085 - val_loss: 0.0201\n",
      "Epoch 7/24\n",
      "3622/3622 [==============================] - 1s 194us/step - loss: 0.0045 - val_loss: 0.0036\n",
      "Epoch 8/24\n",
      "3622/3622 [==============================] - 1s 194us/step - loss: 0.0038 - val_loss: 0.0677\n",
      "Epoch 9/24\n",
      "3622/3622 [==============================] - 1s 192us/step - loss: 0.0047 - val_loss: 0.0218\n",
      "Epoch 10/24\n",
      "3622/3622 [==============================] - 1s 194us/step - loss: 0.0036 - val_loss: 0.0161\n",
      "Epoch 11/24\n",
      "3622/3622 [==============================] - 1s 192us/step - loss: 0.0045 - val_loss: 0.0305\n",
      "Epoch 12/24\n",
      "3622/3622 [==============================] - 1s 193us/step - loss: 0.0044 - val_loss: 9.2618e-04\n",
      "Epoch 13/24\n",
      "3622/3622 [==============================] - 1s 194us/step - loss: 0.0024 - val_loss: 0.0023\n",
      "Epoch 14/24\n",
      "3622/3622 [==============================] - 1s 193us/step - loss: 0.0069 - val_loss: 9.2174e-04\n",
      "Epoch 15/24\n",
      "3622/3622 [==============================] - 1s 194us/step - loss: 0.0030 - val_loss: 0.0024\n",
      "Epoch 16/24\n",
      "3622/3622 [==============================] - 1s 192us/step - loss: 0.0032 - val_loss: 0.0022\n",
      "Epoch 17/24\n",
      "3622/3622 [==============================] - 1s 193us/step - loss: 0.0030 - val_loss: 0.0014\n",
      "Epoch 18/24\n",
      "3622/3622 [==============================] - 1s 193us/step - loss: 0.0027 - val_loss: 0.0016\n",
      "Epoch 19/24\n",
      "3622/3622 [==============================] - 1s 194us/step - loss: 0.0040 - val_loss: 9.7508e-04\n",
      "Epoch 20/24\n",
      "3622/3622 [==============================] - 1s 196us/step - loss: 0.0031 - val_loss: 0.0042\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21/24\n",
      "3622/3622 [==============================] - 1s 195us/step - loss: 0.0022 - val_loss: 0.0046\n",
      "Epoch 22/24\n",
      "3622/3622 [==============================] - 1s 197us/step - loss: 0.0045 - val_loss: 0.0013\n",
      "Epoch 23/24\n",
      "3622/3622 [==============================] - 1s 198us/step - loss: 0.0018 - val_loss: 0.0012\n",
      "Epoch 24/24\n",
      "3622/3622 [==============================] - 1s 193us/step - loss: 0.0035 - val_loss: 0.0029\n",
      "\n",
      "GENERATION 2\n",
      "\n",
      "Train on 3622 samples, validate on 403 samples\n",
      "Epoch 1/24\n",
      "3622/3622 [==============================] - 3s 877us/step - loss: 0.0555 - val_loss: 0.0181\n",
      "Epoch 2/24\n",
      "3622/3622 [==============================] - 2s 424us/step - loss: 0.0093 - val_loss: 0.0011\n",
      "Epoch 3/24\n",
      "3622/3622 [==============================] - 2s 425us/step - loss: 0.0042 - val_loss: 0.0101\n",
      "Epoch 4/24\n",
      "3622/3622 [==============================] - 2s 425us/step - loss: 0.0031 - val_loss: 0.0012\n",
      "Epoch 5/24\n",
      "3622/3622 [==============================] - 2s 425us/step - loss: 0.0022 - val_loss: 0.0019\n",
      "Epoch 6/24\n",
      "3622/3622 [==============================] - 2s 429us/step - loss: 0.0020 - val_loss: 0.0031\n",
      "Epoch 7/24\n",
      "3622/3622 [==============================] - 2s 424us/step - loss: 0.0019 - val_loss: 9.5891e-04\n",
      "Epoch 8/24\n",
      "3622/3622 [==============================] - 2s 427us/step - loss: 0.0017 - val_loss: 0.0014\n",
      "Epoch 9/24\n",
      "3622/3622 [==============================] - 2s 428us/step - loss: 0.0015 - val_loss: 0.0018\n",
      "Epoch 10/24\n",
      "3622/3622 [==============================] - 2s 430us/step - loss: 0.0014 - val_loss: 0.0013\n",
      "Epoch 11/24\n",
      "3622/3622 [==============================] - 2s 424us/step - loss: 0.0014 - val_loss: 9.6180e-04\n",
      "Epoch 12/24\n",
      "3622/3622 [==============================] - 2s 427us/step - loss: 0.0013 - val_loss: 9.5714e-04\n",
      "Epoch 13/24\n",
      "3622/3622 [==============================] - 2s 427us/step - loss: 0.0013 - val_loss: 0.0012\n",
      "Epoch 14/24\n",
      "3622/3622 [==============================] - 2s 424us/step - loss: 0.0013 - val_loss: 0.0011\n",
      "Epoch 15/24\n",
      "3622/3622 [==============================] - 2s 426us/step - loss: 0.0012 - val_loss: 0.0016\n",
      "Epoch 16/24\n",
      "3622/3622 [==============================] - 2s 438us/step - loss: 0.0012 - val_loss: 0.0011\n",
      "Epoch 17/24\n",
      "3622/3622 [==============================] - 2s 424us/step - loss: 0.0012 - val_loss: 0.0011\n",
      "Epoch 18/24\n",
      "3622/3622 [==============================] - 2s 433us/step - loss: 0.0012 - val_loss: 8.9943e-04\n",
      "Epoch 19/24\n",
      "3622/3622 [==============================] - 2s 431us/step - loss: 0.0012 - val_loss: 9.2006e-04\n",
      "Epoch 20/24\n",
      "3622/3622 [==============================] - 2s 424us/step - loss: 0.0012 - val_loss: 8.9732e-04\n",
      "Epoch 21/24\n",
      "3622/3622 [==============================] - 2s 427us/step - loss: 0.0011 - val_loss: 9.4909e-04\n",
      "Epoch 22/24\n",
      "3622/3622 [==============================] - 2s 426us/step - loss: 0.0011 - val_loss: 0.0010\n",
      "Epoch 23/24\n",
      "3622/3622 [==============================] - 2s 423us/step - loss: 0.0011 - val_loss: 7.6866e-04\n",
      "Epoch 24/24\n",
      "3622/3622 [==============================] - 2s 425us/step - loss: 0.0010 - val_loss: 7.3478e-04\n",
      "Train on 3622 samples, validate on 403 samples\n",
      "Epoch 1/24\n",
      "3622/3622 [==============================] - 2s 593us/step - loss: 0.0958 - val_loss: 0.0036\n",
      "Epoch 2/24\n",
      "3622/3622 [==============================] - 1s 183us/step - loss: 0.0311 - val_loss: 0.0141\n",
      "Epoch 3/24\n",
      "3622/3622 [==============================] - 1s 183us/step - loss: 0.0337 - val_loss: 0.0161\n",
      "Epoch 4/24\n",
      "3622/3622 [==============================] - 1s 183us/step - loss: 0.0033 - val_loss: 0.0029\n",
      "Epoch 5/24\n",
      "3622/3622 [==============================] - 1s 183us/step - loss: 0.0093 - val_loss: 7.8670e-04\n",
      "Epoch 6/24\n",
      "3622/3622 [==============================] - 1s 182us/step - loss: 0.0054 - val_loss: 9.8862e-04\n",
      "Epoch 7/24\n",
      "3622/3622 [==============================] - 1s 183us/step - loss: 0.0146 - val_loss: 0.0021\n",
      "Epoch 8/24\n",
      "3622/3622 [==============================] - 1s 184us/step - loss: 0.0098 - val_loss: 7.1849e-04\n",
      "Epoch 9/24\n",
      "3622/3622 [==============================] - 1s 183us/step - loss: 0.0193 - val_loss: 0.0120\n",
      "Epoch 10/24\n",
      "3622/3622 [==============================] - 1s 184us/step - loss: 0.0017 - val_loss: 0.0041\n",
      "Epoch 11/24\n",
      "3622/3622 [==============================] - 1s 184us/step - loss: 0.0022 - val_loss: 7.8006e-04\n",
      "Epoch 12/24\n",
      "3622/3622 [==============================] - 1s 185us/step - loss: 7.0273e-04 - val_loss: 9.2985e-04\n",
      "Epoch 13/24\n",
      "3622/3622 [==============================] - 1s 183us/step - loss: 6.7913e-04 - val_loss: 6.0638e-04\n",
      "Epoch 14/24\n",
      "3622/3622 [==============================] - 1s 184us/step - loss: 7.4950e-04 - val_loss: 7.2653e-04\n",
      "Epoch 15/24\n",
      "3622/3622 [==============================] - 1s 182us/step - loss: 6.1475e-04 - val_loss: 5.6933e-04\n",
      "Epoch 16/24\n",
      "3622/3622 [==============================] - 1s 184us/step - loss: 9.3479e-04 - val_loss: 7.7569e-04\n",
      "Epoch 17/24\n",
      "3622/3622 [==============================] - 1s 181us/step - loss: 7.8328e-04 - val_loss: 5.4555e-04\n",
      "Epoch 18/24\n",
      "3622/3622 [==============================] - 1s 183us/step - loss: 0.0016 - val_loss: 8.5773e-04\n",
      "Epoch 19/24\n",
      "3622/3622 [==============================] - 1s 182us/step - loss: 0.0013 - val_loss: 6.1803e-04\n",
      "Epoch 20/24\n",
      "3622/3622 [==============================] - 1s 189us/step - loss: 0.0029 - val_loss: 6.4153e-04\n",
      "Epoch 21/24\n",
      "3622/3622 [==============================] - 1s 183us/step - loss: 0.0020 - val_loss: 6.3845e-04\n",
      "Epoch 22/24\n",
      "3622/3622 [==============================] - 1s 182us/step - loss: 0.0046 - val_loss: 6.9681e-04\n",
      "Epoch 23/24\n",
      "3622/3622 [==============================] - 1s 187us/step - loss: 0.0031 - val_loss: 6.0926e-04\n",
      "Epoch 24/24\n",
      "3622/3622 [==============================] - 1s 184us/step - loss: 0.0064 - val_loss: 0.0012\n",
      "Train on 3622 samples, validate on 403 samples\n",
      "Epoch 1/24\n",
      "3622/3622 [==============================] - 2s 589us/step - loss: 0.0317 - val_loss: 0.0344\n",
      "Epoch 2/24\n",
      "3622/3622 [==============================] - 1s 182us/step - loss: 0.0291 - val_loss: 0.0103\n",
      "Epoch 3/24\n",
      "3622/3622 [==============================] - 1s 182us/step - loss: 0.0025 - val_loss: 0.0038\n",
      "Epoch 4/24\n",
      "3622/3622 [==============================] - 1s 183us/step - loss: 0.0046 - val_loss: 0.0013\n",
      "Epoch 5/24\n",
      "3622/3622 [==============================] - 1s 182us/step - loss: 0.0013 - val_loss: 0.0019\n",
      "Epoch 6/24\n",
      "3622/3622 [==============================] - 1s 183us/step - loss: 0.0041 - val_loss: 0.0060\n",
      "Epoch 7/24\n",
      "3622/3622 [==============================] - 1s 183us/step - loss: 0.0025 - val_loss: 0.0023\n",
      "Epoch 8/24\n",
      "3622/3622 [==============================] - 1s 182us/step - loss: 0.0090 - val_loss: 0.0040\n",
      "Epoch 9/24\n",
      "3622/3622 [==============================] - 1s 182us/step - loss: 0.0129 - val_loss: 0.0046\n",
      "Epoch 10/24\n",
      "3622/3622 [==============================] - 1s 184us/step - loss: 0.0370 - val_loss: 0.0128\n",
      "Epoch 11/24\n",
      "3622/3622 [==============================] - 1s 182us/step - loss: 0.0076 - val_loss: 0.0041\n",
      "Epoch 12/24\n",
      "3622/3622 [==============================] - 1s 183us/step - loss: 0.0125 - val_loss: 0.0044\n",
      "Epoch 13/24\n",
      "3622/3622 [==============================] - 1s 184us/step - loss: 0.0014 - val_loss: 0.0025\n",
      "Epoch 14/24\n",
      "3622/3622 [==============================] - 1s 184us/step - loss: 0.0022 - val_loss: 5.9612e-04\n",
      "Epoch 15/24\n",
      "3622/3622 [==============================] - 1s 183us/step - loss: 6.5301e-04 - val_loss: 8.1068e-04\n",
      "Epoch 16/24\n",
      "3622/3622 [==============================] - 1s 184us/step - loss: 7.6439e-04 - val_loss: 6.7121e-04\n",
      "Epoch 17/24\n",
      "3622/3622 [==============================] - 1s 187us/step - loss: 6.1755e-04 - val_loss: 6.4952e-04\n",
      "Epoch 18/24\n",
      "3622/3622 [==============================] - 1s 184us/step - loss: 9.1766e-04 - val_loss: 7.1386e-04\n",
      "Epoch 19/24\n",
      "3622/3622 [==============================] - 1s 183us/step - loss: 7.0156e-04 - val_loss: 5.7932e-04\n",
      "Epoch 20/24\n",
      "3622/3622 [==============================] - 1s 182us/step - loss: 0.0017 - val_loss: 7.5985e-04\n",
      "Epoch 21/24\n",
      "3622/3622 [==============================] - 1s 183us/step - loss: 0.0016 - val_loss: 5.5129e-04\n",
      "Epoch 22/24\n",
      "3622/3622 [==============================] - 1s 183us/step - loss: 0.0048 - val_loss: 6.5857e-04\n",
      "Epoch 23/24\n",
      "3622/3622 [==============================] - 1s 184us/step - loss: 0.0050 - val_loss: 5.6098e-04\n",
      "Epoch 24/24\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3622/3622 [==============================] - 1s 183us/step - loss: 0.0129 - val_loss: 0.0024\n",
      "Train on 3622 samples, validate on 403 samples\n",
      "Epoch 1/24\n",
      "3622/3622 [==============================] - 2s 662us/step - loss: 0.1422 - val_loss: 0.0441\n",
      "Epoch 2/24\n",
      "3622/3622 [==============================] - 1s 234us/step - loss: 0.0163 - val_loss: 0.0115\n",
      "Epoch 3/24\n",
      "3622/3622 [==============================] - 1s 239us/step - loss: 0.0077 - val_loss: 0.0012\n",
      "Epoch 4/24\n",
      "3622/3622 [==============================] - 1s 237us/step - loss: 0.0044 - val_loss: 0.0014\n",
      "Epoch 5/24\n",
      "3622/3622 [==============================] - 1s 234us/step - loss: 0.0027 - val_loss: 0.0011\n",
      "Epoch 6/24\n",
      "3622/3622 [==============================] - 1s 238us/step - loss: 0.0020 - val_loss: 7.3319e-04\n",
      "Epoch 7/24\n",
      "3622/3622 [==============================] - 1s 240us/step - loss: 0.0018 - val_loss: 0.0013\n",
      "Epoch 8/24\n",
      "3622/3622 [==============================] - 1s 237us/step - loss: 0.0017 - val_loss: 9.3001e-04\n",
      "Epoch 9/24\n",
      "3622/3622 [==============================] - 1s 235us/step - loss: 0.0017 - val_loss: 7.1016e-04\n",
      "Epoch 10/24\n",
      "3622/3622 [==============================] - 1s 235us/step - loss: 0.0016 - val_loss: 0.0015\n",
      "Epoch 11/24\n",
      "3622/3622 [==============================] - 1s 235us/step - loss: 0.0014 - val_loss: 0.0017\n",
      "Epoch 12/24\n",
      "3622/3622 [==============================] - 1s 235us/step - loss: 0.0014 - val_loss: 9.7372e-04\n",
      "Epoch 13/24\n",
      "3622/3622 [==============================] - 1s 237us/step - loss: 0.0013 - val_loss: 0.0012\n",
      "Epoch 14/24\n",
      "3622/3622 [==============================] - 1s 235us/step - loss: 0.0014 - val_loss: 0.0016\n",
      "Epoch 15/24\n",
      "3622/3622 [==============================] - 1s 235us/step - loss: 0.0013 - val_loss: 0.0023\n",
      "Epoch 16/24\n",
      "3622/3622 [==============================] - 1s 234us/step - loss: 0.0014 - val_loss: 0.0025\n",
      "Epoch 17/24\n",
      "3622/3622 [==============================] - 1s 234us/step - loss: 0.0013 - val_loss: 0.0019\n",
      "Epoch 18/24\n",
      "3622/3622 [==============================] - 1s 236us/step - loss: 0.0012 - val_loss: 0.0020\n",
      "Epoch 19/24\n",
      "3622/3622 [==============================] - 1s 234us/step - loss: 0.0011 - val_loss: 0.0017\n",
      "Epoch 20/24\n",
      "3622/3622 [==============================] - 1s 235us/step - loss: 0.0011 - val_loss: 0.0017\n",
      "Epoch 21/24\n",
      "3622/3622 [==============================] - 1s 234us/step - loss: 0.0011 - val_loss: 0.0015\n",
      "Epoch 22/24\n",
      "3622/3622 [==============================] - 1s 235us/step - loss: 0.0011 - val_loss: 0.0023\n",
      "Epoch 23/24\n",
      "3622/3622 [==============================] - 1s 234us/step - loss: 0.0011 - val_loss: 0.0025\n",
      "Epoch 24/24\n",
      "3622/3622 [==============================] - 1s 236us/step - loss: 0.0010 - val_loss: 0.0023\n",
      "Train on 3622 samples, validate on 403 samples\n",
      "Epoch 1/24\n",
      "3622/3622 [==============================] - 4s 974us/step - loss: 0.0859 - val_loss: 0.0146\n",
      "Epoch 2/24\n",
      "3622/3622 [==============================] - 2s 431us/step - loss: 0.0120 - val_loss: 0.0207\n",
      "Epoch 3/24\n",
      "3622/3622 [==============================] - 2s 430us/step - loss: 0.0079 - val_loss: 0.0017\n",
      "Epoch 4/24\n",
      "3622/3622 [==============================] - 2s 431us/step - loss: 0.0069 - val_loss: 0.0102\n",
      "Epoch 5/24\n",
      "3622/3622 [==============================] - 2s 433us/step - loss: 0.0067 - val_loss: 0.0016\n",
      "Epoch 6/24\n",
      "3622/3622 [==============================] - 2s 432us/step - loss: 0.0041 - val_loss: 0.0065\n",
      "Epoch 7/24\n",
      "3622/3622 [==============================] - 2s 430us/step - loss: 0.0087 - val_loss: 0.0018\n",
      "Epoch 8/24\n",
      "3622/3622 [==============================] - 2s 432us/step - loss: 0.0043 - val_loss: 0.0053\n",
      "Epoch 9/24\n",
      "3622/3622 [==============================] - 2s 431us/step - loss: 0.0068 - val_loss: 0.0095\n",
      "Epoch 10/24\n",
      "3622/3622 [==============================] - 2s 428us/step - loss: 0.0046 - val_loss: 0.0017\n",
      "Epoch 11/24\n",
      "3622/3622 [==============================] - 2s 430us/step - loss: 0.0031 - val_loss: 0.0065\n",
      "Epoch 12/24\n",
      "3622/3622 [==============================] - 2s 428us/step - loss: 0.0063 - val_loss: 0.0062\n",
      "Epoch 13/24\n",
      "3622/3622 [==============================] - 2s 432us/step - loss: 0.0038 - val_loss: 0.0014\n",
      "Epoch 14/24\n",
      "3622/3622 [==============================] - 2s 433us/step - loss: 0.0023 - val_loss: 0.0014\n",
      "Epoch 15/24\n",
      "3622/3622 [==============================] - 2s 430us/step - loss: 0.0044 - val_loss: 0.0058\n",
      "Epoch 16/24\n",
      "3622/3622 [==============================] - 2s 428us/step - loss: 0.0067 - val_loss: 0.0065\n",
      "Epoch 17/24\n",
      "3622/3622 [==============================] - 2s 430us/step - loss: 0.0055 - val_loss: 0.0025\n",
      "Epoch 18/24\n",
      "3622/3622 [==============================] - 2s 433us/step - loss: 0.0038 - val_loss: 0.0011\n",
      "Epoch 19/24\n",
      "3622/3622 [==============================] - 2s 433us/step - loss: 0.0016 - val_loss: 0.0041\n",
      "Epoch 20/24\n",
      "3622/3622 [==============================] - 2s 430us/step - loss: 0.0043 - val_loss: 0.0035\n",
      "Epoch 21/24\n",
      "3622/3622 [==============================] - 2s 431us/step - loss: 0.0035 - val_loss: 0.0031\n",
      "Epoch 22/24\n",
      "3622/3622 [==============================] - 2s 436us/step - loss: 0.0030 - val_loss: 9.1870e-04\n",
      "Epoch 23/24\n",
      "3622/3622 [==============================] - 2s 431us/step - loss: 0.0020 - val_loss: 0.0076\n",
      "Epoch 24/24\n",
      "3622/3622 [==============================] - 2s 433us/step - loss: 0.0064 - val_loss: 9.1888e-04\n",
      "Train on 3622 samples, validate on 403 samples\n",
      "Epoch 1/24\n",
      "3622/3622 [==============================] - 4s 999us/step - loss: 0.0825 - val_loss: 0.0114\n",
      "Epoch 2/24\n",
      "3622/3622 [==============================] - 2s 435us/step - loss: 0.0116 - val_loss: 0.0054\n",
      "Epoch 3/24\n",
      "3622/3622 [==============================] - 2s 431us/step - loss: 0.0102 - val_loss: 0.0155\n",
      "Epoch 4/24\n",
      "3622/3622 [==============================] - 2s 431us/step - loss: 0.0079 - val_loss: 0.0013\n",
      "Epoch 5/24\n",
      "3622/3622 [==============================] - 2s 431us/step - loss: 0.0035 - val_loss: 0.0170\n",
      "Epoch 6/24\n",
      "3622/3622 [==============================] - 2s 429us/step - loss: 0.0096 - val_loss: 0.0097\n",
      "Epoch 7/24\n",
      "3622/3622 [==============================] - 2s 429us/step - loss: 0.0056 - val_loss: 0.0078\n",
      "Epoch 8/24\n",
      "3622/3622 [==============================] - 2s 429us/step - loss: 0.0054 - val_loss: 0.0053\n",
      "Epoch 9/24\n",
      "3622/3622 [==============================] - 2s 427us/step - loss: 0.0039 - val_loss: 0.0238\n",
      "Epoch 10/24\n",
      "3622/3622 [==============================] - 2s 432us/step - loss: 0.0082 - val_loss: 0.0138\n",
      "Epoch 11/24\n",
      "3622/3622 [==============================] - 2s 430us/step - loss: 0.0064 - val_loss: 0.0061\n",
      "Epoch 12/24\n",
      "3622/3622 [==============================] - 2s 433us/step - loss: 0.0029 - val_loss: 0.0107\n",
      "Epoch 13/24\n",
      "3622/3622 [==============================] - 2s 429us/step - loss: 0.0050 - val_loss: 0.0085\n",
      "Epoch 14/24\n",
      "3622/3622 [==============================] - 2s 427us/step - loss: 0.0050 - val_loss: 0.0038\n",
      "Epoch 15/24\n",
      "3622/3622 [==============================] - 2s 434us/step - loss: 0.0041 - val_loss: 0.0015\n",
      "Epoch 16/24\n",
      "3622/3622 [==============================] - 2s 432us/step - loss: 0.0023 - val_loss: 0.0028\n",
      "Epoch 17/24\n",
      "3622/3622 [==============================] - 2s 432us/step - loss: 0.0030 - val_loss: 0.0090\n",
      "Epoch 18/24\n",
      "3622/3622 [==============================] - 2s 428us/step - loss: 0.0045 - val_loss: 0.0060\n",
      "Epoch 19/24\n",
      "3622/3622 [==============================] - 2s 430us/step - loss: 0.0033 - val_loss: 0.0076\n",
      "Epoch 20/24\n",
      "3622/3622 [==============================] - 2s 430us/step - loss: 0.0052 - val_loss: 0.0050\n",
      "Epoch 21/24\n",
      "3622/3622 [==============================] - 2s 430us/step - loss: 0.0032 - val_loss: 0.0021\n",
      "Epoch 22/24\n",
      "3622/3622 [==============================] - 2s 432us/step - loss: 0.0030 - val_loss: 0.0023\n",
      "Epoch 23/24\n",
      "3622/3622 [==============================] - 2s 430us/step - loss: 0.0021 - val_loss: 0.0060\n",
      "Epoch 24/24\n",
      "3622/3622 [==============================] - 2s 429us/step - loss: 0.0054 - val_loss: 0.0032\n",
      "Train on 3622 samples, validate on 403 samples\n",
      "Epoch 1/24\n",
      "3622/3622 [==============================] - 4s 1ms/step - loss: 0.0984 - val_loss: 0.0225\n",
      "Epoch 2/24\n",
      "3622/3622 [==============================] - 2s 430us/step - loss: 0.0118 - val_loss: 0.0171\n",
      "Epoch 3/24\n",
      "3622/3622 [==============================] - 2s 430us/step - loss: 0.0141 - val_loss: 0.0057\n",
      "Epoch 4/24\n",
      "3622/3622 [==============================] - 2s 433us/step - loss: 0.0100 - val_loss: 0.0098\n",
      "Epoch 5/24\n",
      "3622/3622 [==============================] - 2s 435us/step - loss: 0.0101 - val_loss: 0.0053\n",
      "Epoch 6/24\n",
      "3622/3622 [==============================] - 2s 432us/step - loss: 0.0055 - val_loss: 0.0165\n",
      "Epoch 7/24\n",
      "3622/3622 [==============================] - 2s 433us/step - loss: 0.0087 - val_loss: 0.0055\n",
      "Epoch 8/24\n",
      "3622/3622 [==============================] - 2s 432us/step - loss: 0.0063 - val_loss: 0.0089\n",
      "Epoch 9/24\n",
      "3622/3622 [==============================] - 2s 428us/step - loss: 0.0072 - val_loss: 0.0125\n",
      "Epoch 10/24\n",
      "3622/3622 [==============================] - 2s 435us/step - loss: 0.0066 - val_loss: 0.0029\n",
      "Epoch 11/24\n",
      "3622/3622 [==============================] - 2s 432us/step - loss: 0.0039 - val_loss: 0.0050\n",
      "Epoch 12/24\n",
      "3622/3622 [==============================] - 2s 430us/step - loss: 0.0062 - val_loss: 0.0098\n",
      "Epoch 13/24\n",
      "3622/3622 [==============================] - 2s 430us/step - loss: 0.0046 - val_loss: 0.0012\n",
      "Epoch 14/24\n",
      "3622/3622 [==============================] - 2s 431us/step - loss: 0.0028 - val_loss: 0.0018\n",
      "Epoch 15/24\n",
      "3622/3622 [==============================] - 2s 430us/step - loss: 0.0043 - val_loss: 0.0045\n",
      "Epoch 16/24\n",
      "3622/3622 [==============================] - 2s 429us/step - loss: 0.0057 - val_loss: 0.0016\n",
      "Epoch 17/24\n",
      "3622/3622 [==============================] - 2s 431us/step - loss: 0.0030 - val_loss: 0.0011\n",
      "Epoch 18/24\n",
      "3622/3622 [==============================] - 2s 429us/step - loss: 0.0036 - val_loss: 0.0021\n",
      "Epoch 19/24\n",
      "3622/3622 [==============================] - 2s 429us/step - loss: 0.0055 - val_loss: 0.0012\n",
      "Epoch 20/24\n",
      "3622/3622 [==============================] - 2s 431us/step - loss: 0.0022 - val_loss: 9.7762e-04\n",
      "Epoch 21/24\n",
      "3622/3622 [==============================] - 2s 428us/step - loss: 0.0032 - val_loss: 0.0037\n",
      "Epoch 22/24\n",
      "3622/3622 [==============================] - 2s 429us/step - loss: 0.0045 - val_loss: 0.0010\n",
      "Epoch 23/24\n",
      "3622/3622 [==============================] - 2s 430us/step - loss: 0.0017 - val_loss: 0.0010\n",
      "Epoch 24/24\n",
      "3622/3622 [==============================] - 2s 430us/step - loss: 0.0034 - val_loss: 0.0016\n",
      "Train on 3622 samples, validate on 403 samples\n",
      "Epoch 1/24\n",
      "3622/3622 [==============================] - 4s 1ms/step - loss: 0.3051 - val_loss: 0.0553\n",
      "Epoch 2/24\n",
      "3622/3622 [==============================] - 2s 430us/step - loss: 0.0704 - val_loss: 0.0381\n",
      "Epoch 3/24\n",
      "3622/3622 [==============================] - 2s 430us/step - loss: 0.0697 - val_loss: 0.0408\n",
      "Epoch 4/24\n",
      "3622/3622 [==============================] - 2s 430us/step - loss: 0.0697 - val_loss: 0.0332\n",
      "Epoch 5/24\n",
      "3622/3622 [==============================] - 2s 429us/step - loss: 0.0697 - val_loss: 0.0281\n",
      "Epoch 6/24\n",
      "3622/3622 [==============================] - 2s 428us/step - loss: 0.0699 - val_loss: 0.0351\n",
      "Epoch 7/24\n",
      "3622/3622 [==============================] - 2s 435us/step - loss: 0.0696 - val_loss: 0.0456\n",
      "Epoch 8/24\n",
      "3622/3622 [==============================] - 2s 431us/step - loss: 0.0698 - val_loss: 0.0411\n",
      "Epoch 9/24\n",
      "3622/3622 [==============================] - 2s 428us/step - loss: 0.0694 - val_loss: 0.0268\n",
      "Epoch 10/24\n",
      "3622/3622 [==============================] - 2s 431us/step - loss: 0.0693 - val_loss: 0.0369\n",
      "Epoch 11/24\n",
      "3622/3622 [==============================] - 2s 429us/step - loss: 0.0656 - val_loss: 0.0283\n",
      "Epoch 12/24\n",
      "3622/3622 [==============================] - 2s 430us/step - loss: 0.0458 - val_loss: 0.0108\n",
      "Epoch 13/24\n",
      "3622/3622 [==============================] - 2s 430us/step - loss: 0.0391 - val_loss: 0.0060\n",
      "Epoch 14/24\n",
      "3622/3622 [==============================] - 2s 430us/step - loss: 0.0235 - val_loss: 0.0145\n",
      "Epoch 15/24\n",
      "3622/3622 [==============================] - 2s 431us/step - loss: 0.0652 - val_loss: 0.0097\n",
      "Epoch 16/24\n",
      "3622/3622 [==============================] - 2s 431us/step - loss: 0.0253 - val_loss: 0.0123\n",
      "Epoch 17/24\n",
      "3622/3622 [==============================] - 2s 435us/step - loss: 0.0198 - val_loss: 0.0224\n",
      "Epoch 18/24\n",
      "3622/3622 [==============================] - 2s 433us/step - loss: 0.0174 - val_loss: 0.0322\n",
      "Epoch 19/24\n",
      "3622/3622 [==============================] - 2s 428us/step - loss: 0.0161 - val_loss: 0.0394\n",
      "Epoch 20/24\n",
      "3622/3622 [==============================] - 2s 432us/step - loss: 0.0155 - val_loss: 0.0433\n",
      "Epoch 21/24\n",
      "3622/3622 [==============================] - 2s 429us/step - loss: 0.0152 - val_loss: 0.0511\n",
      "Epoch 22/24\n",
      "3622/3622 [==============================] - 2s 430us/step - loss: 0.0150 - val_loss: 0.0509\n",
      "Epoch 23/24\n",
      "3622/3622 [==============================] - 2s 430us/step - loss: 0.0150 - val_loss: 0.0572\n",
      "Epoch 24/24\n",
      "3622/3622 [==============================] - 2s 430us/step - loss: 0.0149 - val_loss: 0.0575\n",
      "Train on 3622 samples, validate on 403 samples\n",
      "Epoch 1/24\n",
      "3622/3622 [==============================] - 4s 1ms/step - loss: 0.1558 - val_loss: 0.1015\n",
      "Epoch 2/24\n",
      "3622/3622 [==============================] - 2s 432us/step - loss: 0.0755 - val_loss: 0.0254\n",
      "Epoch 3/24\n",
      "3622/3622 [==============================] - 2s 431us/step - loss: 0.0733 - val_loss: 0.0158\n",
      "Epoch 4/24\n",
      "3622/3622 [==============================] - 2s 429us/step - loss: 0.0734 - val_loss: 0.0259\n",
      "Epoch 5/24\n",
      "3622/3622 [==============================] - 2s 431us/step - loss: 0.0695 - val_loss: 0.0368\n",
      "Epoch 6/24\n",
      "3622/3622 [==============================] - 2s 431us/step - loss: 0.0686 - val_loss: 0.0419\n",
      "Epoch 7/24\n",
      "3622/3622 [==============================] - 2s 431us/step - loss: 0.0670 - val_loss: 0.0342\n",
      "Epoch 8/24\n",
      "3622/3622 [==============================] - 2s 433us/step - loss: 0.0642 - val_loss: 0.0263\n",
      "Epoch 9/24\n",
      "3622/3622 [==============================] - 2s 435us/step - loss: 0.0587 - val_loss: 0.0239\n",
      "Epoch 10/24\n",
      "3622/3622 [==============================] - 2s 433us/step - loss: 0.0513 - val_loss: 0.0155\n",
      "Epoch 11/24\n",
      "3622/3622 [==============================] - 2s 431us/step - loss: 0.0411 - val_loss: 0.0101\n",
      "Epoch 12/24\n",
      "3622/3622 [==============================] - 2s 430us/step - loss: 0.0304 - val_loss: 0.0060\n",
      "Epoch 13/24\n",
      "3622/3622 [==============================] - 2s 430us/step - loss: 0.0231 - val_loss: 0.0089\n",
      "Epoch 14/24\n",
      "3622/3622 [==============================] - 2s 434us/step - loss: 0.0162 - val_loss: 0.0110\n",
      "Epoch 15/24\n",
      "3622/3622 [==============================] - 2s 431us/step - loss: 0.0132 - val_loss: 0.0160\n",
      "Epoch 16/24\n",
      "3622/3622 [==============================] - 2s 433us/step - loss: 0.0101 - val_loss: 0.0116\n",
      "Epoch 17/24\n",
      "3622/3622 [==============================] - 2s 430us/step - loss: 0.0066 - val_loss: 0.0061\n",
      "Epoch 18/24\n",
      "3622/3622 [==============================] - 2s 431us/step - loss: 0.0044 - val_loss: 0.0033\n",
      "Epoch 19/24\n",
      "3622/3622 [==============================] - 2s 430us/step - loss: 0.0032 - val_loss: 0.0011\n",
      "Epoch 20/24\n",
      "3622/3622 [==============================] - 2s 431us/step - loss: 0.0027 - val_loss: 0.0018\n",
      "Epoch 21/24\n",
      "3622/3622 [==============================] - 2s 429us/step - loss: 0.0023 - val_loss: 0.0012\n",
      "Epoch 22/24\n",
      "3622/3622 [==============================] - 2s 428us/step - loss: 0.0020 - val_loss: 0.0016\n",
      "Epoch 23/24\n",
      "3622/3622 [==============================] - 2s 432us/step - loss: 0.0018 - val_loss: 0.0013\n",
      "Epoch 24/24\n",
      "3622/3622 [==============================] - 2s 432us/step - loss: 0.0017 - val_loss: 7.5437e-04\n",
      "\n",
      "GENERATION 3\n",
      "\n",
      "Train on 3622 samples, validate on 403 samples\n",
      "Epoch 1/24\n",
      "3622/3622 [==============================] - 4s 1ms/step - loss: 0.0381 - val_loss: 0.0048\n",
      "Epoch 2/24\n",
      "3622/3622 [==============================] - 1s 382us/step - loss: 0.0716 - val_loss: 0.0329\n",
      "Epoch 3/24\n",
      "3622/3622 [==============================] - 1s 383us/step - loss: 0.0081 - val_loss: 0.0072\n",
      "Epoch 4/24\n",
      "3622/3622 [==============================] - 1s 378us/step - loss: 0.0254 - val_loss: 0.0017\n",
      "Epoch 5/24\n",
      "3622/3622 [==============================] - 1s 380us/step - loss: 0.0091 - val_loss: 0.0016\n",
      "Epoch 6/24\n",
      "3622/3622 [==============================] - 1s 381us/step - loss: 0.0226 - val_loss: 0.0014\n",
      "Epoch 7/24\n",
      "3622/3622 [==============================] - 1s 379us/step - loss: 0.0155 - val_loss: 0.0014\n",
      "Epoch 8/24\n",
      "3622/3622 [==============================] - 1s 380us/step - loss: 0.0389 - val_loss: 0.0249\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/24\n",
      "3622/3622 [==============================] - 1s 380us/step - loss: 0.0023 - val_loss: 0.0133\n",
      "Epoch 10/24\n",
      "3622/3622 [==============================] - 1s 378us/step - loss: 0.0074 - val_loss: 0.0013\n",
      "Epoch 11/24\n",
      "3622/3622 [==============================] - 1s 380us/step - loss: 0.0030 - val_loss: 0.0022\n",
      "Epoch 12/24\n",
      "3622/3622 [==============================] - 1s 379us/step - loss: 0.0018 - val_loss: 0.0018\n",
      "Epoch 13/24\n",
      "3622/3622 [==============================] - 1s 379us/step - loss: 0.0050 - val_loss: 0.0014\n",
      "Epoch 14/24\n",
      "3622/3622 [==============================] - 1s 384us/step - loss: 0.0024 - val_loss: 0.0017\n",
      "Epoch 15/24\n",
      "3622/3622 [==============================] - 1s 379us/step - loss: 0.0071 - val_loss: 0.0011\n",
      "Epoch 16/24\n",
      "3622/3622 [==============================] - 1s 378us/step - loss: 0.0037 - val_loss: 0.0018\n",
      "Epoch 17/24\n",
      "3622/3622 [==============================] - 1s 379us/step - loss: 0.0113 - val_loss: 0.0019\n",
      "Epoch 18/24\n",
      "3622/3622 [==============================] - 1s 377us/step - loss: 0.0041 - val_loss: 0.0028\n",
      "Epoch 19/24\n",
      "3622/3622 [==============================] - 1s 381us/step - loss: 0.0108 - val_loss: 0.0023\n",
      "Epoch 20/24\n",
      "3622/3622 [==============================] - 1s 379us/step - loss: 0.0030 - val_loss: 0.0036\n",
      "Epoch 21/24\n",
      "3622/3622 [==============================] - 1s 387us/step - loss: 0.0071 - val_loss: 0.0013\n",
      "Epoch 22/24\n",
      "3622/3622 [==============================] - 1s 378us/step - loss: 0.0022 - val_loss: 0.0030\n",
      "Epoch 23/24\n",
      "3622/3622 [==============================] - 1s 379us/step - loss: 0.0051 - val_loss: 0.0010\n",
      "Epoch 24/24\n",
      "3622/3622 [==============================] - 1s 380us/step - loss: 0.0019 - val_loss: 0.0024\n",
      "Train on 3622 samples, validate on 403 samples\n",
      "Epoch 1/24\n",
      "3622/3622 [==============================] - 3s 779us/step - loss: 0.0402 - val_loss: 0.0317\n",
      "Epoch 2/24\n",
      "3622/3622 [==============================] - 1s 198us/step - loss: 0.0156 - val_loss: 0.0021\n",
      "Epoch 3/24\n",
      "3622/3622 [==============================] - 1s 199us/step - loss: 0.0028 - val_loss: 0.0011\n",
      "Epoch 4/24\n",
      "3622/3622 [==============================] - 1s 197us/step - loss: 0.0023 - val_loss: 7.3964e-04\n",
      "Epoch 5/24\n",
      "3622/3622 [==============================] - 1s 198us/step - loss: 8.7668e-04 - val_loss: 9.0806e-04\n",
      "Epoch 6/24\n",
      "3622/3622 [==============================] - 1s 197us/step - loss: 0.0019 - val_loss: 0.0013\n",
      "Epoch 7/24\n",
      "3622/3622 [==============================] - 1s 198us/step - loss: 0.0017 - val_loss: 0.0030\n",
      "Epoch 8/24\n",
      "3622/3622 [==============================] - 1s 199us/step - loss: 0.0087 - val_loss: 0.0023\n",
      "Epoch 9/24\n",
      "3622/3622 [==============================] - 1s 198us/step - loss: 0.0126 - val_loss: 0.0096\n",
      "Epoch 10/24\n",
      "3622/3622 [==============================] - 1s 199us/step - loss: 0.0424 - val_loss: 0.0183\n",
      "Epoch 11/24\n",
      "3622/3622 [==============================] - 1s 198us/step - loss: 0.0066 - val_loss: 9.2608e-04\n",
      "Epoch 12/24\n",
      "3622/3622 [==============================] - 1s 198us/step - loss: 0.0206 - val_loss: 0.0025\n",
      "Epoch 13/24\n",
      "3622/3622 [==============================] - 1s 197us/step - loss: 0.0096 - val_loss: 8.3614e-04\n",
      "Epoch 14/24\n",
      "3622/3622 [==============================] - 1s 197us/step - loss: 0.0156 - val_loss: 0.0042\n",
      "Epoch 15/24\n",
      "3622/3622 [==============================] - 1s 199us/step - loss: 0.0037 - val_loss: 0.0028\n",
      "Epoch 16/24\n",
      "3622/3622 [==============================] - 1s 196us/step - loss: 0.0053 - val_loss: 0.0016\n",
      "Epoch 17/24\n",
      "3622/3622 [==============================] - 1s 203us/step - loss: 7.4773e-04 - val_loss: 0.0011\n",
      "Epoch 18/24\n",
      "3622/3622 [==============================] - 1s 196us/step - loss: 9.8867e-04 - val_loss: 5.4013e-04\n",
      "Epoch 19/24\n",
      "3622/3622 [==============================] - 1s 199us/step - loss: 7.0594e-04 - val_loss: 5.4136e-04\n",
      "Epoch 20/24\n",
      "3622/3622 [==============================] - 1s 196us/step - loss: 0.0014 - val_loss: 6.3585e-04\n",
      "Epoch 21/24\n",
      "3622/3622 [==============================] - 1s 197us/step - loss: 0.0012 - val_loss: 5.0544e-04\n",
      "Epoch 22/24\n",
      "3622/3622 [==============================] - 1s 198us/step - loss: 0.0029 - val_loss: 4.9571e-04\n",
      "Epoch 23/24\n",
      "3622/3622 [==============================] - 1s 198us/step - loss: 0.0026 - val_loss: 6.5541e-04\n",
      "Epoch 24/24\n",
      "3622/3622 [==============================] - 1s 198us/step - loss: 0.0052 - val_loss: 0.0012\n",
      "Train on 3622 samples, validate on 403 samples\n",
      "Epoch 1/24\n",
      "3622/3622 [==============================] - 3s 805us/step - loss: 0.1252 - val_loss: 0.0548\n",
      "Epoch 2/24\n",
      "3622/3622 [==============================] - 1s 199us/step - loss: 0.0136 - val_loss: 0.0059\n",
      "Epoch 3/24\n",
      "3622/3622 [==============================] - 1s 197us/step - loss: 0.0032 - val_loss: 8.3353e-04\n",
      "Epoch 4/24\n",
      "3622/3622 [==============================] - 1s 198us/step - loss: 0.0010 - val_loss: 0.0017\n",
      "Epoch 5/24\n",
      "3622/3622 [==============================] - 1s 198us/step - loss: 8.1048e-04 - val_loss: 0.0014\n",
      "Epoch 6/24\n",
      "3622/3622 [==============================] - 1s 199us/step - loss: 7.3220e-04 - val_loss: 6.2659e-04\n",
      "Epoch 7/24\n",
      "3622/3622 [==============================] - 1s 202us/step - loss: 6.1303e-04 - val_loss: 5.9618e-04\n",
      "Epoch 8/24\n",
      "3622/3622 [==============================] - 1s 197us/step - loss: 5.8142e-04 - val_loss: 6.1813e-04\n",
      "Epoch 9/24\n",
      "3622/3622 [==============================] - 1s 198us/step - loss: 5.4839e-04 - val_loss: 5.7247e-04\n",
      "Epoch 10/24\n",
      "3622/3622 [==============================] - 1s 197us/step - loss: 5.2110e-04 - val_loss: 5.0372e-04\n",
      "Epoch 11/24\n",
      "3622/3622 [==============================] - 1s 198us/step - loss: 5.0118e-04 - val_loss: 8.9417e-04\n",
      "Epoch 12/24\n",
      "3622/3622 [==============================] - 1s 197us/step - loss: 5.8587e-04 - val_loss: 4.6336e-04\n",
      "Epoch 13/24\n",
      "3622/3622 [==============================] - 1s 198us/step - loss: 4.8041e-04 - val_loss: 4.5444e-04\n",
      "Epoch 14/24\n",
      "3622/3622 [==============================] - 1s 197us/step - loss: 4.3667e-04 - val_loss: 4.4470e-04\n",
      "Epoch 15/24\n",
      "3622/3622 [==============================] - 1s 198us/step - loss: 4.1602e-04 - val_loss: 4.3478e-04\n",
      "Epoch 16/24\n",
      "3622/3622 [==============================] - 1s 197us/step - loss: 4.0693e-04 - val_loss: 4.2740e-04\n",
      "Epoch 17/24\n",
      "3622/3622 [==============================] - 1s 197us/step - loss: 3.9325e-04 - val_loss: 4.5240e-04\n",
      "Epoch 18/24\n",
      "3622/3622 [==============================] - 1s 199us/step - loss: 4.0322e-04 - val_loss: 4.3004e-04\n",
      "Epoch 19/24\n",
      "3622/3622 [==============================] - 1s 199us/step - loss: 3.9882e-04 - val_loss: 4.7393e-04\n",
      "Epoch 20/24\n",
      "3622/3622 [==============================] - 1s 197us/step - loss: 4.1337e-04 - val_loss: 3.8280e-04\n",
      "Epoch 21/24\n",
      "3622/3622 [==============================] - 1s 198us/step - loss: 3.7601e-04 - val_loss: 3.9287e-04\n",
      "Epoch 22/24\n",
      "3622/3622 [==============================] - 1s 198us/step - loss: 3.5726e-04 - val_loss: 3.7126e-04\n",
      "Epoch 23/24\n",
      "3622/3622 [==============================] - 1s 199us/step - loss: 3.4459e-04 - val_loss: 4.0942e-04\n",
      "Epoch 24/24\n",
      "3622/3622 [==============================] - 1s 198us/step - loss: 3.5828e-04 - val_loss: 3.9356e-04\n",
      "Train on 3622 samples, validate on 403 samples\n",
      "Epoch 1/24\n",
      "3622/3622 [==============================] - 3s 827us/step - loss: 0.1192 - val_loss: 0.0533\n",
      "Epoch 2/24\n",
      "3622/3622 [==============================] - 1s 198us/step - loss: 0.0144 - val_loss: 0.0111\n",
      "Epoch 3/24\n",
      "3622/3622 [==============================] - 1s 198us/step - loss: 0.0055 - val_loss: 0.0030\n",
      "Epoch 4/24\n",
      "3622/3622 [==============================] - 1s 201us/step - loss: 0.0024 - val_loss: 9.9528e-04\n",
      "Epoch 5/24\n",
      "3622/3622 [==============================] - 1s 203us/step - loss: 0.0012 - val_loss: 6.8574e-04\n",
      "Epoch 6/24\n",
      "3622/3622 [==============================] - 1s 198us/step - loss: 9.3333e-04 - val_loss: 0.0020\n",
      "Epoch 7/24\n",
      "3622/3622 [==============================] - 1s 200us/step - loss: 8.2399e-04 - val_loss: 6.4514e-04\n",
      "Epoch 8/24\n",
      "3622/3622 [==============================] - 1s 199us/step - loss: 6.9738e-04 - val_loss: 6.6859e-04\n",
      "Epoch 9/24\n",
      "3622/3622 [==============================] - 1s 199us/step - loss: 6.8476e-04 - val_loss: 9.4065e-04\n",
      "Epoch 10/24\n",
      "3622/3622 [==============================] - 1s 200us/step - loss: 6.2594e-04 - val_loss: 7.7420e-04\n",
      "Epoch 11/24\n",
      "3622/3622 [==============================] - 1s 199us/step - loss: 5.8856e-04 - val_loss: 6.2427e-04\n",
      "Epoch 12/24\n",
      "3622/3622 [==============================] - 1s 200us/step - loss: 5.7895e-04 - val_loss: 6.2173e-04\n",
      "Epoch 13/24\n",
      "3622/3622 [==============================] - 1s 199us/step - loss: 5.8972e-04 - val_loss: 7.0799e-04\n",
      "Epoch 14/24\n",
      "3622/3622 [==============================] - 1s 199us/step - loss: 5.4837e-04 - val_loss: 7.7930e-04\n",
      "Epoch 15/24\n",
      "3622/3622 [==============================] - 1s 200us/step - loss: 5.2056e-04 - val_loss: 6.5786e-04\n",
      "Epoch 16/24\n",
      "3622/3622 [==============================] - 1s 199us/step - loss: 4.8984e-04 - val_loss: 5.3591e-04\n",
      "Epoch 17/24\n",
      "3622/3622 [==============================] - 1s 198us/step - loss: 4.6208e-04 - val_loss: 5.5489e-04\n",
      "Epoch 18/24\n",
      "3622/3622 [==============================] - 1s 200us/step - loss: 4.4666e-04 - val_loss: 5.1410e-04\n",
      "Epoch 19/24\n",
      "3622/3622 [==============================] - 1s 202us/step - loss: 4.2356e-04 - val_loss: 4.9826e-04\n",
      "Epoch 20/24\n",
      "3622/3622 [==============================] - 1s 197us/step - loss: 4.2604e-04 - val_loss: 4.3439e-04\n",
      "Epoch 21/24\n",
      "3622/3622 [==============================] - 1s 200us/step - loss: 4.0937e-04 - val_loss: 5.6097e-04\n",
      "Epoch 22/24\n",
      "3622/3622 [==============================] - 1s 199us/step - loss: 4.2547e-04 - val_loss: 6.3846e-04\n",
      "Epoch 23/24\n",
      "3622/3622 [==============================] - 1s 199us/step - loss: 4.2411e-04 - val_loss: 5.5646e-04\n",
      "Epoch 24/24\n",
      "3622/3622 [==============================] - 1s 199us/step - loss: 4.3048e-04 - val_loss: 4.0880e-04\n",
      "Train on 3622 samples, validate on 403 samples\n",
      "Epoch 1/24\n",
      "3622/3622 [==============================] - 5s 1ms/step - loss: 0.1179 - val_loss: 0.0232\n",
      "Epoch 2/24\n",
      "3622/3622 [==============================] - 2s 523us/step - loss: 0.0262 - val_loss: 0.0206\n",
      "Epoch 3/24\n",
      "3622/3622 [==============================] - 2s 526us/step - loss: 0.0095 - val_loss: 0.0070\n",
      "Epoch 4/24\n",
      "3622/3622 [==============================] - 2s 527us/step - loss: 0.0043 - val_loss: 0.0049\n",
      "Epoch 5/24\n",
      "3622/3622 [==============================] - 2s 530us/step - loss: 0.0021 - val_loss: 0.0025\n",
      "Epoch 6/24\n",
      "3622/3622 [==============================] - 2s 523us/step - loss: 0.0015 - val_loss: 0.0013\n",
      "Epoch 7/24\n",
      "3622/3622 [==============================] - 2s 525us/step - loss: 0.0014 - val_loss: 0.0019\n",
      "Epoch 8/24\n",
      "3622/3622 [==============================] - 2s 525us/step - loss: 0.0013 - val_loss: 0.0018\n",
      "Epoch 9/24\n",
      "3622/3622 [==============================] - 2s 525us/step - loss: 0.0012 - val_loss: 0.0015\n",
      "Epoch 10/24\n",
      "3622/3622 [==============================] - 2s 526us/step - loss: 0.0012 - val_loss: 0.0013\n",
      "Epoch 11/24\n",
      "3622/3622 [==============================] - 2s 533us/step - loss: 0.0012 - val_loss: 0.0013\n",
      "Epoch 12/24\n",
      "3622/3622 [==============================] - 2s 525us/step - loss: 0.0011 - val_loss: 0.0012\n",
      "Epoch 13/24\n",
      "3622/3622 [==============================] - 2s 525us/step - loss: 0.0011 - val_loss: 0.0012\n",
      "Epoch 14/24\n",
      "3622/3622 [==============================] - 2s 524us/step - loss: 0.0010 - val_loss: 0.0015\n",
      "Epoch 15/24\n",
      "3622/3622 [==============================] - 2s 528us/step - loss: 0.0011 - val_loss: 0.0015\n",
      "Epoch 16/24\n",
      "3622/3622 [==============================] - 2s 522us/step - loss: 0.0010 - val_loss: 0.0011\n",
      "Epoch 17/24\n",
      "3622/3622 [==============================] - 2s 525us/step - loss: 9.4438e-04 - val_loss: 0.0010\n",
      "Epoch 18/24\n",
      "3622/3622 [==============================] - 2s 525us/step - loss: 9.2392e-04 - val_loss: 0.0013\n",
      "Epoch 19/24\n",
      "3622/3622 [==============================] - 2s 529us/step - loss: 9.6328e-04 - val_loss: 0.0011\n",
      "Epoch 20/24\n",
      "3622/3622 [==============================] - 2s 528us/step - loss: 9.1920e-04 - val_loss: 9.2547e-04\n",
      "Epoch 21/24\n",
      "3622/3622 [==============================] - 2s 527us/step - loss: 8.7692e-04 - val_loss: 0.0015\n",
      "Epoch 22/24\n",
      "3622/3622 [==============================] - 2s 523us/step - loss: 9.6816e-04 - val_loss: 8.4989e-04\n",
      "Epoch 23/24\n",
      "3622/3622 [==============================] - 2s 527us/step - loss: 8.0938e-04 - val_loss: 9.2713e-04\n",
      "Epoch 24/24\n",
      "3622/3622 [==============================] - 2s 524us/step - loss: 7.9537e-04 - val_loss: 8.9270e-04\n",
      "Train on 3622 samples, validate on 403 samples\n",
      "Epoch 1/24\n",
      "3622/3622 [==============================] - 4s 1ms/step - loss: 0.0547 - val_loss: 0.0155\n",
      "Epoch 2/24\n",
      "3622/3622 [==============================] - 2s 476us/step - loss: 0.0096 - val_loss: 0.0012\n",
      "Epoch 3/24\n",
      "3622/3622 [==============================] - 2s 477us/step - loss: 0.0035 - val_loss: 0.0012\n",
      "Epoch 4/24\n",
      "3622/3622 [==============================] - 2s 474us/step - loss: 0.0027 - val_loss: 0.0058\n",
      "Epoch 5/24\n",
      "3622/3622 [==============================] - 2s 480us/step - loss: 0.0019 - val_loss: 0.0025\n",
      "Epoch 6/24\n",
      "3622/3622 [==============================] - 2s 476us/step - loss: 0.0015 - val_loss: 9.6243e-04\n",
      "Epoch 7/24\n",
      "3622/3622 [==============================] - 2s 478us/step - loss: 0.0012 - val_loss: 0.0010\n",
      "Epoch 8/24\n",
      "3622/3622 [==============================] - 2s 477us/step - loss: 0.0010 - val_loss: 0.0011\n",
      "Epoch 9/24\n",
      "3622/3622 [==============================] - 2s 487us/step - loss: 9.2206e-04 - val_loss: 0.0012\n",
      "Epoch 10/24\n",
      "3622/3622 [==============================] - 2s 476us/step - loss: 8.5670e-04 - val_loss: 0.0011\n",
      "Epoch 11/24\n",
      "3622/3622 [==============================] - 2s 477us/step - loss: 8.2031e-04 - val_loss: 0.0011\n",
      "Epoch 12/24\n",
      "3622/3622 [==============================] - 2s 476us/step - loss: 8.1672e-04 - val_loss: 9.3587e-04\n",
      "Epoch 13/24\n",
      "3622/3622 [==============================] - 2s 478us/step - loss: 7.9084e-04 - val_loss: 8.1858e-04\n",
      "Epoch 14/24\n",
      "3622/3622 [==============================] - 2s 477us/step - loss: 7.6131e-04 - val_loss: 7.7919e-04\n",
      "Epoch 15/24\n",
      "3622/3622 [==============================] - 2s 478us/step - loss: 7.3217e-04 - val_loss: 7.7507e-04\n",
      "Epoch 16/24\n",
      "3622/3622 [==============================] - 2s 476us/step - loss: 7.1730e-04 - val_loss: 7.3919e-04\n",
      "Epoch 17/24\n",
      "3622/3622 [==============================] - 2s 476us/step - loss: 7.1007e-04 - val_loss: 7.5584e-04\n",
      "Epoch 18/24\n",
      "3622/3622 [==============================] - 2s 482us/step - loss: 6.9262e-04 - val_loss: 9.6358e-04\n",
      "Epoch 19/24\n",
      "3622/3622 [==============================] - 2s 479us/step - loss: 7.5100e-04 - val_loss: 7.4184e-04\n",
      "Epoch 20/24\n",
      "3622/3622 [==============================] - 2s 477us/step - loss: 6.6613e-04 - val_loss: 7.3312e-04\n",
      "Epoch 21/24\n",
      "3622/3622 [==============================] - 2s 475us/step - loss: 6.8308e-04 - val_loss: 7.5634e-04\n",
      "Epoch 22/24\n",
      "3622/3622 [==============================] - 2s 479us/step - loss: 6.7288e-04 - val_loss: 6.6647e-04\n",
      "Epoch 23/24\n",
      "3622/3622 [==============================] - 2s 477us/step - loss: 6.3836e-04 - val_loss: 7.0591e-04\n",
      "Epoch 24/24\n",
      "3622/3622 [==============================] - 2s 476us/step - loss: 6.4730e-04 - val_loss: 6.6880e-04\n",
      "Train on 3622 samples, validate on 403 samples\n",
      "Epoch 1/24\n",
      "3622/3622 [==============================] - 5s 1ms/step - loss: 0.0458 - val_loss: 0.0152\n",
      "Epoch 2/24\n",
      "3622/3622 [==============================] - 2s 476us/step - loss: 0.0072 - val_loss: 0.0047\n",
      "Epoch 3/24\n",
      "3622/3622 [==============================] - 2s 477us/step - loss: 0.0029 - val_loss: 0.0012\n",
      "Epoch 4/24\n",
      "3622/3622 [==============================] - 2s 476us/step - loss: 0.0019 - val_loss: 0.0011\n",
      "Epoch 5/24\n",
      "3622/3622 [==============================] - 2s 477us/step - loss: 0.0015 - val_loss: 0.0013\n",
      "Epoch 6/24\n",
      "3622/3622 [==============================] - 2s 479us/step - loss: 0.0012 - val_loss: 0.0015\n",
      "Epoch 7/24\n",
      "3622/3622 [==============================] - 2s 476us/step - loss: 1.0000e-03 - val_loss: 0.0016\n",
      "Epoch 8/24\n",
      "3622/3622 [==============================] - 2s 482us/step - loss: 9.6310e-04 - val_loss: 0.0014\n",
      "Epoch 9/24\n",
      "3622/3622 [==============================] - 2s 480us/step - loss: 9.4658e-04 - val_loss: 0.0013\n",
      "Epoch 10/24\n",
      "3622/3622 [==============================] - 2s 478us/step - loss: 8.9145e-04 - val_loss: 8.9203e-04\n",
      "Epoch 11/24\n",
      "3622/3622 [==============================] - 2s 477us/step - loss: 8.5529e-04 - val_loss: 8.3128e-04\n",
      "Epoch 12/24\n",
      "3622/3622 [==============================] - 2s 476us/step - loss: 7.9046e-04 - val_loss: 8.1689e-04\n",
      "Epoch 13/24\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3622/3622 [==============================] - 2s 478us/step - loss: 7.7164e-04 - val_loss: 7.8830e-04\n",
      "Epoch 14/24\n",
      "3622/3622 [==============================] - 2s 479us/step - loss: 7.5433e-04 - val_loss: 8.1752e-04\n",
      "Epoch 15/24\n",
      "3622/3622 [==============================] - 2s 480us/step - loss: 7.4344e-04 - val_loss: 8.1600e-04\n",
      "Epoch 16/24\n",
      "3622/3622 [==============================] - 2s 477us/step - loss: 7.3959e-04 - val_loss: 8.2877e-04\n",
      "Epoch 17/24\n",
      "3622/3622 [==============================] - 2s 480us/step - loss: 7.3147e-04 - val_loss: 9.8292e-04\n",
      "Epoch 18/24\n",
      "3622/3622 [==============================] - 2s 479us/step - loss: 7.6243e-04 - val_loss: 7.6592e-04\n",
      "Epoch 19/24\n",
      "3622/3622 [==============================] - 2s 477us/step - loss: 7.0378e-04 - val_loss: 8.5307e-04\n",
      "Epoch 20/24\n",
      "3622/3622 [==============================] - 2s 480us/step - loss: 7.3891e-04 - val_loss: 7.9222e-04\n",
      "Epoch 21/24\n",
      "3622/3622 [==============================] - 2s 476us/step - loss: 6.9758e-04 - val_loss: 6.5091e-04\n",
      "Epoch 22/24\n",
      "3622/3622 [==============================] - 2s 476us/step - loss: 6.5358e-04 - val_loss: 6.4392e-04\n",
      "Epoch 23/24\n",
      "3622/3622 [==============================] - 2s 477us/step - loss: 6.3738e-04 - val_loss: 6.4703e-04\n",
      "Epoch 24/24\n",
      "3622/3622 [==============================] - 2s 477us/step - loss: 6.3128e-04 - val_loss: 6.2385e-04\n",
      "Train on 3622 samples, validate on 403 samples\n",
      "Epoch 1/24\n",
      "3622/3622 [==============================] - 5s 1ms/step - loss: nan - val_loss: nan\n",
      "Epoch 2/24\n",
      "3622/3622 [==============================] - 2s 555us/step - loss: nan - val_loss: nan\n",
      "Epoch 3/24\n",
      "3622/3622 [==============================] - 2s 558us/step - loss: nan - val_loss: nan\n",
      "Epoch 4/24\n",
      "3622/3622 [==============================] - 2s 552us/step - loss: nan - val_loss: nan\n",
      "Epoch 5/24\n",
      "3622/3622 [==============================] - 2s 557us/step - loss: nan - val_loss: nan\n",
      "Epoch 6/24\n",
      "3622/3622 [==============================] - 2s 559us/step - loss: nan - val_loss: nan\n",
      "Epoch 7/24\n",
      "3622/3622 [==============================] - 2s 552us/step - loss: nan - val_loss: nan\n",
      "Epoch 8/24\n",
      "3622/3622 [==============================] - 2s 554us/step - loss: nan - val_loss: nan\n",
      "Epoch 9/24\n",
      "3622/3622 [==============================] - 2s 556us/step - loss: nan - val_loss: nan\n",
      "Epoch 10/24\n",
      "3622/3622 [==============================] - 2s 555us/step - loss: nan - val_loss: nan\n",
      "Epoch 11/24\n",
      "3622/3622 [==============================] - 2s 552us/step - loss: nan - val_loss: nan\n",
      "Epoch 12/24\n",
      "3622/3622 [==============================] - 2s 552us/step - loss: nan - val_loss: nan\n",
      "Epoch 13/24\n",
      "3622/3622 [==============================] - 2s 557us/step - loss: nan - val_loss: nan\n",
      "Epoch 14/24\n",
      "3622/3622 [==============================] - 2s 555us/step - loss: nan - val_loss: nan\n",
      "Epoch 15/24\n",
      "3622/3622 [==============================] - 2s 553us/step - loss: nan - val_loss: nan\n",
      "Epoch 16/24\n",
      "3622/3622 [==============================] - 2s 557us/step - loss: nan - val_loss: nan\n",
      "Epoch 17/24\n",
      "3622/3622 [==============================] - 2s 556us/step - loss: nan - val_loss: nan\n",
      "Epoch 18/24\n",
      "3622/3622 [==============================] - 2s 556us/step - loss: nan - val_loss: nan\n",
      "Epoch 19/24\n",
      "3622/3622 [==============================] - 2s 557us/step - loss: nan - val_loss: nan\n",
      "Epoch 20/24\n",
      "3622/3622 [==============================] - 2s 552us/step - loss: nan - val_loss: nan\n",
      "Epoch 21/24\n",
      "3622/3622 [==============================] - 2s 556us/step - loss: nan - val_loss: nan\n",
      "Epoch 22/24\n",
      "3622/3622 [==============================] - 2s 558us/step - loss: nan - val_loss: nan\n",
      "Epoch 23/24\n",
      "3622/3622 [==============================] - 2s 554us/step - loss: nan - val_loss: nan\n",
      "Epoch 24/24\n",
      "3622/3622 [==============================] - 2s 554us/step - loss: nan - val_loss: nan\n",
      "Train on 3622 samples, validate on 403 samples\n",
      "Epoch 1/24\n",
      "3622/3622 [==============================] - 5s 1ms/step - loss: nan - val_loss: nan\n",
      "Epoch 2/24\n",
      "3622/3622 [==============================] - 1s 390us/step - loss: nan - val_loss: nan\n",
      "Epoch 3/24\n",
      "3622/3622 [==============================] - 1s 392us/step - loss: nan - val_loss: nan\n",
      "Epoch 4/24\n",
      "3622/3622 [==============================] - 1s 390us/step - loss: nan - val_loss: nan\n",
      "Epoch 5/24\n",
      "3622/3622 [==============================] - 1s 388us/step - loss: nan - val_loss: nan\n",
      "Epoch 6/24\n",
      "3622/3622 [==============================] - 1s 393us/step - loss: nan - val_loss: nan\n",
      "Epoch 7/24\n",
      "3622/3622 [==============================] - 1s 392us/step - loss: nan - val_loss: nan\n",
      "Epoch 8/24\n",
      "3622/3622 [==============================] - 1s 391us/step - loss: nan - val_loss: nan\n",
      "Epoch 9/24\n",
      "3622/3622 [==============================] - 1s 389us/step - loss: nan - val_loss: nan\n",
      "Epoch 10/24\n",
      "3622/3622 [==============================] - 1s 392us/step - loss: nan - val_loss: nan\n",
      "Epoch 11/24\n",
      "3622/3622 [==============================] - 1s 398us/step - loss: nan - val_loss: nan\n",
      "Epoch 12/24\n",
      "3622/3622 [==============================] - 1s 389us/step - loss: nan - val_loss: nan\n",
      "Epoch 13/24\n",
      "3622/3622 [==============================] - 1s 394us/step - loss: nan - val_loss: nan\n",
      "Epoch 14/24\n",
      "3622/3622 [==============================] - 1s 394us/step - loss: nan - val_loss: nan\n",
      "Epoch 15/24\n",
      "3622/3622 [==============================] - 1s 390us/step - loss: nan - val_loss: nan\n",
      "Epoch 16/24\n",
      "3622/3622 [==============================] - 1s 390us/step - loss: nan - val_loss: nan\n",
      "Epoch 17/24\n",
      "3622/3622 [==============================] - 1s 388us/step - loss: nan - val_loss: nan\n",
      "Epoch 18/24\n",
      "3622/3622 [==============================] - 1s 391us/step - loss: nan - val_loss: nan\n",
      "Epoch 19/24\n",
      "3622/3622 [==============================] - 1s 390us/step - loss: nan - val_loss: nan\n",
      "Epoch 20/24\n",
      "3622/3622 [==============================] - 1s 390us/step - loss: nan - val_loss: nan\n",
      "Epoch 21/24\n",
      "3622/3622 [==============================] - 1s 387us/step - loss: nan - val_loss: nan\n",
      "Epoch 22/24\n",
      "3622/3622 [==============================] - 1s 389us/step - loss: nan - val_loss: nan\n",
      "Epoch 23/24\n",
      "3622/3622 [==============================] - 1s 389us/step - loss: nan - val_loss: nan\n",
      "Epoch 24/24\n",
      "3622/3622 [==============================] - 1s 389us/step - loss: nan - val_loss: nan\n",
      "\n",
      "GENERATION 4\n",
      "\n",
      "Train on 3622 samples, validate on 403 samples\n",
      "Epoch 1/24\n",
      "3622/3622 [==============================] - 4s 1ms/step - loss: 0.1153 - val_loss: 0.0367\n",
      "Epoch 2/24\n",
      "3622/3622 [==============================] - 1s 185us/step - loss: 0.0108 - val_loss: 0.0022\n",
      "Epoch 3/24\n",
      "3622/3622 [==============================] - 1s 185us/step - loss: 0.0034 - val_loss: 0.0014\n",
      "Epoch 4/24\n",
      "3622/3622 [==============================] - 1s 185us/step - loss: 0.0018 - val_loss: 0.0034\n",
      "Epoch 5/24\n",
      "3622/3622 [==============================] - 1s 188us/step - loss: 0.0011 - val_loss: 0.0020\n",
      "Epoch 6/24\n",
      "3622/3622 [==============================] - 1s 185us/step - loss: 9.9781e-04 - val_loss: 7.0878e-04\n",
      "Epoch 7/24\n",
      "3622/3622 [==============================] - 1s 185us/step - loss: 8.0189e-04 - val_loss: 7.1350e-04\n",
      "Epoch 8/24\n",
      "3622/3622 [==============================] - 1s 185us/step - loss: 7.2330e-04 - val_loss: 7.4841e-04\n",
      "Epoch 9/24\n",
      "3622/3622 [==============================] - 1s 184us/step - loss: 6.0907e-04 - val_loss: 6.4624e-04\n",
      "Epoch 10/24\n",
      "3622/3622 [==============================] - 1s 187us/step - loss: 5.4759e-04 - val_loss: 5.4731e-04\n",
      "Epoch 11/24\n",
      "3622/3622 [==============================] - 1s 184us/step - loss: 5.1733e-04 - val_loss: 5.2178e-04\n",
      "Epoch 12/24\n",
      "3622/3622 [==============================] - 1s 185us/step - loss: 4.7986e-04 - val_loss: 4.9510e-04\n",
      "Epoch 13/24\n",
      "3622/3622 [==============================] - 1s 186us/step - loss: 4.5444e-04 - val_loss: 4.6601e-04\n",
      "Epoch 14/24\n",
      "3622/3622 [==============================] - 1s 187us/step - loss: 4.2501e-04 - val_loss: 4.4441e-04\n",
      "Epoch 15/24\n",
      "3622/3622 [==============================] - 1s 186us/step - loss: 4.2271e-04 - val_loss: 4.2219e-04\n",
      "Epoch 16/24\n",
      "3622/3622 [==============================] - 1s 184us/step - loss: 4.1216e-04 - val_loss: 4.2520e-04\n",
      "Epoch 17/24\n",
      "3622/3622 [==============================] - 1s 185us/step - loss: 3.8881e-04 - val_loss: 4.4688e-04\n",
      "Epoch 18/24\n",
      "3622/3622 [==============================] - 1s 185us/step - loss: 3.8993e-04 - val_loss: 4.1881e-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19/24\n",
      "3622/3622 [==============================] - 1s 184us/step - loss: 3.8051e-04 - val_loss: 5.9783e-04\n",
      "Epoch 20/24\n",
      "3622/3622 [==============================] - 1s 185us/step - loss: 4.1091e-04 - val_loss: 5.4835e-04\n",
      "Epoch 21/24\n",
      "3622/3622 [==============================] - 1s 184us/step - loss: 4.1040e-04 - val_loss: 4.6013e-04\n",
      "Epoch 22/24\n",
      "3622/3622 [==============================] - 1s 185us/step - loss: 4.1521e-04 - val_loss: 4.2976e-04\n",
      "Epoch 23/24\n",
      "3622/3622 [==============================] - 1s 183us/step - loss: 3.8134e-04 - val_loss: 3.7879e-04\n",
      "Epoch 24/24\n",
      "3622/3622 [==============================] - 1s 186us/step - loss: 3.5662e-04 - val_loss: 3.8276e-04\n",
      "Train on 3622 samples, validate on 403 samples\n",
      "Epoch 1/24\n",
      "3622/3622 [==============================] - 4s 1ms/step - loss: 0.0882 - val_loss: 0.0182\n",
      "Epoch 2/24\n",
      "3622/3622 [==============================] - 1s 203us/step - loss: 0.0100 - val_loss: 0.0045\n",
      "Epoch 3/24\n",
      "3622/3622 [==============================] - 1s 200us/step - loss: 0.0039 - val_loss: 9.8094e-04\n",
      "Epoch 4/24\n",
      "3622/3622 [==============================] - 1s 201us/step - loss: 0.0018 - val_loss: 0.0015\n",
      "Epoch 5/24\n",
      "3622/3622 [==============================] - 1s 203us/step - loss: 0.0011 - val_loss: 8.0590e-04\n",
      "Epoch 6/24\n",
      "3622/3622 [==============================] - 1s 200us/step - loss: 8.2338e-04 - val_loss: 0.0014\n",
      "Epoch 7/24\n",
      "3622/3622 [==============================] - 1s 199us/step - loss: 6.8219e-04 - val_loss: 6.5339e-04\n",
      "Epoch 8/24\n",
      "3622/3622 [==============================] - 1s 199us/step - loss: 6.1784e-04 - val_loss: 6.3546e-04\n",
      "Epoch 9/24\n",
      "3622/3622 [==============================] - 1s 200us/step - loss: 6.0089e-04 - val_loss: 7.8930e-04\n",
      "Epoch 10/24\n",
      "3622/3622 [==============================] - 1s 202us/step - loss: 5.6840e-04 - val_loss: 6.4398e-04\n",
      "Epoch 11/24\n",
      "3622/3622 [==============================] - 1s 201us/step - loss: 5.4371e-04 - val_loss: 6.1632e-04\n",
      "Epoch 12/24\n",
      "3622/3622 [==============================] - 1s 200us/step - loss: 5.2180e-04 - val_loss: 5.8108e-04\n",
      "Epoch 13/24\n",
      "3622/3622 [==============================] - 1s 199us/step - loss: 5.0155e-04 - val_loss: 6.6974e-04\n",
      "Epoch 14/24\n",
      "3622/3622 [==============================] - 1s 199us/step - loss: 4.8818e-04 - val_loss: 5.8732e-04\n",
      "Epoch 15/24\n",
      "3622/3622 [==============================] - 1s 200us/step - loss: 4.5634e-04 - val_loss: 6.3275e-04\n",
      "Epoch 16/24\n",
      "3622/3622 [==============================] - 1s 200us/step - loss: 4.4656e-04 - val_loss: 4.9898e-04\n",
      "Epoch 17/24\n",
      "3622/3622 [==============================] - 1s 199us/step - loss: 4.1152e-04 - val_loss: 4.4717e-04\n",
      "Epoch 18/24\n",
      "3622/3622 [==============================] - 1s 199us/step - loss: 4.0154e-04 - val_loss: 4.6206e-04\n",
      "Epoch 19/24\n",
      "3622/3622 [==============================] - 1s 200us/step - loss: 3.7588e-04 - val_loss: 5.1501e-04\n",
      "Epoch 20/24\n",
      "3622/3622 [==============================] - 1s 200us/step - loss: 4.0472e-04 - val_loss: 4.7005e-04\n",
      "Epoch 21/24\n",
      "3622/3622 [==============================] - 1s 201us/step - loss: 3.6389e-04 - val_loss: 4.1021e-04\n",
      "Epoch 22/24\n",
      "3622/3622 [==============================] - 1s 200us/step - loss: 3.6046e-04 - val_loss: 3.8624e-04\n",
      "Epoch 23/24\n",
      "3622/3622 [==============================] - 1s 200us/step - loss: 3.4908e-04 - val_loss: 3.9144e-04\n",
      "Epoch 24/24\n",
      "3622/3622 [==============================] - 1s 206us/step - loss: 3.3939e-04 - val_loss: 3.8628e-04\n",
      "Train on 3622 samples, validate on 403 samples\n",
      "Epoch 1/24\n",
      "3622/3622 [==============================] - 4s 1ms/step - loss: 0.0842 - val_loss: 0.0099\n",
      "Epoch 2/24\n",
      "3622/3622 [==============================] - 1s 196us/step - loss: 0.0707 - val_loss: 0.0406\n",
      "Epoch 3/24\n",
      "3622/3622 [==============================] - 1s 197us/step - loss: 0.0605 - val_loss: 0.0259\n",
      "Epoch 4/24\n",
      "3622/3622 [==============================] - 1s 196us/step - loss: 0.0503 - val_loss: 0.0167\n",
      "Epoch 5/24\n",
      "3622/3622 [==============================] - 1s 196us/step - loss: 0.0369 - val_loss: 0.0075\n",
      "Epoch 6/24\n",
      "3622/3622 [==============================] - 1s 197us/step - loss: 0.0244 - val_loss: 0.0142\n",
      "Epoch 7/24\n",
      "3622/3622 [==============================] - 1s 198us/step - loss: 0.0120 - val_loss: 0.0434\n",
      "Epoch 8/24\n",
      "3622/3622 [==============================] - 1s 197us/step - loss: 0.0104 - val_loss: 0.0568\n",
      "Epoch 9/24\n",
      "3622/3622 [==============================] - 1s 197us/step - loss: 0.0071 - val_loss: 0.0659\n",
      "Epoch 10/24\n",
      "3622/3622 [==============================] - 1s 194us/step - loss: 0.0044 - val_loss: 0.0367\n",
      "Epoch 11/24\n",
      "3622/3622 [==============================] - 1s 208us/step - loss: 0.0031 - val_loss: 0.0389\n",
      "Epoch 12/24\n",
      "3622/3622 [==============================] - 1s 199us/step - loss: 0.0028 - val_loss: 0.0053\n",
      "Epoch 13/24\n",
      "3622/3622 [==============================] - 1s 197us/step - loss: 0.0023 - val_loss: 0.0033\n",
      "Epoch 14/24\n",
      "3622/3622 [==============================] - 1s 197us/step - loss: 0.0020 - val_loss: 0.0038\n",
      "Epoch 15/24\n",
      "3622/3622 [==============================] - 1s 196us/step - loss: 0.0018 - val_loss: 0.0035\n",
      "Epoch 16/24\n",
      "3622/3622 [==============================] - 1s 196us/step - loss: 0.0017 - val_loss: 0.0023\n",
      "Epoch 17/24\n",
      "3622/3622 [==============================] - 1s 197us/step - loss: 0.0015 - val_loss: 0.0024\n",
      "Epoch 18/24\n",
      "3622/3622 [==============================] - 1s 197us/step - loss: 0.0015 - val_loss: 0.0014\n",
      "Epoch 19/24\n",
      "3622/3622 [==============================] - 1s 197us/step - loss: 0.0013 - val_loss: 0.0017\n",
      "Epoch 20/24\n",
      "3622/3622 [==============================] - 1s 197us/step - loss: 0.0013 - val_loss: 0.0016\n",
      "Epoch 21/24\n",
      "3622/3622 [==============================] - 1s 196us/step - loss: 0.0012 - val_loss: 0.0018\n",
      "Epoch 22/24\n",
      "3622/3622 [==============================] - 1s 196us/step - loss: 0.0012 - val_loss: 0.0014\n",
      "Epoch 23/24\n",
      "3622/3622 [==============================] - 1s 196us/step - loss: 0.0011 - val_loss: 0.0012\n",
      "Epoch 24/24\n",
      "3622/3622 [==============================] - 1s 199us/step - loss: 0.0011 - val_loss: 0.0011\n",
      "Train on 3622 samples, validate on 403 samples\n",
      "Epoch 1/24\n",
      "3622/3622 [==============================] - 4s 1ms/step - loss: 0.2287 - val_loss: 0.1018\n",
      "Epoch 2/24\n",
      "3622/3622 [==============================] - 1s 198us/step - loss: 0.0768 - val_loss: 0.0463\n",
      "Epoch 3/24\n",
      "3622/3622 [==============================] - 1s 199us/step - loss: 0.0694 - val_loss: 0.0421\n",
      "Epoch 4/24\n",
      "3622/3622 [==============================] - 1s 199us/step - loss: 0.0692 - val_loss: 0.0421\n",
      "Epoch 5/24\n",
      "3622/3622 [==============================] - 1s 198us/step - loss: 0.0691 - val_loss: 0.0419\n",
      "Epoch 6/24\n",
      "3622/3622 [==============================] - 1s 198us/step - loss: 0.0689 - val_loss: 0.0395\n",
      "Epoch 7/24\n",
      "3622/3622 [==============================] - 1s 198us/step - loss: 0.0688 - val_loss: 0.0379\n",
      "Epoch 8/24\n",
      "3622/3622 [==============================] - 1s 197us/step - loss: 0.0686 - val_loss: 0.0453\n",
      "Epoch 9/24\n",
      "3622/3622 [==============================] - 1s 198us/step - loss: 0.0687 - val_loss: 0.0277\n",
      "Epoch 10/24\n",
      "3622/3622 [==============================] - 1s 198us/step - loss: 0.0685 - val_loss: 0.0321\n",
      "Epoch 11/24\n",
      "3622/3622 [==============================] - 1s 199us/step - loss: 0.0683 - val_loss: 0.0266\n",
      "Epoch 12/24\n",
      "3622/3622 [==============================] - 1s 199us/step - loss: 0.0684 - val_loss: 0.0270\n",
      "Epoch 13/24\n",
      "3622/3622 [==============================] - 1s 198us/step - loss: 0.0681 - val_loss: 0.0336\n",
      "Epoch 14/24\n",
      "3622/3622 [==============================] - 1s 198us/step - loss: 0.0678 - val_loss: 0.0315\n",
      "Epoch 15/24\n",
      "3622/3622 [==============================] - 1s 198us/step - loss: 0.0675 - val_loss: 0.0264\n",
      "Epoch 16/24\n",
      "3622/3622 [==============================] - 1s 201us/step - loss: 0.0674 - val_loss: 0.0283\n",
      "Epoch 17/24\n",
      "3622/3622 [==============================] - 1s 200us/step - loss: 0.0670 - val_loss: 0.0221\n",
      "Epoch 18/24\n",
      "3622/3622 [==============================] - 1s 198us/step - loss: 0.0668 - val_loss: 0.0467\n",
      "Epoch 19/24\n",
      "3622/3622 [==============================] - 1s 200us/step - loss: 0.0663 - val_loss: 0.0479\n",
      "Epoch 20/24\n",
      "3622/3622 [==============================] - 1s 201us/step - loss: 0.0657 - val_loss: 0.0390\n",
      "Epoch 21/24\n",
      "3622/3622 [==============================] - 1s 198us/step - loss: 0.0649 - val_loss: 0.0232\n",
      "Epoch 22/24\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3622/3622 [==============================] - 1s 198us/step - loss: 0.0638 - val_loss: 0.0141\n",
      "Epoch 23/24\n",
      "3622/3622 [==============================] - 1s 198us/step - loss: 0.0628 - val_loss: 0.0257\n",
      "Epoch 24/24\n",
      "3622/3622 [==============================] - 1s 197us/step - loss: 0.0595 - val_loss: 0.0158\n",
      "Train on 3622 samples, validate on 403 samples\n",
      "Epoch 1/24\n",
      "3622/3622 [==============================] - 4s 1ms/step - loss: 0.0379 - val_loss: 0.0029\n",
      "Epoch 2/24\n",
      "3622/3622 [==============================] - 1s 188us/step - loss: 0.0039 - val_loss: 0.0049\n",
      "Epoch 3/24\n",
      "3622/3622 [==============================] - 1s 188us/step - loss: 0.0032 - val_loss: 0.0071\n",
      "Epoch 4/24\n",
      "3622/3622 [==============================] - 1s 188us/step - loss: 0.0060 - val_loss: 0.0220\n",
      "Epoch 5/24\n",
      "3622/3622 [==============================] - 1s 189us/step - loss: 0.0071 - val_loss: 0.0114\n",
      "Epoch 6/24\n",
      "3622/3622 [==============================] - 1s 191us/step - loss: 0.0038 - val_loss: 0.0108\n",
      "Epoch 7/24\n",
      "3622/3622 [==============================] - 1s 188us/step - loss: 0.0039 - val_loss: 0.0134\n",
      "Epoch 8/24\n",
      "3622/3622 [==============================] - 1s 188us/step - loss: 0.0052 - val_loss: 0.0211\n",
      "Epoch 9/24\n",
      "3622/3622 [==============================] - 1s 188us/step - loss: 0.0058 - val_loss: 0.0123\n",
      "Epoch 10/24\n",
      "3622/3622 [==============================] - 1s 188us/step - loss: 0.0042 - val_loss: 0.0049\n",
      "Epoch 11/24\n",
      "3622/3622 [==============================] - 1s 188us/step - loss: 0.0024 - val_loss: 0.0044\n",
      "Epoch 12/24\n",
      "3622/3622 [==============================] - 1s 202us/step - loss: 0.0027 - val_loss: 0.0230\n",
      "Epoch 13/24\n",
      "3622/3622 [==============================] - 1s 208us/step - loss: 0.0064 - val_loss: 0.0093\n",
      "Epoch 14/24\n",
      "3622/3622 [==============================] - 1s 208us/step - loss: 0.0025 - val_loss: 0.0118\n",
      "Epoch 15/24\n",
      "3622/3622 [==============================] - 1s 220us/step - loss: 0.0038 - val_loss: 0.0067\n",
      "Epoch 16/24\n",
      "3622/3622 [==============================] - 1s 213us/step - loss: 0.0022 - val_loss: 0.0078\n",
      "Epoch 17/24\n",
      "3622/3622 [==============================] - 1s 213us/step - loss: 0.0023 - val_loss: 0.0083\n",
      "Epoch 18/24\n",
      "3622/3622 [==============================] - 1s 208us/step - loss: 0.0032 - val_loss: 0.0109\n",
      "Epoch 19/24\n",
      "3622/3622 [==============================] - 1s 197us/step - loss: 0.0025 - val_loss: 0.0095\n",
      "Epoch 20/24\n",
      "3622/3622 [==============================] - 1s 198us/step - loss: 0.0031 - val_loss: 0.0065\n",
      "Epoch 21/24\n",
      "3622/3622 [==============================] - 1s 196us/step - loss: 0.0020 - val_loss: 0.0073\n",
      "Epoch 22/24\n",
      "3622/3622 [==============================] - 1s 197us/step - loss: 0.0025 - val_loss: 0.0063\n",
      "Epoch 23/24\n",
      "3622/3622 [==============================] - 1s 203us/step - loss: 0.0027 - val_loss: 0.0049\n",
      "Epoch 24/24\n",
      "3622/3622 [==============================] - 1s 205us/step - loss: 0.0025 - val_loss: 0.0105\n",
      "Train on 3622 samples, validate on 403 samples\n",
      "Epoch 1/24\n",
      "3622/3622 [==============================] - 4s 1ms/step - loss: 0.0578 - val_loss: 0.0015\n",
      "Epoch 2/24\n",
      "3622/3622 [==============================] - 1s 239us/step - loss: 0.0064 - val_loss: 0.0079\n",
      "Epoch 3/24\n",
      "3622/3622 [==============================] - 1s 240us/step - loss: 0.0049 - val_loss: 0.0077\n",
      "Epoch 4/24\n",
      "3622/3622 [==============================] - 1s 233us/step - loss: 0.0084 - val_loss: 0.0218\n",
      "Epoch 5/24\n",
      "3622/3622 [==============================] - 1s 227us/step - loss: 0.0065 - val_loss: 0.0179\n",
      "Epoch 6/24\n",
      "3622/3622 [==============================] - 1s 247us/step - loss: 0.0066 - val_loss: 0.0193\n",
      "Epoch 7/24\n",
      "3622/3622 [==============================] - 1s 228us/step - loss: 0.0064 - val_loss: 0.0090\n",
      "Epoch 8/24\n",
      "3622/3622 [==============================] - 1s 227us/step - loss: 0.0051 - val_loss: 0.0117\n",
      "Epoch 9/24\n",
      "3622/3622 [==============================] - 1s 229us/step - loss: 0.0049 - val_loss: 0.0167\n",
      "Epoch 10/24\n",
      "3622/3622 [==============================] - 1s 233us/step - loss: 0.0077 - val_loss: 0.0221\n",
      "Epoch 11/24\n",
      "3622/3622 [==============================] - 1s 236us/step - loss: 0.0071 - val_loss: 0.0118\n",
      "Epoch 12/24\n",
      "3622/3622 [==============================] - 1s 231us/step - loss: 0.0042 - val_loss: 0.0046\n",
      "Epoch 13/24\n",
      "3622/3622 [==============================] - 1s 235us/step - loss: 0.0029 - val_loss: 0.0229\n",
      "Epoch 14/24\n",
      "3622/3622 [==============================] - 1s 233us/step - loss: 0.0073 - val_loss: 0.0144\n",
      "Epoch 15/24\n",
      "3622/3622 [==============================] - 1s 232us/step - loss: 0.0046 - val_loss: 0.0060\n",
      "Epoch 16/24\n",
      "3622/3622 [==============================] - 1s 232us/step - loss: 0.0029 - val_loss: 0.0082\n",
      "Epoch 17/24\n",
      "3622/3622 [==============================] - 1s 233us/step - loss: 0.0034 - val_loss: 0.0151\n",
      "Epoch 18/24\n",
      "3622/3622 [==============================] - 1s 233us/step - loss: 0.0053 - val_loss: 0.0078\n",
      "Epoch 19/24\n",
      "3622/3622 [==============================] - 1s 233us/step - loss: 0.0027 - val_loss: 0.0080\n",
      "Epoch 20/24\n",
      "3622/3622 [==============================] - 1s 243us/step - loss: 0.0031 - val_loss: 0.0135\n",
      "Epoch 21/24\n",
      "3622/3622 [==============================] - 1s 235us/step - loss: 0.0059 - val_loss: 0.0123\n",
      "Epoch 22/24\n",
      "3622/3622 [==============================] - 1s 228us/step - loss: 0.0033 - val_loss: 0.0128\n",
      "Epoch 23/24\n",
      "3622/3622 [==============================] - 1s 236us/step - loss: 0.0042 - val_loss: 0.0057\n",
      "Epoch 24/24\n",
      "3622/3622 [==============================] - 1s 242us/step - loss: 0.0025 - val_loss: 0.0074\n",
      "Train on 3622 samples, validate on 403 samples\n",
      "Epoch 1/24\n",
      "3622/3622 [==============================] - 4s 1ms/step - loss: 0.0303 - val_loss: 0.0062\n",
      "Epoch 2/24\n",
      "3622/3622 [==============================] - 1s 240us/step - loss: 0.0054 - val_loss: 0.0103\n",
      "Epoch 3/24\n",
      "3622/3622 [==============================] - 1s 226us/step - loss: 0.0048 - val_loss: 0.0023\n",
      "Epoch 4/24\n",
      "3622/3622 [==============================] - 1s 229us/step - loss: 0.0047 - val_loss: 8.1919e-04\n",
      "Epoch 5/24\n",
      "3622/3622 [==============================] - 1s 234us/step - loss: 0.0092 - val_loss: 8.9961e-04\n",
      "Epoch 6/24\n",
      "3622/3622 [==============================] - 1s 221us/step - loss: 0.0052 - val_loss: 0.0032\n",
      "Epoch 7/24\n",
      "3622/3622 [==============================] - 1s 221us/step - loss: 0.0037 - val_loss: 7.2172e-04\n",
      "Epoch 8/24\n",
      "3622/3622 [==============================] - 1s 222us/step - loss: 0.0099 - val_loss: 8.8222e-04\n",
      "Epoch 9/24\n",
      "3622/3622 [==============================] - 1s 222us/step - loss: 0.0056 - val_loss: 0.0034\n",
      "Epoch 10/24\n",
      "3622/3622 [==============================] - 1s 222us/step - loss: 0.0031 - val_loss: 0.0043\n",
      "Epoch 11/24\n",
      "3622/3622 [==============================] - 1s 222us/step - loss: 0.0027 - val_loss: 0.0020\n",
      "Epoch 12/24\n",
      "3622/3622 [==============================] - 1s 225us/step - loss: 0.0056 - val_loss: 6.4011e-04\n",
      "Epoch 13/24\n",
      "3622/3622 [==============================] - 1s 228us/step - loss: 0.0057 - val_loss: 6.6837e-04\n",
      "Epoch 14/24\n",
      "3622/3622 [==============================] - 1s 222us/step - loss: 0.0043 - val_loss: 0.0013\n",
      "Epoch 15/24\n",
      "3622/3622 [==============================] - 1s 227us/step - loss: 0.0047 - val_loss: 6.1943e-04\n",
      "Epoch 16/24\n",
      "3622/3622 [==============================] - 1s 245us/step - loss: 0.0042 - val_loss: 7.8796e-04\n",
      "Epoch 17/24\n",
      "3622/3622 [==============================] - 1s 251us/step - loss: 0.0032 - val_loss: 0.0012\n",
      "Epoch 18/24\n",
      "3622/3622 [==============================] - 1s 257us/step - loss: 0.0023 - val_loss: 7.6375e-04\n",
      "Epoch 19/24\n",
      "3622/3622 [==============================] - 1s 236us/step - loss: 0.0036 - val_loss: 7.1039e-04\n",
      "Epoch 20/24\n",
      "3622/3622 [==============================] - 1s 248us/step - loss: 0.0035 - val_loss: 8.9106e-04\n",
      "Epoch 21/24\n",
      "3622/3622 [==============================] - 1s 252us/step - loss: 0.0031 - val_loss: 6.8262e-04\n",
      "Epoch 22/24\n",
      "3622/3622 [==============================] - 1s 232us/step - loss: 0.0041 - val_loss: 8.7551e-04\n",
      "Epoch 23/24\n",
      "3622/3622 [==============================] - 1s 240us/step - loss: 0.0030 - val_loss: 0.0013\n",
      "Epoch 24/24\n",
      "3622/3622 [==============================] - 1s 237us/step - loss: 0.0039 - val_loss: 6.6932e-04\n",
      "Train on 3622 samples, validate on 403 samples\n",
      "Epoch 1/24\n",
      "3622/3622 [==============================] - 5s 1ms/step - loss: 0.0468 - val_loss: 0.0012\n",
      "Epoch 2/24\n",
      "3622/3622 [==============================] - 1s 238us/step - loss: 0.0030 - val_loss: 0.0127\n",
      "Epoch 3/24\n",
      "3622/3622 [==============================] - 1s 239us/step - loss: 0.0144 - val_loss: 0.0156\n",
      "Epoch 4/24\n",
      "3622/3622 [==============================] - 1s 234us/step - loss: 0.0053 - val_loss: 0.0022\n",
      "Epoch 5/24\n",
      "3622/3622 [==============================] - 1s 234us/step - loss: 0.0018 - val_loss: 0.0010\n",
      "Epoch 6/24\n",
      "3622/3622 [==============================] - 1s 236us/step - loss: 0.0018 - val_loss: 0.0026\n",
      "Epoch 7/24\n",
      "3622/3622 [==============================] - 1s 235us/step - loss: 0.0034 - val_loss: 0.0381\n",
      "Epoch 8/24\n",
      "3622/3622 [==============================] - 1s 234us/step - loss: 0.0140 - val_loss: 0.0051\n",
      "Epoch 9/24\n",
      "3622/3622 [==============================] - 1s 235us/step - loss: 0.0018 - val_loss: 0.0023\n",
      "Epoch 10/24\n",
      "3622/3622 [==============================] - 1s 236us/step - loss: 0.0015 - val_loss: 0.0013\n",
      "Epoch 11/24\n",
      "3622/3622 [==============================] - 1s 235us/step - loss: 0.0013 - val_loss: 0.0013\n",
      "Epoch 12/24\n",
      "3622/3622 [==============================] - 1s 236us/step - loss: 0.0013 - val_loss: 5.9601e-04\n",
      "Epoch 13/24\n",
      "3622/3622 [==============================] - 1s 237us/step - loss: 0.0056 - val_loss: 0.0187\n",
      "Epoch 14/24\n",
      "3622/3622 [==============================] - 1s 234us/step - loss: 0.0105 - val_loss: 0.0031\n",
      "Epoch 15/24\n",
      "3622/3622 [==============================] - 1s 238us/step - loss: 0.0026 - val_loss: 0.0054\n",
      "Epoch 16/24\n",
      "3622/3622 [==============================] - 1s 236us/step - loss: 0.0035 - val_loss: 9.7553e-04\n",
      "Epoch 17/24\n",
      "3622/3622 [==============================] - 1s 239us/step - loss: 0.0018 - val_loss: 5.6984e-04\n",
      "Epoch 18/24\n",
      "3622/3622 [==============================] - 1s 235us/step - loss: 0.0017 - val_loss: 0.0015\n",
      "Epoch 19/24\n",
      "3622/3622 [==============================] - 1s 236us/step - loss: 0.0041 - val_loss: 0.0028\n",
      "Epoch 20/24\n",
      "3622/3622 [==============================] - 1s 235us/step - loss: 0.0041 - val_loss: 0.0017\n",
      "Epoch 21/24\n",
      "3622/3622 [==============================] - 1s 235us/step - loss: 0.0033 - val_loss: 0.0021\n",
      "Epoch 22/24\n",
      "3622/3622 [==============================] - 1s 234us/step - loss: 0.0045 - val_loss: 0.0016\n",
      "Epoch 23/24\n",
      "3622/3622 [==============================] - 1s 239us/step - loss: 0.0040 - val_loss: 8.1054e-04\n",
      "Epoch 24/24\n",
      "3622/3622 [==============================] - 1s 240us/step - loss: 0.0023 - val_loss: 0.0014\n",
      "Train on 3622 samples, validate on 403 samples\n",
      "Epoch 1/24\n",
      "3622/3622 [==============================] - 5s 1ms/step - loss: 0.1340 - val_loss: 0.0021\n",
      "Epoch 2/24\n",
      "3622/3622 [==============================] - 1s 237us/step - loss: 0.0020 - val_loss: 0.0010\n",
      "Epoch 3/24\n",
      "3622/3622 [==============================] - 1s 236us/step - loss: 0.0017 - val_loss: 6.0299e-04\n",
      "Epoch 4/24\n",
      "3622/3622 [==============================] - 1s 239us/step - loss: 0.0226 - val_loss: 5.4393e-04\n",
      "Epoch 5/24\n",
      "3622/3622 [==============================] - 1s 238us/step - loss: 0.0015 - val_loss: 5.1784e-04\n",
      "Epoch 6/24\n",
      "3622/3622 [==============================] - 1s 238us/step - loss: 0.0168 - val_loss: 0.0014\n",
      "Epoch 7/24\n",
      "3622/3622 [==============================] - 1s 236us/step - loss: 0.0049 - val_loss: 0.0231\n",
      "Epoch 8/24\n",
      "3622/3622 [==============================] - 1s 237us/step - loss: 0.0087 - val_loss: 0.0011\n",
      "Epoch 9/24\n",
      "3622/3622 [==============================] - 1s 238us/step - loss: 0.0093 - val_loss: 0.0141\n",
      "Epoch 10/24\n",
      "3622/3622 [==============================] - 1s 236us/step - loss: 0.0095 - val_loss: 4.5301e-04\n",
      "Epoch 11/24\n",
      "3622/3622 [==============================] - 1s 236us/step - loss: 0.0026 - val_loss: 0.0217\n",
      "Epoch 12/24\n",
      "3622/3622 [==============================] - 1s 237us/step - loss: 0.0109 - val_loss: 0.0011\n",
      "Epoch 13/24\n",
      "3622/3622 [==============================] - 1s 236us/step - loss: 0.0026 - val_loss: 0.0039\n",
      "Epoch 14/24\n",
      "3622/3622 [==============================] - 1s 235us/step - loss: 0.0105 - val_loss: 0.0029\n",
      "Epoch 15/24\n",
      "3622/3622 [==============================] - 1s 236us/step - loss: 0.0026 - val_loss: 4.3103e-04\n",
      "Epoch 16/24\n",
      "3622/3622 [==============================] - 1s 236us/step - loss: 0.0064 - val_loss: 0.0068\n",
      "Epoch 17/24\n",
      "3622/3622 [==============================] - 1s 235us/step - loss: 0.0060 - val_loss: 6.4243e-04\n",
      "Epoch 18/24\n",
      "3622/3622 [==============================] - 1s 238us/step - loss: 0.0040 - val_loss: 0.0034\n",
      "Epoch 19/24\n",
      "3622/3622 [==============================] - 1s 236us/step - loss: 0.0080 - val_loss: 0.0020\n",
      "Epoch 20/24\n",
      "3622/3622 [==============================] - 1s 235us/step - loss: 0.0019 - val_loss: 6.6666e-04\n",
      "Epoch 21/24\n",
      "3622/3622 [==============================] - 1s 238us/step - loss: 0.0059 - val_loss: 0.0018\n",
      "Epoch 22/24\n",
      "3622/3622 [==============================] - 1s 236us/step - loss: 0.0038 - val_loss: 0.0013\n",
      "Epoch 23/24\n",
      "3622/3622 [==============================] - 1s 240us/step - loss: 0.0044 - val_loss: 0.0030\n",
      "Epoch 24/24\n",
      "3622/3622 [==============================] - 1s 241us/step - loss: 0.0050 - val_loss: 4.5005e-04\n",
      "\n",
      "GENERATION 5\n",
      "\n",
      "Train on 3622 samples, validate on 403 samples\n",
      "Epoch 1/24\n",
      "3622/3622 [==============================] - 5s 1ms/step - loss: 0.1041 - val_loss: 0.0029\n",
      "Epoch 2/24\n",
      "3622/3622 [==============================] - 1s 215us/step - loss: 0.0050 - val_loss: 0.0020\n",
      "Epoch 3/24\n",
      "3622/3622 [==============================] - 1s 212us/step - loss: 0.0036 - val_loss: 0.0011\n",
      "Epoch 4/24\n",
      "3622/3622 [==============================] - 1s 214us/step - loss: 0.0075 - val_loss: 0.0054\n",
      "Epoch 5/24\n",
      "3622/3622 [==============================] - 1s 213us/step - loss: 0.0073 - val_loss: 0.0107\n",
      "Epoch 6/24\n",
      "3622/3622 [==============================] - 1s 214us/step - loss: 0.0063 - val_loss: 0.0069\n",
      "Epoch 7/24\n",
      "3622/3622 [==============================] - 1s 212us/step - loss: 0.0072 - val_loss: 0.0014\n",
      "Epoch 8/24\n",
      "3622/3622 [==============================] - 1s 211us/step - loss: 0.0065 - val_loss: 0.0050\n",
      "Epoch 9/24\n",
      "3622/3622 [==============================] - 1s 213us/step - loss: 0.0063 - val_loss: 0.0071\n",
      "Epoch 10/24\n",
      "3622/3622 [==============================] - 1s 213us/step - loss: 0.0049 - val_loss: 0.0076\n",
      "Epoch 11/24\n",
      "3622/3622 [==============================] - 1s 213us/step - loss: 0.0069 - val_loss: 0.0062\n",
      "Epoch 12/24\n",
      "3622/3622 [==============================] - 1s 212us/step - loss: 0.0049 - val_loss: 0.0026\n",
      "Epoch 13/24\n",
      "3622/3622 [==============================] - 1s 213us/step - loss: 0.0053 - val_loss: 0.0036\n",
      "Epoch 14/24\n",
      "3622/3622 [==============================] - 1s 213us/step - loss: 0.0044 - val_loss: 0.0041\n",
      "Epoch 15/24\n",
      "3622/3622 [==============================] - 1s 210us/step - loss: 0.0049 - val_loss: 0.0040\n",
      "Epoch 16/24\n",
      "3622/3622 [==============================] - 1s 212us/step - loss: 0.0049 - val_loss: 0.0054\n",
      "Epoch 17/24\n",
      "3622/3622 [==============================] - 1s 213us/step - loss: 0.0044 - val_loss: 0.0010\n",
      "Epoch 18/24\n",
      "3622/3622 [==============================] - 1s 213us/step - loss: 0.0047 - val_loss: 4.8109e-04\n",
      "Epoch 19/24\n",
      "3622/3622 [==============================] - 1s 213us/step - loss: 0.0023 - val_loss: 0.0016\n",
      "Epoch 20/24\n",
      "3622/3622 [==============================] - 1s 212us/step - loss: 0.0040 - val_loss: 0.0028\n",
      "Epoch 21/24\n",
      "3622/3622 [==============================] - 1s 212us/step - loss: 0.0039 - val_loss: 7.1992e-04\n",
      "Epoch 22/24\n",
      "3622/3622 [==============================] - 1s 214us/step - loss: 0.0033 - val_loss: 0.0018\n",
      "Epoch 23/24\n",
      "3622/3622 [==============================] - 1s 217us/step - loss: 0.0039 - val_loss: 0.0018\n",
      "Epoch 24/24\n",
      "3622/3622 [==============================] - 1s 213us/step - loss: 0.0037 - val_loss: 0.0017\n",
      "Train on 3622 samples, validate on 403 samples\n",
      "Epoch 1/24\n",
      "3622/3622 [==============================] - 5s 1ms/step - loss: 0.1478 - val_loss: 0.0092\n",
      "Epoch 2/24\n",
      "3622/3622 [==============================] - 1s 199us/step - loss: 0.0173 - val_loss: 0.0167\n",
      "Epoch 3/24\n",
      "3622/3622 [==============================] - 1s 197us/step - loss: 0.0088 - val_loss: 0.0062\n",
      "Epoch 4/24\n",
      "3622/3622 [==============================] - 1s 197us/step - loss: 0.0038 - val_loss: 0.0030\n",
      "Epoch 5/24\n",
      "3622/3622 [==============================] - 1s 200us/step - loss: 0.0016 - val_loss: 0.0028\n",
      "Epoch 6/24\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3622/3622 [==============================] - 1s 198us/step - loss: 0.0010 - val_loss: 7.1539e-04\n",
      "Epoch 7/24\n",
      "3622/3622 [==============================] - 1s 202us/step - loss: 7.9203e-04 - val_loss: 8.5501e-04\n",
      "Epoch 8/24\n",
      "3622/3622 [==============================] - 1s 198us/step - loss: 7.2780e-04 - val_loss: 8.0668e-04\n",
      "Epoch 9/24\n",
      "3622/3622 [==============================] - 1s 198us/step - loss: 6.9514e-04 - val_loss: 6.8089e-04\n",
      "Epoch 10/24\n",
      "3622/3622 [==============================] - 1s 197us/step - loss: 6.6648e-04 - val_loss: 8.4534e-04\n",
      "Epoch 11/24\n",
      "3622/3622 [==============================] - 1s 201us/step - loss: 6.4628e-04 - val_loss: 6.9372e-04\n",
      "Epoch 12/24\n",
      "3622/3622 [==============================] - 1s 200us/step - loss: 6.1925e-04 - val_loss: 6.5741e-04\n",
      "Epoch 13/24\n",
      "3622/3622 [==============================] - 1s 200us/step - loss: 6.0707e-04 - val_loss: 7.8707e-04\n",
      "Epoch 14/24\n",
      "3622/3622 [==============================] - 1s 198us/step - loss: 5.9062e-04 - val_loss: 6.4547e-04\n",
      "Epoch 15/24\n",
      "3622/3622 [==============================] - 1s 198us/step - loss: 5.6002e-04 - val_loss: 6.1319e-04\n",
      "Epoch 16/24\n",
      "3622/3622 [==============================] - 1s 198us/step - loss: 5.4053e-04 - val_loss: 5.7553e-04\n",
      "Epoch 17/24\n",
      "3622/3622 [==============================] - 1s 199us/step - loss: 5.1744e-04 - val_loss: 5.5803e-04\n",
      "Epoch 18/24\n",
      "3622/3622 [==============================] - 1s 199us/step - loss: 4.8783e-04 - val_loss: 5.3993e-04\n",
      "Epoch 19/24\n",
      "3622/3622 [==============================] - 1s 198us/step - loss: 4.5792e-04 - val_loss: 5.1765e-04\n",
      "Epoch 20/24\n",
      "3622/3622 [==============================] - 1s 198us/step - loss: 4.2826e-04 - val_loss: 4.7443e-04\n",
      "Epoch 21/24\n",
      "3622/3622 [==============================] - 1s 199us/step - loss: 4.1093e-04 - val_loss: 4.8033e-04\n",
      "Epoch 22/24\n",
      "3622/3622 [==============================] - 1s 198us/step - loss: 3.9781e-04 - val_loss: 4.7831e-04\n",
      "Epoch 23/24\n",
      "3622/3622 [==============================] - 1s 199us/step - loss: 3.8303e-04 - val_loss: 4.3737e-04\n",
      "Epoch 24/24\n",
      "3622/3622 [==============================] - 1s 198us/step - loss: 3.8375e-04 - val_loss: 5.8687e-04\n",
      "Train on 3622 samples, validate on 403 samples\n",
      "Epoch 1/24\n",
      "3622/3622 [==============================] - 5s 1ms/step - loss: 0.4441 - val_loss: 0.2701\n",
      "Epoch 2/24\n",
      "3622/3622 [==============================] - 1s 199us/step - loss: 0.0709 - val_loss: 0.0673\n",
      "Epoch 3/24\n",
      "3622/3622 [==============================] - 1s 200us/step - loss: 0.0257 - val_loss: 0.0119\n",
      "Epoch 4/24\n",
      "3622/3622 [==============================] - 1s 199us/step - loss: 0.0125 - val_loss: 0.0145\n",
      "Epoch 5/24\n",
      "3622/3622 [==============================] - 1s 201us/step - loss: 0.0065 - val_loss: 7.8941e-04\n",
      "Epoch 6/24\n",
      "3622/3622 [==============================] - 1s 198us/step - loss: 0.0036 - val_loss: 0.0024\n",
      "Epoch 7/24\n",
      "3622/3622 [==============================] - 1s 199us/step - loss: 0.0018 - val_loss: 0.0014\n",
      "Epoch 8/24\n",
      "3622/3622 [==============================] - 1s 199us/step - loss: 0.0012 - val_loss: 0.0016\n",
      "Epoch 9/24\n",
      "3622/3622 [==============================] - 1s 200us/step - loss: 8.3145e-04 - val_loss: 0.0014\n",
      "Epoch 10/24\n",
      "3622/3622 [==============================] - 1s 198us/step - loss: 6.8323e-04 - val_loss: 0.0014\n",
      "Epoch 11/24\n",
      "3622/3622 [==============================] - 1s 199us/step - loss: 6.4070e-04 - val_loss: 0.0011\n",
      "Epoch 12/24\n",
      "3622/3622 [==============================] - 1s 204us/step - loss: 6.1684e-04 - val_loss: 0.0013\n",
      "Epoch 13/24\n",
      "3622/3622 [==============================] - 1s 205us/step - loss: 5.9692e-04 - val_loss: 0.0011\n",
      "Epoch 14/24\n",
      "3622/3622 [==============================] - 1s 200us/step - loss: 5.8448e-04 - val_loss: 0.0013\n",
      "Epoch 15/24\n",
      "3622/3622 [==============================] - 1s 200us/step - loss: 5.8493e-04 - val_loss: 9.5541e-04\n",
      "Epoch 16/24\n",
      "3622/3622 [==============================] - 1s 200us/step - loss: 5.7156e-04 - val_loss: 0.0012\n",
      "Epoch 17/24\n",
      "3622/3622 [==============================] - 1s 201us/step - loss: 5.6393e-04 - val_loss: 0.0010\n",
      "Epoch 18/24\n",
      "3622/3622 [==============================] - 1s 200us/step - loss: 5.5497e-04 - val_loss: 9.9937e-04\n",
      "Epoch 19/24\n",
      "3622/3622 [==============================] - 1s 198us/step - loss: 5.4270e-04 - val_loss: 0.0011\n",
      "Epoch 20/24\n",
      "3622/3622 [==============================] - 1s 198us/step - loss: 5.3981e-04 - val_loss: 8.4436e-04\n",
      "Epoch 21/24\n",
      "3622/3622 [==============================] - 1s 201us/step - loss: 5.3226e-04 - val_loss: 9.8371e-04\n",
      "Epoch 22/24\n",
      "3622/3622 [==============================] - 1s 199us/step - loss: 5.2113e-04 - val_loss: 8.4601e-04\n",
      "Epoch 23/24\n",
      "3622/3622 [==============================] - 1s 199us/step - loss: 5.1805e-04 - val_loss: 9.5179e-04\n",
      "Epoch 24/24\n",
      "3622/3622 [==============================] - 1s 200us/step - loss: 5.0957e-04 - val_loss: 9.0371e-04\n",
      "Train on 3622 samples, validate on 403 samples\n",
      "Epoch 1/24\n",
      "3622/3622 [==============================] - 5s 1ms/step - loss: 0.2124 - val_loss: 0.0139\n",
      "Epoch 2/24\n",
      "3622/3622 [==============================] - 1s 197us/step - loss: 0.0371 - val_loss: 0.0033\n",
      "Epoch 3/24\n",
      "3622/3622 [==============================] - 1s 198us/step - loss: 0.0123 - val_loss: 0.0049\n",
      "Epoch 4/24\n",
      "3622/3622 [==============================] - 1s 201us/step - loss: 0.0045 - val_loss: 0.0013\n",
      "Epoch 5/24\n",
      "3622/3622 [==============================] - 1s 198us/step - loss: 0.0023 - val_loss: 0.0084\n",
      "Epoch 6/24\n",
      "3622/3622 [==============================] - 1s 197us/step - loss: 0.0016 - val_loss: 6.1373e-04\n",
      "Epoch 7/24\n",
      "3622/3622 [==============================] - 1s 200us/step - loss: 0.0013 - val_loss: 0.0033\n",
      "Epoch 8/24\n",
      "3622/3622 [==============================] - 1s 199us/step - loss: 0.0011 - val_loss: 8.4695e-04\n",
      "Epoch 9/24\n",
      "3622/3622 [==============================] - 1s 202us/step - loss: 8.5346e-04 - val_loss: 0.0018\n",
      "Epoch 10/24\n",
      "3622/3622 [==============================] - 1s 198us/step - loss: 7.6225e-04 - val_loss: 0.0011\n",
      "Epoch 11/24\n",
      "3622/3622 [==============================] - 1s 199us/step - loss: 6.9802e-04 - val_loss: 0.0014\n",
      "Epoch 12/24\n",
      "3622/3622 [==============================] - 1s 201us/step - loss: 6.5776e-04 - val_loss: 0.0010\n",
      "Epoch 13/24\n",
      "3622/3622 [==============================] - 1s 199us/step - loss: 6.2370e-04 - val_loss: 0.0011\n",
      "Epoch 14/24\n",
      "3622/3622 [==============================] - 1s 199us/step - loss: 5.9309e-04 - val_loss: 8.9404e-04\n",
      "Epoch 15/24\n",
      "3622/3622 [==============================] - 1s 198us/step - loss: 5.8453e-04 - val_loss: 0.0012\n",
      "Epoch 16/24\n",
      "3622/3622 [==============================] - 1s 199us/step - loss: 5.6958e-04 - val_loss: 9.1152e-04\n",
      "Epoch 17/24\n",
      "3622/3622 [==============================] - 1s 199us/step - loss: 5.4590e-04 - val_loss: 9.4052e-04\n",
      "Epoch 18/24\n",
      "3622/3622 [==============================] - 1s 202us/step - loss: 5.3127e-04 - val_loss: 8.3762e-04\n",
      "Epoch 19/24\n",
      "3622/3622 [==============================] - 1s 199us/step - loss: 5.2149e-04 - val_loss: 8.5468e-04\n",
      "Epoch 20/24\n",
      "3622/3622 [==============================] - 1s 198us/step - loss: 5.1486e-04 - val_loss: 9.3271e-04\n",
      "Epoch 21/24\n",
      "3622/3622 [==============================] - 1s 200us/step - loss: 5.1860e-04 - val_loss: 6.4056e-04\n",
      "Epoch 22/24\n",
      "3622/3622 [==============================] - 1s 204us/step - loss: 5.0923e-04 - val_loss: 9.4766e-04\n",
      "Epoch 23/24\n",
      "3622/3622 [==============================] - 1s 216us/step - loss: 5.0029e-04 - val_loss: 6.5767e-04\n",
      "Epoch 24/24\n",
      "3622/3622 [==============================] - 1s 217us/step - loss: 4.9023e-04 - val_loss: 6.6836e-04\n",
      "Train on 3622 samples, validate on 403 samples\n",
      "Epoch 1/24\n",
      "3622/3622 [==============================] - 5s 1ms/step - loss: 0.5695 - val_loss: 0.0611\n",
      "Epoch 2/24\n",
      "3622/3622 [==============================] - 1s 200us/step - loss: 0.0587 - val_loss: 0.0061\n",
      "Epoch 3/24\n",
      "3622/3622 [==============================] - 1s 200us/step - loss: 0.0257 - val_loss: 0.0392\n",
      "Epoch 4/24\n",
      "3622/3622 [==============================] - 1s 201us/step - loss: 0.0125 - val_loss: 0.0099\n",
      "Epoch 5/24\n",
      "3622/3622 [==============================] - 1s 199us/step - loss: 0.0060 - val_loss: 0.0136\n",
      "Epoch 6/24\n",
      "3622/3622 [==============================] - 1s 200us/step - loss: 0.0027 - val_loss: 5.5050e-04\n",
      "Epoch 7/24\n",
      "3622/3622 [==============================] - 1s 199us/step - loss: 0.0015 - val_loss: 0.0020\n",
      "Epoch 8/24\n",
      "3622/3622 [==============================] - 1s 201us/step - loss: 0.0010 - val_loss: 0.0028\n",
      "Epoch 9/24\n",
      "3622/3622 [==============================] - 1s 202us/step - loss: 8.2835e-04 - val_loss: 5.5938e-04\n",
      "Epoch 10/24\n",
      "3622/3622 [==============================] - 1s 207us/step - loss: 7.5987e-04 - val_loss: 0.0013\n",
      "Epoch 11/24\n",
      "3622/3622 [==============================] - 1s 202us/step - loss: 6.5811e-04 - val_loss: 0.0015\n",
      "Epoch 12/24\n",
      "3622/3622 [==============================] - 1s 200us/step - loss: 6.2074e-04 - val_loss: 6.5462e-04\n",
      "Epoch 13/24\n",
      "3622/3622 [==============================] - 1s 200us/step - loss: 5.9544e-04 - val_loss: 8.6610e-04\n",
      "Epoch 14/24\n",
      "3622/3622 [==============================] - 1s 201us/step - loss: 5.6438e-04 - val_loss: 9.4390e-04\n",
      "Epoch 15/24\n",
      "3622/3622 [==============================] - 1s 202us/step - loss: 5.5555e-04 - val_loss: 8.1872e-04\n",
      "Epoch 16/24\n",
      "3622/3622 [==============================] - 1s 201us/step - loss: 5.4788e-04 - val_loss: 8.1412e-04\n",
      "Epoch 17/24\n",
      "3622/3622 [==============================] - 1s 200us/step - loss: 5.4159e-04 - val_loss: 9.3777e-04\n",
      "Epoch 18/24\n",
      "3622/3622 [==============================] - 1s 199us/step - loss: 5.3101e-04 - val_loss: 8.5807e-04\n",
      "Epoch 19/24\n",
      "3622/3622 [==============================] - 1s 198us/step - loss: 5.2513e-04 - val_loss: 7.8460e-04\n",
      "Epoch 20/24\n",
      "3622/3622 [==============================] - 1s 201us/step - loss: 5.1981e-04 - val_loss: 6.3180e-04\n",
      "Epoch 21/24\n",
      "3622/3622 [==============================] - 1s 202us/step - loss: 5.0937e-04 - val_loss: 8.2385e-04\n",
      "Epoch 22/24\n",
      "3622/3622 [==============================] - 1s 209us/step - loss: 5.0147e-04 - val_loss: 7.3767e-04\n",
      "Epoch 23/24\n",
      "3622/3622 [==============================] - 1s 202us/step - loss: 4.9427e-04 - val_loss: 7.3568e-04\n",
      "Epoch 24/24\n",
      "3622/3622 [==============================] - 1s 201us/step - loss: 4.9018e-04 - val_loss: 7.4068e-04\n",
      "Train on 3622 samples, validate on 403 samples\n",
      "Epoch 1/24\n",
      "3622/3622 [==============================] - 6s 2ms/step - loss: 1.0699 - val_loss: 0.0027\n",
      "Epoch 2/24\n",
      "3622/3622 [==============================] - 1s 406us/step - loss: 0.0027 - val_loss: 0.0116\n",
      "Epoch 3/24\n",
      "3622/3622 [==============================] - 1s 403us/step - loss: 0.0295 - val_loss: 0.0178\n",
      "Epoch 4/24\n",
      "3622/3622 [==============================] - 1s 404us/step - loss: 0.0030 - val_loss: 0.0034\n",
      "Epoch 5/24\n",
      "3622/3622 [==============================] - 1s 405us/step - loss: 0.0013 - val_loss: 0.0037\n",
      "Epoch 6/24\n",
      "3622/3622 [==============================] - 1s 404us/step - loss: 0.0013 - val_loss: 0.0037\n",
      "Epoch 7/24\n",
      "3622/3622 [==============================] - 1s 405us/step - loss: 0.0023 - val_loss: 0.0163\n",
      "Epoch 8/24\n",
      "3622/3622 [==============================] - 1s 403us/step - loss: 0.0161 - val_loss: 0.0079\n",
      "Epoch 9/24\n",
      "3622/3622 [==============================] - 1s 403us/step - loss: 0.0023 - val_loss: 0.0042\n",
      "Epoch 10/24\n",
      "3622/3622 [==============================] - 1s 404us/step - loss: 0.0016 - val_loss: 0.0028\n",
      "Epoch 11/24\n",
      "3622/3622 [==============================] - 1s 404us/step - loss: 0.0014 - val_loss: 0.0039\n",
      "Epoch 12/24\n",
      "3622/3622 [==============================] - 1s 404us/step - loss: 0.0023 - val_loss: 0.0119\n",
      "Epoch 13/24\n",
      "3622/3622 [==============================] - 1s 411us/step - loss: 0.0056 - val_loss: 0.0126\n",
      "Epoch 14/24\n",
      "3622/3622 [==============================] - 1s 404us/step - loss: 0.0047 - val_loss: 0.0061\n",
      "Epoch 15/24\n",
      "3622/3622 [==============================] - 1s 408us/step - loss: 0.0020 - val_loss: 0.0037\n",
      "Epoch 16/24\n",
      "3622/3622 [==============================] - 1s 405us/step - loss: 0.0014 - val_loss: 0.0035\n",
      "Epoch 17/24\n",
      "3622/3622 [==============================] - 1s 406us/step - loss: 0.0012 - val_loss: 0.0022\n",
      "Epoch 18/24\n",
      "3622/3622 [==============================] - 1s 413us/step - loss: 8.7488e-04 - val_loss: 0.0010\n",
      "Epoch 19/24\n",
      "3622/3622 [==============================] - 2s 426us/step - loss: 0.0012 - val_loss: 0.0014\n",
      "Epoch 20/24\n",
      "3622/3622 [==============================] - 2s 447us/step - loss: 0.0036 - val_loss: 0.0086\n",
      "Epoch 21/24\n",
      "3622/3622 [==============================] - 2s 438us/step - loss: 0.0124 - val_loss: 0.0018\n",
      "Epoch 22/24\n",
      "3622/3622 [==============================] - 2s 419us/step - loss: 0.0016 - val_loss: 0.0010\n",
      "Epoch 23/24\n",
      "3622/3622 [==============================] - 2s 415us/step - loss: 0.0010 - val_loss: 0.0010\n",
      "Epoch 24/24\n",
      "3622/3622 [==============================] - 1s 410us/step - loss: 9.0330e-04 - val_loss: 0.0012\n",
      "Train on 3622 samples, validate on 403 samples\n",
      "Epoch 1/24\n",
      "3622/3622 [==============================] - 7s 2ms/step - loss: 0.2469 - val_loss: 0.0228\n",
      "Epoch 2/24\n",
      "3622/3622 [==============================] - 2s 600us/step - loss: 0.0875 - val_loss: 0.0364\n",
      "Epoch 3/24\n",
      "3622/3622 [==============================] - 2s 603us/step - loss: 0.0574 - val_loss: 0.0030\n",
      "Epoch 4/24\n",
      "3622/3622 [==============================] - 2s 601us/step - loss: 0.0190 - val_loss: 0.0104\n",
      "Epoch 5/24\n",
      "3622/3622 [==============================] - 2s 599us/step - loss: 0.0026 - val_loss: 0.0106\n",
      "Epoch 6/24\n",
      "3622/3622 [==============================] - 2s 605us/step - loss: 0.0023 - val_loss: 0.0030\n",
      "Epoch 7/24\n",
      "3622/3622 [==============================] - 2s 615us/step - loss: 0.0023 - val_loss: 0.0043\n",
      "Epoch 8/24\n",
      "3622/3622 [==============================] - 2s 619us/step - loss: 0.0020 - val_loss: 0.0162\n",
      "Epoch 9/24\n",
      "3622/3622 [==============================] - 2s 597us/step - loss: 0.0170 - val_loss: 0.0224\n",
      "Epoch 10/24\n",
      "3622/3622 [==============================] - 2s 623us/step - loss: 0.0048 - val_loss: 0.0118\n",
      "Epoch 11/24\n",
      "3622/3622 [==============================] - 2s 601us/step - loss: 0.0022 - val_loss: 0.0078\n",
      "Epoch 12/24\n",
      "3622/3622 [==============================] - 2s 620us/step - loss: 0.0019 - val_loss: 0.0063\n",
      "Epoch 13/24\n",
      "3622/3622 [==============================] - 2s 653us/step - loss: 0.0025 - val_loss: 0.0075\n",
      "Epoch 14/24\n",
      "3622/3622 [==============================] - 2s 640us/step - loss: 0.0080 - val_loss: 0.0430\n",
      "Epoch 15/24\n",
      "3622/3622 [==============================] - 2s 603us/step - loss: 0.0144 - val_loss: 0.0186\n",
      "Epoch 16/24\n",
      "3622/3622 [==============================] - 2s 600us/step - loss: 0.0032 - val_loss: 0.0039\n",
      "Epoch 17/24\n",
      "3622/3622 [==============================] - 2s 613us/step - loss: 0.0017 - val_loss: 0.0038\n",
      "Epoch 18/24\n",
      "3622/3622 [==============================] - 2s 645us/step - loss: 0.0017 - val_loss: 0.0015\n",
      "Epoch 19/24\n",
      "3622/3622 [==============================] - 2s 620us/step - loss: 0.0033 - val_loss: 0.0067\n",
      "Epoch 20/24\n",
      "3622/3622 [==============================] - 2s 602us/step - loss: 0.0116 - val_loss: 0.0063\n",
      "Epoch 21/24\n",
      "3622/3622 [==============================] - 2s 603us/step - loss: 0.0050 - val_loss: 0.0038\n",
      "Epoch 22/24\n",
      "3622/3622 [==============================] - 2s 603us/step - loss: 0.0016 - val_loss: 0.0020\n",
      "Epoch 23/24\n",
      "3622/3622 [==============================] - 2s 599us/step - loss: 0.0019 - val_loss: 0.0047\n",
      "Epoch 24/24\n",
      "3622/3622 [==============================] - 2s 599us/step - loss: 0.0067 - val_loss: 0.0022\n",
      "Train on 3622 samples, validate on 403 samples\n",
      "Epoch 1/24\n",
      "3622/3622 [==============================] - 7s 2ms/step - loss: 0.1950 - val_loss: 0.0219\n",
      "Epoch 2/24\n",
      "3622/3622 [==============================] - 1s 373us/step - loss: 0.0705 - val_loss: 0.0378\n",
      "Epoch 3/24\n",
      "3622/3622 [==============================] - 1s 377us/step - loss: 0.0696 - val_loss: 0.0392\n",
      "Epoch 4/24\n",
      "3622/3622 [==============================] - 1s 395us/step - loss: 0.0697 - val_loss: 0.0331\n",
      "Epoch 5/24\n",
      "3622/3622 [==============================] - 1s 388us/step - loss: 0.0696 - val_loss: 0.0381\n",
      "Epoch 6/24\n",
      "3622/3622 [==============================] - 1s 385us/step - loss: 0.0696 - val_loss: 0.0400\n",
      "Epoch 7/24\n",
      "3622/3622 [==============================] - 1s 389us/step - loss: 0.0697 - val_loss: 0.0360\n",
      "Epoch 8/24\n",
      "3622/3622 [==============================] - 1s 382us/step - loss: 0.0697 - val_loss: 0.0322\n",
      "Epoch 9/24\n",
      "3622/3622 [==============================] - 1s 384us/step - loss: 0.0696 - val_loss: 0.0375\n",
      "Epoch 10/24\n",
      "3622/3622 [==============================] - 1s 386us/step - loss: 0.0693 - val_loss: 0.0307\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11/24\n",
      "3622/3622 [==============================] - 1s 388us/step - loss: 0.0673 - val_loss: 0.0385\n",
      "Epoch 12/24\n",
      "3622/3622 [==============================] - 1s 395us/step - loss: 0.0658 - val_loss: 0.0170\n",
      "Epoch 13/24\n",
      "3622/3622 [==============================] - 1s 408us/step - loss: 0.0567 - val_loss: 0.0278\n",
      "Epoch 14/24\n",
      "3622/3622 [==============================] - 1s 409us/step - loss: 0.0699 - val_loss: 0.0476\n",
      "Epoch 15/24\n",
      "3622/3622 [==============================] - 1s 398us/step - loss: 0.0604 - val_loss: 0.0065\n",
      "Epoch 16/24\n",
      "3622/3622 [==============================] - 1s 386us/step - loss: 0.0321 - val_loss: 0.0076\n",
      "Epoch 17/24\n",
      "3622/3622 [==============================] - 1s 403us/step - loss: 0.0209 - val_loss: 0.0165\n",
      "Epoch 18/24\n",
      "3622/3622 [==============================] - 1s 406us/step - loss: 0.0177 - val_loss: 0.0091\n",
      "Epoch 19/24\n",
      "3622/3622 [==============================] - 2s 421us/step - loss: 0.0114 - val_loss: 0.0103\n",
      "Epoch 20/24\n",
      "3622/3622 [==============================] - 1s 404us/step - loss: 0.0075 - val_loss: 0.0122\n",
      "Epoch 21/24\n",
      "3622/3622 [==============================] - 1s 368us/step - loss: 0.0062 - val_loss: 0.0151\n",
      "Epoch 22/24\n",
      "3622/3622 [==============================] - 1s 383us/step - loss: 0.0055 - val_loss: 0.0140\n",
      "Epoch 23/24\n",
      "3622/3622 [==============================] - 1s 371us/step - loss: 0.0048 - val_loss: 0.0120\n",
      "Epoch 24/24\n",
      "3622/3622 [==============================] - 1s 379us/step - loss: 0.0038 - val_loss: 0.0093\n",
      "Train on 3622 samples, validate on 403 samples\n",
      "Epoch 1/24\n",
      "3622/3622 [==============================] - 6s 2ms/step - loss: 6.2954 - val_loss: 0.0020\n",
      "Epoch 2/24\n",
      "3622/3622 [==============================] - 1s 388us/step - loss: 0.0094 - val_loss: 0.0020\n",
      "Epoch 3/24\n",
      "3622/3622 [==============================] - 1s 384us/step - loss: 0.0072 - val_loss: 0.0027\n",
      "Epoch 4/24\n",
      "3622/3622 [==============================] - 1s 379us/step - loss: 0.0061 - val_loss: 0.0023\n",
      "Epoch 5/24\n",
      "3622/3622 [==============================] - 1s 381us/step - loss: 0.0055 - val_loss: 0.0021\n",
      "Epoch 6/24\n",
      "3622/3622 [==============================] - 1s 381us/step - loss: 0.0053 - val_loss: 0.0042\n",
      "Epoch 7/24\n",
      "3622/3622 [==============================] - 1s 382us/step - loss: 0.0050 - val_loss: 0.0021\n",
      "Epoch 8/24\n",
      "3622/3622 [==============================] - 1s 384us/step - loss: 0.0050 - val_loss: 0.0018\n",
      "Epoch 9/24\n",
      "3622/3622 [==============================] - 1s 382us/step - loss: 0.0044 - val_loss: 0.0043\n",
      "Epoch 10/24\n",
      "3622/3622 [==============================] - 1s 386us/step - loss: 0.0044 - val_loss: 0.0059\n",
      "Epoch 11/24\n",
      "3622/3622 [==============================] - 1s 388us/step - loss: 0.0042 - val_loss: 0.0027\n",
      "Epoch 12/24\n",
      "3622/3622 [==============================] - 1s 387us/step - loss: 0.0041 - val_loss: 0.0061\n",
      "Epoch 13/24\n",
      "3622/3622 [==============================] - 1s 387us/step - loss: 0.0041 - val_loss: 0.0022\n",
      "Epoch 14/24\n",
      "3622/3622 [==============================] - 1s 394us/step - loss: 0.0038 - val_loss: 0.0026\n",
      "Epoch 15/24\n",
      "3622/3622 [==============================] - 1s 383us/step - loss: 0.0038 - val_loss: 0.0025\n",
      "Epoch 16/24\n",
      "3622/3622 [==============================] - 1s 384us/step - loss: 0.0038 - val_loss: 0.0021\n",
      "Epoch 17/24\n",
      "3622/3622 [==============================] - 1s 382us/step - loss: 0.0037 - val_loss: 0.0034\n",
      "Epoch 18/24\n",
      "3622/3622 [==============================] - 1s 383us/step - loss: 0.0036 - val_loss: 0.0029\n",
      "Epoch 19/24\n",
      "3622/3622 [==============================] - 1s 382us/step - loss: 0.0036 - val_loss: 0.0027\n",
      "Epoch 20/24\n",
      "3622/3622 [==============================] - 1s 384us/step - loss: 0.0035 - val_loss: 0.0027\n",
      "Epoch 21/24\n",
      "3622/3622 [==============================] - 1s 384us/step - loss: 0.0033 - val_loss: 0.0027\n",
      "Epoch 22/24\n",
      "3622/3622 [==============================] - 1s 384us/step - loss: 0.0035 - val_loss: 0.0039\n",
      "Epoch 23/24\n",
      "3622/3622 [==============================] - 1s 388us/step - loss: 0.0034 - val_loss: 0.0022\n",
      "Epoch 24/24\n",
      "3622/3622 [==============================] - 1s 389us/step - loss: 0.0032 - val_loss: 0.0016\n",
      "\n",
      "GENERATION 6\n",
      "\n",
      "Train on 3622 samples, validate on 403 samples\n",
      "Epoch 1/24\n",
      "3622/3622 [==============================] - 6s 2ms/step - loss: 0.0491 - val_loss: 0.0270\n",
      "Epoch 2/24\n",
      "3622/3622 [==============================] - 1s 207us/step - loss: 0.0100 - val_loss: 0.0078\n",
      "Epoch 3/24\n",
      "3622/3622 [==============================] - 1s 208us/step - loss: 0.0043 - val_loss: 5.8140e-04\n",
      "Epoch 4/24\n",
      "3622/3622 [==============================] - 1s 206us/step - loss: 0.0027 - val_loss: 0.0015\n",
      "Epoch 5/24\n",
      "3622/3622 [==============================] - 1s 206us/step - loss: 0.0022 - val_loss: 0.0013\n",
      "Epoch 6/24\n",
      "3622/3622 [==============================] - 1s 207us/step - loss: 0.0018 - val_loss: 7.5719e-04\n",
      "Epoch 7/24\n",
      "3622/3622 [==============================] - 1s 206us/step - loss: 0.0015 - val_loss: 0.0016\n",
      "Epoch 8/24\n",
      "3622/3622 [==============================] - 1s 207us/step - loss: 0.0015 - val_loss: 7.6416e-04\n",
      "Epoch 9/24\n",
      "3622/3622 [==============================] - 1s 205us/step - loss: 0.0013 - val_loss: 0.0013\n",
      "Epoch 10/24\n",
      "3622/3622 [==============================] - 1s 203us/step - loss: 0.0013 - val_loss: 7.3074e-04\n",
      "Epoch 11/24\n",
      "3622/3622 [==============================] - 1s 205us/step - loss: 0.0013 - val_loss: 8.9716e-04\n",
      "Epoch 12/24\n",
      "3622/3622 [==============================] - 1s 204us/step - loss: 0.0012 - val_loss: 0.0013\n",
      "Epoch 13/24\n",
      "3622/3622 [==============================] - 1s 203us/step - loss: 0.0012 - val_loss: 6.9028e-04\n",
      "Epoch 14/24\n",
      "3622/3622 [==============================] - 1s 205us/step - loss: 0.0012 - val_loss: 5.7164e-04\n",
      "Epoch 15/24\n",
      "3622/3622 [==============================] - 1s 207us/step - loss: 0.0012 - val_loss: 6.2259e-04\n",
      "Epoch 16/24\n",
      "3622/3622 [==============================] - 1s 205us/step - loss: 0.0012 - val_loss: 0.0010\n",
      "Epoch 17/24\n",
      "3622/3622 [==============================] - 1s 204us/step - loss: 0.0011 - val_loss: 0.0011\n",
      "Epoch 18/24\n",
      "3622/3622 [==============================] - 1s 205us/step - loss: 0.0011 - val_loss: 5.0217e-04\n",
      "Epoch 19/24\n",
      "3622/3622 [==============================] - 1s 204us/step - loss: 0.0011 - val_loss: 5.1430e-04\n",
      "Epoch 20/24\n",
      "3622/3622 [==============================] - 1s 204us/step - loss: 0.0011 - val_loss: 7.2798e-04\n",
      "Epoch 21/24\n",
      "3622/3622 [==============================] - 1s 203us/step - loss: 9.6463e-04 - val_loss: 7.8906e-04\n",
      "Epoch 22/24\n",
      "3622/3622 [==============================] - 1s 204us/step - loss: 9.6396e-04 - val_loss: 7.3104e-04\n",
      "Epoch 23/24\n",
      "3622/3622 [==============================] - 1s 207us/step - loss: 9.4679e-04 - val_loss: 5.8628e-04\n",
      "Epoch 24/24\n",
      "3622/3622 [==============================] - 1s 206us/step - loss: 9.6862e-04 - val_loss: 4.8040e-04\n",
      "Train on 3622 samples, validate on 403 samples\n",
      "Epoch 1/24\n",
      "3622/3622 [==============================] - 6s 2ms/step - loss: 0.1737 - val_loss: 0.0024\n",
      "Epoch 2/24\n",
      "3622/3622 [==============================] - 1s 205us/step - loss: 0.0206 - val_loss: 0.0175\n",
      "Epoch 3/24\n",
      "3622/3622 [==============================] - 1s 207us/step - loss: 0.0072 - val_loss: 0.0046\n",
      "Epoch 4/24\n",
      "3622/3622 [==============================] - 1s 206us/step - loss: 0.0026 - val_loss: 0.0017\n",
      "Epoch 5/24\n",
      "3622/3622 [==============================] - 1s 203us/step - loss: 0.0014 - val_loss: 0.0011\n",
      "Epoch 6/24\n",
      "3622/3622 [==============================] - 1s 208us/step - loss: 0.0010 - val_loss: 0.0011\n",
      "Epoch 7/24\n",
      "3622/3622 [==============================] - 1s 206us/step - loss: 8.9337e-04 - val_loss: 8.6923e-04\n",
      "Epoch 8/24\n",
      "3622/3622 [==============================] - 1s 208us/step - loss: 8.1151e-04 - val_loss: 8.2803e-04\n",
      "Epoch 9/24\n",
      "3622/3622 [==============================] - 1s 204us/step - loss: 7.7324e-04 - val_loss: 9.0288e-04\n",
      "Epoch 10/24\n",
      "3622/3622 [==============================] - 1s 203us/step - loss: 7.3452e-04 - val_loss: 9.3731e-04\n",
      "Epoch 11/24\n",
      "3622/3622 [==============================] - 1s 205us/step - loss: 7.2009e-04 - val_loss: 7.4123e-04\n",
      "Epoch 12/24\n",
      "3622/3622 [==============================] - 1s 204us/step - loss: 6.6929e-04 - val_loss: 7.1110e-04\n",
      "Epoch 13/24\n",
      "3622/3622 [==============================] - 1s 213us/step - loss: 6.4721e-04 - val_loss: 6.7549e-04\n",
      "Epoch 14/24\n",
      "3622/3622 [==============================] - 1s 209us/step - loss: 6.1354e-04 - val_loss: 0.0013\n",
      "Epoch 15/24\n",
      "3622/3622 [==============================] - 1s 219us/step - loss: 7.4406e-04 - val_loss: 6.6371e-04\n",
      "Epoch 16/24\n",
      "3622/3622 [==============================] - 1s 213us/step - loss: 6.6052e-04 - val_loss: 6.3100e-04\n",
      "Epoch 17/24\n",
      "3622/3622 [==============================] - 1s 212us/step - loss: 6.0806e-04 - val_loss: 6.5926e-04\n",
      "Epoch 18/24\n",
      "3622/3622 [==============================] - 1s 204us/step - loss: 5.4864e-04 - val_loss: 5.5364e-04\n",
      "Epoch 19/24\n",
      "3622/3622 [==============================] - 1s 202us/step - loss: 4.9901e-04 - val_loss: 5.7461e-04\n",
      "Epoch 20/24\n",
      "3622/3622 [==============================] - 1s 205us/step - loss: 4.6155e-04 - val_loss: 4.7911e-04\n",
      "Epoch 21/24\n",
      "3622/3622 [==============================] - 1s 208us/step - loss: 4.3222e-04 - val_loss: 4.5250e-04\n",
      "Epoch 22/24\n",
      "3622/3622 [==============================] - 1s 208us/step - loss: 4.0798e-04 - val_loss: 5.6330e-04\n",
      "Epoch 23/24\n",
      "3622/3622 [==============================] - 1s 202us/step - loss: 4.1528e-04 - val_loss: 4.2275e-04\n",
      "Epoch 24/24\n",
      "3622/3622 [==============================] - 1s 204us/step - loss: 3.8496e-04 - val_loss: 4.1396e-04\n",
      "Train on 3622 samples, validate on 403 samples\n",
      "Epoch 1/24\n",
      "3622/3622 [==============================] - 6s 2ms/step - loss: 0.1235 - val_loss: 0.0052\n",
      "Epoch 2/24\n",
      "3622/3622 [==============================] - 1s 251us/step - loss: 0.0129 - val_loss: 0.0041\n",
      "Epoch 3/24\n",
      "3622/3622 [==============================] - 1s 252us/step - loss: 0.0072 - val_loss: 0.0019\n",
      "Epoch 4/24\n",
      "3622/3622 [==============================] - 1s 252us/step - loss: 0.0044 - val_loss: 7.9211e-04\n",
      "Epoch 5/24\n",
      "3622/3622 [==============================] - 1s 250us/step - loss: 0.0029 - val_loss: 0.0024\n",
      "Epoch 6/24\n",
      "3622/3622 [==============================] - 1s 250us/step - loss: 0.0024 - val_loss: 8.4282e-04\n",
      "Epoch 7/24\n",
      "3622/3622 [==============================] - 1s 254us/step - loss: 0.0021 - val_loss: 0.0015\n",
      "Epoch 8/24\n",
      "3622/3622 [==============================] - 1s 251us/step - loss: 0.0020 - val_loss: 0.0018\n",
      "Epoch 9/24\n",
      "3622/3622 [==============================] - 1s 251us/step - loss: 0.0019 - val_loss: 0.0012\n",
      "Epoch 10/24\n",
      "3622/3622 [==============================] - 1s 250us/step - loss: 0.0017 - val_loss: 0.0013\n",
      "Epoch 11/24\n",
      "3622/3622 [==============================] - 1s 250us/step - loss: 0.0016 - val_loss: 0.0012\n",
      "Epoch 12/24\n",
      "3622/3622 [==============================] - 1s 250us/step - loss: 0.0015 - val_loss: 0.0023\n",
      "Epoch 13/24\n",
      "3622/3622 [==============================] - 1s 249us/step - loss: 0.0015 - val_loss: 0.0017\n",
      "Epoch 14/24\n",
      "3622/3622 [==============================] - 1s 251us/step - loss: 0.0014 - val_loss: 0.0016\n",
      "Epoch 15/24\n",
      "3622/3622 [==============================] - 1s 253us/step - loss: 0.0013 - val_loss: 0.0014\n",
      "Epoch 16/24\n",
      "3622/3622 [==============================] - 1s 251us/step - loss: 0.0013 - val_loss: 0.0018\n",
      "Epoch 17/24\n",
      "3622/3622 [==============================] - 1s 251us/step - loss: 0.0012 - val_loss: 0.0012\n",
      "Epoch 18/24\n",
      "3622/3622 [==============================] - 1s 254us/step - loss: 0.0012 - val_loss: 0.0018\n",
      "Epoch 19/24\n",
      "3622/3622 [==============================] - 1s 251us/step - loss: 0.0012 - val_loss: 0.0017\n",
      "Epoch 20/24\n",
      "3622/3622 [==============================] - 1s 251us/step - loss: 0.0011 - val_loss: 0.0021\n",
      "Epoch 21/24\n",
      "3622/3622 [==============================] - 1s 251us/step - loss: 0.0011 - val_loss: 0.0018\n",
      "Epoch 22/24\n",
      "3622/3622 [==============================] - 1s 273us/step - loss: 0.0010 - val_loss: 0.0026\n",
      "Epoch 23/24\n",
      "3622/3622 [==============================] - 1s 255us/step - loss: 0.0010 - val_loss: 0.0023\n",
      "Epoch 24/24\n",
      "3622/3622 [==============================] - 1s 270us/step - loss: 9.4844e-04 - val_loss: 0.0026\n",
      "Train on 3622 samples, validate on 403 samples\n",
      "Epoch 1/24\n",
      "3622/3622 [==============================] - 7s 2ms/step - loss: 0.0742 - val_loss: 0.0212\n",
      "Epoch 2/24\n",
      "3622/3622 [==============================] - 2s 560us/step - loss: 0.0101 - val_loss: 0.0069\n",
      "Epoch 3/24\n",
      "3622/3622 [==============================] - 2s 550us/step - loss: 0.0059 - val_loss: 0.0047\n",
      "Epoch 4/24\n",
      "3622/3622 [==============================] - 2s 617us/step - loss: 0.0040 - val_loss: 0.0012\n",
      "Epoch 5/24\n",
      "3622/3622 [==============================] - 2s 630us/step - loss: 0.0031 - val_loss: 0.0024\n",
      "Epoch 6/24\n",
      "3622/3622 [==============================] - 2s 624us/step - loss: 0.0028 - val_loss: 0.0012\n",
      "Epoch 7/24\n",
      "3622/3622 [==============================] - 2s 626us/step - loss: 0.0026 - val_loss: 0.0013\n",
      "Epoch 8/24\n",
      "3622/3622 [==============================] - 2s 622us/step - loss: 0.0025 - val_loss: 0.0011\n",
      "Epoch 9/24\n",
      "3622/3622 [==============================] - 2s 628us/step - loss: 0.0024 - val_loss: 0.0023\n",
      "Epoch 10/24\n",
      "3622/3622 [==============================] - 2s 631us/step - loss: 0.0023 - val_loss: 0.0016\n",
      "Epoch 11/24\n",
      "3622/3622 [==============================] - 2s 627us/step - loss: 0.0022 - val_loss: 0.0011\n",
      "Epoch 12/24\n",
      "3622/3622 [==============================] - 2s 621us/step - loss: 0.0021 - val_loss: 0.0010\n",
      "Epoch 13/24\n",
      "3622/3622 [==============================] - 2s 619us/step - loss: 0.0021 - val_loss: 0.0013\n",
      "Epoch 14/24\n",
      "3622/3622 [==============================] - 2s 619us/step - loss: 0.0020 - val_loss: 0.0019\n",
      "Epoch 15/24\n",
      "3622/3622 [==============================] - 2s 619us/step - loss: 0.0019 - val_loss: 0.0011\n",
      "Epoch 16/24\n",
      "3622/3622 [==============================] - 2s 625us/step - loss: 0.0019 - val_loss: 0.0011\n",
      "Epoch 17/24\n",
      "3622/3622 [==============================] - 2s 619us/step - loss: 0.0019 - val_loss: 0.0010\n",
      "Epoch 18/24\n",
      "3622/3622 [==============================] - 2s 621us/step - loss: 0.0018 - val_loss: 0.0011\n",
      "Epoch 19/24\n",
      "3622/3622 [==============================] - 2s 582us/step - loss: 0.0019 - val_loss: 0.0011\n",
      "Epoch 20/24\n",
      "3622/3622 [==============================] - 2s 607us/step - loss: 0.0017 - val_loss: 0.0016\n",
      "Epoch 21/24\n",
      "3622/3622 [==============================] - 2s 593us/step - loss: 0.0018 - val_loss: 0.0017\n",
      "Epoch 22/24\n",
      "3622/3622 [==============================] - 2s 589us/step - loss: 0.0017 - val_loss: 0.0024\n",
      "Epoch 23/24\n",
      "3622/3622 [==============================] - 2s 593us/step - loss: 0.0017 - val_loss: 0.0015\n",
      "Epoch 24/24\n",
      "3622/3622 [==============================] - 2s 588us/step - loss: 0.0017 - val_loss: 0.0014\n",
      "Train on 3622 samples, validate on 403 samples\n",
      "Epoch 1/24\n",
      "3622/3622 [==============================] - 8s 2ms/step - loss: 0.1137 - val_loss: 0.0132\n",
      "Epoch 2/24\n",
      "3622/3622 [==============================] - 2s 582us/step - loss: 0.0232 - val_loss: 0.0063\n",
      "Epoch 3/24\n",
      "3622/3622 [==============================] - 2s 587us/step - loss: 0.0100 - val_loss: 0.0080\n",
      "Epoch 4/24\n",
      "3622/3622 [==============================] - 2s 586us/step - loss: 0.0055 - val_loss: 0.0045\n",
      "Epoch 5/24\n",
      "3622/3622 [==============================] - 2s 584us/step - loss: 0.0037 - val_loss: 0.0015\n",
      "Epoch 6/24\n",
      "3622/3622 [==============================] - 2s 587us/step - loss: 0.0029 - val_loss: 0.0018\n",
      "Epoch 7/24\n",
      "3622/3622 [==============================] - 2s 594us/step - loss: 0.0030 - val_loss: 0.0016\n",
      "Epoch 8/24\n",
      "3622/3622 [==============================] - 2s 583us/step - loss: 0.0027 - val_loss: 0.0013\n",
      "Epoch 9/24\n",
      "3622/3622 [==============================] - 2s 583us/step - loss: 0.0026 - val_loss: 0.0022\n",
      "Epoch 10/24\n",
      "3622/3622 [==============================] - 2s 581us/step - loss: 0.0026 - val_loss: 0.0015\n",
      "Epoch 11/24\n",
      "3622/3622 [==============================] - 2s 582us/step - loss: 0.0023 - val_loss: 0.0015\n",
      "Epoch 12/24\n",
      "3622/3622 [==============================] - 2s 588us/step - loss: 0.0023 - val_loss: 0.0018\n",
      "Epoch 13/24\n",
      "3622/3622 [==============================] - 2s 584us/step - loss: 0.0022 - val_loss: 0.0015\n",
      "Epoch 14/24\n",
      "3622/3622 [==============================] - 2s 584us/step - loss: 0.0021 - val_loss: 0.0013\n",
      "Epoch 15/24\n",
      "3622/3622 [==============================] - 2s 587us/step - loss: 0.0022 - val_loss: 0.0013\n",
      "Epoch 16/24\n",
      "3622/3622 [==============================] - 2s 595us/step - loss: 0.0021 - val_loss: 0.0013\n",
      "Epoch 17/24\n",
      "3622/3622 [==============================] - 2s 618us/step - loss: 0.0024 - val_loss: 0.0013\n",
      "Epoch 18/24\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3622/3622 [==============================] - 2s 620us/step - loss: 0.0022 - val_loss: 0.0017\n",
      "Epoch 19/24\n",
      "3622/3622 [==============================] - 2s 591us/step - loss: 0.0020 - val_loss: 0.0018\n",
      "Epoch 20/24\n",
      "3622/3622 [==============================] - 2s 556us/step - loss: 0.0020 - val_loss: 0.0021\n",
      "Epoch 21/24\n",
      "3622/3622 [==============================] - 2s 561us/step - loss: 0.0020 - val_loss: 0.0033\n",
      "Epoch 22/24\n",
      "3622/3622 [==============================] - 2s 593us/step - loss: 0.0020 - val_loss: 0.0033\n",
      "Epoch 23/24\n",
      "3622/3622 [==============================] - 2s 599us/step - loss: 0.0020 - val_loss: 0.0027\n",
      "Epoch 24/24\n",
      "3622/3622 [==============================] - 2s 586us/step - loss: 0.0018 - val_loss: 0.0024\n",
      "Train on 3622 samples, validate on 403 samples\n",
      "Epoch 1/24\n",
      "3622/3622 [==============================] - 8s 2ms/step - loss: 0.6988 - val_loss: 0.8588\n",
      "Epoch 2/24\n",
      "3622/3622 [==============================] - 2s 550us/step - loss: 0.2757 - val_loss: 0.0126\n",
      "Epoch 3/24\n",
      "3622/3622 [==============================] - 2s 553us/step - loss: 0.0733 - val_loss: 0.0504\n",
      "Epoch 4/24\n",
      "3622/3622 [==============================] - 2s 563us/step - loss: 0.0600 - val_loss: 0.1008\n",
      "Epoch 5/24\n",
      "3622/3622 [==============================] - 2s 564us/step - loss: 0.0285 - val_loss: 0.0237\n",
      "Epoch 6/24\n",
      "3622/3622 [==============================] - 2s 560us/step - loss: 0.0078 - val_loss: 0.0055\n",
      "Epoch 7/24\n",
      "3622/3622 [==============================] - 2s 551us/step - loss: 0.0049 - val_loss: 0.0044\n",
      "Epoch 8/24\n",
      "3622/3622 [==============================] - 2s 551us/step - loss: 0.0041 - val_loss: 0.0031\n",
      "Epoch 9/24\n",
      "3622/3622 [==============================] - 2s 551us/step - loss: 0.0040 - val_loss: 0.0069\n",
      "Epoch 10/24\n",
      "3622/3622 [==============================] - 2s 566us/step - loss: 0.0078 - val_loss: 0.0296\n",
      "Epoch 11/24\n",
      "3622/3622 [==============================] - 2s 569us/step - loss: 0.0053 - val_loss: 0.0021\n",
      "Epoch 12/24\n",
      "3622/3622 [==============================] - 2s 599us/step - loss: 0.0035 - val_loss: 0.0028\n",
      "Epoch 13/24\n",
      "3622/3622 [==============================] - 2s 609us/step - loss: 0.0034 - val_loss: 0.0036\n",
      "Epoch 14/24\n",
      "3622/3622 [==============================] - 2s 615us/step - loss: 0.0044 - val_loss: 0.0081\n",
      "Epoch 15/24\n",
      "3622/3622 [==============================] - 2s 578us/step - loss: 0.0040 - val_loss: 0.0079\n",
      "Epoch 16/24\n",
      "3622/3622 [==============================] - 2s 570us/step - loss: 0.0042 - val_loss: 0.0031\n",
      "Epoch 17/24\n",
      "3622/3622 [==============================] - 2s 604us/step - loss: 0.0032 - val_loss: 0.0125\n",
      "Epoch 18/24\n",
      "3622/3622 [==============================] - 2s 581us/step - loss: 0.0048 - val_loss: 0.0014\n",
      "Epoch 19/24\n",
      "3622/3622 [==============================] - 2s 592us/step - loss: 0.0030 - val_loss: 0.0015\n",
      "Epoch 20/24\n",
      "3622/3622 [==============================] - 2s 585us/step - loss: 0.0031 - val_loss: 0.0025\n",
      "Epoch 21/24\n",
      "3622/3622 [==============================] - 2s 581us/step - loss: 0.0031 - val_loss: 0.0014\n",
      "Epoch 22/24\n",
      "3622/3622 [==============================] - 2s 586us/step - loss: 0.0030 - val_loss: 0.0031\n",
      "Epoch 23/24\n",
      "3622/3622 [==============================] - 2s 582us/step - loss: 0.0029 - val_loss: 0.0021\n",
      "Epoch 24/24\n",
      "3622/3622 [==============================] - 2s 578us/step - loss: 0.0032 - val_loss: 0.0013\n",
      "Train on 3622 samples, validate on 403 samples\n",
      "Epoch 1/24\n",
      "3622/3622 [==============================] - 8s 2ms/step - loss: 0.1172 - val_loss: 0.0050\n",
      "Epoch 2/24\n",
      "3622/3622 [==============================] - 2s 580us/step - loss: 0.0070 - val_loss: 0.0667\n",
      "Epoch 3/24\n",
      "3622/3622 [==============================] - 2s 560us/step - loss: 0.0097 - val_loss: 0.0019\n",
      "Epoch 4/24\n",
      "3622/3622 [==============================] - 2s 561us/step - loss: 0.0033 - val_loss: 0.0140\n",
      "Epoch 5/24\n",
      "3622/3622 [==============================] - 2s 565us/step - loss: 0.0075 - val_loss: 0.0033\n",
      "Epoch 6/24\n",
      "3622/3622 [==============================] - 2s 555us/step - loss: 0.0037 - val_loss: 0.0026\n",
      "Epoch 7/24\n",
      "3622/3622 [==============================] - 2s 556us/step - loss: 0.0043 - val_loss: 0.0050\n",
      "Epoch 8/24\n",
      "3622/3622 [==============================] - 2s 597us/step - loss: 0.0047 - val_loss: 0.0029\n",
      "Epoch 9/24\n",
      "3622/3622 [==============================] - 2s 627us/step - loss: 0.0035 - val_loss: 0.0062\n",
      "Epoch 10/24\n",
      "3622/3622 [==============================] - 2s 605us/step - loss: 0.0044 - val_loss: 0.0014\n",
      "Epoch 11/24\n",
      "3622/3622 [==============================] - 2s 589us/step - loss: 0.0018 - val_loss: 0.0018\n",
      "Epoch 12/24\n",
      "3622/3622 [==============================] - 2s 587us/step - loss: 0.0041 - val_loss: 0.0093\n",
      "Epoch 13/24\n",
      "3622/3622 [==============================] - 2s 591us/step - loss: 0.0058 - val_loss: 0.0027\n",
      "Epoch 14/24\n",
      "3622/3622 [==============================] - 2s 592us/step - loss: 0.0022 - val_loss: 0.0013\n",
      "Epoch 15/24\n",
      "3622/3622 [==============================] - 2s 588us/step - loss: 0.0019 - val_loss: 0.0018\n",
      "Epoch 16/24\n",
      "3622/3622 [==============================] - 2s 592us/step - loss: 0.0029 - val_loss: 0.0086\n",
      "Epoch 17/24\n",
      "3622/3622 [==============================] - 2s 584us/step - loss: 0.0060 - val_loss: 0.0013\n",
      "Epoch 18/24\n",
      "3622/3622 [==============================] - 2s 587us/step - loss: 0.0019 - val_loss: 0.0016\n",
      "Epoch 19/24\n",
      "3622/3622 [==============================] - 2s 588us/step - loss: 0.0023 - val_loss: 0.0048\n",
      "Epoch 20/24\n",
      "3622/3622 [==============================] - 2s 585us/step - loss: 0.0042 - val_loss: 0.0074\n",
      "Epoch 21/24\n",
      "3622/3622 [==============================] - 2s 588us/step - loss: 0.0032 - val_loss: 0.0011\n",
      "Epoch 22/24\n",
      "3622/3622 [==============================] - 2s 585us/step - loss: 0.0018 - val_loss: 0.0011\n",
      "Epoch 23/24\n",
      "3622/3622 [==============================] - 2s 589us/step - loss: 0.0018 - val_loss: 0.0016\n",
      "Epoch 24/24\n",
      "3622/3622 [==============================] - 2s 588us/step - loss: 0.0024 - val_loss: 0.0027\n",
      "Train on 3622 samples, validate on 403 samples\n",
      "Epoch 1/24\n",
      "3622/3622 [==============================] - 9s 2ms/step - loss: 3.3890 - val_loss: 0.0084\n",
      "Epoch 2/24\n",
      "3622/3622 [==============================] - 2s 664us/step - loss: 0.0741 - val_loss: 0.0721\n",
      "Epoch 3/24\n",
      "3622/3622 [==============================] - 3s 695us/step - loss: 0.0776 - val_loss: 0.0375\n",
      "Epoch 4/24\n",
      "3622/3622 [==============================] - 2s 660us/step - loss: 0.0759 - val_loss: 0.1341\n",
      "Epoch 5/24\n",
      "3622/3622 [==============================] - 2s 659us/step - loss: 0.0757 - val_loss: 0.0988\n",
      "Epoch 6/24\n",
      "3622/3622 [==============================] - 2s 660us/step - loss: 0.0773 - val_loss: 0.1526\n",
      "Epoch 7/24\n",
      "3622/3622 [==============================] - 2s 666us/step - loss: 0.0873 - val_loss: 0.0383\n",
      "Epoch 8/24\n",
      "3622/3622 [==============================] - 2s 654us/step - loss: 0.0612 - val_loss: 0.0210\n",
      "Epoch 9/24\n",
      "3622/3622 [==============================] - 2s 647us/step - loss: 0.0552 - val_loss: 0.0047\n",
      "Epoch 10/24\n",
      "3622/3622 [==============================] - 2s 659us/step - loss: 0.0673 - val_loss: 0.0030\n",
      "Epoch 11/24\n",
      "3622/3622 [==============================] - 2s 669us/step - loss: 0.0090 - val_loss: 0.0187\n",
      "Epoch 12/24\n",
      "3622/3622 [==============================] - 2s 665us/step - loss: 0.0129 - val_loss: 0.0084\n",
      "Epoch 13/24\n",
      "3622/3622 [==============================] - 2s 659us/step - loss: 0.0049 - val_loss: 0.0142\n",
      "Epoch 14/24\n",
      "3622/3622 [==============================] - 2s 666us/step - loss: 0.0111 - val_loss: 0.0100\n",
      "Epoch 15/24\n",
      "3622/3622 [==============================] - 2s 595us/step - loss: 0.0077 - val_loss: 0.0306\n",
      "Epoch 16/24\n",
      "3622/3622 [==============================] - 2s 565us/step - loss: 0.0127 - val_loss: 0.0230\n",
      "Epoch 17/24\n",
      "3622/3622 [==============================] - 2s 552us/step - loss: 0.0065 - val_loss: 0.0247\n",
      "Epoch 18/24\n",
      "3622/3622 [==============================] - 2s 557us/step - loss: 0.0088 - val_loss: 0.0215\n",
      "Epoch 19/24\n",
      "3622/3622 [==============================] - 2s 555us/step - loss: 0.0118 - val_loss: 0.0165\n",
      "Epoch 20/24\n",
      "3622/3622 [==============================] - 2s 552us/step - loss: 0.0048 - val_loss: 0.0105\n",
      "Epoch 21/24\n",
      "3622/3622 [==============================] - 2s 552us/step - loss: 0.0080 - val_loss: 0.0135\n",
      "Epoch 22/24\n",
      "3622/3622 [==============================] - 2s 553us/step - loss: 0.0086 - val_loss: 0.0216\n",
      "Epoch 23/24\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3622/3622 [==============================] - 2s 555us/step - loss: 0.0092 - val_loss: 0.0115\n",
      "Epoch 24/24\n",
      "3622/3622 [==============================] - 2s 556us/step - loss: 0.0066 - val_loss: 0.0250\n",
      "Train on 3622 samples, validate on 403 samples\n",
      "Epoch 1/24\n",
      "3622/3622 [==============================] - 8s 2ms/step - loss: 0.0524 - val_loss: 0.0013\n",
      "Epoch 2/24\n",
      "3622/3622 [==============================] - 2s 485us/step - loss: 0.0033 - val_loss: 0.0362\n",
      "Epoch 3/24\n",
      "3622/3622 [==============================] - 2s 483us/step - loss: 0.0103 - val_loss: 9.8509e-04\n",
      "Epoch 4/24\n",
      "3622/3622 [==============================] - 2s 481us/step - loss: 0.0016 - val_loss: 0.0021\n",
      "Epoch 5/24\n",
      "3622/3622 [==============================] - 2s 483us/step - loss: 0.0028 - val_loss: 0.0030\n",
      "Epoch 6/24\n",
      "3622/3622 [==============================] - 2s 483us/step - loss: 0.0032 - val_loss: 0.0237\n",
      "Epoch 7/24\n",
      "3622/3622 [==============================] - 2s 481us/step - loss: 0.0101 - val_loss: 0.0068\n",
      "Epoch 8/24\n",
      "3622/3622 [==============================] - 2s 484us/step - loss: 0.0041 - val_loss: 0.0010\n",
      "Epoch 9/24\n",
      "3622/3622 [==============================] - 2s 486us/step - loss: 0.0014 - val_loss: 0.0010\n",
      "Epoch 10/24\n",
      "3622/3622 [==============================] - 2s 484us/step - loss: 0.0016 - val_loss: 0.0018\n",
      "Epoch 11/24\n",
      "3622/3622 [==============================] - 2s 485us/step - loss: 0.0054 - val_loss: 0.0115\n",
      "Epoch 12/24\n",
      "3622/3622 [==============================] - 2s 483us/step - loss: 0.0062 - val_loss: 0.0022\n",
      "Epoch 13/24\n",
      "3622/3622 [==============================] - 2s 480us/step - loss: 0.0029 - val_loss: 0.0014\n",
      "Epoch 14/24\n",
      "3622/3622 [==============================] - 2s 483us/step - loss: 0.0023 - val_loss: 0.0040\n",
      "Epoch 15/24\n",
      "3622/3622 [==============================] - 2s 482us/step - loss: 0.0036 - val_loss: 0.0037\n",
      "Epoch 16/24\n",
      "3622/3622 [==============================] - 2s 483us/step - loss: 0.0035 - val_loss: 0.0073\n",
      "Epoch 17/24\n",
      "3622/3622 [==============================] - 2s 508us/step - loss: 0.0034 - val_loss: 7.4392e-04\n",
      "Epoch 18/24\n",
      "3622/3622 [==============================] - 2s 481us/step - loss: 0.0016 - val_loss: 9.5584e-04\n",
      "Epoch 19/24\n",
      "3622/3622 [==============================] - 2s 489us/step - loss: 0.0016 - val_loss: 9.9228e-04\n",
      "Epoch 20/24\n",
      "3622/3622 [==============================] - 2s 482us/step - loss: 0.0015 - val_loss: 0.0017\n",
      "Epoch 21/24\n",
      "3622/3622 [==============================] - 2s 485us/step - loss: 0.0039 - val_loss: 0.0020\n",
      "Epoch 22/24\n",
      "3622/3622 [==============================] - 2s 481us/step - loss: 0.0025 - val_loss: 0.0014\n",
      "Epoch 23/24\n",
      "3622/3622 [==============================] - 2s 484us/step - loss: 0.0031 - val_loss: 0.0024\n",
      "Epoch 24/24\n",
      "3622/3622 [==============================] - 2s 483us/step - loss: 0.0023 - val_loss: 6.8799e-04\n",
      "\n",
      "GENERATION 7\n",
      "\n",
      "Train on 3622 samples, validate on 403 samples\n",
      "Epoch 1/24\n",
      "3622/3622 [==============================] - 8s 2ms/step - loss: 0.1430 - val_loss: 0.0101\n",
      "Epoch 2/24\n",
      "3622/3622 [==============================] - 2s 434us/step - loss: 0.0174 - val_loss: 0.0200\n",
      "Epoch 3/24\n",
      "3622/3622 [==============================] - 2s 432us/step - loss: 0.0099 - val_loss: 0.0084\n",
      "Epoch 4/24\n",
      "3622/3622 [==============================] - 2s 435us/step - loss: 0.0048 - val_loss: 0.0010\n",
      "Epoch 5/24\n",
      "3622/3622 [==============================] - 2s 436us/step - loss: 0.0033 - val_loss: 0.0010\n",
      "Epoch 6/24\n",
      "3622/3622 [==============================] - 2s 444us/step - loss: 0.0027 - val_loss: 0.0015\n",
      "Epoch 7/24\n",
      "3622/3622 [==============================] - 2s 435us/step - loss: 0.0023 - val_loss: 0.0010\n",
      "Epoch 8/24\n",
      "3622/3622 [==============================] - 2s 436us/step - loss: 0.0023 - val_loss: 9.7056e-04\n",
      "Epoch 9/24\n",
      "3622/3622 [==============================] - 2s 434us/step - loss: 0.0021 - val_loss: 9.9084e-04\n",
      "Epoch 10/24\n",
      "3622/3622 [==============================] - 2s 442us/step - loss: 0.0021 - val_loss: 0.0012\n",
      "Epoch 11/24\n",
      "3622/3622 [==============================] - 2s 436us/step - loss: 0.0020 - val_loss: 9.9063e-04\n",
      "Epoch 12/24\n",
      "3622/3622 [==============================] - 2s 438us/step - loss: 0.0019 - val_loss: 0.0010\n",
      "Epoch 13/24\n",
      "3622/3622 [==============================] - 2s 435us/step - loss: 0.0019 - val_loss: 9.6907e-04\n",
      "Epoch 14/24\n",
      "3622/3622 [==============================] - 2s 432us/step - loss: 0.0018 - val_loss: 0.0010\n",
      "Epoch 15/24\n",
      "3622/3622 [==============================] - 2s 436us/step - loss: 0.0017 - val_loss: 0.0010\n",
      "Epoch 16/24\n",
      "3622/3622 [==============================] - 2s 438us/step - loss: 0.0017 - val_loss: 9.9686e-04\n",
      "Epoch 17/24\n",
      "3622/3622 [==============================] - 2s 434us/step - loss: 0.0017 - val_loss: 9.6748e-04\n",
      "Epoch 18/24\n",
      "3622/3622 [==============================] - 2s 434us/step - loss: 0.0017 - val_loss: 9.6301e-04\n",
      "Epoch 19/24\n",
      "3622/3622 [==============================] - 2s 436us/step - loss: 0.0017 - val_loss: 9.1097e-04\n",
      "Epoch 20/24\n",
      "3622/3622 [==============================] - 2s 436us/step - loss: 0.0016 - val_loss: 9.5732e-04\n",
      "Epoch 21/24\n",
      "3622/3622 [==============================] - 2s 441us/step - loss: 0.0016 - val_loss: 8.8790e-04\n",
      "Epoch 22/24\n",
      "3622/3622 [==============================] - 2s 432us/step - loss: 0.0016 - val_loss: 8.9664e-04\n",
      "Epoch 23/24\n",
      "3622/3622 [==============================] - 2s 432us/step - loss: 0.0015 - val_loss: 8.8416e-04\n",
      "Epoch 24/24\n",
      "3622/3622 [==============================] - 2s 436us/step - loss: 0.0016 - val_loss: 0.0011\n",
      "Train on 3622 samples, validate on 403 samples\n",
      "Epoch 1/24\n",
      "3622/3622 [==============================] - 7s 2ms/step - loss: 0.1386 - val_loss: 0.0547\n",
      "Epoch 2/24\n",
      "3622/3622 [==============================] - 1s 206us/step - loss: 0.0156 - val_loss: 0.0138\n",
      "Epoch 3/24\n",
      "3622/3622 [==============================] - 1s 205us/step - loss: 0.0055 - val_loss: 0.0065\n",
      "Epoch 4/24\n",
      "3622/3622 [==============================] - 1s 208us/step - loss: 0.0025 - val_loss: 0.0020\n",
      "Epoch 5/24\n",
      "3622/3622 [==============================] - 1s 209us/step - loss: 0.0014 - val_loss: 0.0014\n",
      "Epoch 6/24\n",
      "3622/3622 [==============================] - 1s 206us/step - loss: 8.4318e-04 - val_loss: 7.7037e-04\n",
      "Epoch 7/24\n",
      "3622/3622 [==============================] - 1s 207us/step - loss: 7.5359e-04 - val_loss: 6.7159e-04\n",
      "Epoch 8/24\n",
      "3622/3622 [==============================] - 1s 205us/step - loss: 6.8692e-04 - val_loss: 8.2264e-04\n",
      "Epoch 9/24\n",
      "3622/3622 [==============================] - 1s 205us/step - loss: 6.5202e-04 - val_loss: 7.6185e-04\n",
      "Epoch 10/24\n",
      "3622/3622 [==============================] - 1s 206us/step - loss: 6.3088e-04 - val_loss: 6.2115e-04\n",
      "Epoch 11/24\n",
      "3622/3622 [==============================] - 1s 205us/step - loss: 6.0294e-04 - val_loss: 6.1076e-04\n",
      "Epoch 12/24\n",
      "3622/3622 [==============================] - 1s 207us/step - loss: 5.7983e-04 - val_loss: 5.8215e-04\n",
      "Epoch 13/24\n",
      "3622/3622 [==============================] - 1s 208us/step - loss: 5.6485e-04 - val_loss: 5.8906e-04\n",
      "Epoch 14/24\n",
      "3622/3622 [==============================] - 1s 207us/step - loss: 5.2535e-04 - val_loss: 5.4878e-04\n",
      "Epoch 15/24\n",
      "3622/3622 [==============================] - 1s 205us/step - loss: 4.9332e-04 - val_loss: 4.8930e-04\n",
      "Epoch 16/24\n",
      "3622/3622 [==============================] - 1s 205us/step - loss: 4.7346e-04 - val_loss: 4.6951e-04\n",
      "Epoch 17/24\n",
      "3622/3622 [==============================] - 1s 206us/step - loss: 4.4557e-04 - val_loss: 5.7978e-04\n",
      "Epoch 18/24\n",
      "3622/3622 [==============================] - 1s 205us/step - loss: 4.5981e-04 - val_loss: 7.1133e-04\n",
      "Epoch 19/24\n",
      "3622/3622 [==============================] - 1s 204us/step - loss: 4.8566e-04 - val_loss: 6.2677e-04\n",
      "Epoch 20/24\n",
      "3622/3622 [==============================] - 1s 207us/step - loss: 4.3632e-04 - val_loss: 4.7515e-04\n",
      "Epoch 21/24\n",
      "3622/3622 [==============================] - 1s 205us/step - loss: 4.1334e-04 - val_loss: 4.7589e-04\n",
      "Epoch 22/24\n",
      "3622/3622 [==============================] - 1s 206us/step - loss: 3.9467e-04 - val_loss: 4.5012e-04\n",
      "Epoch 23/24\n",
      "3622/3622 [==============================] - 1s 206us/step - loss: 4.0096e-04 - val_loss: 5.5649e-04\n",
      "Epoch 24/24\n",
      "3622/3622 [==============================] - 1s 207us/step - loss: 4.3356e-04 - val_loss: 4.0602e-04\n",
      "Train on 3622 samples, validate on 403 samples\n",
      "Epoch 1/24\n",
      "3622/3622 [==============================] - 7s 2ms/step - loss: 0.1372 - val_loss: 0.0647\n",
      "Epoch 2/24\n",
      "3622/3622 [==============================] - 1s 205us/step - loss: 0.0199 - val_loss: 0.0147\n",
      "Epoch 3/24\n",
      "3622/3622 [==============================] - 1s 215us/step - loss: 0.0056 - val_loss: 0.0037\n",
      "Epoch 4/24\n",
      "3622/3622 [==============================] - 1s 208us/step - loss: 0.0023 - val_loss: 7.9965e-04\n",
      "Epoch 5/24\n",
      "3622/3622 [==============================] - 1s 206us/step - loss: 0.0010 - val_loss: 7.4356e-04\n",
      "Epoch 6/24\n",
      "3622/3622 [==============================] - 1s 205us/step - loss: 7.5899e-04 - val_loss: 0.0010\n",
      "Epoch 7/24\n",
      "3622/3622 [==============================] - 1s 206us/step - loss: 7.1027e-04 - val_loss: 6.9272e-04\n",
      "Epoch 8/24\n",
      "3622/3622 [==============================] - 1s 206us/step - loss: 6.5504e-04 - val_loss: 6.9057e-04\n",
      "Epoch 9/24\n",
      "3622/3622 [==============================] - 1s 205us/step - loss: 6.4299e-04 - val_loss: 6.9652e-04\n",
      "Epoch 10/24\n",
      "3622/3622 [==============================] - 1s 205us/step - loss: 6.0366e-04 - val_loss: 7.3793e-04\n",
      "Epoch 11/24\n",
      "3622/3622 [==============================] - 1s 205us/step - loss: 5.9526e-04 - val_loss: 6.5888e-04\n",
      "Epoch 12/24\n",
      "3622/3622 [==============================] - 1s 205us/step - loss: 5.7614e-04 - val_loss: 6.1262e-04\n",
      "Epoch 13/24\n",
      "3622/3622 [==============================] - 1s 205us/step - loss: 5.7696e-04 - val_loss: 5.9054e-04\n",
      "Epoch 14/24\n",
      "3622/3622 [==============================] - 1s 206us/step - loss: 5.3282e-04 - val_loss: 5.3411e-04\n",
      "Epoch 15/24\n",
      "3622/3622 [==============================] - 1s 204us/step - loss: 4.9418e-04 - val_loss: 5.1566e-04\n",
      "Epoch 16/24\n",
      "3622/3622 [==============================] - 1s 206us/step - loss: 4.6444e-04 - val_loss: 4.8312e-04\n",
      "Epoch 17/24\n",
      "3622/3622 [==============================] - 1s 206us/step - loss: 4.4347e-04 - val_loss: 5.2441e-04\n",
      "Epoch 18/24\n",
      "3622/3622 [==============================] - 1s 205us/step - loss: 4.3697e-04 - val_loss: 4.5121e-04\n",
      "Epoch 19/24\n",
      "3622/3622 [==============================] - 1s 206us/step - loss: 4.0399e-04 - val_loss: 4.5800e-04\n",
      "Epoch 20/24\n",
      "3622/3622 [==============================] - 1s 204us/step - loss: 3.9195e-04 - val_loss: 0.0011\n",
      "Epoch 21/24\n",
      "3622/3622 [==============================] - 1s 204us/step - loss: 4.8095e-04 - val_loss: 5.7412e-04\n",
      "Epoch 22/24\n",
      "3622/3622 [==============================] - 1s 205us/step - loss: 4.3555e-04 - val_loss: 4.7851e-04\n",
      "Epoch 23/24\n",
      "3622/3622 [==============================] - 1s 205us/step - loss: 4.1081e-04 - val_loss: 4.5073e-04\n",
      "Epoch 24/24\n",
      "3622/3622 [==============================] - 1s 208us/step - loss: 3.8958e-04 - val_loss: 4.1208e-04\n",
      "Train on 3622 samples, validate on 403 samples\n",
      "Epoch 1/24\n",
      "3622/3622 [==============================] - 7s 2ms/step - loss: 0.0399 - val_loss: 0.0144\n",
      "Epoch 2/24\n",
      "3622/3622 [==============================] - 1s 206us/step - loss: 0.0108 - val_loss: 0.0076\n",
      "Epoch 3/24\n",
      "3622/3622 [==============================] - 1s 204us/step - loss: 0.0037 - val_loss: 0.0013\n",
      "Epoch 4/24\n",
      "3622/3622 [==============================] - 1s 204us/step - loss: 0.0021 - val_loss: 0.0033\n",
      "Epoch 5/24\n",
      "3622/3622 [==============================] - 1s 205us/step - loss: 0.0014 - val_loss: 5.8233e-04\n",
      "Epoch 6/24\n",
      "3622/3622 [==============================] - 1s 206us/step - loss: 0.0010 - val_loss: 0.0017\n",
      "Epoch 7/24\n",
      "3622/3622 [==============================] - 1s 206us/step - loss: 8.0007e-04 - val_loss: 6.1070e-04\n",
      "Epoch 8/24\n",
      "3622/3622 [==============================] - 1s 205us/step - loss: 6.7057e-04 - val_loss: 0.0011\n",
      "Epoch 9/24\n",
      "3622/3622 [==============================] - 1s 205us/step - loss: 6.2871e-04 - val_loss: 6.8211e-04\n",
      "Epoch 10/24\n",
      "3622/3622 [==============================] - 1s 205us/step - loss: 6.0211e-04 - val_loss: 7.2522e-04\n",
      "Epoch 11/24\n",
      "3622/3622 [==============================] - 1s 203us/step - loss: 5.7346e-04 - val_loss: 7.0560e-04\n",
      "Epoch 12/24\n",
      "3622/3622 [==============================] - 1s 205us/step - loss: 5.5348e-04 - val_loss: 6.4905e-04\n",
      "Epoch 13/24\n",
      "3622/3622 [==============================] - 1s 205us/step - loss: 5.3796e-04 - val_loss: 5.9386e-04\n",
      "Epoch 14/24\n",
      "3622/3622 [==============================] - 1s 204us/step - loss: 5.2478e-04 - val_loss: 6.7700e-04\n",
      "Epoch 15/24\n",
      "3622/3622 [==============================] - 1s 205us/step - loss: 5.1375e-04 - val_loss: 5.8156e-04\n",
      "Epoch 16/24\n",
      "3622/3622 [==============================] - 1s 204us/step - loss: 5.0281e-04 - val_loss: 4.9986e-04\n",
      "Epoch 17/24\n",
      "3622/3622 [==============================] - 1s 205us/step - loss: 4.9294e-04 - val_loss: 5.5009e-04\n",
      "Epoch 18/24\n",
      "3622/3622 [==============================] - 1s 205us/step - loss: 4.8273e-04 - val_loss: 6.0199e-04\n",
      "Epoch 19/24\n",
      "3622/3622 [==============================] - 1s 204us/step - loss: 4.7043e-04 - val_loss: 5.4321e-04\n",
      "Epoch 20/24\n",
      "3622/3622 [==============================] - 1s 204us/step - loss: 4.5941e-04 - val_loss: 4.7014e-04\n",
      "Epoch 21/24\n",
      "3622/3622 [==============================] - 1s 205us/step - loss: 4.5164e-04 - val_loss: 4.6472e-04\n",
      "Epoch 22/24\n",
      "3622/3622 [==============================] - 1s 207us/step - loss: 4.6043e-04 - val_loss: 4.8777e-04\n",
      "Epoch 23/24\n",
      "3622/3622 [==============================] - 1s 210us/step - loss: 4.3722e-04 - val_loss: 4.5151e-04\n",
      "Epoch 24/24\n",
      "3622/3622 [==============================] - 1s 205us/step - loss: 4.3140e-04 - val_loss: 4.6875e-04\n",
      "Train on 3622 samples, validate on 403 samples\n",
      "Epoch 1/24\n",
      "3622/3622 [==============================] - 7s 2ms/step - loss: 0.0863 - val_loss: 0.0121\n",
      "Epoch 2/24\n",
      "3622/3622 [==============================] - 1s 204us/step - loss: 0.0228 - val_loss: 0.0049\n",
      "Epoch 3/24\n",
      "3622/3622 [==============================] - 1s 205us/step - loss: 0.0157 - val_loss: 0.0044\n",
      "Epoch 4/24\n",
      "3622/3622 [==============================] - 1s 204us/step - loss: 0.0109 - val_loss: 0.0029\n",
      "Epoch 5/24\n",
      "3622/3622 [==============================] - 1s 205us/step - loss: 0.0075 - val_loss: 0.0021\n",
      "Epoch 6/24\n",
      "3622/3622 [==============================] - 1s 204us/step - loss: 0.0052 - val_loss: 0.0011\n",
      "Epoch 7/24\n",
      "3622/3622 [==============================] - 1s 204us/step - loss: 0.0037 - val_loss: 0.0011\n",
      "Epoch 8/24\n",
      "3622/3622 [==============================] - 1s 205us/step - loss: 0.0027 - val_loss: 8.9894e-04\n",
      "Epoch 9/24\n",
      "3622/3622 [==============================] - 1s 203us/step - loss: 0.0020 - val_loss: 8.3292e-04\n",
      "Epoch 10/24\n",
      "3622/3622 [==============================] - 1s 205us/step - loss: 0.0016 - val_loss: 8.4521e-04\n",
      "Epoch 11/24\n",
      "3622/3622 [==============================] - 1s 203us/step - loss: 0.0013 - val_loss: 8.9755e-04\n",
      "Epoch 12/24\n",
      "3622/3622 [==============================] - 1s 203us/step - loss: 0.0011 - val_loss: 8.4913e-04\n",
      "Epoch 13/24\n",
      "3622/3622 [==============================] - 1s 204us/step - loss: 0.0010 - val_loss: 8.9802e-04\n",
      "Epoch 14/24\n",
      "3622/3622 [==============================] - 1s 204us/step - loss: 9.3879e-04 - val_loss: 9.6255e-04\n",
      "Epoch 15/24\n",
      "3622/3622 [==============================] - 1s 206us/step - loss: 8.9613e-04 - val_loss: 8.8814e-04\n",
      "Epoch 16/24\n",
      "3622/3622 [==============================] - 1s 204us/step - loss: 8.7211e-04 - val_loss: 9.7390e-04\n",
      "Epoch 17/24\n",
      "3622/3622 [==============================] - 1s 205us/step - loss: 8.5190e-04 - val_loss: 0.0011\n",
      "Epoch 18/24\n",
      "3622/3622 [==============================] - 1s 205us/step - loss: 8.4380e-04 - val_loss: 9.5946e-04\n",
      "Epoch 19/24\n",
      "3622/3622 [==============================] - 1s 205us/step - loss: 8.3311e-04 - val_loss: 9.3377e-04\n",
      "Epoch 20/24\n",
      "3622/3622 [==============================] - 1s 206us/step - loss: 8.3068e-04 - val_loss: 9.4717e-04\n",
      "Epoch 21/24\n",
      "3622/3622 [==============================] - 1s 212us/step - loss: 8.2656e-04 - val_loss: 0.0010\n",
      "Epoch 22/24\n",
      "3622/3622 [==============================] - 1s 203us/step - loss: 8.2253e-04 - val_loss: 9.7413e-04\n",
      "Epoch 23/24\n",
      "3622/3622 [==============================] - 1s 205us/step - loss: 8.2061e-04 - val_loss: 0.0012\n",
      "Epoch 24/24\n",
      "3622/3622 [==============================] - 1s 203us/step - loss: 8.1878e-04 - val_loss: 9.8564e-04\n",
      "Train on 3622 samples, validate on 403 samples\n",
      "Epoch 1/24\n",
      "3622/3622 [==============================] - 7s 2ms/step - loss: 0.0673 - val_loss: 0.0012\n",
      "Epoch 2/24\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3622/3622 [==============================] - 1s 206us/step - loss: 0.0045 - val_loss: 0.0072\n",
      "Epoch 3/24\n",
      "3622/3622 [==============================] - 1s 205us/step - loss: 0.0038 - val_loss: 0.0010\n",
      "Epoch 4/24\n",
      "3622/3622 [==============================] - 1s 205us/step - loss: 7.1202e-04 - val_loss: 7.1498e-04\n",
      "Epoch 5/24\n",
      "3622/3622 [==============================] - 1s 205us/step - loss: 8.2401e-04 - val_loss: 0.0017\n",
      "Epoch 6/24\n",
      "3622/3622 [==============================] - 1s 206us/step - loss: 0.0059 - val_loss: 0.0149\n",
      "Epoch 7/24\n",
      "3622/3622 [==============================] - 1s 206us/step - loss: 0.0034 - val_loss: 6.1035e-04\n",
      "Epoch 8/24\n",
      "3622/3622 [==============================] - 1s 205us/step - loss: 6.5897e-04 - val_loss: 0.0016\n",
      "Epoch 9/24\n",
      "3622/3622 [==============================] - 1s 207us/step - loss: 7.2379e-04 - val_loss: 0.0020\n",
      "Epoch 10/24\n",
      "3622/3622 [==============================] - 1s 207us/step - loss: 7.9091e-04 - val_loss: 0.0014\n",
      "Epoch 11/24\n",
      "3622/3622 [==============================] - 1s 205us/step - loss: 0.0011 - val_loss: 0.0020\n",
      "Epoch 12/24\n",
      "3622/3622 [==============================] - 1s 207us/step - loss: 0.0013 - val_loss: 0.0015\n",
      "Epoch 13/24\n",
      "3622/3622 [==============================] - 1s 206us/step - loss: 8.8441e-04 - val_loss: 0.0035\n",
      "Epoch 14/24\n",
      "3622/3622 [==============================] - 1s 205us/step - loss: 0.0041 - val_loss: 0.0130\n",
      "Epoch 15/24\n",
      "3622/3622 [==============================] - 1s 209us/step - loss: 0.0042 - val_loss: 0.0045\n",
      "Epoch 16/24\n",
      "3622/3622 [==============================] - 1s 208us/step - loss: 0.0011 - val_loss: 0.0010\n",
      "Epoch 17/24\n",
      "3622/3622 [==============================] - 1s 206us/step - loss: 5.1897e-04 - val_loss: 5.7106e-04\n",
      "Epoch 18/24\n",
      "3622/3622 [==============================] - 1s 216us/step - loss: 5.0517e-04 - val_loss: 4.9076e-04\n",
      "Epoch 19/24\n",
      "3622/3622 [==============================] - 1s 232us/step - loss: 7.1940e-04 - val_loss: 4.9454e-04\n",
      "Epoch 20/24\n",
      "3622/3622 [==============================] - 1s 253us/step - loss: 8.0871e-04 - val_loss: 5.4888e-04\n",
      "Epoch 21/24\n",
      "3622/3622 [==============================] - 1s 250us/step - loss: 4.6375e-04 - val_loss: 0.0010\n",
      "Epoch 22/24\n",
      "3622/3622 [==============================] - 1s 248us/step - loss: 6.5726e-04 - val_loss: 0.0023\n",
      "Epoch 23/24\n",
      "3622/3622 [==============================] - 1s 251us/step - loss: 0.0035 - val_loss: 0.0058\n",
      "Epoch 24/24\n",
      "3622/3622 [==============================] - 1s 244us/step - loss: 0.0026 - val_loss: 0.0025\n",
      "Train on 3622 samples, validate on 403 samples\n",
      "Epoch 1/24\n",
      "3622/3622 [==============================] - 9s 3ms/step - loss: 0.2381 - val_loss: 7.1096e-04\n",
      "Epoch 2/24\n",
      "3622/3622 [==============================] - 1s 249us/step - loss: 0.0032 - val_loss: 5.4434e-04\n",
      "Epoch 3/24\n",
      "3622/3622 [==============================] - 1s 251us/step - loss: 0.0027 - val_loss: 0.0066\n",
      "Epoch 4/24\n",
      "3622/3622 [==============================] - 1s 248us/step - loss: 0.0110 - val_loss: 0.0073\n",
      "Epoch 5/24\n",
      "3622/3622 [==============================] - 1s 251us/step - loss: 0.0060 - val_loss: 0.0241\n",
      "Epoch 6/24\n",
      "3622/3622 [==============================] - 1s 255us/step - loss: 0.0144 - val_loss: 0.0079\n",
      "Epoch 7/24\n",
      "3622/3622 [==============================] - 1s 262us/step - loss: 0.0108 - val_loss: 0.0049\n",
      "Epoch 8/24\n",
      "3622/3622 [==============================] - 1s 251us/step - loss: 0.0067 - val_loss: 0.0145\n",
      "Epoch 9/24\n",
      "3622/3622 [==============================] - 1s 246us/step - loss: 0.0092 - val_loss: 0.0122\n",
      "Epoch 10/24\n",
      "3622/3622 [==============================] - 1s 246us/step - loss: 0.0099 - val_loss: 0.0058\n",
      "Epoch 11/24\n",
      "3622/3622 [==============================] - 1s 247us/step - loss: 0.0057 - val_loss: 0.0222\n",
      "Epoch 12/24\n",
      "3622/3622 [==============================] - 1s 250us/step - loss: 0.0102 - val_loss: 0.0052\n",
      "Epoch 13/24\n",
      "3622/3622 [==============================] - 1s 251us/step - loss: 0.0048 - val_loss: 0.0148\n",
      "Epoch 14/24\n",
      "3622/3622 [==============================] - 1s 245us/step - loss: 0.0080 - val_loss: 0.0186\n",
      "Epoch 15/24\n",
      "3622/3622 [==============================] - 1s 248us/step - loss: 0.0072 - val_loss: 0.0091\n",
      "Epoch 16/24\n",
      "3622/3622 [==============================] - 1s 247us/step - loss: 0.0065 - val_loss: 0.0197\n",
      "Epoch 17/24\n",
      "3622/3622 [==============================] - 1s 244us/step - loss: 0.0073 - val_loss: 0.0073\n",
      "Epoch 18/24\n",
      "3622/3622 [==============================] - 1s 245us/step - loss: 0.0040 - val_loss: 0.0018\n",
      "Epoch 19/24\n",
      "3622/3622 [==============================] - 1s 248us/step - loss: 0.0045 - val_loss: 0.0170\n",
      "Epoch 20/24\n",
      "3622/3622 [==============================] - 1s 253us/step - loss: 0.0084 - val_loss: 0.0029\n",
      "Epoch 21/24\n",
      "3622/3622 [==============================] - 1s 247us/step - loss: 0.0020 - val_loss: 0.0175\n",
      "Epoch 22/24\n",
      "3622/3622 [==============================] - 1s 248us/step - loss: 0.0063 - val_loss: 0.0080\n",
      "Epoch 23/24\n",
      "3622/3622 [==============================] - 1s 247us/step - loss: 0.0053 - val_loss: 0.0040\n",
      "Epoch 24/24\n",
      "3622/3622 [==============================] - 1s 250us/step - loss: 0.0040 - val_loss: 0.0103\n",
      "Train on 3622 samples, validate on 403 samples\n",
      "Epoch 1/24\n",
      "3622/3622 [==============================] - 12s 3ms/step - loss: 0.1249 - val_loss: 0.0019\n",
      "Epoch 2/24\n",
      "3622/3622 [==============================] - 2s 612us/step - loss: 0.0028 - val_loss: 0.0145\n",
      "Epoch 3/24\n",
      "3622/3622 [==============================] - 2s 680us/step - loss: 0.0107 - val_loss: 0.0014\n",
      "Epoch 4/24\n",
      "3622/3622 [==============================] - 2s 687us/step - loss: 0.0022 - val_loss: 0.0011\n",
      "Epoch 5/24\n",
      "3622/3622 [==============================] - 2s 677us/step - loss: 0.0026 - val_loss: 0.0012\n",
      "Epoch 6/24\n",
      "3622/3622 [==============================] - 2s 683us/step - loss: 0.0051 - val_loss: 0.0055\n",
      "Epoch 7/24\n",
      "3622/3622 [==============================] - 2s 682us/step - loss: 0.0058 - val_loss: 0.0012\n",
      "Epoch 8/24\n",
      "3622/3622 [==============================] - 2s 679us/step - loss: 0.0022 - val_loss: 9.2534e-04\n",
      "Epoch 9/24\n",
      "3622/3622 [==============================] - 2s 685us/step - loss: 0.0019 - val_loss: 8.8351e-04\n",
      "Epoch 10/24\n",
      "3622/3622 [==============================] - 2s 687us/step - loss: 0.0027 - val_loss: 8.2579e-04\n",
      "Epoch 11/24\n",
      "3622/3622 [==============================] - 2s 678us/step - loss: 0.0032 - val_loss: 0.0042\n",
      "Epoch 12/24\n",
      "3622/3622 [==============================] - 2s 689us/step - loss: 0.0049 - val_loss: 0.0042\n",
      "Epoch 13/24\n",
      "3622/3622 [==============================] - 2s 681us/step - loss: 0.0042 - val_loss: 0.0014\n",
      "Epoch 14/24\n",
      "3622/3622 [==============================] - 2s 678us/step - loss: 0.0025 - val_loss: 7.9781e-04\n",
      "Epoch 15/24\n",
      "3622/3622 [==============================] - 2s 683us/step - loss: 0.0016 - val_loss: 0.0012\n",
      "Epoch 16/24\n",
      "3622/3622 [==============================] - 2s 679us/step - loss: 0.0015 - val_loss: 0.0117\n",
      "Epoch 17/24\n",
      "3622/3622 [==============================] - 2s 689us/step - loss: 0.0079 - val_loss: 0.0062\n",
      "Epoch 18/24\n",
      "3622/3622 [==============================] - 2s 676us/step - loss: 0.0024 - val_loss: 0.0026\n",
      "Epoch 19/24\n",
      "3622/3622 [==============================] - 2s 597us/step - loss: 0.0016 - val_loss: 9.7165e-04\n",
      "Epoch 20/24\n",
      "3622/3622 [==============================] - 2s 624us/step - loss: 0.0012 - val_loss: 7.1345e-04\n",
      "Epoch 21/24\n",
      "3622/3622 [==============================] - 2s 684us/step - loss: 0.0028 - val_loss: 0.0040\n",
      "Epoch 22/24\n",
      "3622/3622 [==============================] - 3s 692us/step - loss: 0.0041 - val_loss: 0.0010\n",
      "Epoch 23/24\n",
      "3622/3622 [==============================] - 3s 691us/step - loss: 0.0017 - val_loss: 7.0578e-04\n",
      "Epoch 24/24\n",
      "3622/3622 [==============================] - 2s 598us/step - loss: 0.0015 - val_loss: 0.0013\n",
      "Train on 3622 samples, validate on 403 samples\n",
      "Epoch 1/24\n",
      "3622/3622 [==============================] - 12s 3ms/step - loss: 0.1111 - val_loss: 0.0025\n",
      "Epoch 2/24\n",
      "3622/3622 [==============================] - 2s 677us/step - loss: 0.0083 - val_loss: 0.0038\n",
      "Epoch 3/24\n",
      "3622/3622 [==============================] - 2s 681us/step - loss: 0.0087 - val_loss: 0.0043\n",
      "Epoch 4/24\n",
      "3622/3622 [==============================] - 2s 677us/step - loss: 0.0070 - val_loss: 0.0198\n",
      "Epoch 5/24\n",
      "3622/3622 [==============================] - 2s 674us/step - loss: 0.0098 - val_loss: 0.0035\n",
      "Epoch 6/24\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3622/3622 [==============================] - 2s 680us/step - loss: 0.0037 - val_loss: 0.0552\n",
      "Epoch 7/24\n",
      "3622/3622 [==============================] - 2s 681us/step - loss: 0.0125 - val_loss: 0.0012\n",
      "Epoch 8/24\n",
      "3622/3622 [==============================] - 2s 677us/step - loss: 0.0022 - val_loss: 0.0012\n",
      "Epoch 9/24\n",
      "3622/3622 [==============================] - 2s 680us/step - loss: 0.0020 - val_loss: 0.0017\n",
      "Epoch 10/24\n",
      "3622/3622 [==============================] - 2s 682us/step - loss: 0.0040 - val_loss: 0.0012\n",
      "Epoch 11/24\n",
      "3622/3622 [==============================] - 2s 687us/step - loss: 0.0025 - val_loss: 0.0053\n",
      "Epoch 12/24\n",
      "3622/3622 [==============================] - 2s 673us/step - loss: 0.0064 - val_loss: 0.0170\n",
      "Epoch 13/24\n",
      "3622/3622 [==============================] - 2s 676us/step - loss: 0.0068 - val_loss: 0.0055\n",
      "Epoch 14/24\n",
      "3622/3622 [==============================] - 2s 675us/step - loss: 0.0038 - val_loss: 0.0045\n",
      "Epoch 15/24\n",
      "3622/3622 [==============================] - 2s 681us/step - loss: 0.0036 - val_loss: 0.0100\n",
      "Epoch 16/24\n",
      "3622/3622 [==============================] - 2s 683us/step - loss: 0.0064 - val_loss: 0.0111\n",
      "Epoch 17/24\n",
      "3622/3622 [==============================] - 2s 681us/step - loss: 0.0039 - val_loss: 0.0021\n",
      "Epoch 18/24\n",
      "3622/3622 [==============================] - 2s 678us/step - loss: 0.0018 - val_loss: 0.0034\n",
      "Epoch 19/24\n",
      "3622/3622 [==============================] - 2s 680us/step - loss: 0.0034 - val_loss: 0.0014\n",
      "Epoch 20/24\n",
      "3622/3622 [==============================] - 2s 678us/step - loss: 0.0019 - val_loss: 0.0020\n",
      "Epoch 21/24\n",
      "3622/3622 [==============================] - 2s 679us/step - loss: 0.0025 - val_loss: 0.0042\n",
      "Epoch 22/24\n",
      "3622/3622 [==============================] - 2s 675us/step - loss: 0.0039 - val_loss: 0.0067\n",
      "Epoch 23/24\n",
      "3622/3622 [==============================] - 3s 696us/step - loss: 0.0041 - val_loss: 0.0013\n",
      "Epoch 24/24\n",
      "3622/3622 [==============================] - 2s 674us/step - loss: 0.0016 - val_loss: 0.0014\n",
      "\n",
      "GENERATION 8\n",
      "\n",
      "Train on 3622 samples, validate on 403 samples\n",
      "Epoch 1/24\n",
      "3622/3622 [==============================] - 11s 3ms/step - loss: 2.0066 - val_loss: 1.6542\n",
      "Epoch 2/24\n",
      "3622/3622 [==============================] - 2s 471us/step - loss: 0.9955 - val_loss: 0.8217\n",
      "Epoch 3/24\n",
      "3622/3622 [==============================] - 2s 474us/step - loss: 0.4510 - val_loss: 0.3870\n",
      "Epoch 4/24\n",
      "3622/3622 [==============================] - 2s 468us/step - loss: 0.2007 - val_loss: 0.1751\n",
      "Epoch 5/24\n",
      "3622/3622 [==============================] - 2s 472us/step - loss: 0.1026 - val_loss: 0.0798\n",
      "Epoch 6/24\n",
      "3622/3622 [==============================] - 2s 473us/step - loss: 0.0740 - val_loss: 0.0415\n",
      "Epoch 7/24\n",
      "3622/3622 [==============================] - 2s 474us/step - loss: 0.0697 - val_loss: 0.0274\n",
      "Epoch 8/24\n",
      "3622/3622 [==============================] - 2s 469us/step - loss: 0.0709 - val_loss: 0.0236\n",
      "Epoch 9/24\n",
      "3622/3622 [==============================] - 2s 470us/step - loss: 0.0713 - val_loss: 0.0249\n",
      "Epoch 10/24\n",
      "3622/3622 [==============================] - 2s 473us/step - loss: 0.0707 - val_loss: 0.0284\n",
      "Epoch 11/24\n",
      "3622/3622 [==============================] - 2s 473us/step - loss: 0.0701 - val_loss: 0.0325\n",
      "Epoch 12/24\n",
      "3622/3622 [==============================] - 2s 481us/step - loss: 0.0697 - val_loss: 0.0353\n",
      "Epoch 13/24\n",
      "3622/3622 [==============================] - 2s 468us/step - loss: 0.0697 - val_loss: 0.0383\n",
      "Epoch 14/24\n",
      "3622/3622 [==============================] - 2s 476us/step - loss: 0.0697 - val_loss: 0.0391\n",
      "Epoch 15/24\n",
      "3622/3622 [==============================] - 2s 469us/step - loss: 0.0697 - val_loss: 0.0388\n",
      "Epoch 16/24\n",
      "3622/3622 [==============================] - 2s 475us/step - loss: 0.0697 - val_loss: 0.0383\n",
      "Epoch 17/24\n",
      "3622/3622 [==============================] - 2s 476us/step - loss: 0.0697 - val_loss: 0.0382\n",
      "Epoch 18/24\n",
      "3622/3622 [==============================] - 2s 470us/step - loss: 0.0697 - val_loss: 0.0383\n",
      "Epoch 19/24\n",
      "3622/3622 [==============================] - 2s 479us/step - loss: 0.0696 - val_loss: 0.0371\n",
      "Epoch 20/24\n",
      "3622/3622 [==============================] - 2s 473us/step - loss: 0.0696 - val_loss: 0.0354\n",
      "Epoch 21/24\n",
      "3622/3622 [==============================] - 2s 477us/step - loss: 0.0696 - val_loss: 0.0359\n",
      "Epoch 22/24\n",
      "3622/3622 [==============================] - 2s 479us/step - loss: 0.0696 - val_loss: 0.0363\n",
      "Epoch 23/24\n",
      "3622/3622 [==============================] - 2s 467us/step - loss: 0.0696 - val_loss: 0.0369\n",
      "Epoch 24/24\n",
      "3622/3622 [==============================] - 2s 471us/step - loss: 0.0696 - val_loss: 0.0370\n",
      "Train on 3622 samples, validate on 403 samples\n",
      "Epoch 1/24\n",
      "3622/3622 [==============================] - 10s 3ms/step - loss: 0.2215 - val_loss: 0.1613\n",
      "Epoch 2/24\n",
      "3622/3622 [==============================] - 1s 245us/step - loss: 0.0518 - val_loss: 0.0016\n",
      "Epoch 3/24\n",
      "3622/3622 [==============================] - 1s 247us/step - loss: 0.0133 - val_loss: 0.0015\n",
      "Epoch 4/24\n",
      "3622/3622 [==============================] - 1s 245us/step - loss: 0.0057 - val_loss: 9.4285e-04\n",
      "Epoch 5/24\n",
      "3622/3622 [==============================] - 1s 251us/step - loss: 0.0031 - val_loss: 8.5976e-04\n",
      "Epoch 6/24\n",
      "3622/3622 [==============================] - 1s 246us/step - loss: 0.0014 - val_loss: 0.0011\n",
      "Epoch 7/24\n",
      "3622/3622 [==============================] - 1s 247us/step - loss: 0.0010 - val_loss: 0.0013\n",
      "Epoch 8/24\n",
      "3622/3622 [==============================] - 1s 246us/step - loss: 9.3037e-04 - val_loss: 8.5718e-04\n",
      "Epoch 9/24\n",
      "3622/3622 [==============================] - 1s 245us/step - loss: 8.3559e-04 - val_loss: 8.4420e-04\n",
      "Epoch 10/24\n",
      "3622/3622 [==============================] - 1s 241us/step - loss: 7.9046e-04 - val_loss: 8.4955e-04\n",
      "Epoch 11/24\n",
      "3622/3622 [==============================] - 1s 246us/step - loss: 7.6585e-04 - val_loss: 7.8259e-04\n",
      "Epoch 12/24\n",
      "3622/3622 [==============================] - 1s 243us/step - loss: 7.2956e-04 - val_loss: 7.7227e-04\n",
      "Epoch 13/24\n",
      "3622/3622 [==============================] - 1s 245us/step - loss: 7.1814e-04 - val_loss: 7.9670e-04\n",
      "Epoch 14/24\n",
      "3622/3622 [==============================] - 1s 242us/step - loss: 6.8526e-04 - val_loss: 7.4960e-04\n",
      "Epoch 15/24\n",
      "3622/3622 [==============================] - 1s 247us/step - loss: 6.4991e-04 - val_loss: 6.9693e-04\n",
      "Epoch 16/24\n",
      "3622/3622 [==============================] - 1s 241us/step - loss: 6.2186e-04 - val_loss: 6.7245e-04\n",
      "Epoch 17/24\n",
      "3622/3622 [==============================] - 1s 244us/step - loss: 6.0544e-04 - val_loss: 6.8746e-04\n",
      "Epoch 18/24\n",
      "3622/3622 [==============================] - 1s 247us/step - loss: 5.9163e-04 - val_loss: 6.8874e-04\n",
      "Epoch 19/24\n",
      "3622/3622 [==============================] - 1s 253us/step - loss: 5.6671e-04 - val_loss: 6.1335e-04\n",
      "Epoch 20/24\n",
      "3622/3622 [==============================] - 1s 253us/step - loss: 5.3253e-04 - val_loss: 5.9788e-04\n",
      "Epoch 21/24\n",
      "3622/3622 [==============================] - 1s 265us/step - loss: 5.1399e-04 - val_loss: 6.3022e-04\n",
      "Epoch 22/24\n",
      "3622/3622 [==============================] - 1s 260us/step - loss: 4.9563e-04 - val_loss: 5.0972e-04\n",
      "Epoch 23/24\n",
      "3622/3622 [==============================] - 1s 228us/step - loss: 4.5587e-04 - val_loss: 4.8854e-04\n",
      "Epoch 24/24\n",
      "3622/3622 [==============================] - 1s 213us/step - loss: 4.4229e-04 - val_loss: 4.6586e-04\n",
      "Train on 3622 samples, validate on 403 samples\n",
      "Epoch 1/24\n",
      "3622/3622 [==============================] - 11s 3ms/step - loss: 0.1221 - val_loss: 0.0117\n",
      "Epoch 2/24\n",
      "3622/3622 [==============================] - 2s 469us/step - loss: 0.0202 - val_loss: 0.0035\n",
      "Epoch 3/24\n",
      "3622/3622 [==============================] - 2s 464us/step - loss: 0.0062 - val_loss: 0.0051\n",
      "Epoch 4/24\n",
      "3622/3622 [==============================] - 2s 472us/step - loss: 0.0037 - val_loss: 0.0067\n",
      "Epoch 5/24\n",
      "3622/3622 [==============================] - 2s 464us/step - loss: 0.0022 - val_loss: 0.0031\n",
      "Epoch 6/24\n",
      "3622/3622 [==============================] - 2s 461us/step - loss: 0.0017 - val_loss: 0.0014\n",
      "Epoch 7/24\n",
      "3622/3622 [==============================] - 2s 464us/step - loss: 0.0015 - val_loss: 0.0021\n",
      "Epoch 8/24\n",
      "3622/3622 [==============================] - 2s 464us/step - loss: 0.0013 - val_loss: 0.0020\n",
      "Epoch 9/24\n",
      "3622/3622 [==============================] - 2s 465us/step - loss: 0.0013 - val_loss: 0.0017\n",
      "Epoch 10/24\n",
      "3622/3622 [==============================] - 2s 462us/step - loss: 0.0012 - val_loss: 0.0014\n",
      "Epoch 11/24\n",
      "3622/3622 [==============================] - 2s 472us/step - loss: 0.0013 - val_loss: 0.0017\n",
      "Epoch 12/24\n",
      "3622/3622 [==============================] - 2s 460us/step - loss: 0.0012 - val_loss: 0.0015\n",
      "Epoch 13/24\n",
      "3622/3622 [==============================] - 2s 469us/step - loss: 0.0012 - val_loss: 0.0014\n",
      "Epoch 14/24\n",
      "3622/3622 [==============================] - 2s 466us/step - loss: 0.0011 - val_loss: 0.0013\n",
      "Epoch 15/24\n",
      "3622/3622 [==============================] - 2s 463us/step - loss: 0.0011 - val_loss: 0.0012\n",
      "Epoch 16/24\n",
      "3622/3622 [==============================] - 2s 461us/step - loss: 0.0011 - val_loss: 0.0012\n",
      "Epoch 17/24\n",
      "3622/3622 [==============================] - 2s 458us/step - loss: 0.0011 - val_loss: 0.0011\n",
      "Epoch 18/24\n",
      "3622/3622 [==============================] - 2s 462us/step - loss: 0.0010 - val_loss: 0.0011\n",
      "Epoch 19/24\n",
      "3622/3622 [==============================] - 2s 464us/step - loss: 9.9421e-04 - val_loss: 0.0012\n",
      "Epoch 20/24\n",
      "3622/3622 [==============================] - 2s 458us/step - loss: 9.6866e-04 - val_loss: 9.7284e-04\n",
      "Epoch 21/24\n",
      "3622/3622 [==============================] - 2s 464us/step - loss: 9.5956e-04 - val_loss: 0.0011\n",
      "Epoch 22/24\n",
      "3622/3622 [==============================] - 2s 459us/step - loss: 9.5737e-04 - val_loss: 9.6051e-04\n",
      "Epoch 23/24\n",
      "3622/3622 [==============================] - 2s 468us/step - loss: 9.0620e-04 - val_loss: 8.8713e-04\n",
      "Epoch 24/24\n",
      "3622/3622 [==============================] - 2s 462us/step - loss: 8.6649e-04 - val_loss: 0.0010\n",
      "Train on 3622 samples, validate on 403 samples\n",
      "Epoch 1/24\n",
      "3622/3622 [==============================] - 11s 3ms/step - loss: 0.2074 - val_loss: 0.2210\n",
      "Epoch 2/24\n",
      "3622/3622 [==============================] - 2s 450us/step - loss: 0.1319 - val_loss: 0.1393\n",
      "Epoch 3/24\n",
      "3622/3622 [==============================] - 2s 447us/step - loss: 0.0943 - val_loss: 0.0966\n",
      "Epoch 4/24\n",
      "3622/3622 [==============================] - 2s 445us/step - loss: 0.0771 - val_loss: 0.0707\n",
      "Epoch 5/24\n",
      "3622/3622 [==============================] - 2s 447us/step - loss: 0.0679 - val_loss: 0.0556\n",
      "Epoch 6/24\n",
      "3622/3622 [==============================] - 2s 442us/step - loss: 0.0633 - val_loss: 0.0454\n",
      "Epoch 7/24\n",
      "3622/3622 [==============================] - 2s 446us/step - loss: 0.0603 - val_loss: 0.0392\n",
      "Epoch 8/24\n",
      "3622/3622 [==============================] - 2s 443us/step - loss: 0.0585 - val_loss: 0.0346\n",
      "Epoch 9/24\n",
      "3622/3622 [==============================] - 2s 443us/step - loss: 0.0570 - val_loss: 0.0326\n",
      "Epoch 10/24\n",
      "3622/3622 [==============================] - 2s 439us/step - loss: 0.0559 - val_loss: 0.0308\n",
      "Epoch 11/24\n",
      "3622/3622 [==============================] - 2s 449us/step - loss: 0.0549 - val_loss: 0.0295\n",
      "Epoch 12/24\n",
      "3622/3622 [==============================] - 2s 447us/step - loss: 0.0539 - val_loss: 0.0291\n",
      "Epoch 13/24\n",
      "3622/3622 [==============================] - 2s 456us/step - loss: 0.0530 - val_loss: 0.0281\n",
      "Epoch 14/24\n",
      "3622/3622 [==============================] - 2s 446us/step - loss: 0.0520 - val_loss: 0.0273\n",
      "Epoch 15/24\n",
      "3622/3622 [==============================] - 2s 446us/step - loss: 0.0510 - val_loss: 0.0274\n",
      "Epoch 16/24\n",
      "3622/3622 [==============================] - 2s 446us/step - loss: 0.0499 - val_loss: 0.0264\n",
      "Epoch 17/24\n",
      "3622/3622 [==============================] - 2s 446us/step - loss: 0.0485 - val_loss: 0.0251\n",
      "Epoch 18/24\n",
      "3622/3622 [==============================] - 2s 446us/step - loss: 0.0470 - val_loss: 0.0223\n",
      "Epoch 19/24\n",
      "3622/3622 [==============================] - 2s 452us/step - loss: 0.0456 - val_loss: 0.0226\n",
      "Epoch 20/24\n",
      "3622/3622 [==============================] - 2s 448us/step - loss: 0.0444 - val_loss: 0.0227\n",
      "Epoch 21/24\n",
      "3622/3622 [==============================] - 2s 448us/step - loss: 0.0431 - val_loss: 0.0222\n",
      "Epoch 22/24\n",
      "3622/3622 [==============================] - 2s 440us/step - loss: 0.0419 - val_loss: 0.0202\n",
      "Epoch 23/24\n",
      "3622/3622 [==============================] - 2s 463us/step - loss: 0.0405 - val_loss: 0.0187\n",
      "Epoch 24/24\n",
      "3622/3622 [==============================] - 2s 441us/step - loss: 0.0389 - val_loss: 0.0179\n",
      "Train on 3622 samples, validate on 403 samples\n",
      "Epoch 1/24\n",
      "3622/3622 [==============================] - 12s 3ms/step - loss: 0.0949 - val_loss: 0.0401\n",
      "Epoch 2/24\n",
      "3622/3622 [==============================] - 2s 453us/step - loss: 0.0696 - val_loss: 0.0307\n",
      "Epoch 3/24\n",
      "3622/3622 [==============================] - 2s 460us/step - loss: 0.0696 - val_loss: 0.0339\n",
      "Epoch 4/24\n",
      "3622/3622 [==============================] - 2s 448us/step - loss: 0.0695 - val_loss: 0.0403\n",
      "Epoch 5/24\n",
      "3622/3622 [==============================] - 2s 447us/step - loss: 0.0696 - val_loss: 0.0376\n",
      "Epoch 6/24\n",
      "3622/3622 [==============================] - 2s 444us/step - loss: 0.0695 - val_loss: 0.0351\n",
      "Epoch 7/24\n",
      "3622/3622 [==============================] - 2s 443us/step - loss: 0.0694 - val_loss: 0.0423\n",
      "Epoch 8/24\n",
      "3622/3622 [==============================] - 2s 440us/step - loss: 0.0693 - val_loss: 0.0276\n",
      "Epoch 9/24\n",
      "3622/3622 [==============================] - 2s 442us/step - loss: 0.0694 - val_loss: 0.0362\n",
      "Epoch 10/24\n",
      "3622/3622 [==============================] - 2s 448us/step - loss: 0.0692 - val_loss: 0.0415\n",
      "Epoch 11/24\n",
      "3622/3622 [==============================] - 2s 441us/step - loss: 0.0693 - val_loss: 0.0421\n",
      "Epoch 12/24\n",
      "3622/3622 [==============================] - 2s 442us/step - loss: 0.0692 - val_loss: 0.0356\n",
      "Epoch 13/24\n",
      "3622/3622 [==============================] - 2s 449us/step - loss: 0.0692 - val_loss: 0.0362\n",
      "Epoch 14/24\n",
      "3622/3622 [==============================] - 2s 444us/step - loss: 0.0691 - val_loss: 0.0379\n",
      "Epoch 15/24\n",
      "3622/3622 [==============================] - 2s 442us/step - loss: 0.0690 - val_loss: 0.0437\n",
      "Epoch 16/24\n",
      "3622/3622 [==============================] - 2s 446us/step - loss: 0.0690 - val_loss: 0.0337\n",
      "Epoch 17/24\n",
      "3622/3622 [==============================] - 2s 441us/step - loss: 0.0689 - val_loss: 0.0325\n",
      "Epoch 18/24\n",
      "3622/3622 [==============================] - 2s 437us/step - loss: 0.0689 - val_loss: 0.0239\n",
      "Epoch 19/24\n",
      "3622/3622 [==============================] - 2s 444us/step - loss: 0.0693 - val_loss: 0.0275\n",
      "Epoch 20/24\n",
      "3622/3622 [==============================] - 2s 444us/step - loss: 0.0689 - val_loss: 0.0317\n",
      "Epoch 21/24\n",
      "3622/3622 [==============================] - 2s 451us/step - loss: 0.0688 - val_loss: 0.0477\n",
      "Epoch 22/24\n",
      "3622/3622 [==============================] - 2s 449us/step - loss: 0.0688 - val_loss: 0.0357\n",
      "Epoch 23/24\n",
      "3622/3622 [==============================] - 2s 451us/step - loss: 0.0686 - val_loss: 0.0312\n",
      "Epoch 24/24\n",
      "3622/3622 [==============================] - 2s 448us/step - loss: 0.0686 - val_loss: 0.0427\n",
      "Train on 3622 samples, validate on 403 samples\n",
      "Epoch 1/24\n",
      "3622/3622 [==============================] - 12s 3ms/step - loss: 1.9921 - val_loss: 0.0304\n",
      "Epoch 2/24\n",
      "3622/3622 [==============================] - 2s 446us/step - loss: 0.0699 - val_loss: 0.0339\n",
      "Epoch 3/24\n",
      "3622/3622 [==============================] - 2s 452us/step - loss: 0.0697 - val_loss: 0.0447\n",
      "Epoch 4/24\n",
      "3622/3622 [==============================] - 2s 442us/step - loss: 0.0697 - val_loss: 0.0426\n",
      "Epoch 5/24\n",
      "3622/3622 [==============================] - 2s 447us/step - loss: 0.0698 - val_loss: 0.0286\n",
      "Epoch 6/24\n",
      "3622/3622 [==============================] - 2s 441us/step - loss: 0.0697 - val_loss: 0.0422\n",
      "Epoch 7/24\n",
      "3622/3622 [==============================] - 2s 447us/step - loss: 0.0696 - val_loss: 0.0388\n",
      "Epoch 8/24\n",
      "3622/3622 [==============================] - 2s 446us/step - loss: 0.0695 - val_loss: 0.0297\n",
      "Epoch 9/24\n",
      "3622/3622 [==============================] - 2s 447us/step - loss: 0.0695 - val_loss: 0.0259\n",
      "Epoch 10/24\n",
      "3622/3622 [==============================] - 2s 441us/step - loss: 0.0694 - val_loss: 0.0436\n",
      "Epoch 11/24\n",
      "3622/3622 [==============================] - 2s 441us/step - loss: 0.0693 - val_loss: 0.0454\n",
      "Epoch 12/24\n",
      "3622/3622 [==============================] - 2s 447us/step - loss: 0.0694 - val_loss: 0.0286\n",
      "Epoch 13/24\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3622/3622 [==============================] - 2s 448us/step - loss: 0.0693 - val_loss: 0.0225\n",
      "Epoch 14/24\n",
      "3622/3622 [==============================] - 2s 451us/step - loss: 0.0694 - val_loss: 0.0384\n",
      "Epoch 15/24\n",
      "3622/3622 [==============================] - 2s 445us/step - loss: 0.0691 - val_loss: 0.0472\n",
      "Epoch 16/24\n",
      "3622/3622 [==============================] - 2s 449us/step - loss: 0.0693 - val_loss: 0.0441\n",
      "Epoch 17/24\n",
      "3622/3622 [==============================] - 2s 448us/step - loss: 0.0691 - val_loss: 0.0345\n",
      "Epoch 18/24\n",
      "3622/3622 [==============================] - 2s 444us/step - loss: 0.0690 - val_loss: 0.0337\n",
      "Epoch 19/24\n",
      "3622/3622 [==============================] - 2s 448us/step - loss: 0.0689 - val_loss: 0.0386\n",
      "Epoch 20/24\n",
      "3622/3622 [==============================] - 2s 451us/step - loss: 0.0689 - val_loss: 0.0341\n",
      "Epoch 21/24\n",
      "3622/3622 [==============================] - 2s 447us/step - loss: 0.0689 - val_loss: 0.0288\n",
      "Epoch 22/24\n",
      "3622/3622 [==============================] - 2s 444us/step - loss: 0.0689 - val_loss: 0.0372\n",
      "Epoch 23/24\n",
      "3622/3622 [==============================] - 2s 447us/step - loss: 0.0688 - val_loss: 0.0525\n",
      "Epoch 24/24\n",
      "3622/3622 [==============================] - 2s 445us/step - loss: 0.0690 - val_loss: 0.0419\n",
      "Train on 3622 samples, validate on 403 samples\n",
      "Epoch 1/24\n",
      "3622/3622 [==============================] - 12s 3ms/step - loss: 0.0731 - val_loss: 0.0088\n",
      "Epoch 2/24\n",
      "3622/3622 [==============================] - 1s 387us/step - loss: 0.0192 - val_loss: 0.0038\n",
      "Epoch 3/24\n",
      "3622/3622 [==============================] - 1s 381us/step - loss: 0.0096 - val_loss: 0.0021\n",
      "Epoch 4/24\n",
      "3622/3622 [==============================] - 1s 372us/step - loss: 0.0052 - val_loss: 0.0017\n",
      "Epoch 5/24\n",
      "3622/3622 [==============================] - 1s 373us/step - loss: 0.0032 - val_loss: 0.0016\n",
      "Epoch 6/24\n",
      "3622/3622 [==============================] - 1s 372us/step - loss: 0.0024 - val_loss: 0.0026\n",
      "Epoch 7/24\n",
      "3622/3622 [==============================] - 1s 372us/step - loss: 0.0021 - val_loss: 0.0023\n",
      "Epoch 8/24\n",
      "3622/3622 [==============================] - 1s 372us/step - loss: 0.0019 - val_loss: 0.0016\n",
      "Epoch 9/24\n",
      "3622/3622 [==============================] - 1s 373us/step - loss: 0.0019 - val_loss: 0.0022\n",
      "Epoch 10/24\n",
      "3622/3622 [==============================] - 1s 374us/step - loss: 0.0019 - val_loss: 0.0025\n",
      "Epoch 11/24\n",
      "3622/3622 [==============================] - 1s 374us/step - loss: 0.0019 - val_loss: 0.0020\n",
      "Epoch 12/24\n",
      "3622/3622 [==============================] - 1s 374us/step - loss: 0.0019 - val_loss: 0.0018\n",
      "Epoch 13/24\n",
      "3622/3622 [==============================] - 1s 373us/step - loss: 0.0019 - val_loss: 0.0021\n",
      "Epoch 14/24\n",
      "3622/3622 [==============================] - 1s 377us/step - loss: 0.0018 - val_loss: 0.0025\n",
      "Epoch 15/24\n",
      "3622/3622 [==============================] - 1s 374us/step - loss: 0.0018 - val_loss: 0.0020\n",
      "Epoch 16/24\n",
      "3622/3622 [==============================] - 1s 371us/step - loss: 0.0018 - val_loss: 0.0017\n",
      "Epoch 17/24\n",
      "3622/3622 [==============================] - 1s 380us/step - loss: 0.0018 - val_loss: 0.0019\n",
      "Epoch 18/24\n",
      "3622/3622 [==============================] - 1s 372us/step - loss: 0.0018 - val_loss: 0.0018\n",
      "Epoch 19/24\n",
      "3622/3622 [==============================] - 1s 371us/step - loss: 0.0018 - val_loss: 0.0017\n",
      "Epoch 20/24\n",
      "3622/3622 [==============================] - 1s 372us/step - loss: 0.0018 - val_loss: 0.0022\n",
      "Epoch 21/24\n",
      "3622/3622 [==============================] - 1s 371us/step - loss: 0.0018 - val_loss: 0.0023\n",
      "Epoch 22/24\n",
      "3622/3622 [==============================] - 1s 373us/step - loss: 0.0018 - val_loss: 0.0019\n",
      "Epoch 23/24\n",
      "3622/3622 [==============================] - 1s 374us/step - loss: 0.0018 - val_loss: 0.0021\n",
      "Epoch 24/24\n",
      "3622/3622 [==============================] - 1s 373us/step - loss: 0.0018 - val_loss: 0.0018\n",
      "Train on 3622 samples, validate on 403 samples\n",
      "Epoch 1/24\n",
      "3622/3622 [==============================] - 10s 3ms/step - loss: 0.0436 - val_loss: 0.0077\n",
      "Epoch 2/24\n",
      "3622/3622 [==============================] - 1s 374us/step - loss: 0.0151 - val_loss: 0.0048\n",
      "Epoch 3/24\n",
      "3622/3622 [==============================] - 1s 373us/step - loss: 0.0106 - val_loss: 0.0022\n",
      "Epoch 4/24\n",
      "3622/3622 [==============================] - 1s 374us/step - loss: 0.0073 - val_loss: 0.0020\n",
      "Epoch 5/24\n",
      "3622/3622 [==============================] - 1s 373us/step - loss: 0.0052 - val_loss: 0.0016\n",
      "Epoch 6/24\n",
      "3622/3622 [==============================] - 1s 370us/step - loss: 0.0038 - val_loss: 0.0017\n",
      "Epoch 7/24\n",
      "3622/3622 [==============================] - 1s 370us/step - loss: 0.0030 - val_loss: 0.0017\n",
      "Epoch 8/24\n",
      "3622/3622 [==============================] - 1s 371us/step - loss: 0.0025 - val_loss: 0.0019\n",
      "Epoch 9/24\n",
      "3622/3622 [==============================] - 1s 373us/step - loss: 0.0022 - val_loss: 0.0023\n",
      "Epoch 10/24\n",
      "3622/3622 [==============================] - ETA: 0s - loss: 0.002 - 1s 371us/step - loss: 0.0020 - val_loss: 0.0021\n",
      "Epoch 11/24\n",
      "3622/3622 [==============================] - 1s 372us/step - loss: 0.0019 - val_loss: 0.0025\n",
      "Epoch 12/24\n",
      "3622/3622 [==============================] - 1s 372us/step - loss: 0.0018 - val_loss: 0.0029\n",
      "Epoch 13/24\n",
      "3622/3622 [==============================] - 1s 372us/step - loss: 0.0018 - val_loss: 0.0024\n",
      "Epoch 14/24\n",
      "3622/3622 [==============================] - 1s 373us/step - loss: 0.0017 - val_loss: 0.0022\n",
      "Epoch 15/24\n",
      "3622/3622 [==============================] - 1s 373us/step - loss: 0.0017 - val_loss: 0.0024\n",
      "Epoch 16/24\n",
      "3622/3622 [==============================] - ETA: 0s - loss: 0.001 - 1s 374us/step - loss: 0.0017 - val_loss: 0.0027\n",
      "Epoch 17/24\n",
      "3622/3622 [==============================] - 1s 374us/step - loss: 0.0017 - val_loss: 0.0028\n",
      "Epoch 18/24\n",
      "3622/3622 [==============================] - 1s 373us/step - loss: 0.0017 - val_loss: 0.0022\n",
      "Epoch 19/24\n",
      "3622/3622 [==============================] - 1s 371us/step - loss: 0.0017 - val_loss: 0.0028\n",
      "Epoch 20/24\n",
      "3622/3622 [==============================] - ETA: 0s - loss: 0.001 - 1s 373us/step - loss: 0.0017 - val_loss: 0.0028\n",
      "Epoch 21/24\n",
      "3622/3622 [==============================] - 1s 382us/step - loss: 0.0017 - val_loss: 0.0028\n",
      "Epoch 22/24\n",
      "3622/3622 [==============================] - 1s 378us/step - loss: 0.0017 - val_loss: 0.0030\n",
      "Epoch 23/24\n",
      "3622/3622 [==============================] - 1s 372us/step - loss: 0.0017 - val_loss: 0.0026\n",
      "Epoch 24/24\n",
      "3622/3622 [==============================] - 1s 371us/step - loss: 0.0017 - val_loss: 0.0027\n",
      "Train on 3622 samples, validate on 403 samples\n",
      "Epoch 1/24\n",
      "3622/3622 [==============================] - 10s 3ms/step - loss: 0.1256 - val_loss: 0.0773\n",
      "Epoch 2/24\n",
      "3622/3622 [==============================] - 1s 373us/step - loss: 0.0616 - val_loss: 0.0409\n",
      "Epoch 3/24\n",
      "3622/3622 [==============================] - 1s 374us/step - loss: 0.0523 - val_loss: 0.0289\n",
      "Epoch 4/24\n",
      "3622/3622 [==============================] - 1s 379us/step - loss: 0.0486 - val_loss: 0.0257\n",
      "Epoch 5/24\n",
      "3622/3622 [==============================] - 1s 373us/step - loss: 0.0459 - val_loss: 0.0226\n",
      "Epoch 6/24\n",
      "3622/3622 [==============================] - 1s 376us/step - loss: 0.0431 - val_loss: 0.0213\n",
      "Epoch 7/24\n",
      "3622/3622 [==============================] - 1s 374us/step - loss: 0.0403 - val_loss: 0.0191\n",
      "Epoch 8/24\n",
      "3622/3622 [==============================] - 1s 375us/step - loss: 0.0375 - val_loss: 0.0189\n",
      "Epoch 9/24\n",
      "3622/3622 [==============================] - 1s 371us/step - loss: 0.0346 - val_loss: 0.0164\n",
      "Epoch 10/24\n",
      "3622/3622 [==============================] - 1s 374us/step - loss: 0.0316 - val_loss: 0.0141\n",
      "Epoch 11/24\n",
      "3622/3622 [==============================] - 1s 371us/step - loss: 0.0286 - val_loss: 0.0141\n",
      "Epoch 12/24\n",
      "3622/3622 [==============================] - 1s 373us/step - loss: 0.0257 - val_loss: 0.0117\n",
      "Epoch 13/24\n",
      "3622/3622 [==============================] - 1s 374us/step - loss: 0.0229 - val_loss: 0.0107\n",
      "Epoch 14/24\n",
      "3622/3622 [==============================] - 1s 370us/step - loss: 0.0202 - val_loss: 0.0092\n",
      "Epoch 15/24\n",
      "3622/3622 [==============================] - 1s 373us/step - loss: 0.0179 - val_loss: 0.0075\n",
      "Epoch 16/24\n",
      "3622/3622 [==============================] - 1s 372us/step - loss: 0.0157 - val_loss: 0.0059\n",
      "Epoch 17/24\n",
      "3622/3622 [==============================] - 1s 377us/step - loss: 0.0137 - val_loss: 0.0044\n",
      "Epoch 18/24\n",
      "3622/3622 [==============================] - 1s 374us/step - loss: 0.0120 - val_loss: 0.0036\n",
      "Epoch 19/24\n",
      "3622/3622 [==============================] - 1s 370us/step - loss: 0.0105 - val_loss: 0.0029\n",
      "Epoch 20/24\n",
      "3622/3622 [==============================] - 1s 379us/step - loss: 0.0093 - val_loss: 0.0027\n",
      "Epoch 21/24\n",
      "3622/3622 [==============================] - 1s 372us/step - loss: 0.0083 - val_loss: 0.0023\n",
      "Epoch 22/24\n",
      "3622/3622 [==============================] - 1s 370us/step - loss: 0.0075 - val_loss: 0.0021\n",
      "Epoch 23/24\n",
      "3622/3622 [==============================] - 1s 378us/step - loss: 0.0069 - val_loss: 0.0020\n",
      "Epoch 24/24\n",
      "3622/3622 [==============================] - 1s 381us/step - loss: 0.0063 - val_loss: 0.0021\n",
      "\n",
      "GENERATION 9\n",
      "\n",
      "Train on 3622 samples, validate on 403 samples\n",
      "Epoch 1/24\n",
      "3622/3622 [==============================] - 9s 3ms/step - loss: 0.1738 - val_loss: 0.0190\n",
      "Epoch 2/24\n",
      "3622/3622 [==============================] - 1s 239us/step - loss: 0.0164 - val_loss: 0.0175\n",
      "Epoch 3/24\n",
      "3622/3622 [==============================] - 1s 238us/step - loss: 0.0073 - val_loss: 0.0085\n",
      "Epoch 4/24\n",
      "3622/3622 [==============================] - 1s 237us/step - loss: 0.0035 - val_loss: 0.0025\n",
      "Epoch 5/24\n",
      "3622/3622 [==============================] - 1s 237us/step - loss: 0.0016 - val_loss: 0.0010\n",
      "Epoch 6/24\n",
      "3622/3622 [==============================] - 1s 236us/step - loss: 9.5691e-04 - val_loss: 0.0013\n",
      "Epoch 7/24\n",
      "3622/3622 [==============================] - 1s 238us/step - loss: 7.9916e-04 - val_loss: 7.6561e-04\n",
      "Epoch 8/24\n",
      "3622/3622 [==============================] - 1s 237us/step - loss: 7.4532e-04 - val_loss: 7.6647e-04\n",
      "Epoch 9/24\n",
      "3622/3622 [==============================] - 1s 237us/step - loss: 7.1100e-04 - val_loss: 8.6148e-04\n",
      "Epoch 10/24\n",
      "3622/3622 [==============================] - 1s 237us/step - loss: 6.9145e-04 - val_loss: 7.8937e-04\n",
      "Epoch 11/24\n",
      "3622/3622 [==============================] - 1s 236us/step - loss: 6.7400e-04 - val_loss: 7.1335e-04\n",
      "Epoch 12/24\n",
      "3622/3622 [==============================] - 1s 237us/step - loss: 6.4588e-04 - val_loss: 6.7486e-04\n",
      "Epoch 13/24\n",
      "3622/3622 [==============================] - 1s 241us/step - loss: 6.1930e-04 - val_loss: 6.5735e-04\n",
      "Epoch 14/24\n",
      "3622/3622 [==============================] - 1s 240us/step - loss: 5.7760e-04 - val_loss: 5.8853e-04\n",
      "Epoch 15/24\n",
      "3622/3622 [==============================] - 1s 237us/step - loss: 5.3249e-04 - val_loss: 7.9225e-04\n",
      "Epoch 16/24\n",
      "3622/3622 [==============================] - 1s 240us/step - loss: 5.2679e-04 - val_loss: 5.4227e-04\n",
      "Epoch 17/24\n",
      "3622/3622 [==============================] - 1s 238us/step - loss: 4.7506e-04 - val_loss: 5.8729e-04\n",
      "Epoch 18/24\n",
      "3622/3622 [==============================] - 1s 237us/step - loss: 4.7606e-04 - val_loss: 5.1667e-04\n",
      "Epoch 19/24\n",
      "3622/3622 [==============================] - 1s 236us/step - loss: 4.4357e-04 - val_loss: 5.0727e-04\n",
      "Epoch 20/24\n",
      "3622/3622 [==============================] - 1s 241us/step - loss: 4.1856e-04 - val_loss: 5.7340e-04\n",
      "Epoch 21/24\n",
      "3622/3622 [==============================] - 1s 240us/step - loss: 4.4821e-04 - val_loss: 6.3256e-04\n",
      "Epoch 22/24\n",
      "3622/3622 [==============================] - 1s 237us/step - loss: 4.3346e-04 - val_loss: 4.6911e-04\n",
      "Epoch 23/24\n",
      "3622/3622 [==============================] - 1s 237us/step - loss: 3.9712e-04 - val_loss: 4.4792e-04\n",
      "Epoch 24/24\n",
      "3622/3622 [==============================] - 1s 238us/step - loss: 3.7754e-04 - val_loss: 4.1188e-04\n",
      "Train on 3622 samples, validate on 403 samples\n",
      "Epoch 1/24\n",
      "3622/3622 [==============================] - 9s 3ms/step - loss: 0.0925 - val_loss: 0.0156\n",
      "Epoch 2/24\n",
      "3622/3622 [==============================] - 1s 207us/step - loss: 0.0104 - val_loss: 0.0043\n",
      "Epoch 3/24\n",
      "3622/3622 [==============================] - 1s 207us/step - loss: 0.0049 - val_loss: 0.0011\n",
      "Epoch 4/24\n",
      "3622/3622 [==============================] - 1s 206us/step - loss: 0.0023 - val_loss: 0.0012\n",
      "Epoch 5/24\n",
      "3622/3622 [==============================] - 1s 206us/step - loss: 0.0012 - val_loss: 7.7627e-04\n",
      "Epoch 6/24\n",
      "3622/3622 [==============================] - 1s 207us/step - loss: 9.4706e-04 - val_loss: 0.0017\n",
      "Epoch 7/24\n",
      "3622/3622 [==============================] - 1s 205us/step - loss: 7.3169e-04 - val_loss: 6.6120e-04\n",
      "Epoch 8/24\n",
      "3622/3622 [==============================] - 1s 205us/step - loss: 6.7973e-04 - val_loss: 7.9452e-04\n",
      "Epoch 9/24\n",
      "3622/3622 [==============================] - 1s 207us/step - loss: 6.2552e-04 - val_loss: 7.1191e-04\n",
      "Epoch 10/24\n",
      "3622/3622 [==============================] - 1s 210us/step - loss: 6.0262e-04 - val_loss: 6.6625e-04\n",
      "Epoch 11/24\n",
      "3622/3622 [==============================] - 1s 208us/step - loss: 5.9195e-04 - val_loss: 6.9118e-04\n",
      "Epoch 12/24\n",
      "3622/3622 [==============================] - 1s 207us/step - loss: 5.7522e-04 - val_loss: 6.6014e-04\n",
      "Epoch 13/24\n",
      "3622/3622 [==============================] - 1s 207us/step - loss: 5.5374e-04 - val_loss: 5.8833e-04\n",
      "Epoch 14/24\n",
      "3622/3622 [==============================] - 1s 205us/step - loss: 5.5368e-04 - val_loss: 5.8619e-04\n",
      "Epoch 15/24\n",
      "3622/3622 [==============================] - 1s 206us/step - loss: 5.2335e-04 - val_loss: 6.1171e-04\n",
      "Epoch 16/24\n",
      "3622/3622 [==============================] - 1s 205us/step - loss: 5.0186e-04 - val_loss: 5.6206e-04\n",
      "Epoch 17/24\n",
      "3622/3622 [==============================] - 1s 205us/step - loss: 4.7295e-04 - val_loss: 4.9572e-04\n",
      "Epoch 18/24\n",
      "3622/3622 [==============================] - 1s 206us/step - loss: 4.4130e-04 - val_loss: 4.8833e-04\n",
      "Epoch 19/24\n",
      "3622/3622 [==============================] - 1s 205us/step - loss: 4.2119e-04 - val_loss: 4.5743e-04\n",
      "Epoch 20/24\n",
      "3622/3622 [==============================] - 1s 206us/step - loss: 4.1584e-04 - val_loss: 4.2609e-04\n",
      "Epoch 21/24\n",
      "3622/3622 [==============================] - 1s 206us/step - loss: 3.9263e-04 - val_loss: 4.1008e-04\n",
      "Epoch 22/24\n",
      "3622/3622 [==============================] - 1s 204us/step - loss: 3.7691e-04 - val_loss: 3.9909e-04\n",
      "Epoch 23/24\n",
      "3622/3622 [==============================] - 1s 206us/step - loss: 3.7297e-04 - val_loss: 8.6388e-04\n",
      "Epoch 24/24\n",
      "3622/3622 [==============================] - 1s 206us/step - loss: 5.0617e-04 - val_loss: 8.3715e-04\n",
      "Train on 3622 samples, validate on 403 samples\n",
      "Epoch 1/24\n",
      "3622/3622 [==============================] - 10s 3ms/step - loss: 0.0745 - val_loss: 0.0033\n",
      "Epoch 2/24\n",
      "3622/3622 [==============================] - 1s 208us/step - loss: 0.0098 - val_loss: 0.0066\n",
      "Epoch 3/24\n",
      "3622/3622 [==============================] - 1s 209us/step - loss: 0.0045 - val_loss: 0.0045\n",
      "Epoch 4/24\n",
      "3622/3622 [==============================] - 1s 207us/step - loss: 0.0023 - val_loss: 0.0027\n",
      "Epoch 5/24\n",
      "3622/3622 [==============================] - 1s 207us/step - loss: 0.0010 - val_loss: 8.8098e-04\n",
      "Epoch 6/24\n",
      "3622/3622 [==============================] - 1s 212us/step - loss: 7.6525e-04 - val_loss: 7.6371e-04\n",
      "Epoch 7/24\n",
      "3622/3622 [==============================] - 1s 209us/step - loss: 6.9083e-04 - val_loss: 7.2492e-04\n",
      "Epoch 8/24\n",
      "3622/3622 [==============================] - 1s 208us/step - loss: 6.1339e-04 - val_loss: 7.0145e-04\n",
      "Epoch 9/24\n",
      "3622/3622 [==============================] - 1s 207us/step - loss: 5.9048e-04 - val_loss: 6.3783e-04\n",
      "Epoch 10/24\n",
      "3622/3622 [==============================] - 1s 210us/step - loss: 5.6637e-04 - val_loss: 6.4430e-04\n",
      "Epoch 11/24\n",
      "3622/3622 [==============================] - 1s 208us/step - loss: 5.5721e-04 - val_loss: 7.3168e-04\n",
      "Epoch 12/24\n",
      "3622/3622 [==============================] - 1s 208us/step - loss: 5.5187e-04 - val_loss: 5.9803e-04\n",
      "Epoch 13/24\n",
      "3622/3622 [==============================] - 1s 208us/step - loss: 5.0925e-04 - val_loss: 5.1322e-04\n",
      "Epoch 14/24\n",
      "3622/3622 [==============================] - 1s 210us/step - loss: 4.8942e-04 - val_loss: 4.9250e-04\n",
      "Epoch 15/24\n",
      "3622/3622 [==============================] - 1s 208us/step - loss: 4.6462e-04 - val_loss: 4.7302e-04\n",
      "Epoch 16/24\n",
      "3622/3622 [==============================] - 1s 208us/step - loss: 4.4323e-04 - val_loss: 4.6301e-04\n",
      "Epoch 17/24\n",
      "3622/3622 [==============================] - 1s 210us/step - loss: 4.3078e-04 - val_loss: 4.3490e-04\n",
      "Epoch 18/24\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3622/3622 [==============================] - 1s 208us/step - loss: 4.0444e-04 - val_loss: 4.3338e-04\n",
      "Epoch 19/24\n",
      "3622/3622 [==============================] - 1s 207us/step - loss: 3.9994e-04 - val_loss: 5.1418e-04\n",
      "Epoch 20/24\n",
      "3622/3622 [==============================] - 1s 209us/step - loss: 4.3285e-04 - val_loss: 3.9643e-04\n",
      "Epoch 21/24\n",
      "3622/3622 [==============================] - 1s 211us/step - loss: 3.6674e-04 - val_loss: 3.8844e-04\n",
      "Epoch 22/24\n",
      "3622/3622 [==============================] - 1s 210us/step - loss: 3.5730e-04 - val_loss: 5.3805e-04\n",
      "Epoch 23/24\n",
      "3622/3622 [==============================] - 1s 209us/step - loss: 3.9529e-04 - val_loss: 6.7236e-04\n",
      "Epoch 24/24\n",
      "3622/3622 [==============================] - 1s 207us/step - loss: 4.8933e-04 - val_loss: 3.9730e-04\n",
      "Train on 3622 samples, validate on 403 samples\n",
      "Epoch 1/24\n",
      "3622/3622 [==============================] - 10s 3ms/step - loss: 0.1391 - val_loss: 0.0357\n",
      "Epoch 2/24\n",
      "3622/3622 [==============================] - 1s 267us/step - loss: 0.0169 - val_loss: 0.0112\n",
      "Epoch 3/24\n",
      "3622/3622 [==============================] - 1s 265us/step - loss: 0.0075 - val_loss: 8.9983e-04\n",
      "Epoch 4/24\n",
      "3622/3622 [==============================] - 1s 266us/step - loss: 0.0043 - val_loss: 0.0017\n",
      "Epoch 5/24\n",
      "3622/3622 [==============================] - 1s 265us/step - loss: 0.0026 - val_loss: 8.8871e-04\n",
      "Epoch 6/24\n",
      "3622/3622 [==============================] - 1s 265us/step - loss: 0.0024 - val_loss: 7.3341e-04\n",
      "Epoch 7/24\n",
      "3622/3622 [==============================] - 1s 265us/step - loss: 0.0020 - val_loss: 0.0018\n",
      "Epoch 8/24\n",
      "3622/3622 [==============================] - 1s 268us/step - loss: 0.0018 - val_loss: 7.1300e-04\n",
      "Epoch 9/24\n",
      "3622/3622 [==============================] - 1s 268us/step - loss: 0.0017 - val_loss: 0.0012\n",
      "Epoch 10/24\n",
      "3622/3622 [==============================] - 1s 268us/step - loss: 0.0016 - val_loss: 0.0011\n",
      "Epoch 11/24\n",
      "3622/3622 [==============================] - 1s 268us/step - loss: 0.0015 - val_loss: 8.6206e-04\n",
      "Epoch 12/24\n",
      "3622/3622 [==============================] - 1s 266us/step - loss: 0.0015 - val_loss: 9.0478e-04\n",
      "Epoch 13/24\n",
      "3622/3622 [==============================] - 1s 265us/step - loss: 0.0014 - val_loss: 0.0012\n",
      "Epoch 14/24\n",
      "3622/3622 [==============================] - 1s 264us/step - loss: 0.0013 - val_loss: 0.0015\n",
      "Epoch 15/24\n",
      "3622/3622 [==============================] - 1s 265us/step - loss: 0.0013 - val_loss: 0.0012\n",
      "Epoch 16/24\n",
      "3622/3622 [==============================] - 1s 275us/step - loss: 0.0014 - val_loss: 9.0129e-04\n",
      "Epoch 17/24\n",
      "3622/3622 [==============================] - 1s 267us/step - loss: 0.0013 - val_loss: 9.6246e-04\n",
      "Epoch 18/24\n",
      "3622/3622 [==============================] - 1s 267us/step - loss: 0.0012 - val_loss: 0.0012\n",
      "Epoch 19/24\n",
      "3622/3622 [==============================] - 1s 267us/step - loss: 0.0012 - val_loss: 0.0015\n",
      "Epoch 20/24\n",
      "3622/3622 [==============================] - 1s 264us/step - loss: 0.0012 - val_loss: 0.0018\n",
      "Epoch 21/24\n",
      "3622/3622 [==============================] - 1s 267us/step - loss: 0.0011 - val_loss: 0.0022\n",
      "Epoch 22/24\n",
      "3622/3622 [==============================] - 1s 266us/step - loss: 0.0012 - val_loss: 0.0022\n",
      "Epoch 23/24\n",
      "3622/3622 [==============================] - 1s 265us/step - loss: 0.0011 - val_loss: 0.0015\n",
      "Epoch 24/24\n",
      "3622/3622 [==============================] - 1s 264us/step - loss: 0.0010 - val_loss: 0.0014\n",
      "Train on 3622 samples, validate on 403 samples\n",
      "Epoch 1/24\n",
      "3622/3622 [==============================] - 11s 3ms/step - loss: 0.2349 - val_loss: 0.2622\n",
      "Epoch 2/24\n",
      "3622/3622 [==============================] - 2s 577us/step - loss: 0.1564 - val_loss: 0.1493\n",
      "Epoch 3/24\n",
      "3622/3622 [==============================] - 2s 582us/step - loss: 0.1016 - val_loss: 0.0857\n",
      "Epoch 4/24\n",
      "3622/3622 [==============================] - 2s 573us/step - loss: 0.0757 - val_loss: 0.0564\n",
      "Epoch 5/24\n",
      "3622/3622 [==============================] - 2s 580us/step - loss: 0.0653 - val_loss: 0.0419\n",
      "Epoch 6/24\n",
      "3622/3622 [==============================] - 2s 578us/step - loss: 0.0608 - val_loss: 0.0327\n",
      "Epoch 7/24\n",
      "3622/3622 [==============================] - 2s 579us/step - loss: 0.0577 - val_loss: 0.0281\n",
      "Epoch 8/24\n",
      "3622/3622 [==============================] - 2s 576us/step - loss: 0.0555 - val_loss: 0.0249\n",
      "Epoch 9/24\n",
      "3622/3622 [==============================] - 2s 577us/step - loss: 0.0533 - val_loss: 0.0232\n",
      "Epoch 10/24\n",
      "3622/3622 [==============================] - 2s 582us/step - loss: 0.0515 - val_loss: 0.0236\n",
      "Epoch 11/24\n",
      "3622/3622 [==============================] - 2s 583us/step - loss: 0.0498 - val_loss: 0.0217\n",
      "Epoch 12/24\n",
      "3622/3622 [==============================] - 2s 582us/step - loss: 0.0480 - val_loss: 0.0204\n",
      "Epoch 13/24\n",
      "3622/3622 [==============================] - 2s 580us/step - loss: 0.0463 - val_loss: 0.0189\n",
      "Epoch 14/24\n",
      "3622/3622 [==============================] - 2s 580us/step - loss: 0.0441 - val_loss: 0.0179\n",
      "Epoch 15/24\n",
      "3622/3622 [==============================] - 2s 581us/step - loss: 0.0420 - val_loss: 0.0155\n",
      "Epoch 16/24\n",
      "3622/3622 [==============================] - 2s 578us/step - loss: 0.0403 - val_loss: 0.0148\n",
      "Epoch 17/24\n",
      "3622/3622 [==============================] - 2s 580us/step - loss: 0.0380 - val_loss: 0.0150\n",
      "Epoch 18/24\n",
      "3622/3622 [==============================] - 2s 580us/step - loss: 0.0356 - val_loss: 0.0124\n",
      "Epoch 19/24\n",
      "3622/3622 [==============================] - 2s 578us/step - loss: 0.0331 - val_loss: 0.0114\n",
      "Epoch 20/24\n",
      "3622/3622 [==============================] - 2s 584us/step - loss: 0.0303 - val_loss: 0.0096\n",
      "Epoch 21/24\n",
      "3622/3622 [==============================] - 2s 578us/step - loss: 0.0284 - val_loss: 0.0074\n",
      "Epoch 22/24\n",
      "3622/3622 [==============================] - 2s 578us/step - loss: 0.0256 - val_loss: 0.0063\n",
      "Epoch 23/24\n",
      "3622/3622 [==============================] - 2s 582us/step - loss: 0.0230 - val_loss: 0.0049\n",
      "Epoch 24/24\n",
      "3622/3622 [==============================] - 2s 582us/step - loss: 0.0206 - val_loss: 0.0045\n",
      "Train on 3622 samples, validate on 403 samples\n",
      "Epoch 1/24\n",
      "3622/3622 [==============================] - 11s 3ms/step - loss: nan - val_loss: nan\n",
      "Epoch 2/24\n",
      "3622/3622 [==============================] - 2s 523us/step - loss: nan - val_loss: nan\n",
      "Epoch 3/24\n",
      "3622/3622 [==============================] - 2s 525us/step - loss: nan - val_loss: nan\n",
      "Epoch 4/24\n",
      "3622/3622 [==============================] - 2s 527us/step - loss: nan - val_loss: nan\n",
      "Epoch 5/24\n",
      "3622/3622 [==============================] - 2s 527us/step - loss: nan - val_loss: nan\n",
      "Epoch 6/24\n",
      "3622/3622 [==============================] - 2s 529us/step - loss: nan - val_loss: nan\n",
      "Epoch 7/24\n",
      "3622/3622 [==============================] - 2s 526us/step - loss: nan - val_loss: nan\n",
      "Epoch 8/24\n",
      "3622/3622 [==============================] - 2s 523us/step - loss: nan - val_loss: nan\n",
      "Epoch 9/24\n",
      "3622/3622 [==============================] - 2s 526us/step - loss: nan - val_loss: nan\n",
      "Epoch 10/24\n",
      "3622/3622 [==============================] - 2s 525us/step - loss: nan - val_loss: nan\n",
      "Epoch 11/24\n",
      "3622/3622 [==============================] - 2s 526us/step - loss: nan - val_loss: nan\n",
      "Epoch 12/24\n",
      "3622/3622 [==============================] - 2s 526us/step - loss: nan - val_loss: nan\n",
      "Epoch 13/24\n",
      "3622/3622 [==============================] - 2s 523us/step - loss: nan - val_loss: nan\n",
      "Epoch 14/24\n",
      "3622/3622 [==============================] - 2s 525us/step - loss: nan - val_loss: nan\n",
      "Epoch 15/24\n",
      "3622/3622 [==============================] - 2s 524us/step - loss: nan - val_loss: nan\n",
      "Epoch 16/24\n",
      "3622/3622 [==============================] - 2s 525us/step - loss: nan - val_loss: nan\n",
      "Epoch 17/24\n",
      "3622/3622 [==============================] - 2s 530us/step - loss: nan - val_loss: nan\n",
      "Epoch 18/24\n",
      "3622/3622 [==============================] - 2s 528us/step - loss: nan - val_loss: nan\n",
      "Epoch 19/24\n",
      "3622/3622 [==============================] - 2s 525us/step - loss: nan - val_loss: nan\n",
      "Epoch 20/24\n",
      "3622/3622 [==============================] - 2s 528us/step - loss: nan - val_loss: nan\n",
      "Epoch 21/24\n",
      "3622/3622 [==============================] - 2s 529us/step - loss: nan - val_loss: nan\n",
      "Epoch 22/24\n",
      "3622/3622 [==============================] - 2s 528us/step - loss: nan - val_loss: nan\n",
      "Epoch 23/24\n",
      "3622/3622 [==============================] - 2s 527us/step - loss: nan - val_loss: nan\n",
      "Epoch 24/24\n",
      "3622/3622 [==============================] - 2s 523us/step - loss: nan - val_loss: nan\n",
      "Train on 3622 samples, validate on 403 samples\n",
      "Epoch 1/24\n",
      "3622/3622 [==============================] - 11s 3ms/step - loss: nan - val_loss: nan\n",
      "Epoch 2/24\n",
      "3622/3622 [==============================] - 2s 526us/step - loss: nan - val_loss: nan\n",
      "Epoch 3/24\n",
      "3622/3622 [==============================] - 2s 526us/step - loss: nan - val_loss: nan\n",
      "Epoch 4/24\n",
      "3622/3622 [==============================] - 2s 526us/step - loss: nan - val_loss: nan\n",
      "Epoch 5/24\n",
      "3622/3622 [==============================] - 2s 527us/step - loss: nan - val_loss: nan\n",
      "Epoch 6/24\n",
      "3622/3622 [==============================] - 2s 534us/step - loss: nan - val_loss: nan\n",
      "Epoch 7/24\n",
      "3622/3622 [==============================] - 2s 525us/step - loss: nan - val_loss: nan\n",
      "Epoch 8/24\n",
      "3622/3622 [==============================] - 2s 526us/step - loss: nan - val_loss: nan\n",
      "Epoch 9/24\n",
      "3622/3622 [==============================] - 2s 526us/step - loss: nan - val_loss: nan\n",
      "Epoch 10/24\n",
      "3622/3622 [==============================] - 2s 523us/step - loss: nan - val_loss: nan\n",
      "Epoch 11/24\n",
      "3622/3622 [==============================] - 2s 540us/step - loss: nan - val_loss: nan\n",
      "Epoch 12/24\n",
      "3622/3622 [==============================] - 2s 633us/step - loss: nan - val_loss: nan\n",
      "Epoch 13/24\n",
      "3622/3622 [==============================] - 2s 631us/step - loss: nan - val_loss: nan\n",
      "Epoch 14/24\n",
      "3622/3622 [==============================] - 2s 633us/step - loss: nan - val_loss: nan\n",
      "Epoch 15/24\n",
      "3622/3622 [==============================] - 2s 628us/step - loss: nan - val_loss: nan\n",
      "Epoch 16/24\n",
      "3622/3622 [==============================] - 2s 626us/step - loss: nan - val_loss: nan\n",
      "Epoch 17/24\n",
      "3622/3622 [==============================] - 2s 624us/step - loss: nan - val_loss: nan\n",
      "Epoch 18/24\n",
      "3622/3622 [==============================] - 2s 621us/step - loss: nan - val_loss: nan\n",
      "Epoch 19/24\n",
      "3622/3622 [==============================] - 2s 625us/step - loss: nan - val_loss: nan\n",
      "Epoch 20/24\n",
      "3622/3622 [==============================] - 2s 620us/step - loss: nan - val_loss: nan\n",
      "Epoch 21/24\n",
      "3622/3622 [==============================] - 2s 631us/step - loss: nan - val_loss: nan\n",
      "Epoch 22/24\n",
      "3622/3622 [==============================] - 2s 620us/step - loss: nan - val_loss: nan\n",
      "Epoch 23/24\n",
      "3622/3622 [==============================] - 2s 626us/step - loss: nan - val_loss: nan\n",
      "Epoch 24/24\n",
      "3622/3622 [==============================] - 2s 630us/step - loss: nan - val_loss: nan\n",
      "Train on 3622 samples, validate on 403 samples\n",
      "Epoch 1/24\n",
      "3622/3622 [==============================] - 15s 4ms/step - loss: nan - val_loss: nan\n",
      "Epoch 2/24\n",
      "3622/3622 [==============================] - 2s 645us/step - loss: nan - val_loss: nan\n",
      "Epoch 3/24\n",
      "3622/3622 [==============================] - 2s 631us/step - loss: nan - val_loss: nan\n",
      "Epoch 4/24\n",
      "3622/3622 [==============================] - 2s 637us/step - loss: nan - val_loss: nan\n",
      "Epoch 5/24\n",
      "3622/3622 [==============================] - 2s 649us/step - loss: nan - val_loss: nan\n",
      "Epoch 6/24\n",
      "3622/3622 [==============================] - 2s 637us/step - loss: nan - val_loss: nan\n",
      "Epoch 7/24\n",
      "3622/3622 [==============================] - 2s 632us/step - loss: nan - val_loss: nan\n",
      "Epoch 8/24\n",
      "3622/3622 [==============================] - 2s 630us/step - loss: nan - val_loss: nan\n",
      "Epoch 9/24\n",
      "3622/3622 [==============================] - 2s 633us/step - loss: nan - val_loss: nan\n",
      "Epoch 10/24\n",
      "3622/3622 [==============================] - 2s 637us/step - loss: nan - val_loss: nan\n",
      "Epoch 11/24\n",
      "3622/3622 [==============================] - 2s 634us/step - loss: nan - val_loss: nan\n",
      "Epoch 12/24\n",
      "3622/3622 [==============================] - 2s 636us/step - loss: nan - val_loss: nan\n",
      "Epoch 13/24\n",
      "3622/3622 [==============================] - 2s 631us/step - loss: nan - val_loss: nan\n",
      "Epoch 14/24\n",
      "3622/3622 [==============================] - 2s 642us/step - loss: nan - val_loss: nan\n",
      "Epoch 15/24\n",
      "3622/3622 [==============================] - 2s 637us/step - loss: nan - val_loss: nan\n",
      "Epoch 16/24\n",
      "3622/3622 [==============================] - 2s 626us/step - loss: nan - val_loss: nan\n",
      "Epoch 17/24\n",
      "3622/3622 [==============================] - 2s 627us/step - loss: nan - val_loss: nan\n",
      "Epoch 18/24\n",
      "3622/3622 [==============================] - 2s 628us/step - loss: nan - val_loss: nan\n",
      "Epoch 19/24\n",
      "3622/3622 [==============================] - 2s 635us/step - loss: nan - val_loss: nan\n",
      "Epoch 20/24\n",
      "3622/3622 [==============================] - 2s 635us/step - loss: nan - val_loss: nan\n",
      "Epoch 21/24\n",
      "3622/3622 [==============================] - 2s 637us/step - loss: nan - val_loss: nan\n",
      "Epoch 22/24\n",
      "3622/3622 [==============================] - 2s 628us/step - loss: nan - val_loss: nan\n",
      "Epoch 23/24\n",
      "3622/3622 [==============================] - 2s 634us/step - loss: nan - val_loss: nan\n",
      "Epoch 24/24\n",
      "3622/3622 [==============================] - 2s 632us/step - loss: nan - val_loss: nan\n",
      "Train on 3622 samples, validate on 403 samples\n",
      "Epoch 1/24\n",
      "3622/3622 [==============================] - 15s 4ms/step - loss: 0.1633 - val_loss: 0.0307\n",
      "Epoch 2/24\n",
      "3622/3622 [==============================] - 2s 489us/step - loss: 0.0156 - val_loss: 0.0068\n",
      "Epoch 3/24\n",
      "3622/3622 [==============================] - 2s 484us/step - loss: 0.0083 - val_loss: 0.0064\n",
      "Epoch 4/24\n",
      "3622/3622 [==============================] - 2s 473us/step - loss: 0.0059 - val_loss: 0.0060\n",
      "Epoch 5/24\n",
      "3622/3622 [==============================] - 2s 483us/step - loss: 0.0042 - val_loss: 0.0011\n",
      "Epoch 6/24\n",
      "3622/3622 [==============================] - 2s 477us/step - loss: 0.0040 - val_loss: 0.0022\n",
      "Epoch 7/24\n",
      "3622/3622 [==============================] - 2s 475us/step - loss: 0.0036 - val_loss: 0.0022\n",
      "Epoch 8/24\n",
      "3622/3622 [==============================] - 2s 475us/step - loss: 0.0033 - val_loss: 0.0015\n",
      "Epoch 9/24\n",
      "3622/3622 [==============================] - 2s 478us/step - loss: 0.0033 - val_loss: 0.0018\n",
      "Epoch 10/24\n",
      "3622/3622 [==============================] - 2s 471us/step - loss: 0.0031 - val_loss: 0.0017\n",
      "Epoch 11/24\n",
      "3622/3622 [==============================] - 2s 481us/step - loss: 0.0030 - val_loss: 0.0017\n",
      "Epoch 12/24\n",
      "3622/3622 [==============================] - 2s 473us/step - loss: 0.0030 - val_loss: 0.0015\n",
      "Epoch 13/24\n",
      "3622/3622 [==============================] - 2s 479us/step - loss: 0.0028 - val_loss: 0.0017\n",
      "Epoch 14/24\n",
      "3622/3622 [==============================] - 2s 472us/step - loss: 0.0028 - val_loss: 0.0016\n",
      "Epoch 15/24\n",
      "3622/3622 [==============================] - 2s 478us/step - loss: 0.0028 - val_loss: 0.0017\n",
      "Epoch 16/24\n",
      "3622/3622 [==============================] - 2s 480us/step - loss: 0.0030 - val_loss: 0.0017\n",
      "Epoch 17/24\n",
      "3622/3622 [==============================] - 2s 482us/step - loss: 0.0027 - val_loss: 0.0016\n",
      "Epoch 18/24\n",
      "3622/3622 [==============================] - 2s 478us/step - loss: 0.0026 - val_loss: 0.0017\n",
      "Epoch 19/24\n",
      "3622/3622 [==============================] - 2s 484us/step - loss: 0.0026 - val_loss: 0.0014\n",
      "Epoch 20/24\n",
      "3622/3622 [==============================] - 2s 480us/step - loss: 0.0026 - val_loss: 0.0017\n",
      "Epoch 21/24\n",
      "3622/3622 [==============================] - 2s 487us/step - loss: 0.0026 - val_loss: 0.0015\n",
      "Epoch 22/24\n",
      "3622/3622 [==============================] - 2s 476us/step - loss: 0.0026 - val_loss: 0.0014\n",
      "Epoch 23/24\n",
      "3622/3622 [==============================] - 2s 486us/step - loss: 0.0025 - val_loss: 0.0015\n",
      "Epoch 24/24\n",
      "3622/3622 [==============================] - 2s 480us/step - loss: 0.0025 - val_loss: 0.0013\n",
      "\n",
      "GENERATION 10\n",
      "\n",
      "Train on 3622 samples, validate on 403 samples\n",
      "Epoch 1/24\n",
      "3622/3622 [==============================] - 14s 4ms/step - loss: 0.0820 - val_loss: 0.0023\n",
      "Epoch 2/24\n",
      "3622/3622 [==============================] - 1s 251us/step - loss: 0.0100 - val_loss: 0.0035\n",
      "Epoch 3/24\n",
      "3622/3622 [==============================] - 1s 253us/step - loss: 0.0055 - val_loss: 0.0012\n",
      "Epoch 4/24\n",
      "3622/3622 [==============================] - 1s 250us/step - loss: 0.0030 - val_loss: 8.1287e-04\n",
      "Epoch 5/24\n",
      "3622/3622 [==============================] - 1s 251us/step - loss: 0.0022 - val_loss: 0.0026\n",
      "Epoch 6/24\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3622/3622 [==============================] - 1s 255us/step - loss: 0.0020 - val_loss: 8.9578e-04\n",
      "Epoch 7/24\n",
      "3622/3622 [==============================] - 1s 250us/step - loss: 0.0018 - val_loss: 0.0019\n",
      "Epoch 8/24\n",
      "3622/3622 [==============================] - 1s 251us/step - loss: 0.0016 - val_loss: 0.0039\n",
      "Epoch 9/24\n",
      "3622/3622 [==============================] - 1s 252us/step - loss: 0.0016 - val_loss: 0.0030\n",
      "Epoch 10/24\n",
      "3622/3622 [==============================] - 1s 252us/step - loss: 0.0016 - val_loss: 0.0019\n",
      "Epoch 11/24\n",
      "3622/3622 [==============================] - 1s 258us/step - loss: 0.0014 - val_loss: 0.0027\n",
      "Epoch 12/24\n",
      "3622/3622 [==============================] - 1s 252us/step - loss: 0.0013 - val_loss: 0.0043\n",
      "Epoch 13/24\n",
      "3622/3622 [==============================] - 1s 251us/step - loss: 0.0013 - val_loss: 0.0045\n",
      "Epoch 14/24\n",
      "3622/3622 [==============================] - 1s 253us/step - loss: 0.0012 - val_loss: 0.0062\n",
      "Epoch 15/24\n",
      "3622/3622 [==============================] - 1s 252us/step - loss: 0.0012 - val_loss: 0.0079\n",
      "Epoch 16/24\n",
      "3622/3622 [==============================] - 1s 254us/step - loss: 0.0011 - val_loss: 0.0075\n",
      "Epoch 17/24\n",
      "3622/3622 [==============================] - 1s 248us/step - loss: 0.0011 - val_loss: 0.0075\n",
      "Epoch 18/24\n",
      "3622/3622 [==============================] - 1s 252us/step - loss: 0.0011 - val_loss: 0.0073\n",
      "Epoch 19/24\n",
      "3622/3622 [==============================] - 1s 253us/step - loss: 0.0011 - val_loss: 0.0055\n",
      "Epoch 20/24\n",
      "3622/3622 [==============================] - 1s 254us/step - loss: 0.0010 - val_loss: 0.0081\n",
      "Epoch 21/24\n",
      "3622/3622 [==============================] - 1s 250us/step - loss: 0.0010 - val_loss: 0.0086\n",
      "Epoch 22/24\n",
      "3622/3622 [==============================] - 1s 251us/step - loss: 9.4726e-04 - val_loss: 0.0075\n",
      "Epoch 23/24\n",
      "3622/3622 [==============================] - 1s 263us/step - loss: 9.4577e-04 - val_loss: 0.0072\n",
      "Epoch 24/24\n",
      "3622/3622 [==============================] - 1s 255us/step - loss: 8.6400e-04 - val_loss: 0.0069\n",
      "Train on 3622 samples, validate on 403 samples\n",
      "Epoch 1/24\n",
      "3622/3622 [==============================] - 11s 3ms/step - loss: 0.1336 - val_loss: 0.0626\n",
      "Epoch 2/24\n",
      "3622/3622 [==============================] - 1s 207us/step - loss: 0.0175 - val_loss: 0.0114\n",
      "Epoch 3/24\n",
      "3622/3622 [==============================] - 1s 209us/step - loss: 0.0057 - val_loss: 0.0055\n",
      "Epoch 4/24\n",
      "3622/3622 [==============================] - 1s 209us/step - loss: 0.0025 - val_loss: 8.2064e-04\n",
      "Epoch 5/24\n",
      "3622/3622 [==============================] - 1s 210us/step - loss: 0.0013 - val_loss: 8.6860e-04\n",
      "Epoch 6/24\n",
      "3622/3622 [==============================] - 1s 207us/step - loss: 9.6161e-04 - val_loss: 0.0015\n",
      "Epoch 7/24\n",
      "3622/3622 [==============================] - 1s 207us/step - loss: 8.5120e-04 - val_loss: 6.8013e-04\n",
      "Epoch 8/24\n",
      "3622/3622 [==============================] - 1s 208us/step - loss: 7.5819e-04 - val_loss: 0.0011\n",
      "Epoch 9/24\n",
      "3622/3622 [==============================] - 1s 209us/step - loss: 6.8071e-04 - val_loss: 8.7562e-04\n",
      "Epoch 10/24\n",
      "3622/3622 [==============================] - 1s 208us/step - loss: 6.5742e-04 - val_loss: 6.6784e-04\n",
      "Epoch 11/24\n",
      "3622/3622 [==============================] - 1s 207us/step - loss: 6.2237e-04 - val_loss: 8.4159e-04\n",
      "Epoch 12/24\n",
      "3622/3622 [==============================] - 1s 209us/step - loss: 5.9385e-04 - val_loss: 6.9569e-04\n",
      "Epoch 13/24\n",
      "3622/3622 [==============================] - 1s 211us/step - loss: 5.6846e-04 - val_loss: 6.8072e-04\n",
      "Epoch 14/24\n",
      "3622/3622 [==============================] - 1s 208us/step - loss: 5.5031e-04 - val_loss: 7.1311e-04\n",
      "Epoch 15/24\n",
      "3622/3622 [==============================] - 1s 214us/step - loss: 5.1980e-04 - val_loss: 7.1039e-04\n",
      "Epoch 16/24\n",
      "3622/3622 [==============================] - 1s 210us/step - loss: 5.0731e-04 - val_loss: 5.3756e-04\n",
      "Epoch 17/24\n",
      "3622/3622 [==============================] - 1s 216us/step - loss: 4.7251e-04 - val_loss: 4.7062e-04\n",
      "Epoch 18/24\n",
      "3622/3622 [==============================] - 1s 210us/step - loss: 4.3707e-04 - val_loss: 4.5258e-04\n",
      "Epoch 19/24\n",
      "3622/3622 [==============================] - 1s 210us/step - loss: 4.2243e-04 - val_loss: 4.5044e-04\n",
      "Epoch 20/24\n",
      "3622/3622 [==============================] - 1s 209us/step - loss: 3.9732e-04 - val_loss: 4.3039e-04\n",
      "Epoch 21/24\n",
      "3622/3622 [==============================] - 1s 218us/step - loss: 3.8119e-04 - val_loss: 4.3370e-04\n",
      "Epoch 22/24\n",
      "3622/3622 [==============================] - 1s 208us/step - loss: 3.8257e-04 - val_loss: 3.9845e-04\n",
      "Epoch 23/24\n",
      "3622/3622 [==============================] - 1s 212us/step - loss: 3.6070e-04 - val_loss: 5.6451e-04\n",
      "Epoch 24/24\n",
      "3622/3622 [==============================] - 1s 208us/step - loss: 3.8203e-04 - val_loss: 3.8835e-04\n",
      "Train on 3622 samples, validate on 403 samples\n",
      "Epoch 1/24\n",
      "3622/3622 [==============================] - 11s 3ms/step - loss: 0.0873 - val_loss: 0.0824\n",
      "Epoch 2/24\n",
      "3622/3622 [==============================] - 1s 191us/step - loss: 0.0792 - val_loss: 0.0434\n",
      "Epoch 3/24\n",
      "3622/3622 [==============================] - 1s 188us/step - loss: 0.0685 - val_loss: 0.0177\n",
      "Epoch 4/24\n",
      "3622/3622 [==============================] - 1s 189us/step - loss: 0.0607 - val_loss: 0.0234\n",
      "Epoch 5/24\n",
      "3622/3622 [==============================] - 1s 187us/step - loss: 0.0480 - val_loss: 0.0208\n",
      "Epoch 6/24\n",
      "3622/3622 [==============================] - 1s 188us/step - loss: 0.0338 - val_loss: 0.0070\n",
      "Epoch 7/24\n",
      "3622/3622 [==============================] - 1s 188us/step - loss: 0.0195 - val_loss: 0.0183\n",
      "Epoch 8/24\n",
      "3622/3622 [==============================] - 1s 188us/step - loss: 0.0102 - val_loss: 0.0563\n",
      "Epoch 9/24\n",
      "3622/3622 [==============================] - 1s 190us/step - loss: 0.0074 - val_loss: 0.0643\n",
      "Epoch 10/24\n",
      "3622/3622 [==============================] - 1s 187us/step - loss: 0.0054 - val_loss: 0.0215\n",
      "Epoch 11/24\n",
      "3622/3622 [==============================] - 1s 189us/step - loss: 0.0032 - val_loss: 0.0050\n",
      "Epoch 12/24\n",
      "3622/3622 [==============================] - 1s 188us/step - loss: 0.0025 - val_loss: 0.0104\n",
      "Epoch 13/24\n",
      "3622/3622 [==============================] - 1s 188us/step - loss: 0.0023 - val_loss: 0.0020\n",
      "Epoch 14/24\n",
      "3622/3622 [==============================] - 1s 188us/step - loss: 0.0022 - val_loss: 0.0013\n",
      "Epoch 15/24\n",
      "3622/3622 [==============================] - 1s 190us/step - loss: 0.0019 - val_loss: 0.0015\n",
      "Epoch 16/24\n",
      "3622/3622 [==============================] - 1s 192us/step - loss: 0.0015 - val_loss: 0.0013\n",
      "Epoch 17/24\n",
      "3622/3622 [==============================] - 1s 193us/step - loss: 0.0013 - val_loss: 0.0014\n",
      "Epoch 18/24\n",
      "3622/3622 [==============================] - 1s 189us/step - loss: 0.0012 - val_loss: 0.0012\n",
      "Epoch 19/24\n",
      "3622/3622 [==============================] - 1s 188us/step - loss: 0.0011 - val_loss: 0.0013\n",
      "Epoch 20/24\n",
      "3622/3622 [==============================] - 1s 188us/step - loss: 0.0011 - val_loss: 9.1933e-04\n",
      "Epoch 21/24\n",
      "3622/3622 [==============================] - 1s 203us/step - loss: 0.0010 - val_loss: 0.0021\n",
      "Epoch 22/24\n",
      "3622/3622 [==============================] - 1s 189us/step - loss: 0.0011 - val_loss: 0.0012\n",
      "Epoch 23/24\n",
      "3622/3622 [==============================] - 1s 192us/step - loss: 9.6196e-04 - val_loss: 7.6085e-04\n",
      "Epoch 24/24\n",
      "3622/3622 [==============================] - 1s 189us/step - loss: 0.0010 - val_loss: 0.0011\n",
      "Train on 3622 samples, validate on 403 samples\n",
      "Epoch 1/24\n",
      "3622/3622 [==============================] - 11s 3ms/step - loss: 0.7489 - val_loss: 0.3986\n",
      "Epoch 2/24\n",
      "3622/3622 [==============================] - 1s 189us/step - loss: 0.1539 - val_loss: 0.0404\n",
      "Epoch 3/24\n",
      "3622/3622 [==============================] - 1s 189us/step - loss: 0.0785 - val_loss: 0.0062\n",
      "Epoch 4/24\n",
      "3622/3622 [==============================] - 1s 190us/step - loss: 0.0965 - val_loss: 0.0079\n",
      "Epoch 5/24\n",
      "3622/3622 [==============================] - 1s 191us/step - loss: 0.0791 - val_loss: 0.0260\n",
      "Epoch 6/24\n",
      "3622/3622 [==============================] - 1s 192us/step - loss: 0.0696 - val_loss: 0.0481\n",
      "Epoch 7/24\n",
      "3622/3622 [==============================] - 1s 193us/step - loss: 0.0703 - val_loss: 0.0492\n",
      "Epoch 8/24\n",
      "3622/3622 [==============================] - 1s 192us/step - loss: 0.0688 - val_loss: 0.0380\n",
      "Epoch 9/24\n",
      "3622/3622 [==============================] - 1s 189us/step - loss: 0.0673 - val_loss: 0.0295\n",
      "Epoch 10/24\n",
      "3622/3622 [==============================] - 1s 189us/step - loss: 0.0661 - val_loss: 0.0299\n",
      "Epoch 11/24\n",
      "3622/3622 [==============================] - 1s 189us/step - loss: 0.0639 - val_loss: 0.0298\n",
      "Epoch 12/24\n",
      "3622/3622 [==============================] - 1s 189us/step - loss: 0.0608 - val_loss: 0.0270\n",
      "Epoch 13/24\n",
      "3622/3622 [==============================] - 1s 190us/step - loss: 0.0561 - val_loss: 0.0234\n",
      "Epoch 14/24\n",
      "3622/3622 [==============================] - 1s 189us/step - loss: 0.0498 - val_loss: 0.0482\n",
      "Epoch 15/24\n",
      "3622/3622 [==============================] - 1s 189us/step - loss: 0.0427 - val_loss: 0.0079\n",
      "Epoch 16/24\n",
      "3622/3622 [==============================] - 1s 191us/step - loss: 0.0367 - val_loss: 0.0180\n",
      "Epoch 17/24\n",
      "3622/3622 [==============================] - 1s 189us/step - loss: 0.0273 - val_loss: 0.0080\n",
      "Epoch 18/24\n",
      "3622/3622 [==============================] - 1s 189us/step - loss: 0.0208 - val_loss: 0.0138\n",
      "Epoch 19/24\n",
      "3622/3622 [==============================] - 1s 189us/step - loss: 0.0154 - val_loss: 0.0026\n",
      "Epoch 20/24\n",
      "3622/3622 [==============================] - 1s 191us/step - loss: 0.0102 - val_loss: 0.0091\n",
      "Epoch 21/24\n",
      "3622/3622 [==============================] - 1s 192us/step - loss: 0.0074 - val_loss: 0.0135\n",
      "Epoch 22/24\n",
      "3622/3622 [==============================] - ETA: 0s - loss: 0.005 - 1s 191us/step - loss: 0.0055 - val_loss: 0.0169\n",
      "Epoch 23/24\n",
      "3622/3622 [==============================] - 1s 190us/step - loss: 0.0044 - val_loss: 0.0160\n",
      "Epoch 24/24\n",
      "3622/3622 [==============================] - 1s 194us/step - loss: 0.0037 - val_loss: 0.0132\n",
      "Train on 3622 samples, validate on 403 samples\n",
      "Epoch 1/24\n",
      "3622/3622 [==============================] - 15s 4ms/step - loss: 0.1386 - val_loss: 0.0611\n",
      "Epoch 2/24\n",
      "3622/3622 [==============================] - 1s 273us/step - loss: 0.0172 - val_loss: 0.0178\n",
      "Epoch 3/24\n",
      "3622/3622 [==============================] - 1s 271us/step - loss: 0.0076 - val_loss: 0.0061\n",
      "Epoch 4/24\n",
      "3622/3622 [==============================] - 1s 273us/step - loss: 0.0042 - val_loss: 0.0039\n",
      "Epoch 5/24\n",
      "3622/3622 [==============================] - 1s 270us/step - loss: 0.0030 - val_loss: 0.0012\n",
      "Epoch 6/24\n",
      "3622/3622 [==============================] - 1s 273us/step - loss: 0.0021 - val_loss: 0.0015\n",
      "Epoch 7/24\n",
      "3622/3622 [==============================] - 1s 274us/step - loss: 0.0018 - val_loss: 7.7385e-04\n",
      "Epoch 8/24\n",
      "3622/3622 [==============================] - 1s 275us/step - loss: 0.0016 - val_loss: 0.0011\n",
      "Epoch 9/24\n",
      "3622/3622 [==============================] - 1s 276us/step - loss: 0.0015 - val_loss: 0.0013\n",
      "Epoch 10/24\n",
      "3622/3622 [==============================] - 1s 276us/step - loss: 0.0015 - val_loss: 0.0011\n",
      "Epoch 11/24\n",
      "3622/3622 [==============================] - 1s 274us/step - loss: 0.0014 - val_loss: 7.4806e-04\n",
      "Epoch 12/24\n",
      "3622/3622 [==============================] - 1s 271us/step - loss: 0.0014 - val_loss: 9.6528e-04\n",
      "Epoch 13/24\n",
      "3622/3622 [==============================] - 1s 272us/step - loss: 0.0013 - val_loss: 0.0015\n",
      "Epoch 14/24\n",
      "3622/3622 [==============================] - 1s 270us/step - loss: 0.0014 - val_loss: 0.0010\n",
      "Epoch 15/24\n",
      "3622/3622 [==============================] - 1s 274us/step - loss: 0.0013 - val_loss: 7.5418e-04\n",
      "Epoch 16/24\n",
      "3622/3622 [==============================] - 1s 273us/step - loss: 0.0013 - val_loss: 9.3404e-04\n",
      "Epoch 17/24\n",
      "3622/3622 [==============================] - 1s 274us/step - loss: 0.0012 - val_loss: 0.0014\n",
      "Epoch 18/24\n",
      "3622/3622 [==============================] - 1s 272us/step - loss: 0.0013 - val_loss: 0.0013\n",
      "Epoch 19/24\n",
      "3622/3622 [==============================] - 1s 266us/step - loss: 0.0012 - val_loss: 9.3524e-04\n",
      "Epoch 20/24\n",
      "3622/3622 [==============================] - 1s 275us/step - loss: 0.0011 - val_loss: 6.9582e-04\n",
      "Epoch 21/24\n",
      "3622/3622 [==============================] - 1s 277us/step - loss: 0.0011 - val_loss: 7.1308e-04\n",
      "Epoch 22/24\n",
      "3622/3622 [==============================] - 1s 275us/step - loss: 0.0011 - val_loss: 9.7031e-04\n",
      "Epoch 23/24\n",
      "3622/3622 [==============================] - 1s 270us/step - loss: 0.0010 - val_loss: 0.0012\n",
      "Epoch 24/24\n",
      "3622/3622 [==============================] - 1s 272us/step - loss: 0.0010 - val_loss: 7.8805e-04\n",
      "Train on 3622 samples, validate on 403 samples\n",
      "Epoch 1/24\n",
      "3622/3622 [==============================] - 15s 4ms/step - loss: 0.1341 - val_loss: 0.0837\n",
      "Epoch 2/24\n",
      "3622/3622 [==============================] - 1s 277us/step - loss: 0.0206 - val_loss: 0.0210\n",
      "Epoch 3/24\n",
      "3622/3622 [==============================] - 1s 270us/step - loss: 0.0089 - val_loss: 0.0087\n",
      "Epoch 4/24\n",
      "3622/3622 [==============================] - 1s 277us/step - loss: 0.0049 - val_loss: 0.0038\n",
      "Epoch 5/24\n",
      "3622/3622 [==============================] - 1s 273us/step - loss: 0.0034 - val_loss: 0.0017\n",
      "Epoch 6/24\n",
      "3622/3622 [==============================] - 1s 271us/step - loss: 0.0023 - val_loss: 0.0016\n",
      "Epoch 7/24\n",
      "3622/3622 [==============================] - 1s 273us/step - loss: 0.0021 - val_loss: 0.0011\n",
      "Epoch 8/24\n",
      "3622/3622 [==============================] - 1s 270us/step - loss: 0.0021 - val_loss: 0.0013\n",
      "Epoch 9/24\n",
      "3622/3622 [==============================] - 1s 275us/step - loss: 0.0018 - val_loss: 8.1715e-04\n",
      "Epoch 10/24\n",
      "3622/3622 [==============================] - 1s 276us/step - loss: 0.0018 - val_loss: 0.0010\n",
      "Epoch 11/24\n",
      "3622/3622 [==============================] - 1s 276us/step - loss: 0.0016 - val_loss: 7.7115e-04\n",
      "Epoch 12/24\n",
      "3622/3622 [==============================] - 1s 275us/step - loss: 0.0017 - val_loss: 9.2003e-04\n",
      "Epoch 13/24\n",
      "3622/3622 [==============================] - 1s 278us/step - loss: 0.0016 - val_loss: 8.5452e-04\n",
      "Epoch 14/24\n",
      "3622/3622 [==============================] - 1s 273us/step - loss: 0.0016 - val_loss: 7.4911e-04\n",
      "Epoch 15/24\n",
      "3622/3622 [==============================] - 1s 272us/step - loss: 0.0015 - val_loss: 9.4897e-04\n",
      "Epoch 16/24\n",
      "3622/3622 [==============================] - 1s 272us/step - loss: 0.0015 - val_loss: 7.7992e-04\n",
      "Epoch 17/24\n",
      "3622/3622 [==============================] - 1s 271us/step - loss: 0.0014 - val_loss: 8.5696e-04\n",
      "Epoch 18/24\n",
      "3622/3622 [==============================] - 1s 275us/step - loss: 0.0014 - val_loss: 9.4598e-04\n",
      "Epoch 19/24\n",
      "3622/3622 [==============================] - 1s 269us/step - loss: 0.0013 - val_loss: 9.3296e-04\n",
      "Epoch 20/24\n",
      "3622/3622 [==============================] - 1s 272us/step - loss: 0.0014 - val_loss: 8.5571e-04\n",
      "Epoch 21/24\n",
      "3622/3622 [==============================] - 1s 270us/step - loss: 0.0013 - val_loss: 8.0235e-04\n",
      "Epoch 22/24\n",
      "3622/3622 [==============================] - 1s 269us/step - loss: 0.0013 - val_loss: 0.0011\n",
      "Epoch 23/24\n",
      "3622/3622 [==============================] - 1s 271us/step - loss: 0.0012 - val_loss: 9.3001e-04\n",
      "Epoch 24/24\n",
      "3622/3622 [==============================] - 1s 278us/step - loss: 0.0012 - val_loss: 0.0010\n",
      "Train on 3622 samples, validate on 403 samples\n",
      "Epoch 1/24\n",
      "3622/3622 [==============================] - 15s 4ms/step - loss: 0.0674 - val_loss: 0.0039\n",
      "Epoch 2/24\n",
      "3622/3622 [==============================] - 1s 270us/step - loss: 0.0133 - val_loss: 6.5665e-04\n",
      "Epoch 3/24\n",
      "3622/3622 [==============================] - 1s 273us/step - loss: 0.0084 - val_loss: 0.0088\n",
      "Epoch 4/24\n",
      "3622/3622 [==============================] - 1s 273us/step - loss: 0.0052 - val_loss: 8.6385e-04\n",
      "Epoch 5/24\n",
      "3622/3622 [==============================] - 1s 273us/step - loss: 0.0037 - val_loss: 0.0037\n",
      "Epoch 6/24\n",
      "3622/3622 [==============================] - 1s 272us/step - loss: 0.0029 - val_loss: 6.2619e-04\n",
      "Epoch 7/24\n",
      "3622/3622 [==============================] - 1s 266us/step - loss: 0.0027 - val_loss: 0.0018\n",
      "Epoch 8/24\n",
      "3622/3622 [==============================] - 1s 269us/step - loss: 0.0023 - val_loss: 5.5371e-04\n",
      "Epoch 9/24\n",
      "3622/3622 [==============================] - 1s 273us/step - loss: 0.0019 - val_loss: 5.5920e-04\n",
      "Epoch 10/24\n",
      "3622/3622 [==============================] - 1s 271us/step - loss: 0.0018 - val_loss: 8.1120e-04\n",
      "Epoch 11/24\n",
      "3622/3622 [==============================] - 1s 277us/step - loss: 0.0017 - val_loss: 4.9447e-04\n",
      "Epoch 12/24\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3622/3622 [==============================] - 1s 273us/step - loss: 0.0016 - val_loss: 5.3607e-04\n",
      "Epoch 13/24\n",
      "3622/3622 [==============================] - 1s 270us/step - loss: 0.0014 - val_loss: 4.8898e-04\n",
      "Epoch 14/24\n",
      "3622/3622 [==============================] - 1s 277us/step - loss: 0.0014 - val_loss: 4.9819e-04\n",
      "Epoch 15/24\n",
      "3622/3622 [==============================] - 1s 274us/step - loss: 0.0013 - val_loss: 5.0061e-04\n",
      "Epoch 16/24\n",
      "3622/3622 [==============================] - 1s 273us/step - loss: 0.0013 - val_loss: 4.8063e-04\n",
      "Epoch 17/24\n",
      "3622/3622 [==============================] - 1s 275us/step - loss: 0.0012 - val_loss: 6.2999e-04\n",
      "Epoch 18/24\n",
      "3622/3622 [==============================] - 1s 276us/step - loss: 0.0012 - val_loss: 4.7594e-04\n",
      "Epoch 19/24\n",
      "3622/3622 [==============================] - 1s 271us/step - loss: 0.0012 - val_loss: 4.7275e-04\n",
      "Epoch 20/24\n",
      "3622/3622 [==============================] - 1s 282us/step - loss: 0.0011 - val_loss: 4.6969e-04\n",
      "Epoch 21/24\n",
      "3622/3622 [==============================] - 1s 271us/step - loss: 0.0011 - val_loss: 4.7027e-04\n",
      "Epoch 22/24\n",
      "3622/3622 [==============================] - 1s 276us/step - loss: 0.0011 - val_loss: 5.8337e-04\n",
      "Epoch 23/24\n",
      "3622/3622 [==============================] - 1s 270us/step - loss: 0.0010 - val_loss: 5.8485e-04\n",
      "Epoch 24/24\n",
      "3622/3622 [==============================] - 1s 267us/step - loss: 9.9171e-04 - val_loss: 8.2919e-04\n",
      "Train on 3622 samples, validate on 403 samples\n",
      "Epoch 1/24\n",
      "3622/3622 [==============================] - 15s 4ms/step - loss: 0.0805 - val_loss: 0.0096\n",
      "Epoch 2/24\n",
      "3622/3622 [==============================] - 1s 273us/step - loss: 0.0152 - val_loss: 0.0016\n",
      "Epoch 3/24\n",
      "3622/3622 [==============================] - 1s 270us/step - loss: 0.0090 - val_loss: 0.0033\n",
      "Epoch 4/24\n",
      "3622/3622 [==============================] - 1s 276us/step - loss: 0.0060 - val_loss: 9.7746e-04\n",
      "Epoch 5/24\n",
      "3622/3622 [==============================] - 1s 268us/step - loss: 0.0042 - val_loss: 0.0027\n",
      "Epoch 6/24\n",
      "3622/3622 [==============================] - 1s 269us/step - loss: 0.0036 - val_loss: 6.3876e-04\n",
      "Epoch 7/24\n",
      "3622/3622 [==============================] - 1s 275us/step - loss: 0.0029 - val_loss: 9.5256e-04\n",
      "Epoch 8/24\n",
      "3622/3622 [==============================] - 1s 275us/step - loss: 0.0024 - val_loss: 4.8747e-04\n",
      "Epoch 9/24\n",
      "3622/3622 [==============================] - 1s 276us/step - loss: 0.0024 - val_loss: 4.9602e-04\n",
      "Epoch 10/24\n",
      "3622/3622 [==============================] - 1s 269us/step - loss: 0.0022 - val_loss: 5.9283e-04\n",
      "Epoch 11/24\n",
      "3622/3622 [==============================] - 1s 276us/step - loss: 0.0021 - val_loss: 5.3532e-04\n",
      "Epoch 12/24\n",
      "3622/3622 [==============================] - 1s 267us/step - loss: 0.0019 - val_loss: 6.3446e-04\n",
      "Epoch 13/24\n",
      "3622/3622 [==============================] - 1s 277us/step - loss: 0.0019 - val_loss: 4.6884e-04\n",
      "Epoch 14/24\n",
      "3622/3622 [==============================] - 1s 273us/step - loss: 0.0018 - val_loss: 5.3810e-04\n",
      "Epoch 15/24\n",
      "3622/3622 [==============================] - 1s 278us/step - loss: 0.0017 - val_loss: 4.9627e-04\n",
      "Epoch 16/24\n",
      "3622/3622 [==============================] - 1s 272us/step - loss: 0.0017 - val_loss: 4.5616e-04\n",
      "Epoch 17/24\n",
      "3622/3622 [==============================] - 1s 272us/step - loss: 0.0016 - val_loss: 5.5433e-04\n",
      "Epoch 18/24\n",
      "3622/3622 [==============================] - 1s 275us/step - loss: 0.0015 - val_loss: 4.5984e-04\n",
      "Epoch 19/24\n",
      "3622/3622 [==============================] - 1s 271us/step - loss: 0.0015 - val_loss: 4.7864e-04\n",
      "Epoch 20/24\n",
      "3622/3622 [==============================] - 1s 281us/step - loss: 0.0015 - val_loss: 4.5893e-04\n",
      "Epoch 21/24\n",
      "3622/3622 [==============================] - 1s 270us/step - loss: 0.0014 - val_loss: 5.8645e-04\n",
      "Epoch 22/24\n",
      "3622/3622 [==============================] - 1s 277us/step - loss: 0.0014 - val_loss: 6.4971e-04\n",
      "Epoch 23/24\n",
      "3622/3622 [==============================] - 1s 270us/step - loss: 0.0014 - val_loss: 4.4103e-04\n",
      "Epoch 24/24\n",
      "3622/3622 [==============================] - 1s 272us/step - loss: 0.0013 - val_loss: 4.6760e-04\n",
      "Train on 3622 samples, validate on 403 samples\n",
      "Epoch 1/24\n",
      "3622/3622 [==============================] - 13s 4ms/step - loss: 0.5402 - val_loss: 0.0015\n",
      "Epoch 2/24\n",
      "3622/3622 [==============================] - 2s 491us/step - loss: 0.0048 - val_loss: 0.0459\n",
      "Epoch 3/24\n",
      "3622/3622 [==============================] - 2s 494us/step - loss: 0.0311 - val_loss: 0.0028\n",
      "Epoch 4/24\n",
      "3622/3622 [==============================] - 2s 489us/step - loss: 0.0028 - val_loss: 0.0034\n",
      "Epoch 5/24\n",
      "3622/3622 [==============================] - 2s 491us/step - loss: 0.0025 - val_loss: 0.0022\n",
      "Epoch 6/24\n",
      "3622/3622 [==============================] - 2s 496us/step - loss: 0.0023 - val_loss: 0.0024\n",
      "Epoch 7/24\n",
      "3622/3622 [==============================] - 2s 492us/step - loss: 0.0022 - val_loss: 0.0056\n",
      "Epoch 8/24\n",
      "3622/3622 [==============================] - 2s 492us/step - loss: 0.0025 - val_loss: 0.0017\n",
      "Epoch 9/24\n",
      "3622/3622 [==============================] - 2s 492us/step - loss: 0.0019 - val_loss: 0.0019\n",
      "Epoch 10/24\n",
      "3622/3622 [==============================] - 2s 495us/step - loss: 0.0018 - val_loss: 0.0010\n",
      "Epoch 11/24\n",
      "3622/3622 [==============================] - 2s 492us/step - loss: 0.0019 - val_loss: 0.0011\n",
      "Epoch 12/24\n",
      "3622/3622 [==============================] - 2s 492us/step - loss: 0.0017 - val_loss: 9.3879e-04\n",
      "Epoch 13/24\n",
      "3622/3622 [==============================] - 2s 490us/step - loss: 0.0034 - val_loss: 0.0015\n",
      "Epoch 14/24\n",
      "3622/3622 [==============================] - 2s 493us/step - loss: 0.0027 - val_loss: 0.0016\n",
      "Epoch 15/24\n",
      "3622/3622 [==============================] - 2s 490us/step - loss: 0.0027 - val_loss: 8.9749e-04\n",
      "Epoch 16/24\n",
      "3622/3622 [==============================] - 2s 493us/step - loss: 0.0021 - val_loss: 8.8614e-04\n",
      "Epoch 17/24\n",
      "3622/3622 [==============================] - 2s 495us/step - loss: 0.0019 - val_loss: 0.0011\n",
      "Epoch 18/24\n",
      "3622/3622 [==============================] - 2s 495us/step - loss: 0.0018 - val_loss: 0.0030\n",
      "Epoch 19/24\n",
      "3622/3622 [==============================] - 2s 496us/step - loss: 0.0029 - val_loss: 9.8109e-04\n",
      "Epoch 20/24\n",
      "3622/3622 [==============================] - 2s 493us/step - loss: 0.0016 - val_loss: 0.0013\n",
      "Epoch 21/24\n",
      "3622/3622 [==============================] - 2s 491us/step - loss: 0.0013 - val_loss: 0.0022\n",
      "Epoch 22/24\n",
      "3622/3622 [==============================] - 2s 495us/step - loss: 0.0015 - val_loss: 0.0021\n",
      "Epoch 23/24\n",
      "3622/3622 [==============================] - 2s 493us/step - loss: 0.0016 - val_loss: 0.0036\n",
      "Epoch 24/24\n",
      "3622/3622 [==============================] - 2s 499us/step - loss: 0.0048 - val_loss: 0.0132\n",
      "\n",
      "GENERATION 11\n",
      "\n",
      "Train on 3622 samples, validate on 403 samples\n",
      "Epoch 1/24\n",
      "3622/3622 [==============================] - 12s 3ms/step - loss: 0.0684 - val_loss: 0.0297\n",
      "Epoch 2/24\n",
      "3622/3622 [==============================] - 1s 174us/step - loss: 0.0536 - val_loss: 0.0207\n",
      "Epoch 3/24\n",
      "3622/3622 [==============================] - 1s 175us/step - loss: 0.0294 - val_loss: 0.0142\n",
      "Epoch 4/24\n",
      "3622/3622 [==============================] - 1s 175us/step - loss: 0.0122 - val_loss: 0.0319\n",
      "Epoch 5/24\n",
      "3622/3622 [==============================] - 1s 176us/step - loss: 0.0069 - val_loss: 0.0707\n",
      "Epoch 6/24\n",
      "3622/3622 [==============================] - 1s 176us/step - loss: 0.0047 - val_loss: 0.0641\n",
      "Epoch 7/24\n",
      "3622/3622 [==============================] - 1s 175us/step - loss: 0.0031 - val_loss: 0.0434\n",
      "Epoch 8/24\n",
      "3622/3622 [==============================] - 1s 176us/step - loss: 0.0027 - val_loss: 0.0087\n",
      "Epoch 9/24\n",
      "3622/3622 [==============================] - 1s 176us/step - loss: 0.0023 - val_loss: 0.0178\n",
      "Epoch 10/24\n",
      "3622/3622 [==============================] - 1s 175us/step - loss: 0.0021 - val_loss: 0.0080\n",
      "Epoch 11/24\n",
      "3622/3622 [==============================] - 1s 176us/step - loss: 0.0020 - val_loss: 0.0034\n",
      "Epoch 12/24\n",
      "3622/3622 [==============================] - 1s 176us/step - loss: 0.0018 - val_loss: 0.0042\n",
      "Epoch 13/24\n",
      "3622/3622 [==============================] - 1s 177us/step - loss: 0.0017 - val_loss: 0.0044\n",
      "Epoch 14/24\n",
      "3622/3622 [==============================] - 1s 177us/step - loss: 0.0015 - val_loss: 0.0014\n",
      "Epoch 15/24\n",
      "3622/3622 [==============================] - 1s 181us/step - loss: 0.0014 - val_loss: 0.0010\n",
      "Epoch 16/24\n",
      "3622/3622 [==============================] - 1s 186us/step - loss: 0.0013 - val_loss: 0.0025\n",
      "Epoch 17/24\n",
      "3622/3622 [==============================] - 1s 177us/step - loss: 0.0012 - val_loss: 0.0013\n",
      "Epoch 18/24\n",
      "3622/3622 [==============================] - 1s 175us/step - loss: 0.0011 - val_loss: 0.0013\n",
      "Epoch 19/24\n",
      "3622/3622 [==============================] - 1s 175us/step - loss: 0.0010 - val_loss: 0.0021\n",
      "Epoch 20/24\n",
      "3622/3622 [==============================] - 1s 176us/step - loss: 0.0012 - val_loss: 9.3893e-04\n",
      "Epoch 21/24\n",
      "3622/3622 [==============================] - 1s 176us/step - loss: 0.0010 - val_loss: 0.0017\n",
      "Epoch 22/24\n",
      "3622/3622 [==============================] - 1s 175us/step - loss: 0.0010 - val_loss: 8.5625e-04\n",
      "Epoch 23/24\n",
      "3622/3622 [==============================] - 1s 176us/step - loss: 9.8259e-04 - val_loss: 0.0015\n",
      "Epoch 24/24\n",
      "3622/3622 [==============================] - 1s 175us/step - loss: 9.5137e-04 - val_loss: 7.5549e-04\n",
      "Train on 3622 samples, validate on 403 samples\n",
      "Epoch 1/24\n",
      "3622/3622 [==============================] - 12s 3ms/step - loss: 0.1623 - val_loss: 0.0343\n",
      "Epoch 2/24\n",
      "3622/3622 [==============================] - 1s 209us/step - loss: 0.0188 - val_loss: 0.0201\n",
      "Epoch 3/24\n",
      "3622/3622 [==============================] - 1s 210us/step - loss: 0.0077 - val_loss: 0.0072\n",
      "Epoch 4/24\n",
      "3622/3622 [==============================] - 1s 210us/step - loss: 0.0034 - val_loss: 0.0029\n",
      "Epoch 5/24\n",
      "3622/3622 [==============================] - 1s 208us/step - loss: 0.0014 - val_loss: 0.0017\n",
      "Epoch 6/24\n",
      "3622/3622 [==============================] - 1s 211us/step - loss: 9.1983e-04 - val_loss: 6.7088e-04\n",
      "Epoch 7/24\n",
      "3622/3622 [==============================] - 1s 211us/step - loss: 7.3698e-04 - val_loss: 6.5850e-04\n",
      "Epoch 8/24\n",
      "3622/3622 [==============================] - 1s 210us/step - loss: 6.7952e-04 - val_loss: 8.1215e-04\n",
      "Epoch 9/24\n",
      "3622/3622 [==============================] - 1s 209us/step - loss: 6.4954e-04 - val_loss: 6.3979e-04\n",
      "Epoch 10/24\n",
      "3622/3622 [==============================] - 1s 211us/step - loss: 6.3264e-04 - val_loss: 6.4128e-04\n",
      "Epoch 11/24\n",
      "3622/3622 [==============================] - 1s 212us/step - loss: 6.1853e-04 - val_loss: 6.9105e-04\n",
      "Epoch 12/24\n",
      "3622/3622 [==============================] - 1s 209us/step - loss: 6.1071e-04 - val_loss: 6.3102e-04\n",
      "Epoch 13/24\n",
      "3622/3622 [==============================] - 1s 209us/step - loss: 5.9754e-04 - val_loss: 6.1690e-04\n",
      "Epoch 14/24\n",
      "3622/3622 [==============================] - 1s 211us/step - loss: 5.9085e-04 - val_loss: 6.2659e-04\n",
      "Epoch 15/24\n",
      "3622/3622 [==============================] - 1s 210us/step - loss: 5.7338e-04 - val_loss: 6.4731e-04\n",
      "Epoch 16/24\n",
      "3622/3622 [==============================] - 1s 210us/step - loss: 5.6282e-04 - val_loss: 5.7700e-04\n",
      "Epoch 17/24\n",
      "3622/3622 [==============================] - 1s 210us/step - loss: 5.4849e-04 - val_loss: 5.6852e-04\n",
      "Epoch 18/24\n",
      "3622/3622 [==============================] - 1s 211us/step - loss: 5.6067e-04 - val_loss: 5.6382e-04\n",
      "Epoch 19/24\n",
      "3622/3622 [==============================] - 1s 212us/step - loss: 5.1988e-04 - val_loss: 6.2992e-04\n",
      "Epoch 20/24\n",
      "3622/3622 [==============================] - 1s 210us/step - loss: 5.0899e-04 - val_loss: 6.2199e-04\n",
      "Epoch 21/24\n",
      "3622/3622 [==============================] - 1s 215us/step - loss: 4.8390e-04 - val_loss: 4.9665e-04\n",
      "Epoch 22/24\n",
      "3622/3622 [==============================] - 1s 210us/step - loss: 4.6382e-04 - val_loss: 4.7206e-04\n",
      "Epoch 23/24\n",
      "3622/3622 [==============================] - 1s 210us/step - loss: 4.3913e-04 - val_loss: 5.1978e-04\n",
      "Epoch 24/24\n",
      "3622/3622 [==============================] - 1s 209us/step - loss: 4.2676e-04 - val_loss: 5.6342e-04\n",
      "Train on 3622 samples, validate on 403 samples\n",
      "Epoch 1/24\n",
      "3622/3622 [==============================] - 14s 4ms/step - loss: 0.2411 - val_loss: 0.2155\n",
      "Epoch 2/24\n",
      "3622/3622 [==============================] - 2s 424us/step - loss: 0.0820 - val_loss: 0.0222\n",
      "Epoch 3/24\n",
      "3622/3622 [==============================] - 2s 422us/step - loss: 0.0273 - val_loss: 0.0171\n",
      "Epoch 4/24\n",
      "3622/3622 [==============================] - 2s 424us/step - loss: 0.0055 - val_loss: 0.0036\n",
      "Epoch 5/24\n",
      "3622/3622 [==============================] - 2s 426us/step - loss: 0.0030 - val_loss: 0.0019\n",
      "Epoch 6/24\n",
      "3622/3622 [==============================] - 2s 423us/step - loss: 0.0014 - val_loss: 0.0013\n",
      "Epoch 7/24\n",
      "3622/3622 [==============================] - 2s 424us/step - loss: 0.0014 - val_loss: 0.0012\n",
      "Epoch 8/24\n",
      "3622/3622 [==============================] - 2s 431us/step - loss: 0.0013 - val_loss: 0.0013\n",
      "Epoch 9/24\n",
      "3622/3622 [==============================] - 2s 425us/step - loss: 0.0012 - val_loss: 0.0013\n",
      "Epoch 10/24\n",
      "3622/3622 [==============================] - 2s 428us/step - loss: 0.0012 - val_loss: 0.0012\n",
      "Epoch 11/24\n",
      "3622/3622 [==============================] - 2s 426us/step - loss: 0.0011 - val_loss: 0.0012\n",
      "Epoch 12/24\n",
      "3622/3622 [==============================] - 2s 422us/step - loss: 0.0011 - val_loss: 0.0011\n",
      "Epoch 13/24\n",
      "3622/3622 [==============================] - 2s 423us/step - loss: 0.0011 - val_loss: 0.0011\n",
      "Epoch 14/24\n",
      "3622/3622 [==============================] - 2s 423us/step - loss: 0.0011 - val_loss: 0.0012\n",
      "Epoch 15/24\n",
      "3622/3622 [==============================] - 2s 427us/step - loss: 0.0011 - val_loss: 0.0011\n",
      "Epoch 16/24\n",
      "3622/3622 [==============================] - 2s 422us/step - loss: 0.0011 - val_loss: 0.0010\n",
      "Epoch 17/24\n",
      "3622/3622 [==============================] - 2s 427us/step - loss: 0.0010 - val_loss: 0.0010\n",
      "Epoch 18/24\n",
      "3622/3622 [==============================] - 2s 427us/step - loss: 0.0010 - val_loss: 0.0015\n",
      "Epoch 19/24\n",
      "3622/3622 [==============================] - 2s 424us/step - loss: 0.0011 - val_loss: 0.0014\n",
      "Epoch 20/24\n",
      "3622/3622 [==============================] - 2s 423us/step - loss: 0.0010 - val_loss: 0.0012\n",
      "Epoch 21/24\n",
      "3622/3622 [==============================] - 2s 423us/step - loss: 9.9463e-04 - val_loss: 0.0011\n",
      "Epoch 22/24\n",
      "3622/3622 [==============================] - 2s 425us/step - loss: 9.5986e-04 - val_loss: 9.2828e-04\n",
      "Epoch 23/24\n",
      "3622/3622 [==============================] - 2s 424us/step - loss: 9.2901e-04 - val_loss: 9.1225e-04\n",
      "Epoch 24/24\n",
      "3622/3622 [==============================] - 2s 423us/step - loss: 9.1096e-04 - val_loss: 8.8836e-04\n",
      "Train on 3622 samples, validate on 403 samples\n",
      "Epoch 1/24\n",
      "3622/3622 [==============================] - 14s 4ms/step - loss: 0.1232 - val_loss: 0.0246\n",
      "Epoch 2/24\n",
      "3622/3622 [==============================] - 2s 442us/step - loss: 0.0193 - val_loss: 0.0156\n",
      "Epoch 3/24\n",
      "3622/3622 [==============================] - 2s 441us/step - loss: 0.0088 - val_loss: 0.0039\n",
      "Epoch 4/24\n",
      "3622/3622 [==============================] - 2s 442us/step - loss: 0.0043 - val_loss: 0.0043\n",
      "Epoch 5/24\n",
      "3622/3622 [==============================] - 2s 442us/step - loss: 0.0031 - val_loss: 0.0017\n",
      "Epoch 6/24\n",
      "3622/3622 [==============================] - 2s 444us/step - loss: 0.0027 - val_loss: 0.0016\n",
      "Epoch 7/24\n",
      "3622/3622 [==============================] - 2s 440us/step - loss: 0.0024 - val_loss: 0.0024\n",
      "Epoch 8/24\n",
      "3622/3622 [==============================] - 2s 442us/step - loss: 0.0024 - val_loss: 0.0033\n",
      "Epoch 9/24\n",
      "3622/3622 [==============================] - 2s 445us/step - loss: 0.0022 - val_loss: 0.0023\n",
      "Epoch 10/24\n",
      "3622/3622 [==============================] - 2s 442us/step - loss: 0.0023 - val_loss: 0.0030\n",
      "Epoch 11/24\n",
      "3622/3622 [==============================] - 2s 445us/step - loss: 0.0022 - val_loss: 0.0038\n",
      "Epoch 12/24\n",
      "3622/3622 [==============================] - 2s 446us/step - loss: 0.0021 - val_loss: 0.0039\n",
      "Epoch 13/24\n",
      "3622/3622 [==============================] - 2s 442us/step - loss: 0.0020 - val_loss: 0.0051\n",
      "Epoch 14/24\n",
      "3622/3622 [==============================] - 2s 443us/step - loss: 0.0019 - val_loss: 0.0055\n",
      "Epoch 15/24\n",
      "3622/3622 [==============================] - 2s 442us/step - loss: 0.0020 - val_loss: 0.0027\n",
      "Epoch 16/24\n",
      "3622/3622 [==============================] - 2s 443us/step - loss: 0.0020 - val_loss: 0.0020\n",
      "Epoch 17/24\n",
      "3622/3622 [==============================] - 2s 444us/step - loss: 0.0021 - val_loss: 0.0035\n",
      "Epoch 18/24\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3622/3622 [==============================] - 2s 439us/step - loss: 0.0018 - val_loss: 0.0050\n",
      "Epoch 19/24\n",
      "3622/3622 [==============================] - 2s 447us/step - loss: 0.0017 - val_loss: 0.0074\n",
      "Epoch 20/24\n",
      "3622/3622 [==============================] - 2s 443us/step - loss: 0.0016 - val_loss: 0.0088\n",
      "Epoch 21/24\n",
      "3622/3622 [==============================] - 2s 442us/step - loss: 0.0017 - val_loss: 0.0050\n",
      "Epoch 22/24\n",
      "3622/3622 [==============================] - 2s 443us/step - loss: 0.0018 - val_loss: 0.0021\n",
      "Epoch 23/24\n",
      "3622/3622 [==============================] - 2s 442us/step - loss: 0.0019 - val_loss: 0.0056\n",
      "Epoch 24/24\n",
      "3622/3622 [==============================] - 2s 443us/step - loss: 0.0016 - val_loss: 0.0109\n",
      "Train on 3622 samples, validate on 403 samples\n",
      "Epoch 1/24\n",
      "3622/3622 [==============================] - 14s 4ms/step - loss: 0.0952 - val_loss: 0.0136\n",
      "Epoch 2/24\n",
      "3622/3622 [==============================] - 2s 443us/step - loss: 0.0142 - val_loss: 0.0048\n",
      "Epoch 3/24\n",
      "3622/3622 [==============================] - 2s 440us/step - loss: 0.0059 - val_loss: 0.0024\n",
      "Epoch 4/24\n",
      "3622/3622 [==============================] - 2s 440us/step - loss: 0.0036 - val_loss: 0.0015\n",
      "Epoch 5/24\n",
      "3622/3622 [==============================] - 2s 439us/step - loss: 0.0032 - val_loss: 0.0019\n",
      "Epoch 6/24\n",
      "3622/3622 [==============================] - 2s 445us/step - loss: 0.0029 - val_loss: 0.0019\n",
      "Epoch 7/24\n",
      "3622/3622 [==============================] - 2s 440us/step - loss: 0.0029 - val_loss: 0.0015\n",
      "Epoch 8/24\n",
      "3622/3622 [==============================] - 2s 442us/step - loss: 0.0027 - val_loss: 0.0015\n",
      "Epoch 9/24\n",
      "3622/3622 [==============================] - 2s 444us/step - loss: 0.0024 - val_loss: 0.0017\n",
      "Epoch 10/24\n",
      "3622/3622 [==============================] - 2s 439us/step - loss: 0.0024 - val_loss: 0.0020\n",
      "Epoch 11/24\n",
      "3622/3622 [==============================] - 2s 441us/step - loss: 0.0022 - val_loss: 0.0023\n",
      "Epoch 12/24\n",
      "3622/3622 [==============================] - 2s 445us/step - loss: 0.0023 - val_loss: 0.0019\n",
      "Epoch 13/24\n",
      "3622/3622 [==============================] - 2s 442us/step - loss: 0.0021 - val_loss: 0.0023\n",
      "Epoch 14/24\n",
      "3622/3622 [==============================] - 2s 442us/step - loss: 0.0021 - val_loss: 0.0027\n",
      "Epoch 15/24\n",
      "3622/3622 [==============================] - 2s 440us/step - loss: 0.0020 - val_loss: 0.0028\n",
      "Epoch 16/24\n",
      "3622/3622 [==============================] - 2s 441us/step - loss: 0.0019 - val_loss: 0.0025\n",
      "Epoch 17/24\n",
      "3622/3622 [==============================] - 2s 442us/step - loss: 0.0020 - val_loss: 0.0034\n",
      "Epoch 18/24\n",
      "3622/3622 [==============================] - 2s 443us/step - loss: 0.0020 - val_loss: 0.0032\n",
      "Epoch 19/24\n",
      "3622/3622 [==============================] - 2s 440us/step - loss: 0.0019 - val_loss: 0.0022\n",
      "Epoch 20/24\n",
      "3622/3622 [==============================] - 2s 441us/step - loss: 0.0018 - val_loss: 0.0042\n",
      "Epoch 21/24\n",
      "3622/3622 [==============================] - 2s 440us/step - loss: 0.0018 - val_loss: 0.0044\n",
      "Epoch 22/24\n",
      "3622/3622 [==============================] - 2s 443us/step - loss: 0.0018 - val_loss: 0.0034\n",
      "Epoch 23/24\n",
      "3622/3622 [==============================] - 2s 441us/step - loss: 0.0017 - val_loss: 0.0027\n",
      "Epoch 24/24\n",
      "3622/3622 [==============================] - 2s 445us/step - loss: 0.0017 - val_loss: 0.0035\n",
      "Train on 3622 samples, validate on 403 samples\n",
      "Epoch 1/24\n",
      "3622/3622 [==============================] - 14s 4ms/step - loss: 0.0831 - val_loss: 0.0310\n",
      "Epoch 2/24\n",
      "3622/3622 [==============================] - 2s 437us/step - loss: 0.0225 - val_loss: 0.0227\n",
      "Epoch 3/24\n",
      "3622/3622 [==============================] - 2s 439us/step - loss: 0.0104 - val_loss: 0.0086\n",
      "Epoch 4/24\n",
      "3622/3622 [==============================] - 2s 438us/step - loss: 0.0057 - val_loss: 0.0031\n",
      "Epoch 5/24\n",
      "3622/3622 [==============================] - 2s 435us/step - loss: 0.0035 - val_loss: 0.0015\n",
      "Epoch 6/24\n",
      "3622/3622 [==============================] - 2s 439us/step - loss: 0.0030 - val_loss: 0.0017\n",
      "Epoch 7/24\n",
      "3622/3622 [==============================] - 2s 439us/step - loss: 0.0029 - val_loss: 0.0014\n",
      "Epoch 8/24\n",
      "3622/3622 [==============================] - 2s 445us/step - loss: 0.0027 - val_loss: 0.0014\n",
      "Epoch 9/24\n",
      "3622/3622 [==============================] - 2s 438us/step - loss: 0.0025 - val_loss: 0.0018\n",
      "Epoch 10/24\n",
      "3622/3622 [==============================] - 2s 436us/step - loss: 0.0025 - val_loss: 0.0023\n",
      "Epoch 11/24\n",
      "3622/3622 [==============================] - 2s 439us/step - loss: 0.0022 - val_loss: 0.0018\n",
      "Epoch 12/24\n",
      "3622/3622 [==============================] - 2s 439us/step - loss: 0.0023 - val_loss: 0.0017\n",
      "Epoch 13/24\n",
      "3622/3622 [==============================] - 2s 435us/step - loss: 0.0022 - val_loss: 0.0019\n",
      "Epoch 14/24\n",
      "3622/3622 [==============================] - 2s 440us/step - loss: 0.0022 - val_loss: 0.0023\n",
      "Epoch 15/24\n",
      "3622/3622 [==============================] - 2s 507us/step - loss: 0.0021 - val_loss: 0.0038\n",
      "Epoch 16/24\n",
      "3622/3622 [==============================] - 2s 526us/step - loss: 0.0020 - val_loss: 0.0029\n",
      "Epoch 17/24\n",
      "3622/3622 [==============================] - 2s 533us/step - loss: 0.0021 - val_loss: 0.0021\n",
      "Epoch 18/24\n",
      "3622/3622 [==============================] - 2s 537us/step - loss: 0.0021 - val_loss: 0.0016\n",
      "Epoch 19/24\n",
      "3622/3622 [==============================] - 2s 527us/step - loss: 0.0019 - val_loss: 0.0032\n",
      "Epoch 20/24\n",
      "3622/3622 [==============================] - 2s 520us/step - loss: 0.0019 - val_loss: 0.0045\n",
      "Epoch 21/24\n",
      "3622/3622 [==============================] - 2s 527us/step - loss: 0.0018 - val_loss: 0.0044\n",
      "Epoch 22/24\n",
      "3622/3622 [==============================] - 2s 520us/step - loss: 0.0018 - val_loss: 0.0066\n",
      "Epoch 23/24\n",
      "3622/3622 [==============================] - 2s 526us/step - loss: 0.0019 - val_loss: 0.0042\n",
      "Epoch 24/24\n",
      "3622/3622 [==============================] - 2s 527us/step - loss: 0.0018 - val_loss: 0.0037\n",
      "Train on 3622 samples, validate on 403 samples\n",
      "Epoch 1/24\n",
      "3622/3622 [==============================] - 20s 5ms/step - loss: 0.0819 - val_loss: 0.0160\n",
      "Epoch 2/24\n",
      "3622/3622 [==============================] - 2s 527us/step - loss: 0.0148 - val_loss: 0.0123\n",
      "Epoch 3/24\n",
      "3622/3622 [==============================] - 2s 525us/step - loss: 0.0071 - val_loss: 0.0018\n",
      "Epoch 4/24\n",
      "3622/3622 [==============================] - 2s 526us/step - loss: 0.0041 - val_loss: 0.0024\n",
      "Epoch 5/24\n",
      "3622/3622 [==============================] - 2s 531us/step - loss: 0.0032 - val_loss: 0.0014\n",
      "Epoch 6/24\n",
      "3622/3622 [==============================] - 2s 522us/step - loss: 0.0028 - val_loss: 0.0012\n",
      "Epoch 7/24\n",
      "3622/3622 [==============================] - 2s 525us/step - loss: 0.0026 - val_loss: 0.0015\n",
      "Epoch 8/24\n",
      "3622/3622 [==============================] - 2s 523us/step - loss: 0.0024 - val_loss: 0.0019\n",
      "Epoch 9/24\n",
      "3622/3622 [==============================] - 2s 520us/step - loss: 0.0025 - val_loss: 0.0014\n",
      "Epoch 10/24\n",
      "3622/3622 [==============================] - 2s 520us/step - loss: 0.0023 - val_loss: 0.0014\n",
      "Epoch 11/24\n",
      "3622/3622 [==============================] - 2s 522us/step - loss: 0.0021 - val_loss: 0.0013\n",
      "Epoch 12/24\n",
      "3622/3622 [==============================] - 2s 538us/step - loss: 0.0022 - val_loss: 0.0016\n",
      "Epoch 13/24\n",
      "3622/3622 [==============================] - 2s 530us/step - loss: 0.0020 - val_loss: 0.0018\n",
      "Epoch 14/24\n",
      "3622/3622 [==============================] - 2s 519us/step - loss: 0.0021 - val_loss: 0.0033\n",
      "Epoch 15/24\n",
      "3622/3622 [==============================] - 2s 492us/step - loss: 0.0024 - val_loss: 0.0032\n",
      "Epoch 16/24\n",
      "3622/3622 [==============================] - 2s 485us/step - loss: 0.0022 - val_loss: 0.0027\n",
      "Epoch 17/24\n",
      "3622/3622 [==============================] - 2s 471us/step - loss: 0.0020 - val_loss: 0.0028\n",
      "Epoch 18/24\n",
      "3622/3622 [==============================] - 2s 440us/step - loss: 0.0020 - val_loss: 0.0027\n",
      "Epoch 19/24\n",
      "3622/3622 [==============================] - 2s 452us/step - loss: 0.0019 - val_loss: 0.0024\n",
      "Epoch 20/24\n",
      "3622/3622 [==============================] - 2s 441us/step - loss: 0.0018 - val_loss: 0.0022\n",
      "Epoch 21/24\n",
      "3622/3622 [==============================] - 2s 439us/step - loss: 0.0018 - val_loss: 0.0025\n",
      "Epoch 22/24\n",
      "3622/3622 [==============================] - 2s 438us/step - loss: 0.0017 - val_loss: 0.0022\n",
      "Epoch 23/24\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3622/3622 [==============================] - 2s 443us/step - loss: 0.0017 - val_loss: 0.0021\n",
      "Epoch 24/24\n",
      "3622/3622 [==============================] - 2s 469us/step - loss: 0.0016 - val_loss: 0.0016\n",
      "Train on 3622 samples, validate on 403 samples\n",
      "Epoch 1/24\n",
      "3622/3622 [==============================] - 16s 4ms/step - loss: 2.0093 - val_loss: 1.8423\n",
      "Epoch 2/24\n",
      "3622/3622 [==============================] - 2s 460us/step - loss: 1.2089 - val_loss: 1.1222\n",
      "Epoch 3/24\n",
      "3622/3622 [==============================] - 2s 461us/step - loss: 0.6896 - val_loss: 0.6566\n",
      "Epoch 4/24\n",
      "3622/3622 [==============================] - 2s 476us/step - loss: 0.3763 - val_loss: 0.3654\n",
      "Epoch 5/24\n",
      "3622/3622 [==============================] - 2s 450us/step - loss: 0.2010 - val_loss: 0.1968\n",
      "Epoch 6/24\n",
      "3622/3622 [==============================] - 2s 455us/step - loss: 0.1163 - val_loss: 0.1059\n",
      "Epoch 7/24\n",
      "3622/3622 [==============================] - 2s 458us/step - loss: 0.0815 - val_loss: 0.0604\n",
      "Epoch 8/24\n",
      "3622/3622 [==============================] - 2s 448us/step - loss: 0.0713 - val_loss: 0.0398\n",
      "Epoch 9/24\n",
      "3622/3622 [==============================] - 2s 457us/step - loss: 0.0696 - val_loss: 0.0317\n",
      "Epoch 10/24\n",
      "3622/3622 [==============================] - 2s 458us/step - loss: 0.0700 - val_loss: 0.0294\n",
      "Epoch 11/24\n",
      "3622/3622 [==============================] - 2s 513us/step - loss: 0.0701 - val_loss: 0.0296\n",
      "Epoch 12/24\n",
      "3622/3622 [==============================] - 2s 466us/step - loss: 0.0700 - val_loss: 0.0315\n",
      "Epoch 13/24\n",
      "3622/3622 [==============================] - 2s 442us/step - loss: 0.0698 - val_loss: 0.0339\n",
      "Epoch 14/24\n",
      "3622/3622 [==============================] - 2s 444us/step - loss: 0.0697 - val_loss: 0.0356\n",
      "Epoch 15/24\n",
      "3622/3622 [==============================] - 2s 441us/step - loss: 0.0696 - val_loss: 0.0369\n",
      "Epoch 16/24\n",
      "3622/3622 [==============================] - 2s 442us/step - loss: 0.0696 - val_loss: 0.0372\n",
      "Epoch 17/24\n",
      "3622/3622 [==============================] - 2s 441us/step - loss: 0.0696 - val_loss: 0.0365\n",
      "Epoch 18/24\n",
      "3622/3622 [==============================] - 2s 439us/step - loss: 0.0696 - val_loss: 0.0365\n",
      "Epoch 19/24\n",
      "3622/3622 [==============================] - 2s 446us/step - loss: 0.0696 - val_loss: 0.0361\n",
      "Epoch 20/24\n",
      "3622/3622 [==============================] - 2s 466us/step - loss: 0.0696 - val_loss: 0.0363\n",
      "Epoch 21/24\n",
      "3622/3622 [==============================] - 2s 457us/step - loss: 0.0696 - val_loss: 0.0367\n",
      "Epoch 22/24\n",
      "3622/3622 [==============================] - 2s 440us/step - loss: 0.0696 - val_loss: 0.0366\n",
      "Epoch 23/24\n",
      "3622/3622 [==============================] - 2s 447us/step - loss: 0.0696 - val_loss: 0.0355\n",
      "Epoch 24/24\n",
      "3622/3622 [==============================] - 2s 474us/step - loss: 0.0696 - val_loss: 0.0346\n",
      "Train on 3622 samples, validate on 403 samples\n",
      "Epoch 1/24\n",
      "3622/3622 [==============================] - 18s 5ms/step - loss: 0.2241 - val_loss: 0.0383\n",
      "Epoch 2/24\n",
      "3622/3622 [==============================] - 2s 463us/step - loss: 0.0315 - val_loss: 0.0406\n",
      "Epoch 3/24\n",
      "3622/3622 [==============================] - 2s 464us/step - loss: 0.0181 - val_loss: 0.0249\n",
      "Epoch 4/24\n",
      "3622/3622 [==============================] - 2s 463us/step - loss: 0.0089 - val_loss: 0.0056\n",
      "Epoch 5/24\n",
      "3622/3622 [==============================] - 2s 461us/step - loss: 0.0051 - val_loss: 0.0017\n",
      "Epoch 6/24\n",
      "3622/3622 [==============================] - 2s 462us/step - loss: 0.0036 - val_loss: 0.0023\n",
      "Epoch 7/24\n",
      "3622/3622 [==============================] - 2s 461us/step - loss: 0.0034 - val_loss: 0.0021\n",
      "Epoch 8/24\n",
      "3622/3622 [==============================] - 2s 466us/step - loss: 0.0030 - val_loss: 0.0015\n",
      "Epoch 9/24\n",
      "3622/3622 [==============================] - 2s 472us/step - loss: 0.0028 - val_loss: 0.0015\n",
      "Epoch 10/24\n",
      "3622/3622 [==============================] - 2s 473us/step - loss: 0.0027 - val_loss: 0.0015\n",
      "Epoch 11/24\n",
      "3622/3622 [==============================] - 2s 479us/step - loss: 0.0024 - val_loss: 0.0015\n",
      "Epoch 12/24\n",
      "3622/3622 [==============================] - 2s 479us/step - loss: 0.0026 - val_loss: 0.0015\n",
      "Epoch 13/24\n",
      "3622/3622 [==============================] - 2s 480us/step - loss: 0.0025 - val_loss: 0.0019\n",
      "Epoch 14/24\n",
      "3622/3622 [==============================] - 2s 486us/step - loss: 0.0025 - val_loss: 0.0024\n",
      "Epoch 15/24\n",
      "3622/3622 [==============================] - 2s 479us/step - loss: 0.0023 - val_loss: 0.0023\n",
      "Epoch 16/24\n",
      "3622/3622 [==============================] - 2s 484us/step - loss: 0.0023 - val_loss: 0.0034\n",
      "Epoch 17/24\n",
      "3622/3622 [==============================] - 2s 493us/step - loss: 0.0023 - val_loss: 0.0026\n",
      "Epoch 18/24\n",
      "3622/3622 [==============================] - 2s 482us/step - loss: 0.0023 - val_loss: 0.0020\n",
      "Epoch 19/24\n",
      "3622/3622 [==============================] - 2s 482us/step - loss: 0.0022 - val_loss: 0.0024\n",
      "Epoch 20/24\n",
      "3622/3622 [==============================] - 2s 479us/step - loss: 0.0020 - val_loss: 0.0023\n",
      "Epoch 21/24\n",
      "3622/3622 [==============================] - 2s 467us/step - loss: 0.0020 - val_loss: 0.0020\n",
      "Epoch 22/24\n",
      "3622/3622 [==============================] - 2s 467us/step - loss: 0.0021 - val_loss: 0.0018\n",
      "Epoch 23/24\n",
      "3622/3622 [==============================] - 2s 464us/step - loss: 0.0020 - val_loss: 0.0028\n",
      "Epoch 24/24\n",
      "3622/3622 [==============================] - 2s 461us/step - loss: 0.0019 - val_loss: 0.0033\n",
      "\n",
      "GENERATION 12\n",
      "\n",
      "Train on 3622 samples, validate on 403 samples\n",
      "Epoch 1/24\n",
      "3622/3622 [==============================] - 17s 5ms/step - loss: 0.0710 - val_loss: 0.0013\n",
      "Epoch 2/24\n",
      "3622/3622 [==============================] - 1s 235us/step - loss: 0.0086 - val_loss: 7.0956e-04\n",
      "Epoch 3/24\n",
      "3622/3622 [==============================] - 1s 237us/step - loss: 0.0028 - val_loss: 0.0012\n",
      "Epoch 4/24\n",
      "3622/3622 [==============================] - 1s 234us/step - loss: 0.0015 - val_loss: 0.0023\n",
      "Epoch 5/24\n",
      "3622/3622 [==============================] - 1s 235us/step - loss: 8.4747e-04 - val_loss: 6.6936e-04\n",
      "Epoch 6/24\n",
      "3622/3622 [==============================] - 1s 238us/step - loss: 6.3541e-04 - val_loss: 5.6134e-04\n",
      "Epoch 7/24\n",
      "3622/3622 [==============================] - 1s 237us/step - loss: 5.8135e-04 - val_loss: 6.9603e-04\n",
      "Epoch 8/24\n",
      "3622/3622 [==============================] - 1s 236us/step - loss: 5.4241e-04 - val_loss: 6.8655e-04\n",
      "Epoch 9/24\n",
      "3622/3622 [==============================] - 1s 239us/step - loss: 5.3684e-04 - val_loss: 5.7848e-04\n",
      "Epoch 10/24\n",
      "3622/3622 [==============================] - 1s 240us/step - loss: 5.1851e-04 - val_loss: 5.2953e-04\n",
      "Epoch 11/24\n",
      "3622/3622 [==============================] - 1s 245us/step - loss: 5.2374e-04 - val_loss: 6.3630e-04\n",
      "Epoch 12/24\n",
      "3622/3622 [==============================] - 1s 238us/step - loss: 4.9360e-04 - val_loss: 6.4884e-04\n",
      "Epoch 13/24\n",
      "3622/3622 [==============================] - 1s 237us/step - loss: 4.7849e-04 - val_loss: 5.3171e-04\n",
      "Epoch 14/24\n",
      "3622/3622 [==============================] - 1s 238us/step - loss: 4.4892e-04 - val_loss: 4.7768e-04\n",
      "Epoch 15/24\n",
      "3622/3622 [==============================] - 1s 235us/step - loss: 4.3047e-04 - val_loss: 5.3901e-04\n",
      "Epoch 16/24\n",
      "3622/3622 [==============================] - 1s 241us/step - loss: 4.0167e-04 - val_loss: 4.5797e-04\n",
      "Epoch 17/24\n",
      "3622/3622 [==============================] - 1s 236us/step - loss: 3.8151e-04 - val_loss: 5.3570e-04\n",
      "Epoch 18/24\n",
      "3622/3622 [==============================] - 1s 236us/step - loss: 3.8649e-04 - val_loss: 4.0668e-04\n",
      "Epoch 19/24\n",
      "3622/3622 [==============================] - 1s 233us/step - loss: 3.5592e-04 - val_loss: 4.0240e-04\n",
      "Epoch 20/24\n",
      "3622/3622 [==============================] - 1s 235us/step - loss: 3.4387e-04 - val_loss: 5.3435e-04\n",
      "Epoch 21/24\n",
      "3622/3622 [==============================] - 1s 235us/step - loss: 3.8445e-04 - val_loss: 6.2023e-04\n",
      "Epoch 22/24\n",
      "3622/3622 [==============================] - 1s 234us/step - loss: 3.8400e-04 - val_loss: 3.9654e-04\n",
      "Epoch 23/24\n",
      "3622/3622 [==============================] - 1s 237us/step - loss: 3.3436e-04 - val_loss: 3.7438e-04\n",
      "Epoch 24/24\n",
      "3622/3622 [==============================] - 1s 234us/step - loss: 3.2475e-04 - val_loss: 3.7669e-04\n",
      "Train on 3622 samples, validate on 403 samples\n",
      "Epoch 1/24\n",
      "3622/3622 [==============================] - 17s 5ms/step - loss: 0.1282 - val_loss: 0.0759\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/24\n",
      "3622/3622 [==============================] - 1s 241us/step - loss: 0.0167 - val_loss: 0.0151\n",
      "Epoch 3/24\n",
      "3622/3622 [==============================] - 1s 240us/step - loss: 0.0062 - val_loss: 0.0060\n",
      "Epoch 4/24\n",
      "3622/3622 [==============================] - 1s 236us/step - loss: 0.0032 - val_loss: 0.0024\n",
      "Epoch 5/24\n",
      "3622/3622 [==============================] - 1s 235us/step - loss: 0.0016 - val_loss: 0.0015\n",
      "Epoch 6/24\n",
      "3622/3622 [==============================] - 1s 236us/step - loss: 9.6492e-04 - val_loss: 7.9999e-04\n",
      "Epoch 7/24\n",
      "3622/3622 [==============================] - 1s 239us/step - loss: 8.5452e-04 - val_loss: 7.7747e-04\n",
      "Epoch 8/24\n",
      "3622/3622 [==============================] - 1s 242us/step - loss: 7.9758e-04 - val_loss: 0.0010\n",
      "Epoch 9/24\n",
      "3622/3622 [==============================] - 1s 237us/step - loss: 7.5950e-04 - val_loss: 7.3852e-04\n",
      "Epoch 10/24\n",
      "3622/3622 [==============================] - 1s 234us/step - loss: 7.1347e-04 - val_loss: 7.3673e-04\n",
      "Epoch 11/24\n",
      "3622/3622 [==============================] - 1s 239us/step - loss: 6.8047e-04 - val_loss: 7.1349e-04\n",
      "Epoch 12/24\n",
      "3622/3622 [==============================] - 1s 242us/step - loss: 6.4739e-04 - val_loss: 7.0178e-04\n",
      "Epoch 13/24\n",
      "3622/3622 [==============================] - 1s 235us/step - loss: 6.1611e-04 - val_loss: 6.6333e-04\n",
      "Epoch 14/24\n",
      "3622/3622 [==============================] - 1s 234us/step - loss: 5.8056e-04 - val_loss: 6.3821e-04\n",
      "Epoch 15/24\n",
      "3622/3622 [==============================] - 1s 236us/step - loss: 5.4332e-04 - val_loss: 5.8886e-04\n",
      "Epoch 16/24\n",
      "3622/3622 [==============================] - 1s 240us/step - loss: 5.2072e-04 - val_loss: 5.4912e-04\n",
      "Epoch 17/24\n",
      "3622/3622 [==============================] - 1s 239us/step - loss: 4.9928e-04 - val_loss: 5.0817e-04\n",
      "Epoch 18/24\n",
      "3622/3622 [==============================] - 1s 237us/step - loss: 4.9384e-04 - val_loss: 5.0877e-04\n",
      "Epoch 19/24\n",
      "3622/3622 [==============================] - 1s 235us/step - loss: 4.5811e-04 - val_loss: 4.7502e-04\n",
      "Epoch 20/24\n",
      "3622/3622 [==============================] - 1s 237us/step - loss: 4.3140e-04 - val_loss: 6.3446e-04\n",
      "Epoch 21/24\n",
      "3622/3622 [==============================] - 1s 240us/step - loss: 4.4980e-04 - val_loss: 4.3418e-04\n",
      "Epoch 22/24\n",
      "3622/3622 [==============================] - 1s 239us/step - loss: 4.2555e-04 - val_loss: 4.5192e-04\n",
      "Epoch 23/24\n",
      "3622/3622 [==============================] - 1s 240us/step - loss: 4.2723e-04 - val_loss: 5.1329e-04\n",
      "Epoch 24/24\n",
      "3622/3622 [==============================] - 1s 238us/step - loss: 4.0084e-04 - val_loss: 4.5936e-04\n",
      "Train on 3622 samples, validate on 403 samples\n",
      "Epoch 1/24\n",
      "3622/3622 [==============================] - 18s 5ms/step - loss: 0.1015 - val_loss: 0.0210\n",
      "Epoch 2/24\n",
      "3622/3622 [==============================] - 2s 495us/step - loss: 0.0201 - val_loss: 0.0172\n",
      "Epoch 3/24\n",
      "3622/3622 [==============================] - 2s 494us/step - loss: 0.0082 - val_loss: 0.0055\n",
      "Epoch 4/24\n",
      "3622/3622 [==============================] - 2s 492us/step - loss: 0.0037 - val_loss: 0.0013\n",
      "Epoch 5/24\n",
      "3622/3622 [==============================] - 2s 495us/step - loss: 0.0026 - val_loss: 0.0035\n",
      "Epoch 6/24\n",
      "3622/3622 [==============================] - 2s 490us/step - loss: 0.0020 - val_loss: 0.0013\n",
      "Epoch 7/24\n",
      "3622/3622 [==============================] - 2s 502us/step - loss: 0.0017 - val_loss: 0.0013\n",
      "Epoch 8/24\n",
      "3622/3622 [==============================] - 2s 492us/step - loss: 0.0015 - val_loss: 0.0012\n",
      "Epoch 9/24\n",
      "3622/3622 [==============================] - 2s 491us/step - loss: 0.0015 - val_loss: 0.0013\n",
      "Epoch 10/24\n",
      "3622/3622 [==============================] - 2s 495us/step - loss: 0.0015 - val_loss: 0.0012\n",
      "Epoch 11/24\n",
      "3622/3622 [==============================] - 2s 491us/step - loss: 0.0015 - val_loss: 0.0012\n",
      "Epoch 12/24\n",
      "3622/3622 [==============================] - 2s 496us/step - loss: 0.0014 - val_loss: 0.0011\n",
      "Epoch 13/24\n",
      "3622/3622 [==============================] - 2s 491us/step - loss: 0.0013 - val_loss: 0.0012\n",
      "Epoch 14/24\n",
      "3622/3622 [==============================] - 2s 493us/step - loss: 0.0013 - val_loss: 0.0014\n",
      "Epoch 15/24\n",
      "3622/3622 [==============================] - 2s 492us/step - loss: 0.0013 - val_loss: 0.0011\n",
      "Epoch 16/24\n",
      "3622/3622 [==============================] - 2s 492us/step - loss: 0.0013 - val_loss: 9.9479e-04\n",
      "Epoch 17/24\n",
      "3622/3622 [==============================] - 2s 492us/step - loss: 0.0012 - val_loss: 0.0011\n",
      "Epoch 18/24\n",
      "3622/3622 [==============================] - 2s 493us/step - loss: 0.0012 - val_loss: 0.0012\n",
      "Epoch 19/24\n",
      "3622/3622 [==============================] - 2s 489us/step - loss: 0.0012 - val_loss: 9.6028e-04\n",
      "Epoch 20/24\n",
      "3622/3622 [==============================] - 2s 488us/step - loss: 0.0011 - val_loss: 8.8174e-04\n",
      "Epoch 21/24\n",
      "3622/3622 [==============================] - 2s 490us/step - loss: 0.0012 - val_loss: 0.0017\n",
      "Epoch 22/24\n",
      "3622/3622 [==============================] - 2s 496us/step - loss: 0.0011 - val_loss: 0.0012\n",
      "Epoch 23/24\n",
      "3622/3622 [==============================] - 2s 493us/step - loss: 0.0011 - val_loss: 7.9422e-04\n",
      "Epoch 24/24\n",
      "3622/3622 [==============================] - 2s 491us/step - loss: 0.0011 - val_loss: 0.0013\n",
      "Train on 3622 samples, validate on 403 samples\n",
      "Epoch 1/24\n",
      "3622/3622 [==============================] - 19s 5ms/step - loss: 0.1428 - val_loss: 0.0048\n",
      "Epoch 2/24\n",
      "3622/3622 [==============================] - 2s 488us/step - loss: 0.0205 - val_loss: 0.0101\n",
      "Epoch 3/24\n",
      "3622/3622 [==============================] - 2s 490us/step - loss: 0.0066 - val_loss: 0.0050\n",
      "Epoch 4/24\n",
      "3622/3622 [==============================] - 2s 492us/step - loss: 0.0034 - val_loss: 0.0023\n",
      "Epoch 5/24\n",
      "3622/3622 [==============================] - 2s 492us/step - loss: 0.0023 - val_loss: 0.0021\n",
      "Epoch 6/24\n",
      "3622/3622 [==============================] - 2s 488us/step - loss: 0.0019 - val_loss: 0.0016\n",
      "Epoch 7/24\n",
      "3622/3622 [==============================] - 2s 489us/step - loss: 0.0018 - val_loss: 0.0020\n",
      "Epoch 8/24\n",
      "3622/3622 [==============================] - 2s 501us/step - loss: 0.0017 - val_loss: 0.0014\n",
      "Epoch 9/24\n",
      "3622/3622 [==============================] - 2s 502us/step - loss: 0.0017 - val_loss: 0.0014\n",
      "Epoch 10/24\n",
      "3622/3622 [==============================] - 2s 493us/step - loss: 0.0016 - val_loss: 0.0015\n",
      "Epoch 11/24\n",
      "3622/3622 [==============================] - 2s 492us/step - loss: 0.0016 - val_loss: 0.0014\n",
      "Epoch 12/24\n",
      "3622/3622 [==============================] - 2s 491us/step - loss: 0.0016 - val_loss: 0.0015\n",
      "Epoch 13/24\n",
      "3622/3622 [==============================] - 2s 493us/step - loss: 0.0016 - val_loss: 0.0013\n",
      "Epoch 14/24\n",
      "3622/3622 [==============================] - 2s 494us/step - loss: 0.0016 - val_loss: 0.0013\n",
      "Epoch 15/24\n",
      "3622/3622 [==============================] - 2s 495us/step - loss: 0.0016 - val_loss: 0.0013\n",
      "Epoch 16/24\n",
      "3622/3622 [==============================] - 2s 494us/step - loss: 0.0015 - val_loss: 0.0014\n",
      "Epoch 17/24\n",
      "3622/3622 [==============================] - 2s 494us/step - loss: 0.0015 - val_loss: 0.0013\n",
      "Epoch 18/24\n",
      "3622/3622 [==============================] - 2s 496us/step - loss: 0.0015 - val_loss: 0.0013\n",
      "Epoch 19/24\n",
      "3622/3622 [==============================] - 2s 494us/step - loss: 0.0016 - val_loss: 0.0013\n",
      "Epoch 20/24\n",
      "3622/3622 [==============================] - 2s 490us/step - loss: 0.0015 - val_loss: 0.0012\n",
      "Epoch 21/24\n",
      "3622/3622 [==============================] - 2s 491us/step - loss: 0.0015 - val_loss: 0.0012\n",
      "Epoch 22/24\n",
      "3622/3622 [==============================] - 2s 493us/step - loss: 0.0015 - val_loss: 0.0012\n",
      "Epoch 23/24\n",
      "3622/3622 [==============================] - 2s 496us/step - loss: 0.0014 - val_loss: 0.0012\n",
      "Epoch 24/24\n",
      "3622/3622 [==============================] - 2s 495us/step - loss: 0.0014 - val_loss: 0.0013\n",
      "Train on 3622 samples, validate on 403 samples\n",
      "Epoch 1/24\n",
      "3622/3622 [==============================] - 19s 5ms/step - loss: nan - val_loss: nan\n",
      "Epoch 2/24\n",
      "3622/3622 [==============================] - 2s 487us/step - loss: nan - val_loss: nan\n",
      "Epoch 3/24\n",
      "3622/3622 [==============================] - 2s 488us/step - loss: nan - val_loss: nan\n",
      "Epoch 4/24\n",
      "3622/3622 [==============================] - 2s 488us/step - loss: nan - val_loss: nan\n",
      "Epoch 5/24\n",
      "3622/3622 [==============================] - 2s 489us/step - loss: nan - val_loss: nan\n",
      "Epoch 6/24\n",
      "3622/3622 [==============================] - 2s 498us/step - loss: nan - val_loss: nan\n",
      "Epoch 7/24\n",
      "3622/3622 [==============================] - 2s 500us/step - loss: nan - val_loss: nan\n",
      "Epoch 8/24\n",
      "3622/3622 [==============================] - 2s 497us/step - loss: nan - val_loss: nan\n",
      "Epoch 9/24\n",
      "3622/3622 [==============================] - 2s 492us/step - loss: nan - val_loss: nan\n",
      "Epoch 10/24\n",
      "3622/3622 [==============================] - 2s 502us/step - loss: nan - val_loss: nan\n",
      "Epoch 11/24\n",
      "3622/3622 [==============================] - 2s 458us/step - loss: nan - val_loss: nan\n",
      "Epoch 12/24\n",
      "3622/3622 [==============================] - 2s 451us/step - loss: nan - val_loss: nan\n",
      "Epoch 13/24\n",
      "3622/3622 [==============================] - 2s 452us/step - loss: nan - val_loss: nan\n",
      "Epoch 14/24\n",
      "3622/3622 [==============================] - 2s 445us/step - loss: nan - val_loss: nan\n",
      "Epoch 15/24\n",
      "3622/3622 [==============================] - 2s 444us/step - loss: nan - val_loss: nan\n",
      "Epoch 16/24\n",
      "3622/3622 [==============================] - 2s 447us/step - loss: nan - val_loss: nan\n",
      "Epoch 17/24\n",
      "3622/3622 [==============================] - 2s 443us/step - loss: nan - val_loss: nan\n",
      "Epoch 18/24\n",
      "3622/3622 [==============================] - 2s 444us/step - loss: nan - val_loss: nan\n",
      "Epoch 19/24\n",
      "3622/3622 [==============================] - 2s 445us/step - loss: nan - val_loss: nan\n",
      "Epoch 20/24\n",
      "3622/3622 [==============================] - 2s 442us/step - loss: nan - val_loss: nan\n",
      "Epoch 21/24\n",
      "3622/3622 [==============================] - 2s 442us/step - loss: nan - val_loss: nan\n",
      "Epoch 22/24\n",
      "3622/3622 [==============================] - 2s 446us/step - loss: nan - val_loss: nan\n",
      "Epoch 23/24\n",
      "3622/3622 [==============================] - 2s 446us/step - loss: nan - val_loss: nan\n",
      "Epoch 24/24\n",
      "3622/3622 [==============================] - 2s 440us/step - loss: nan - val_loss: nan\n",
      "Train on 3622 samples, validate on 403 samples\n",
      "Epoch 1/24\n",
      "3622/3622 [==============================] - 17s 5ms/step - loss: nan - val_loss: nan\n",
      "Epoch 2/24\n",
      "3622/3622 [==============================] - 2s 447us/step - loss: nan - val_loss: nan\n",
      "Epoch 3/24\n",
      "3622/3622 [==============================] - 2s 442us/step - loss: nan - val_loss: nan\n",
      "Epoch 4/24\n",
      "3622/3622 [==============================] - 2s 444us/step - loss: nan - val_loss: nan\n",
      "Epoch 5/24\n",
      "3622/3622 [==============================] - 2s 444us/step - loss: nan - val_loss: nan\n",
      "Epoch 6/24\n",
      "3622/3622 [==============================] - 2s 446us/step - loss: nan - val_loss: nan\n",
      "Epoch 7/24\n",
      "3622/3622 [==============================] - 2s 446us/step - loss: nan - val_loss: nan\n",
      "Epoch 8/24\n",
      "3622/3622 [==============================] - 2s 448us/step - loss: nan - val_loss: nan\n",
      "Epoch 9/24\n",
      "3622/3622 [==============================] - 2s 441us/step - loss: nan - val_loss: nan\n",
      "Epoch 10/24\n",
      "3622/3622 [==============================] - 2s 445us/step - loss: nan - val_loss: nan\n",
      "Epoch 11/24\n",
      "3622/3622 [==============================] - 2s 444us/step - loss: nan - val_loss: nan\n",
      "Epoch 12/24\n",
      "3622/3622 [==============================] - 2s 441us/step - loss: nan - val_loss: nan\n",
      "Epoch 13/24\n",
      "3622/3622 [==============================] - 2s 454us/step - loss: nan - val_loss: nan\n",
      "Epoch 14/24\n",
      "3622/3622 [==============================] - 2s 442us/step - loss: nan - val_loss: nan\n",
      "Epoch 15/24\n",
      "3622/3622 [==============================] - 2s 442us/step - loss: nan - val_loss: nan\n",
      "Epoch 16/24\n",
      "3622/3622 [==============================] - 2s 444us/step - loss: nan - val_loss: nan\n",
      "Epoch 17/24\n",
      "3622/3622 [==============================] - 2s 445us/step - loss: nan - val_loss: nan\n",
      "Epoch 18/24\n",
      "3622/3622 [==============================] - 2s 449us/step - loss: nan - val_loss: nan\n",
      "Epoch 19/24\n",
      "3622/3622 [==============================] - 2s 450us/step - loss: nan - val_loss: nan\n",
      "Epoch 20/24\n",
      "3622/3622 [==============================] - 2s 445us/step - loss: nan - val_loss: nan\n",
      "Epoch 21/24\n",
      "3622/3622 [==============================] - 2s 444us/step - loss: nan - val_loss: nan\n",
      "Epoch 22/24\n",
      "3622/3622 [==============================] - 2s 446us/step - loss: nan - val_loss: nan\n",
      "Epoch 23/24\n",
      "3622/3622 [==============================] - 2s 445us/step - loss: nan - val_loss: nan\n",
      "Epoch 24/24\n",
      "3622/3622 [==============================] - 2s 444us/step - loss: nan - val_loss: nan\n",
      "Train on 3622 samples, validate on 403 samples\n",
      "Epoch 1/24\n",
      "3622/3622 [==============================] - 18s 5ms/step - loss: nan - val_loss: nan\n",
      "Epoch 2/24\n",
      "3622/3622 [==============================] - 2s 471us/step - loss: nan - val_loss: nan\n",
      "Epoch 3/24\n",
      "3622/3622 [==============================] - 2s 452us/step - loss: nan - val_loss: nan\n",
      "Epoch 4/24\n",
      "3622/3622 [==============================] - 2s 455us/step - loss: nan - val_loss: nan\n",
      "Epoch 5/24\n",
      "3622/3622 [==============================] - 2s 449us/step - loss: nan - val_loss: nan\n",
      "Epoch 6/24\n",
      "3622/3622 [==============================] - 2s 458us/step - loss: nan - val_loss: nan\n",
      "Epoch 7/24\n",
      "3622/3622 [==============================] - 2s 447us/step - loss: nan - val_loss: nan\n",
      "Epoch 8/24\n",
      "3622/3622 [==============================] - 2s 449us/step - loss: nan - val_loss: nan\n",
      "Epoch 9/24\n",
      "3622/3622 [==============================] - 2s 447us/step - loss: nan - val_loss: nan\n",
      "Epoch 10/24\n",
      "3622/3622 [==============================] - 2s 443us/step - loss: nan - val_loss: nan\n",
      "Epoch 11/24\n",
      "3622/3622 [==============================] - 2s 442us/step - loss: nan - val_loss: nan\n",
      "Epoch 12/24\n",
      "3622/3622 [==============================] - 2s 441us/step - loss: nan - val_loss: nan\n",
      "Epoch 13/24\n",
      "3622/3622 [==============================] - 2s 441us/step - loss: nan - val_loss: nan\n",
      "Epoch 14/24\n",
      "3622/3622 [==============================] - 2s 443us/step - loss: nan - val_loss: nan\n",
      "Epoch 15/24\n",
      "3622/3622 [==============================] - 2s 439us/step - loss: nan - val_loss: nan\n",
      "Epoch 16/24\n",
      "3622/3622 [==============================] - 2s 442us/step - loss: nan - val_loss: nan\n",
      "Epoch 17/24\n",
      "3622/3622 [==============================] - 2s 442us/step - loss: nan - val_loss: nan\n",
      "Epoch 18/24\n",
      "3622/3622 [==============================] - 2s 442us/step - loss: nan - val_loss: nan\n",
      "Epoch 19/24\n",
      "3622/3622 [==============================] - 2s 442us/step - loss: nan - val_loss: nan\n",
      "Epoch 20/24\n",
      "3622/3622 [==============================] - 2s 446us/step - loss: nan - val_loss: nan\n",
      "Epoch 21/24\n",
      "3622/3622 [==============================] - 2s 440us/step - loss: nan - val_loss: nan\n",
      "Epoch 22/24\n",
      "3622/3622 [==============================] - 2s 443us/step - loss: nan - val_loss: nan\n",
      "Epoch 23/24\n",
      "3622/3622 [==============================] - 2s 441us/step - loss: nan - val_loss: nan\n",
      "Epoch 24/24\n",
      "3622/3622 [==============================] - 2s 438us/step - loss: nan - val_loss: nan\n",
      "Train on 3622 samples, validate on 403 samples\n",
      "Epoch 1/24\n",
      "3622/3622 [==============================] - 17s 5ms/step - loss: nan - val_loss: nan\n",
      "Epoch 2/24\n",
      "3622/3622 [==============================] - 2s 443us/step - loss: nan - val_loss: nan\n",
      "Epoch 3/24\n",
      "3622/3622 [==============================] - 2s 440us/step - loss: nan - val_loss: nan\n",
      "Epoch 4/24\n",
      "3622/3622 [==============================] - 2s 440us/step - loss: nan - val_loss: nan\n",
      "Epoch 5/24\n",
      "3622/3622 [==============================] - 2s 441us/step - loss: nan - val_loss: nan\n",
      "Epoch 6/24\n",
      "3622/3622 [==============================] - 2s 441us/step - loss: nan - val_loss: nan\n",
      "Epoch 7/24\n",
      "3622/3622 [==============================] - 2s 441us/step - loss: nan - val_loss: nan\n",
      "Epoch 8/24\n",
      "3622/3622 [==============================] - 2s 441us/step - loss: nan - val_loss: nan\n",
      "Epoch 9/24\n",
      "3622/3622 [==============================] - 2s 440us/step - loss: nan - val_loss: nan\n",
      "Epoch 10/24\n",
      "3622/3622 [==============================] - 2s 443us/step - loss: nan - val_loss: nan\n",
      "Epoch 11/24\n",
      "3622/3622 [==============================] - 2s 440us/step - loss: nan - val_loss: nan\n",
      "Epoch 12/24\n",
      "3622/3622 [==============================] - 2s 442us/step - loss: nan - val_loss: nan\n",
      "Epoch 13/24\n",
      "3622/3622 [==============================] - 2s 440us/step - loss: nan - val_loss: nan\n",
      "Epoch 14/24\n",
      "3622/3622 [==============================] - 2s 440us/step - loss: nan - val_loss: nan\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15/24\n",
      "3622/3622 [==============================] - 2s 442us/step - loss: nan - val_loss: nan\n",
      "Epoch 16/24\n",
      "3622/3622 [==============================] - 2s 441us/step - loss: nan - val_loss: nan\n",
      "Epoch 17/24\n",
      "3622/3622 [==============================] - 2s 447us/step - loss: nan - val_loss: nan\n",
      "Epoch 18/24\n",
      "3622/3622 [==============================] - 2s 440us/step - loss: nan - val_loss: nan\n",
      "Epoch 19/24\n",
      "3622/3622 [==============================] - 2s 441us/step - loss: nan - val_loss: nan\n",
      "Epoch 20/24\n",
      "3622/3622 [==============================] - 2s 442us/step - loss: nan - val_loss: nan\n",
      "Epoch 21/24\n",
      "3622/3622 [==============================] - 2s 439us/step - loss: nan - val_loss: nan\n",
      "Epoch 22/24\n",
      "3622/3622 [==============================] - 2s 444us/step - loss: nan - val_loss: nan\n",
      "Epoch 23/24\n",
      "3622/3622 [==============================] - 2s 440us/step - loss: nan - val_loss: nan\n",
      "Epoch 24/24\n",
      "3622/3622 [==============================] - 2s 444us/step - loss: nan - val_loss: nan\n",
      "Train on 3622 samples, validate on 403 samples\n",
      "Epoch 1/24\n",
      "3622/3622 [==============================] - 18s 5ms/step - loss: 0.3949 - val_loss: 0.1346\n",
      "Epoch 2/24\n",
      "3622/3622 [==============================] - 2s 441us/step - loss: 0.0835 - val_loss: 0.0486\n",
      "Epoch 3/24\n",
      "3622/3622 [==============================] - 2s 441us/step - loss: 0.0702 - val_loss: 0.0359\n",
      "Epoch 4/24\n",
      "3622/3622 [==============================] - 2s 444us/step - loss: 0.0698 - val_loss: 0.0359\n",
      "Epoch 5/24\n",
      "3622/3622 [==============================] - 2s 443us/step - loss: 0.0698 - val_loss: 0.0392\n",
      "Epoch 6/24\n",
      "3622/3622 [==============================] - 2s 448us/step - loss: 0.0698 - val_loss: 0.0386\n",
      "Epoch 7/24\n",
      "3622/3622 [==============================] - 2s 446us/step - loss: 0.0698 - val_loss: 0.0373\n",
      "Epoch 8/24\n",
      "3622/3622 [==============================] - 2s 442us/step - loss: 0.0698 - val_loss: 0.0358\n",
      "Epoch 9/24\n",
      "3622/3622 [==============================] - 2s 444us/step - loss: 0.0699 - val_loss: 0.0381\n",
      "Epoch 10/24\n",
      "3622/3622 [==============================] - 2s 445us/step - loss: 0.0698 - val_loss: 0.0288\n",
      "Epoch 11/24\n",
      "3622/3622 [==============================] - 2s 442us/step - loss: 0.0700 - val_loss: 0.0364\n",
      "Epoch 12/24\n",
      "3622/3622 [==============================] - 2s 447us/step - loss: 0.0698 - val_loss: 0.0413\n",
      "Epoch 13/24\n",
      "3622/3622 [==============================] - 2s 444us/step - loss: 0.0699 - val_loss: 0.0360\n",
      "Epoch 14/24\n",
      "3622/3622 [==============================] - 2s 445us/step - loss: 0.0698 - val_loss: 0.0368\n",
      "Epoch 15/24\n",
      "3622/3622 [==============================] - 2s 443us/step - loss: 0.0698 - val_loss: 0.0349\n",
      "Epoch 16/24\n",
      "3622/3622 [==============================] - 2s 439us/step - loss: 0.0698 - val_loss: 0.0377\n",
      "Epoch 17/24\n",
      "3622/3622 [==============================] - 2s 439us/step - loss: 0.0698 - val_loss: 0.0394\n",
      "Epoch 18/24\n",
      "3622/3622 [==============================] - 2s 443us/step - loss: 0.0699 - val_loss: 0.0352\n",
      "Epoch 19/24\n",
      "3622/3622 [==============================] - 2s 482us/step - loss: 0.0698 - val_loss: 0.0374\n",
      "Epoch 20/24\n",
      "3622/3622 [==============================] - 2s 505us/step - loss: 0.0698 - val_loss: 0.0368\n",
      "Epoch 21/24\n",
      "3622/3622 [==============================] - 2s 499us/step - loss: 0.0698 - val_loss: 0.0352\n",
      "Epoch 22/24\n",
      "3622/3622 [==============================] - 2s 493us/step - loss: 0.0698 - val_loss: 0.0350\n",
      "Epoch 23/24\n",
      "3622/3622 [==============================] - 2s 491us/step - loss: 0.0698 - val_loss: 0.0420\n",
      "Epoch 24/24\n",
      "3622/3622 [==============================] - 2s 475us/step - loss: 0.0699 - val_loss: 0.0335\n",
      "\n",
      "GENERATION 13\n",
      "\n",
      "Train on 3622 samples, validate on 403 samples\n",
      "Epoch 1/24\n",
      "3622/3622 [==============================] - 20s 6ms/step - loss: 0.6052 - val_loss: 0.0024\n",
      "Epoch 2/24\n",
      "3622/3622 [==============================] - 2s 571us/step - loss: 0.0080 - val_loss: 0.0024\n",
      "Epoch 3/24\n",
      "3622/3622 [==============================] - 2s 567us/step - loss: 0.0024 - val_loss: 0.0024\n",
      "Epoch 4/24\n",
      "3622/3622 [==============================] - 2s 571us/step - loss: 0.0042 - val_loss: 0.0027\n",
      "Epoch 5/24\n",
      "3622/3622 [==============================] - 2s 569us/step - loss: 0.0036 - val_loss: 0.0033\n",
      "Epoch 6/24\n",
      "3622/3622 [==============================] - 2s 570us/step - loss: 0.0022 - val_loss: 0.0039\n",
      "Epoch 7/24\n",
      "3622/3622 [==============================] - 2s 568us/step - loss: 0.0022 - val_loss: 0.0031\n",
      "Epoch 8/24\n",
      "3622/3622 [==============================] - 2s 573us/step - loss: 0.0022 - val_loss: 0.0047\n",
      "Epoch 9/24\n",
      "3622/3622 [==============================] - 2s 572us/step - loss: 0.0021 - val_loss: 0.0028\n",
      "Epoch 10/24\n",
      "3622/3622 [==============================] - 2s 568us/step - loss: 0.0020 - val_loss: 0.0024\n",
      "Epoch 11/24\n",
      "3622/3622 [==============================] - 2s 570us/step - loss: 0.0019 - val_loss: 0.0042\n",
      "Epoch 12/24\n",
      "3622/3622 [==============================] - 2s 571us/step - loss: 0.0021 - val_loss: 0.0027\n",
      "Epoch 13/24\n",
      "3622/3622 [==============================] - 2s 570us/step - loss: 0.0020 - val_loss: 0.0024\n",
      "Epoch 14/24\n",
      "3622/3622 [==============================] - 2s 565us/step - loss: 0.0019 - val_loss: 0.0031\n",
      "Epoch 15/24\n",
      "3622/3622 [==============================] - 2s 569us/step - loss: 0.0019 - val_loss: 0.0017\n",
      "Epoch 16/24\n",
      "3622/3622 [==============================] - 2s 566us/step - loss: 0.0020 - val_loss: 0.0017\n",
      "Epoch 17/24\n",
      "3622/3622 [==============================] - 2s 572us/step - loss: 0.0022 - val_loss: 0.0029\n",
      "Epoch 18/24\n",
      "3622/3622 [==============================] - 2s 568us/step - loss: 0.0018 - val_loss: 0.0026\n",
      "Epoch 19/24\n",
      "3622/3622 [==============================] - 2s 570us/step - loss: 0.0018 - val_loss: 0.0030\n",
      "Epoch 20/24\n",
      "3622/3622 [==============================] - 2s 568us/step - loss: 0.0018 - val_loss: 0.0019\n",
      "Epoch 21/24\n",
      "3622/3622 [==============================] - 2s 570us/step - loss: 0.0018 - val_loss: 0.0019\n",
      "Epoch 22/24\n",
      "3622/3622 [==============================] - 2s 569us/step - loss: 0.0017 - val_loss: 0.0021\n",
      "Epoch 23/24\n",
      "3622/3622 [==============================] - 2s 577us/step - loss: 0.0018 - val_loss: 0.0025\n",
      "Epoch 24/24\n",
      "3622/3622 [==============================] - 2s 572us/step - loss: 0.0017 - val_loss: 0.0015\n",
      "Train on 3622 samples, validate on 403 samples\n",
      "Epoch 1/24\n",
      "3622/3622 [==============================] - 19s 5ms/step - loss: 0.0976 - val_loss: 0.0350\n",
      "Epoch 2/24\n",
      "3622/3622 [==============================] - 1s 231us/step - loss: 0.0100 - val_loss: 0.0038\n",
      "Epoch 3/24\n",
      "3622/3622 [==============================] - 1s 228us/step - loss: 0.0037 - val_loss: 9.5583e-04\n",
      "Epoch 4/24\n",
      "3622/3622 [==============================] - 1s 228us/step - loss: 0.0018 - val_loss: 0.0013\n",
      "Epoch 5/24\n",
      "3622/3622 [==============================] - 1s 230us/step - loss: 0.0011 - val_loss: 8.4389e-04\n",
      "Epoch 6/24\n",
      "3622/3622 [==============================] - 1s 229us/step - loss: 8.7687e-04 - val_loss: 0.0013\n",
      "Epoch 7/24\n",
      "3622/3622 [==============================] - 1s 234us/step - loss: 7.4953e-04 - val_loss: 7.9893e-04\n",
      "Epoch 8/24\n",
      "3622/3622 [==============================] - 1s 228us/step - loss: 6.8860e-04 - val_loss: 6.5803e-04\n",
      "Epoch 9/24\n",
      "3622/3622 [==============================] - 1s 228us/step - loss: 6.7128e-04 - val_loss: 7.3517e-04\n",
      "Epoch 10/24\n",
      "3622/3622 [==============================] - 1s 229us/step - loss: 6.4866e-04 - val_loss: 7.6302e-04\n",
      "Epoch 11/24\n",
      "3622/3622 [==============================] - 1s 230us/step - loss: 6.2945e-04 - val_loss: 6.7714e-04\n",
      "Epoch 12/24\n",
      "3622/3622 [==============================] - 1s 230us/step - loss: 6.1333e-04 - val_loss: 6.7179e-04\n",
      "Epoch 13/24\n",
      "3622/3622 [==============================] - 1s 228us/step - loss: 5.9358e-04 - val_loss: 6.9153e-04\n",
      "Epoch 14/24\n",
      "3622/3622 [==============================] - 1s 231us/step - loss: 5.7123e-04 - val_loss: 6.5458e-04\n",
      "Epoch 15/24\n",
      "3622/3622 [==============================] - 1s 227us/step - loss: 5.4199e-04 - val_loss: 5.8332e-04\n",
      "Epoch 16/24\n",
      "3622/3622 [==============================] - 1s 234us/step - loss: 5.0400e-04 - val_loss: 5.0693e-04\n",
      "Epoch 17/24\n",
      "3622/3622 [==============================] - 1s 229us/step - loss: 4.8522e-04 - val_loss: 5.1594e-04\n",
      "Epoch 18/24\n",
      "3622/3622 [==============================] - 1s 230us/step - loss: 4.7532e-04 - val_loss: 4.6286e-04\n",
      "Epoch 19/24\n",
      "3622/3622 [==============================] - 1s 236us/step - loss: 4.4924e-04 - val_loss: 5.4453e-04\n",
      "Epoch 20/24\n",
      "3622/3622 [==============================] - 1s 230us/step - loss: 4.4578e-04 - val_loss: 4.6295e-04\n",
      "Epoch 21/24\n",
      "3622/3622 [==============================] - 1s 228us/step - loss: 4.1638e-04 - val_loss: 4.2438e-04\n",
      "Epoch 22/24\n",
      "3622/3622 [==============================] - 1s 233us/step - loss: 3.9675e-04 - val_loss: 4.9756e-04\n",
      "Epoch 23/24\n",
      "3622/3622 [==============================] - 1s 229us/step - loss: 4.1863e-04 - val_loss: 4.4227e-04\n",
      "Epoch 24/24\n",
      "3622/3622 [==============================] - 1s 228us/step - loss: 3.8846e-04 - val_loss: 3.9962e-04\n",
      "Train on 3622 samples, validate on 403 samples\n",
      "Epoch 1/24\n",
      "3622/3622 [==============================] - 19s 5ms/step - loss: 0.0770 - val_loss: 0.0229\n",
      "Epoch 2/24\n",
      "3622/3622 [==============================] - 1s 244us/step - loss: 0.0040 - val_loss: 0.0029\n",
      "Epoch 3/24\n",
      "3622/3622 [==============================] - 1s 233us/step - loss: 0.0041 - val_loss: 0.0041\n",
      "Epoch 4/24\n",
      "3622/3622 [==============================] - 1s 224us/step - loss: 0.0023 - val_loss: 0.0015\n",
      "Epoch 5/24\n",
      "3622/3622 [==============================] - 1s 226us/step - loss: 0.0018 - val_loss: 0.0011\n",
      "Epoch 6/24\n",
      "3622/3622 [==============================] - 1s 222us/step - loss: 0.0018 - val_loss: 6.7169e-04\n",
      "Epoch 7/24\n",
      "3622/3622 [==============================] - 1s 221us/step - loss: 0.0010 - val_loss: 0.0035010   \n",
      "Epoch 8/24\n",
      "3622/3622 [==============================] - 1s 220us/step - loss: 0.0039 - val_loss: 0.0032\n",
      "Epoch 9/24\n",
      "3622/3622 [==============================] - 1s 217us/step - loss: 0.0019 - val_loss: 6.0882e-04\n",
      "Epoch 10/24\n",
      "3622/3622 [==============================] - 1s 218us/step - loss: 7.1573e-04 - val_loss: 5.9145e-04\n",
      "Epoch 11/24\n",
      "3622/3622 [==============================] - 1s 217us/step - loss: 8.9726e-04 - val_loss: 0.0030\n",
      "Epoch 12/24\n",
      "3622/3622 [==============================] - 1s 222us/step - loss: 0.0033 - val_loss: 0.0055\n",
      "Epoch 13/24\n",
      "3622/3622 [==============================] - 1s 218us/step - loss: 0.0019 - val_loss: 7.2012e-04\n",
      "Epoch 14/24\n",
      "3622/3622 [==============================] - 1s 217us/step - loss: 9.4854e-04 - val_loss: 0.0019\n",
      "Epoch 15/24\n",
      "3622/3622 [==============================] - 1s 218us/step - loss: 0.0022 - val_loss: 0.0022\n",
      "Epoch 16/24\n",
      "3622/3622 [==============================] - 1s 217us/step - loss: 0.0014 - val_loss: 0.0024\n",
      "Epoch 17/24\n",
      "3622/3622 [==============================] - 1s 217us/step - loss: 0.0018 - val_loss: 5.4160e-04\n",
      "Epoch 18/24\n",
      "3622/3622 [==============================] - 1s 218us/step - loss: 5.5730e-04 - val_loss: 5.2339e-04\n",
      "Epoch 19/24\n",
      "3622/3622 [==============================] - 1s 217us/step - loss: 7.5459e-04 - val_loss: 5.6168e-04\n",
      "Epoch 20/24\n",
      "3622/3622 [==============================] - 1s 216us/step - loss: 0.0011 - val_loss: 0.0019\n",
      "Epoch 21/24\n",
      "3622/3622 [==============================] - 1s 217us/step - loss: 0.0018 - val_loss: 0.0055\n",
      "Epoch 22/24\n",
      "3622/3622 [==============================] - 1s 218us/step - loss: 0.0028 - val_loss: 0.0021\n",
      "Epoch 23/24\n",
      "3622/3622 [==============================] - 1s 217us/step - loss: 0.0013 - val_loss: 6.7576e-04\n",
      "Epoch 24/24\n",
      "3622/3622 [==============================] - 1s 219us/step - loss: 5.9049e-04 - val_loss: 7.8649e-04\n",
      "Train on 3622 samples, validate on 403 samples\n",
      "Epoch 1/24\n",
      "3622/3622 [==============================] - 17s 5ms/step - loss: 0.1033 - val_loss: 0.0334\n",
      "Epoch 2/24\n",
      "3622/3622 [==============================] - 1s 218us/step - loss: 0.0627 - val_loss: 0.0218\n",
      "Epoch 3/24\n",
      "3622/3622 [==============================] - 1s 218us/step - loss: 0.0540 - val_loss: 0.0127\n",
      "Epoch 4/24\n",
      "3622/3622 [==============================] - 1s 218us/step - loss: 0.0431 - val_loss: 0.0089\n",
      "Epoch 5/24\n",
      "3622/3622 [==============================] - 1s 218us/step - loss: 0.0592 - val_loss: 0.0127\n",
      "Epoch 6/24\n",
      "3622/3622 [==============================] - 1s 217us/step - loss: 0.0552 - val_loss: 0.0127\n",
      "Epoch 7/24\n",
      "3622/3622 [==============================] - 1s 217us/step - loss: 0.0558 - val_loss: 0.0107\n",
      "Epoch 8/24\n",
      "3622/3622 [==============================] - 1s 219us/step - loss: 0.0552 - val_loss: 0.0128\n",
      "Epoch 9/24\n",
      "3622/3622 [==============================] - 1s 218us/step - loss: 0.0512 - val_loss: 0.0210\n",
      "Epoch 10/24\n",
      "3622/3622 [==============================] - 1s 218us/step - loss: 0.0510 - val_loss: 0.0123\n",
      "Epoch 11/24\n",
      "3622/3622 [==============================] - 1s 221us/step - loss: 0.0500 - val_loss: 0.0067\n",
      "Epoch 12/24\n",
      "3622/3622 [==============================] - 1s 218us/step - loss: 0.0497 - val_loss: 0.0326\n",
      "Epoch 13/24\n",
      "3622/3622 [==============================] - 1s 223us/step - loss: 0.0477 - val_loss: 0.0076\n",
      "Epoch 14/24\n",
      "3622/3622 [==============================] - 1s 224us/step - loss: 0.0448 - val_loss: 0.0102\n",
      "Epoch 15/24\n",
      "3622/3622 [==============================] - 1s 218us/step - loss: 0.0237 - val_loss: 0.0353\n",
      "Epoch 16/24\n",
      "3622/3622 [==============================] - 1s 217us/step - loss: 0.0142 - val_loss: 0.1112\n",
      "Epoch 17/24\n",
      "3622/3622 [==============================] - 1s 219us/step - loss: 0.0733 - val_loss: 0.0398\n",
      "Epoch 18/24\n",
      "3622/3622 [==============================] - 1s 219us/step - loss: 0.0696 - val_loss: 0.0429\n",
      "Epoch 19/24\n",
      "3622/3622 [==============================] - 1s 218us/step - loss: 0.0691 - val_loss: 0.0395\n",
      "Epoch 20/24\n",
      "3622/3622 [==============================] - 1s 218us/step - loss: 0.0685 - val_loss: 0.0326\n",
      "Epoch 21/24\n",
      "3622/3622 [==============================] - 1s 218us/step - loss: 0.0660 - val_loss: 0.0111\n",
      "Epoch 22/24\n",
      "3622/3622 [==============================] - 1s 218us/step - loss: 0.0680 - val_loss: 0.0089\n",
      "Epoch 23/24\n",
      "3622/3622 [==============================] - 1s 219us/step - loss: 0.0746 - val_loss: 0.0854\n",
      "Epoch 24/24\n",
      "3622/3622 [==============================] - 1s 217us/step - loss: 0.0661 - val_loss: 0.0084\n",
      "Train on 3622 samples, validate on 403 samples\n",
      "Epoch 1/24\n",
      "3622/3622 [==============================] - 18s 5ms/step - loss: 0.2384 - val_loss: 0.0119\n",
      "Epoch 2/24\n",
      "3622/3622 [==============================] - 1s 217us/step - loss: 0.0162 - val_loss: 0.0015\n",
      "Epoch 3/24\n",
      "3622/3622 [==============================] - 1s 218us/step - loss: 0.0059 - val_loss: 0.0065\n",
      "Epoch 4/24\n",
      "3622/3622 [==============================] - 1s 216us/step - loss: 0.0031 - val_loss: 0.0011\n",
      "Epoch 5/24\n",
      "3622/3622 [==============================] - 1s 217us/step - loss: 0.0015 - val_loss: 0.0067\n",
      "Epoch 6/24\n",
      "3622/3622 [==============================] - 1s 216us/step - loss: 0.0047 - val_loss: 0.0012\n",
      "Epoch 7/24\n",
      "3622/3622 [==============================] - 1s 216us/step - loss: 0.0014 - val_loss: 0.0011\n",
      "Epoch 8/24\n",
      "3622/3622 [==============================] - 1s 216us/step - loss: 0.0020 - val_loss: 0.0156\n",
      "Epoch 9/24\n",
      "3622/3622 [==============================] - 1s 215us/step - loss: 0.0040 - val_loss: 0.0018\n",
      "Epoch 10/24\n",
      "3622/3622 [==============================] - 1s 217us/step - loss: 0.0015 - val_loss: 9.4633e-04\n",
      "Epoch 11/24\n",
      "3622/3622 [==============================] - 1s 217us/step - loss: 0.0012 - val_loss: 7.9380e-04\n",
      "Epoch 12/24\n",
      "3622/3622 [==============================] - 1s 217us/step - loss: 0.0011 - val_loss: 7.9044e-04\n",
      "Epoch 13/24\n",
      "3622/3622 [==============================] - 1s 223us/step - loss: 0.0015 - val_loss: 0.0031\n",
      "Epoch 14/24\n",
      "3622/3622 [==============================] - 1s 220us/step - loss: 0.0035 - val_loss: 0.0045\n",
      "Epoch 15/24\n",
      "3622/3622 [==============================] - 1s 217us/step - loss: 0.0029 - val_loss: 0.0028\n",
      "Epoch 16/24\n",
      "3622/3622 [==============================] - 1s 216us/step - loss: 0.0020 - val_loss: 0.0012\n",
      "Epoch 17/24\n",
      "3622/3622 [==============================] - 1s 216us/step - loss: 0.0018 - val_loss: 0.0023\n",
      "Epoch 18/24\n",
      "3622/3622 [==============================] - 1s 218us/step - loss: 0.0016 - val_loss: 0.0015\n",
      "Epoch 19/24\n",
      "3622/3622 [==============================] - 1s 221us/step - loss: 0.0018 - val_loss: 6.8049e-04\n",
      "Epoch 20/24\n",
      "3622/3622 [==============================] - 1s 216us/step - loss: 0.0013 - val_loss: 0.0031\n",
      "Epoch 21/24\n",
      "3622/3622 [==============================] - 1s 216us/step - loss: 0.0018 - val_loss: 0.0011\n",
      "Epoch 22/24\n",
      "3622/3622 [==============================] - 1s 217us/step - loss: 0.0011 - val_loss: 0.0016\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23/24\n",
      "3622/3622 [==============================] - 1s 216us/step - loss: 0.0013 - val_loss: 0.0013\n",
      "Epoch 24/24\n",
      "3622/3622 [==============================] - 1s 216us/step - loss: 0.0016 - val_loss: 0.0015\n",
      "Train on 3622 samples, validate on 403 samples\n",
      "Epoch 1/24\n",
      "3622/3622 [==============================] - 17s 5ms/step - loss: 1.4261 - val_loss: 0.0018\n",
      "Epoch 2/24\n",
      "3622/3622 [==============================] - 1s 218us/step - loss: 0.0082 - val_loss: 8.7725e-04\n",
      "Epoch 3/24\n",
      "3622/3622 [==============================] - 1s 216us/step - loss: 0.0051 - val_loss: 9.1594e-04\n",
      "Epoch 4/24\n",
      "3622/3622 [==============================] - 1s 220us/step - loss: 0.0043 - val_loss: 0.0017\n",
      "Epoch 5/24\n",
      "3622/3622 [==============================] - 1s 218us/step - loss: 0.0040 - val_loss: 0.0021\n",
      "Epoch 6/24\n",
      "3622/3622 [==============================] - 1s 217us/step - loss: 0.0035 - val_loss: 0.0011\n",
      "Epoch 7/24\n",
      "3622/3622 [==============================] - 1s 217us/step - loss: 0.0035 - val_loss: 0.0027\n",
      "Epoch 8/24\n",
      "3622/3622 [==============================] - 1s 215us/step - loss: 0.0034 - val_loss: 0.0018\n",
      "Epoch 9/24\n",
      "3622/3622 [==============================] - 1s 219us/step - loss: 0.0032 - val_loss: 0.0022\n",
      "Epoch 10/24\n",
      "3622/3622 [==============================] - 1s 217us/step - loss: 0.0032 - val_loss: 0.0019\n",
      "Epoch 11/24\n",
      "3622/3622 [==============================] - 1s 216us/step - loss: 0.0030 - val_loss: 0.0011\n",
      "Epoch 12/24\n",
      "3622/3622 [==============================] - 1s 217us/step - loss: 0.0030 - val_loss: 0.0019\n",
      "Epoch 13/24\n",
      "3622/3622 [==============================] - 1s 217us/step - loss: 0.0029 - val_loss: 0.0016\n",
      "Epoch 14/24\n",
      "3622/3622 [==============================] - 1s 221us/step - loss: 0.0028 - val_loss: 0.0025\n",
      "Epoch 15/24\n",
      "3622/3622 [==============================] - 1s 217us/step - loss: 0.0028 - val_loss: 0.0013\n",
      "Epoch 16/24\n",
      "3622/3622 [==============================] - 1s 216us/step - loss: 0.0027 - val_loss: 0.0016\n",
      "Epoch 17/24\n",
      "3622/3622 [==============================] - 1s 217us/step - loss: 0.0027 - val_loss: 0.0018\n",
      "Epoch 18/24\n",
      "3622/3622 [==============================] - 1s 215us/step - loss: 0.0023 - val_loss: 0.0012\n",
      "Epoch 19/24\n",
      "3622/3622 [==============================] - 1s 215us/step - loss: 0.0025 - val_loss: 0.0011\n",
      "Epoch 20/24\n",
      "3622/3622 [==============================] - 1s 215us/step - loss: 0.0024 - val_loss: 0.0020\n",
      "Epoch 21/24\n",
      "3622/3622 [==============================] - 1s 216us/step - loss: 0.0024 - val_loss: 0.0012\n",
      "Epoch 22/24\n",
      "3622/3622 [==============================] - 1s 217us/step - loss: 0.0024 - val_loss: 9.4430e-04\n",
      "Epoch 23/24\n",
      "3622/3622 [==============================] - 1s 216us/step - loss: 0.0022 - val_loss: 0.0023\n",
      "Epoch 24/24\n",
      "3622/3622 [==============================] - 1s 219us/step - loss: 0.0023 - val_loss: 0.0012\n",
      "Train on 3622 samples, validate on 403 samples\n",
      "Epoch 1/24\n",
      "3622/3622 [==============================] - 18s 5ms/step - loss: 0.3332 - val_loss: 0.1334\n",
      "Epoch 2/24\n",
      "3622/3622 [==============================] - 1s 217us/step - loss: 0.0332 - val_loss: 0.0017\n",
      "Epoch 3/24\n",
      "3622/3622 [==============================] - 1s 216us/step - loss: 0.0066 - val_loss: 0.0132\n",
      "Epoch 4/24\n",
      "3622/3622 [==============================] - 1s 217us/step - loss: 0.0070 - val_loss: 0.0035\n",
      "Epoch 5/24\n",
      "3622/3622 [==============================] - 1s 218us/step - loss: 0.0045 - val_loss: 0.0019\n",
      "Epoch 6/24\n",
      "3622/3622 [==============================] - 1s 240us/step - loss: 0.0039 - val_loss: 0.0026\n",
      "Epoch 7/24\n",
      "3622/3622 [==============================] - 1s 254us/step - loss: 0.0036 - val_loss: 0.0033\n",
      "Epoch 8/24\n",
      "3622/3622 [==============================] - 1s 223us/step - loss: 0.0032 - val_loss: 0.0018\n",
      "Epoch 9/24\n",
      "3622/3622 [==============================] - 1s 220us/step - loss: 0.0032 - val_loss: 0.0019\n",
      "Epoch 10/24\n",
      "3622/3622 [==============================] - 1s 220us/step - loss: 0.0030 - val_loss: 0.0016\n",
      "Epoch 11/24\n",
      "3622/3622 [==============================] - 1s 216us/step - loss: 0.0028 - val_loss: 0.0015\n",
      "Epoch 12/24\n",
      "3622/3622 [==============================] - 1s 217us/step - loss: 0.0028 - val_loss: 0.0032\n",
      "Epoch 13/24\n",
      "3622/3622 [==============================] - 1s 217us/step - loss: 0.0029 - val_loss: 0.0024\n",
      "Epoch 14/24\n",
      "3622/3622 [==============================] - 1s 221us/step - loss: 0.0026 - val_loss: 0.0020\n",
      "Epoch 15/24\n",
      "3622/3622 [==============================] - 1s 219us/step - loss: 0.0026 - val_loss: 0.0039\n",
      "Epoch 16/24\n",
      "3622/3622 [==============================] - 1s 216us/step - loss: 0.0028 - val_loss: 0.0014\n",
      "Epoch 17/24\n",
      "3622/3622 [==============================] - 1s 219us/step - loss: 0.0024 - val_loss: 0.0014\n",
      "Epoch 18/24\n",
      "3622/3622 [==============================] - 1s 215us/step - loss: 0.0025 - val_loss: 0.0030\n",
      "Epoch 19/24\n",
      "3622/3622 [==============================] - 1s 217us/step - loss: 0.0026 - val_loss: 0.0017\n",
      "Epoch 20/24\n",
      "3622/3622 [==============================] - 1s 217us/step - loss: 0.0024 - val_loss: 0.0020\n",
      "Epoch 21/24\n",
      "3622/3622 [==============================] - 1s 216us/step - loss: 0.0026 - val_loss: 0.0020\n",
      "Epoch 22/24\n",
      "3622/3622 [==============================] - 1s 216us/step - loss: 0.0024 - val_loss: 0.0013\n",
      "Epoch 23/24\n",
      "3622/3622 [==============================] - 1s 216us/step - loss: 0.0020 - val_loss: 0.0013\n",
      "Epoch 24/24\n",
      "3622/3622 [==============================] - 1s 216us/step - loss: 0.0022 - val_loss: 0.0015\n",
      "Train on 3622 samples, validate on 403 samples\n",
      "Epoch 1/24\n",
      "3622/3622 [==============================] - 18s 5ms/step - loss: 0.1296 - val_loss: 0.0282\n",
      "Epoch 2/24\n",
      "3622/3622 [==============================] - 1s 210us/step - loss: 0.0137 - val_loss: 0.0113\n",
      "Epoch 3/24\n",
      "3622/3622 [==============================] - 1s 208us/step - loss: 0.0069 - val_loss: 0.0050\n",
      "Epoch 4/24\n",
      "3622/3622 [==============================] - 1s 204us/step - loss: 0.0044 - val_loss: 0.0024\n",
      "Epoch 5/24\n",
      "3622/3622 [==============================] - 1s 203us/step - loss: 0.0044 - val_loss: 0.0050\n",
      "Epoch 6/24\n",
      "3622/3622 [==============================] - 1s 210us/step - loss: 0.0059 - val_loss: 0.0045\n",
      "Epoch 7/24\n",
      "3622/3622 [==============================] - 1s 204us/step - loss: 0.0046 - val_loss: 0.0034\n",
      "Epoch 8/24\n",
      "3622/3622 [==============================] - 1s 204us/step - loss: 0.0047 - val_loss: 0.0148\n",
      "Epoch 9/24\n",
      "3622/3622 [==============================] - 1s 204us/step - loss: 0.0074 - val_loss: 0.0017\n",
      "Epoch 10/24\n",
      "3622/3622 [==============================] - 1s 203us/step - loss: 0.0024 - val_loss: 0.0061\n",
      "Epoch 11/24\n",
      "3622/3622 [==============================] - 1s 205us/step - loss: 0.0050 - val_loss: 7.9747e-04\n",
      "Epoch 12/24\n",
      "3622/3622 [==============================] - 1s 204us/step - loss: 0.0021 - val_loss: 0.0010\n",
      "Epoch 13/24\n",
      "3622/3622 [==============================] - 1s 205us/step - loss: 0.0040 - val_loss: 7.6750e-04\n",
      "Epoch 14/24\n",
      "3622/3622 [==============================] - 1s 212us/step - loss: 0.0024 - val_loss: 0.0049\n",
      "Epoch 15/24\n",
      "3622/3622 [==============================] - 1s 210us/step - loss: 0.0063 - val_loss: 0.0034\n",
      "Epoch 16/24\n",
      "3622/3622 [==============================] - 1s 203us/step - loss: 0.0047 - val_loss: 0.0022\n",
      "Epoch 17/24\n",
      "3622/3622 [==============================] - 1s 203us/step - loss: 0.0045 - val_loss: 0.0021\n",
      "Epoch 18/24\n",
      "3622/3622 [==============================] - 1s 203us/step - loss: 0.0040 - val_loss: 9.8454e-04\n",
      "Epoch 19/24\n",
      "3622/3622 [==============================] - 1s 205us/step - loss: 0.0030 - val_loss: 0.0015\n",
      "Epoch 20/24\n",
      "3622/3622 [==============================] - 1s 203us/step - loss: 0.0039 - val_loss: 0.0013\n",
      "Epoch 21/24\n",
      "3622/3622 [==============================] - 1s 205us/step - loss: 0.0037 - val_loss: 0.0049\n",
      "Epoch 22/24\n",
      "3622/3622 [==============================] - 1s 203us/step - loss: 0.0062 - val_loss: 0.0017\n",
      "Epoch 23/24\n",
      "3622/3622 [==============================] - 1s 202us/step - loss: 0.0034 - val_loss: 0.0057\n",
      "Epoch 24/24\n",
      "3622/3622 [==============================] - 1s 204us/step - loss: 0.0055 - val_loss: 6.2254e-04\n",
      "Train on 3622 samples, validate on 403 samples\n",
      "Epoch 1/24\n",
      "3622/3622 [==============================] - 20s 5ms/step - loss: 0.0959 - val_loss: 0.0203\n",
      "Epoch 2/24\n",
      "3622/3622 [==============================] - 2s 415us/step - loss: 0.0135 - val_loss: 0.0251\n",
      "Epoch 3/24\n",
      "3622/3622 [==============================] - 2s 416us/step - loss: 0.0102 - val_loss: 0.0098\n",
      "Epoch 4/24\n",
      "3622/3622 [==============================] - 2s 414us/step - loss: 0.0083 - val_loss: 0.0078\n",
      "Epoch 5/24\n",
      "3622/3622 [==============================] - 2s 421us/step - loss: 0.0074 - val_loss: 0.0231\n",
      "Epoch 6/24\n",
      "3622/3622 [==============================] - 2s 417us/step - loss: 0.0093 - val_loss: 0.0208\n",
      "Epoch 7/24\n",
      "3622/3622 [==============================] - 1s 413us/step - loss: 0.0088 - val_loss: 0.0096\n",
      "Epoch 8/24\n",
      "3622/3622 [==============================] - 2s 416us/step - loss: 0.0055 - val_loss: 0.0154\n",
      "Epoch 9/24\n",
      "3622/3622 [==============================] - 2s 414us/step - loss: 0.0067 - val_loss: 0.0131\n",
      "Epoch 10/24\n",
      "3622/3622 [==============================] - 2s 417us/step - loss: 0.0073 - val_loss: 0.0153\n",
      "Epoch 11/24\n",
      "3622/3622 [==============================] - 1s 412us/step - loss: 0.0072 - val_loss: 0.0135\n",
      "Epoch 12/24\n",
      "3622/3622 [==============================] - 2s 416us/step - loss: 0.0058 - val_loss: 0.0016\n",
      "Epoch 13/24\n",
      "3622/3622 [==============================] - 1s 410us/step - loss: 0.0033 - val_loss: 0.0014\n",
      "Epoch 14/24\n",
      "3622/3622 [==============================] - 2s 416us/step - loss: 0.0033 - val_loss: 0.0033\n",
      "Epoch 15/24\n",
      "3622/3622 [==============================] - 1s 413us/step - loss: 0.0048 - val_loss: 0.0069\n",
      "Epoch 16/24\n",
      "3622/3622 [==============================] - 1s 414us/step - loss: 0.0072 - val_loss: 0.0032\n",
      "Epoch 17/24\n",
      "3622/3622 [==============================] - 1s 412us/step - loss: 0.0051 - val_loss: 0.0064\n",
      "Epoch 18/24\n",
      "3622/3622 [==============================] - 1s 413us/step - loss: 0.0050 - val_loss: 0.0062\n",
      "Epoch 19/24\n",
      "3622/3622 [==============================] - 2s 419us/step - loss: 0.0058 - val_loss: 0.0153\n",
      "Epoch 20/24\n",
      "3622/3622 [==============================] - 2s 416us/step - loss: 0.0063 - val_loss: 0.0038\n",
      "Epoch 21/24\n",
      "3622/3622 [==============================] - 1s 413us/step - loss: 0.0044 - val_loss: 0.0019\n",
      "Epoch 22/24\n",
      "3622/3622 [==============================] - 1s 413us/step - loss: 0.0027 - val_loss: 0.0020\n",
      "Epoch 23/24\n",
      "3622/3622 [==============================] - 1s 412us/step - loss: 0.0037 - val_loss: 0.0019\n",
      "Epoch 24/24\n",
      "3622/3622 [==============================] - ETA: 0s - loss: 0.0026- ETA: 0s - loss: 0.0 - 1s 413us/step - loss: 0.0026 - val_loss: 0.0017\n",
      "\n",
      "GENERATION 14\n",
      "\n",
      "Train on 3622 samples, validate on 403 samples\n",
      "Epoch 1/24\n",
      "3622/3622 [==============================] - 18s 5ms/step - loss: 0.1402 - val_loss: 0.0199\n",
      "Epoch 2/24\n",
      "3622/3622 [==============================] - 1s 216us/step - loss: 0.0153 - val_loss: 0.0137\n",
      "Epoch 3/24\n",
      "3622/3622 [==============================] - 1s 216us/step - loss: 0.0067 - val_loss: 0.0049\n",
      "Epoch 4/24\n",
      "3622/3622 [==============================] - 1s 217us/step - loss: 0.0026 - val_loss: 7.6298e-04\n",
      "Epoch 5/24\n",
      "3622/3622 [==============================] - 1s 218us/step - loss: 0.0013 - val_loss: 0.0013\n",
      "Epoch 6/24\n",
      "3622/3622 [==============================] - 1s 215us/step - loss: 0.0011 - val_loss: 0.0012\n",
      "Epoch 7/24\n",
      "3622/3622 [==============================] - 1s 216us/step - loss: 8.1917e-04 - val_loss: 9.3106e-04\n",
      "Epoch 8/24\n",
      "3622/3622 [==============================] - 1s 216us/step - loss: 7.6497e-04 - val_loss: 7.2770e-04\n",
      "Epoch 9/24\n",
      "3622/3622 [==============================] - 1s 216us/step - loss: 7.0295e-04 - val_loss: 7.2034e-04\n",
      "Epoch 10/24\n",
      "3622/3622 [==============================] - 1s 215us/step - loss: 6.4673e-04 - val_loss: 6.9261e-04\n",
      "Epoch 11/24\n",
      "3622/3622 [==============================] - 1s 215us/step - loss: 6.3062e-04 - val_loss: 6.5550e-04\n",
      "Epoch 12/24\n",
      "3622/3622 [==============================] - 1s 216us/step - loss: 6.2394e-04 - val_loss: 6.8258e-04\n",
      "Epoch 13/24\n",
      "3622/3622 [==============================] - 1s 214us/step - loss: 6.0635e-04 - val_loss: 7.2055e-04\n",
      "Epoch 14/24\n",
      "3622/3622 [==============================] - 1s 215us/step - loss: 5.9052e-04 - val_loss: 7.1758e-04\n",
      "Epoch 15/24\n",
      "3622/3622 [==============================] - 1s 215us/step - loss: 5.8045e-04 - val_loss: 7.0937e-04\n",
      "Epoch 16/24\n",
      "3622/3622 [==============================] - 1s 219us/step - loss: 5.7609e-04 - val_loss: 5.9837e-04\n",
      "Epoch 17/24\n",
      "3622/3622 [==============================] - 1s 219us/step - loss: 5.6877e-04 - val_loss: 6.6942e-04\n",
      "Epoch 18/24\n",
      "3622/3622 [==============================] - 1s 217us/step - loss: 5.5470e-04 - val_loss: 5.4175e-04\n",
      "Epoch 19/24\n",
      "3622/3622 [==============================] - 1s 216us/step - loss: 5.0711e-04 - val_loss: 5.2919e-04\n",
      "Epoch 20/24\n",
      "3622/3622 [==============================] - 1s 218us/step - loss: 4.9456e-04 - val_loss: 5.2176e-04\n",
      "Epoch 21/24\n",
      "3622/3622 [==============================] - 1s 216us/step - loss: 4.7172e-04 - val_loss: 4.7606e-04\n",
      "Epoch 22/24\n",
      "3622/3622 [==============================] - 1s 216us/step - loss: 4.6749e-04 - val_loss: 6.4702e-04\n",
      "Epoch 23/24\n",
      "3622/3622 [==============================] - 1s 218us/step - loss: 5.2725e-04 - val_loss: 4.5562e-04\n",
      "Epoch 24/24\n",
      "3622/3622 [==============================] - 1s 222us/step - loss: 4.4646e-04 - val_loss: 4.4745e-04\n",
      "Train on 3622 samples, validate on 403 samples\n",
      "Epoch 1/24\n",
      "3622/3622 [==============================] - 19s 5ms/step - loss: 0.0777 - val_loss: 0.0041\n",
      "Epoch 2/24\n",
      "3622/3622 [==============================] - 1s 217us/step - loss: 0.0076 - val_loss: 0.0011\n",
      "Epoch 3/24\n",
      "3622/3622 [==============================] - 1s 214us/step - loss: 0.0036 - val_loss: 0.0024\n",
      "Epoch 4/24\n",
      "3622/3622 [==============================] - 1s 215us/step - loss: 0.0016 - val_loss: 0.0014\n",
      "Epoch 5/24\n",
      "3622/3622 [==============================] - 1s 215us/step - loss: 8.5294e-04 - val_loss: 0.0012\n",
      "Epoch 6/24\n",
      "3622/3622 [==============================] - 1s 214us/step - loss: 7.6860e-04 - val_loss: 7.9778e-04\n",
      "Epoch 7/24\n",
      "3622/3622 [==============================] - 1s 214us/step - loss: 6.6591e-04 - val_loss: 6.7024e-04\n",
      "Epoch 8/24\n",
      "3622/3622 [==============================] - 1s 215us/step - loss: 6.2405e-04 - val_loss: 6.2593e-04\n",
      "Epoch 9/24\n",
      "3622/3622 [==============================] - 1s 215us/step - loss: 5.8833e-04 - val_loss: 6.0898e-04\n",
      "Epoch 10/24\n",
      "3622/3622 [==============================] - 1s 216us/step - loss: 5.6509e-04 - val_loss: 5.8831e-04\n",
      "Epoch 11/24\n",
      "3622/3622 [==============================] - 1s 215us/step - loss: 5.3431e-04 - val_loss: 5.8602e-04\n",
      "Epoch 12/24\n",
      "3622/3622 [==============================] - 1s 213us/step - loss: 5.3447e-04 - val_loss: 5.3995e-04\n",
      "Epoch 13/24\n",
      "3622/3622 [==============================] - 1s 216us/step - loss: 4.9313e-04 - val_loss: 9.1114e-04\n",
      "Epoch 14/24\n",
      "3622/3622 [==============================] - 1s 215us/step - loss: 5.6311e-04 - val_loss: 4.9593e-04\n",
      "Epoch 15/24\n",
      "3622/3622 [==============================] - 1s 214us/step - loss: 4.7633e-04 - val_loss: 6.3166e-04\n",
      "Epoch 16/24\n",
      "3622/3622 [==============================] - 1s 214us/step - loss: 4.7941e-04 - val_loss: 5.2823e-04\n",
      "Epoch 17/24\n",
      "3622/3622 [==============================] - 1s 214us/step - loss: 4.5074e-04 - val_loss: 4.4314e-04\n",
      "Epoch 18/24\n",
      "3622/3622 [==============================] - 1s 214us/step - loss: 4.1572e-04 - val_loss: 6.6874e-04\n",
      "Epoch 19/24\n",
      "3622/3622 [==============================] - 1s 217us/step - loss: 4.3597e-04 - val_loss: 4.6007e-04\n",
      "Epoch 20/24\n",
      "3622/3622 [==============================] - 1s 216us/step - loss: 4.4395e-04 - val_loss: 4.6460e-04\n",
      "Epoch 21/24\n",
      "3622/3622 [==============================] - 1s 214us/step - loss: 3.9712e-04 - val_loss: 4.9249e-04\n",
      "Epoch 22/24\n",
      "3622/3622 [==============================] - 1s 221us/step - loss: 4.1061e-04 - val_loss: 7.2032e-04\n",
      "Epoch 23/24\n",
      "3622/3622 [==============================] - 1s 214us/step - loss: 4.2830e-04 - val_loss: 4.2003e-04\n",
      "Epoch 24/24\n",
      "3622/3622 [==============================] - 1s 215us/step - loss: 3.9104e-04 - val_loss: 4.2347e-04\n",
      "Train on 3622 samples, validate on 403 samples\n",
      "Epoch 1/24\n",
      "3622/3622 [==============================] - 19s 5ms/step - loss: 0.1772 - val_loss: 0.0192\n",
      "Epoch 2/24\n",
      "3622/3622 [==============================] - 1s 219us/step - loss: 0.0220 - val_loss: 0.0249\n",
      "Epoch 3/24\n",
      "3622/3622 [==============================] - 1s 217us/step - loss: 0.0116 - val_loss: 0.0079\n",
      "Epoch 4/24\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3622/3622 [==============================] - 1s 215us/step - loss: 0.0063 - val_loss: 0.0040\n",
      "Epoch 5/24\n",
      "3622/3622 [==============================] - 1s 216us/step - loss: 0.0041 - val_loss: 0.0017\n",
      "Epoch 6/24\n",
      "3622/3622 [==============================] - 1s 216us/step - loss: 0.0029 - val_loss: 0.0014\n",
      "Epoch 7/24\n",
      "3622/3622 [==============================] - 1s 215us/step - loss: 0.0026 - val_loss: 9.0328e-04\n",
      "Epoch 8/24\n",
      "3622/3622 [==============================] - 1s 215us/step - loss: 0.0026 - val_loss: 0.0018\n",
      "Epoch 9/24\n",
      "3622/3622 [==============================] - 1s 216us/step - loss: 0.0023 - val_loss: 8.9126e-04\n",
      "Epoch 10/24\n",
      "3622/3622 [==============================] - 1s 214us/step - loss: 0.0022 - val_loss: 9.2276e-04\n",
      "Epoch 11/24\n",
      "3622/3622 [==============================] - 1s 215us/step - loss: 0.0021 - val_loss: 0.0018\n",
      "Epoch 12/24\n",
      "3622/3622 [==============================] - 1s 217us/step - loss: 0.0019 - val_loss: 0.0019\n",
      "Epoch 13/24\n",
      "3622/3622 [==============================] - 1s 216us/step - loss: 0.0020 - val_loss: 7.6504e-04\n",
      "Epoch 14/24\n",
      "3622/3622 [==============================] - 1s 214us/step - loss: 0.0018 - val_loss: 9.8296e-04\n",
      "Epoch 15/24\n",
      "3622/3622 [==============================] - 1s 216us/step - loss: 0.0018 - val_loss: 0.0023\n",
      "Epoch 16/24\n",
      "3622/3622 [==============================] - 1s 215us/step - loss: 0.0016 - val_loss: 0.0024\n",
      "Epoch 17/24\n",
      "3622/3622 [==============================] - 1s 216us/step - loss: 0.0016 - val_loss: 0.0020\n",
      "Epoch 18/24\n",
      "3622/3622 [==============================] - 1s 215us/step - loss: 0.0014 - val_loss: 0.0025\n",
      "Epoch 19/24\n",
      "3622/3622 [==============================] - 1s 219us/step - loss: 0.0015 - val_loss: 0.0026\n",
      "Epoch 20/24\n",
      "3622/3622 [==============================] - 1s 221us/step - loss: 0.0014 - val_loss: 0.0031\n",
      "Epoch 21/24\n",
      "3622/3622 [==============================] - 1s 216us/step - loss: 0.0015 - val_loss: 0.0034\n",
      "Epoch 22/24\n",
      "3622/3622 [==============================] - 1s 218us/step - loss: 0.0016 - val_loss: 0.0028\n",
      "Epoch 23/24\n",
      "3622/3622 [==============================] - 1s 216us/step - loss: 0.0014 - val_loss: 0.0019\n",
      "Epoch 24/24\n",
      "3622/3622 [==============================] - 1s 216us/step - loss: 0.0013 - val_loss: 0.0011\n",
      "Train on 3622 samples, validate on 403 samples\n",
      "Epoch 1/24\n",
      "3622/3622 [==============================] - 19s 5ms/step - loss: 0.1460 - val_loss: 0.0671\n",
      "Epoch 2/24\n",
      "3622/3622 [==============================] - 1s 215us/step - loss: 0.0219 - val_loss: 0.0218\n",
      "Epoch 3/24\n",
      "3622/3622 [==============================] - 1s 220us/step - loss: 0.0086 - val_loss: 0.0064\n",
      "Epoch 4/24\n",
      "3622/3622 [==============================] - 1s 215us/step - loss: 0.0047 - val_loss: 0.0061\n",
      "Epoch 5/24\n",
      "3622/3622 [==============================] - 1s 218us/step - loss: 0.0033 - val_loss: 0.0013\n",
      "Epoch 6/24\n",
      "3622/3622 [==============================] - 1s 216us/step - loss: 0.0026 - val_loss: 0.0024\n",
      "Epoch 7/24\n",
      "3622/3622 [==============================] - 1s 216us/step - loss: 0.0021 - val_loss: 9.1012e-04\n",
      "Epoch 8/24\n",
      "3622/3622 [==============================] - 1s 216us/step - loss: 0.0021 - val_loss: 0.0017\n",
      "Epoch 9/24\n",
      "3622/3622 [==============================] - 1s 215us/step - loss: 0.0021 - val_loss: 0.0013\n",
      "Epoch 10/24\n",
      "3622/3622 [==============================] - 1s 215us/step - loss: 0.0019 - val_loss: 0.0014\n",
      "Epoch 11/24\n",
      "3622/3622 [==============================] - 1s 218us/step - loss: 0.0019 - val_loss: 0.0021\n",
      "Epoch 12/24\n",
      "3622/3622 [==============================] - 1s 217us/step - loss: 0.0020 - val_loss: 0.0012\n",
      "Epoch 13/24\n",
      "3622/3622 [==============================] - 1s 216us/step - loss: 0.0019 - val_loss: 0.0019\n",
      "Epoch 14/24\n",
      "3622/3622 [==============================] - 1s 215us/step - loss: 0.0017 - val_loss: 0.0018\n",
      "Epoch 15/24\n",
      "3622/3622 [==============================] - 1s 218us/step - loss: 0.0017 - val_loss: 0.0026\n",
      "Epoch 16/24\n",
      "3622/3622 [==============================] - 1s 219us/step - loss: 0.0017 - val_loss: 0.0025\n",
      "Epoch 17/24\n",
      "3622/3622 [==============================] - 1s 216us/step - loss: 0.0016 - val_loss: 0.0039\n",
      "Epoch 18/24\n",
      "3622/3622 [==============================] - 1s 216us/step - loss: 0.0019 - val_loss: 0.0019\n",
      "Epoch 19/24\n",
      "3622/3622 [==============================] - 1s 216us/step - loss: 0.0017 - val_loss: 0.0011\n",
      "Epoch 20/24\n",
      "3622/3622 [==============================] - 1s 218us/step - loss: 0.0016 - val_loss: 0.0021\n",
      "Epoch 21/24\n",
      "3622/3622 [==============================] - 1s 216us/step - loss: 0.0016 - val_loss: 0.0033\n",
      "Epoch 22/24\n",
      "3622/3622 [==============================] - 1s 215us/step - loss: 0.0014 - val_loss: 0.0046\n",
      "Epoch 23/24\n",
      "3622/3622 [==============================] - 1s 221us/step - loss: 0.0014 - val_loss: 0.0036\n",
      "Epoch 24/24\n",
      "3622/3622 [==============================] - 1s 216us/step - loss: 0.0013 - val_loss: 0.0036\n",
      "Train on 3622 samples, validate on 403 samples\n",
      "Epoch 1/24\n",
      "3622/3622 [==============================] - 19s 5ms/step - loss: 4.0925 - val_loss: 0.0018\n",
      "Epoch 2/24\n",
      "3622/3622 [==============================] - 1s 214us/step - loss: 0.0128 - val_loss: 0.0073\n",
      "Epoch 3/24\n",
      "3622/3622 [==============================] - 1s 213us/step - loss: 0.0097 - val_loss: 0.0029\n",
      "Epoch 4/24\n",
      "3622/3622 [==============================] - 1s 218us/step - loss: 0.0082 - val_loss: 0.0032\n",
      "Epoch 5/24\n",
      "3622/3622 [==============================] - 1s 215us/step - loss: 0.0071 - val_loss: 0.0076\n",
      "Epoch 6/24\n",
      "3622/3622 [==============================] - 1s 214us/step - loss: 0.0066 - val_loss: 0.0027\n",
      "Epoch 7/24\n",
      "3622/3622 [==============================] - 1s 215us/step - loss: 0.0058 - val_loss: 0.0024\n",
      "Epoch 8/24\n",
      "3622/3622 [==============================] - 1s 212us/step - loss: 0.0055 - val_loss: 0.0024\n",
      "Epoch 9/24\n",
      "3622/3622 [==============================] - 1s 214us/step - loss: 0.0053 - val_loss: 0.0042\n",
      "Epoch 10/24\n",
      "3622/3622 [==============================] - 1s 214us/step - loss: 0.0049 - val_loss: 0.0017\n",
      "Epoch 11/24\n",
      "3622/3622 [==============================] - 1s 213us/step - loss: 0.0044 - val_loss: 0.0020\n",
      "Epoch 12/24\n",
      "3622/3622 [==============================] - 1s 221us/step - loss: 0.0042 - val_loss: 6.9889e-04\n",
      "Epoch 13/24\n",
      "3622/3622 [==============================] - 1s 213us/step - loss: 0.0040 - val_loss: 9.1954e-04\n",
      "Epoch 14/24\n",
      "3622/3622 [==============================] - 1s 215us/step - loss: 0.0036 - val_loss: 0.0013\n",
      "Epoch 15/24\n",
      "3622/3622 [==============================] - 1s 214us/step - loss: 0.0037 - val_loss: 0.0011\n",
      "Epoch 16/24\n",
      "3622/3622 [==============================] - 1s 213us/step - loss: 0.0034 - val_loss: 7.3679e-04\n",
      "Epoch 17/24\n",
      "3622/3622 [==============================] - 1s 214us/step - loss: 0.0031 - val_loss: 5.5893e-04\n",
      "Epoch 18/24\n",
      "3622/3622 [==============================] - 1s 214us/step - loss: 0.0032 - val_loss: 8.8082e-04\n",
      "Epoch 19/24\n",
      "3622/3622 [==============================] - 1s 215us/step - loss: 0.0030 - val_loss: 7.6129e-04\n",
      "Epoch 20/24\n",
      "3622/3622 [==============================] - 1s 213us/step - loss: 0.0030 - val_loss: 5.2946e-04\n",
      "Epoch 21/24\n",
      "3622/3622 [==============================] - 1s 214us/step - loss: 0.0028 - val_loss: 6.4953e-04\n",
      "Epoch 22/24\n",
      "3622/3622 [==============================] - 1s 213us/step - loss: 0.0028 - val_loss: 6.8008e-04\n",
      "Epoch 23/24\n",
      "3622/3622 [==============================] - 1s 212us/step - loss: 0.0027 - val_loss: 5.2493e-04\n",
      "Epoch 24/24\n",
      "3622/3622 [==============================] - 1s 214us/step - loss: 0.0026 - val_loss: 6.0999e-04\n",
      "Train on 3622 samples, validate on 403 samples\n",
      "Epoch 1/24\n",
      "3622/3622 [==============================] - 20s 6ms/step - loss: 5.7203 - val_loss: 0.0018\n",
      "Epoch 2/24\n",
      "3622/3622 [==============================] - 1s 214us/step - loss: 0.0139 - val_loss: 0.0017\n",
      "Epoch 3/24\n",
      "3622/3622 [==============================] - 1s 216us/step - loss: 0.0091 - val_loss: 0.0011\n",
      "Epoch 4/24\n",
      "3622/3622 [==============================] - 1s 218us/step - loss: 0.0074 - val_loss: 5.7589e-04\n",
      "Epoch 5/24\n",
      "3622/3622 [==============================] - 1s 215us/step - loss: 0.0072 - val_loss: 0.0013\n",
      "Epoch 6/24\n",
      "3622/3622 [==============================] - 1s 220us/step - loss: 0.0066 - val_loss: 0.0013\n",
      "Epoch 7/24\n",
      "3622/3622 [==============================] - 1s 215us/step - loss: 0.0064 - val_loss: 5.4488e-04\n",
      "Epoch 8/24\n",
      "3622/3622 [==============================] - 1s 216us/step - loss: 0.0060 - val_loss: 0.0013\n",
      "Epoch 9/24\n",
      "3622/3622 [==============================] - 1s 215us/step - loss: 0.0060 - val_loss: 0.0012\n",
      "Epoch 10/24\n",
      "3622/3622 [==============================] - 1s 216us/step - loss: 0.0058 - val_loss: 9.8810e-04\n",
      "Epoch 11/24\n",
      "3622/3622 [==============================] - 1s 215us/step - loss: 0.0054 - val_loss: 9.2836e-04\n",
      "Epoch 12/24\n",
      "3622/3622 [==============================] - 1s 214us/step - loss: 0.0052 - val_loss: 0.0013\n",
      "Epoch 13/24\n",
      "3622/3622 [==============================] - 1s 215us/step - loss: 0.0053 - val_loss: 0.0015\n",
      "Epoch 14/24\n",
      "3622/3622 [==============================] - 1s 214us/step - loss: 0.0055 - val_loss: 0.0019\n",
      "Epoch 15/24\n",
      "3622/3622 [==============================] - 1s 214us/step - loss: 0.0047 - val_loss: 8.5440e-04\n",
      "Epoch 16/24\n",
      "3622/3622 [==============================] - 1s 216us/step - loss: 0.0046 - val_loss: 0.0016\n",
      "Epoch 17/24\n",
      "3622/3622 [==============================] - 1s 213us/step - loss: 0.0047 - val_loss: 0.0016\n",
      "Epoch 18/24\n",
      "3622/3622 [==============================] - 1s 215us/step - loss: 0.0044 - val_loss: 0.0012\n",
      "Epoch 19/24\n",
      "3622/3622 [==============================] - 1s 214us/step - loss: 0.0042 - val_loss: 6.3698e-04\n",
      "Epoch 20/24\n",
      "3622/3622 [==============================] - 1s 216us/step - loss: 0.0043 - val_loss: 0.0018\n",
      "Epoch 21/24\n",
      "3622/3622 [==============================] - 1s 214us/step - loss: 0.0043 - val_loss: 8.6753e-04\n",
      "Epoch 22/24\n",
      "3622/3622 [==============================] - 1s 214us/step - loss: 0.0041 - val_loss: 0.0016\n",
      "Epoch 23/24\n",
      "3622/3622 [==============================] - 1s 215us/step - loss: 0.0041 - val_loss: 0.0013\n",
      "Epoch 24/24\n",
      "3622/3622 [==============================] - 1s 223us/step - loss: 0.0040 - val_loss: 0.0013\n",
      "Train on 3622 samples, validate on 403 samples\n",
      "Epoch 1/24\n",
      "3622/3622 [==============================] - 20s 5ms/step - loss: 6.1377 - val_loss: 0.0113\n",
      "Epoch 2/24\n",
      "3622/3622 [==============================] - 1s 216us/step - loss: 0.0259 - val_loss: 0.0014\n",
      "Epoch 3/24\n",
      "3622/3622 [==============================] - 1s 219us/step - loss: 0.0133 - val_loss: 0.0044\n",
      "Epoch 4/24\n",
      "3622/3622 [==============================] - 1s 217us/step - loss: 0.0114 - val_loss: 0.0039\n",
      "Epoch 5/24\n",
      "3622/3622 [==============================] - 1s 215us/step - loss: 0.0095 - val_loss: 0.0049\n",
      "Epoch 6/24\n",
      "3622/3622 [==============================] - 1s 215us/step - loss: 0.0084 - val_loss: 0.0056\n",
      "Epoch 7/24\n",
      "3622/3622 [==============================] - 1s 217us/step - loss: 0.0075 - val_loss: 0.0033\n",
      "Epoch 8/24\n",
      "3622/3622 [==============================] - 1s 215us/step - loss: 0.0074 - val_loss: 0.0061\n",
      "Epoch 9/24\n",
      "3622/3622 [==============================] - 1s 216us/step - loss: 0.0065 - val_loss: 0.0069\n",
      "Epoch 10/24\n",
      "3622/3622 [==============================] - 1s 214us/step - loss: 0.0063 - val_loss: 0.0019\n",
      "Epoch 11/24\n",
      "3622/3622 [==============================] - 1s 217us/step - loss: 0.0057 - val_loss: 0.0052\n",
      "Epoch 12/24\n",
      "3622/3622 [==============================] - 1s 216us/step - loss: 0.0055 - val_loss: 0.0052\n",
      "Epoch 13/24\n",
      "3622/3622 [==============================] - 1s 216us/step - loss: 0.0051 - val_loss: 0.0024\n",
      "Epoch 14/24\n",
      "3622/3622 [==============================] - 1s 216us/step - loss: 0.0049 - val_loss: 0.0068\n",
      "Epoch 15/24\n",
      "3622/3622 [==============================] - 1s 215us/step - loss: 0.0046 - val_loss: 0.0046\n",
      "Epoch 16/24\n",
      "3622/3622 [==============================] - 1s 217us/step - loss: 0.0045 - val_loss: 0.0028\n",
      "Epoch 17/24\n",
      "3622/3622 [==============================] - 1s 216us/step - loss: 0.0043 - val_loss: 0.0035\n",
      "Epoch 18/24\n",
      "3622/3622 [==============================] - 1s 216us/step - loss: 0.0041 - val_loss: 0.0020\n",
      "Epoch 19/24\n",
      "3622/3622 [==============================] - 1s 215us/step - loss: 0.0041 - val_loss: 0.0044\n",
      "Epoch 20/24\n",
      "3622/3622 [==============================] - 1s 216us/step - loss: 0.0038 - val_loss: 0.0031\n",
      "Epoch 21/24\n",
      "3622/3622 [==============================] - 1s 216us/step - loss: 0.0037 - val_loss: 0.0026\n",
      "Epoch 22/24\n",
      "3622/3622 [==============================] - 1s 216us/step - loss: 0.0035 - val_loss: 0.0020\n",
      "Epoch 23/24\n",
      "3622/3622 [==============================] - 1s 219us/step - loss: 0.0036 - val_loss: 0.0037\n",
      "Epoch 24/24\n",
      "3622/3622 [==============================] - 1s 220us/step - loss: 0.0035 - val_loss: 0.0039\n",
      "Train on 3622 samples, validate on 403 samples\n",
      "Epoch 1/24\n",
      "3622/3622 [==============================] - 22s 6ms/step - loss: 0.2311 - val_loss: 0.0032\n",
      "Epoch 2/24\n",
      "3622/3622 [==============================] - 2s 551us/step - loss: 0.0229 - val_loss: 0.0021\n",
      "Epoch 3/24\n",
      "3622/3622 [==============================] - 2s 549us/step - loss: 0.0045 - val_loss: 0.0421\n",
      "Epoch 4/24\n",
      "3622/3622 [==============================] - 2s 550us/step - loss: 0.0064 - val_loss: 0.0087\n",
      "Epoch 5/24\n",
      "3622/3622 [==============================] - 2s 550us/step - loss: 0.0022 - val_loss: 0.0035\n",
      "Epoch 6/24\n",
      "3622/3622 [==============================] - 2s 552us/step - loss: 0.0020 - val_loss: 0.0017\n",
      "Epoch 7/24\n",
      "3622/3622 [==============================] - 2s 549us/step - loss: 0.0025 - val_loss: 0.0053\n",
      "Epoch 8/24\n",
      "3622/3622 [==============================] - 2s 552us/step - loss: 0.0018 - val_loss: 0.0346\n",
      "Epoch 9/24\n",
      "3622/3622 [==============================] - 2s 552us/step - loss: 0.0022 - val_loss: 0.0070\n",
      "Epoch 10/24\n",
      "3622/3622 [==============================] - 2s 555us/step - loss: 0.0017 - val_loss: 0.0021\n",
      "Epoch 11/24\n",
      "3622/3622 [==============================] - 2s 552us/step - loss: 0.0022 - val_loss: 0.0036\n",
      "Epoch 12/24\n",
      "3622/3622 [==============================] - 2s 555us/step - loss: 0.0017 - val_loss: 0.0028\n",
      "Epoch 13/24\n",
      "3622/3622 [==============================] - 2s 553us/step - loss: 0.0016 - val_loss: 0.0023\n",
      "Epoch 14/24\n",
      "3622/3622 [==============================] - 2s 550us/step - loss: 0.0017 - val_loss: 0.0018\n",
      "Epoch 15/24\n",
      "3622/3622 [==============================] - 2s 552us/step - loss: 0.0024 - val_loss: 0.0016\n",
      "Epoch 16/24\n",
      "3622/3622 [==============================] - 2s 557us/step - loss: 0.0017 - val_loss: 0.0054\n",
      "Epoch 17/24\n",
      "3622/3622 [==============================] - 2s 552us/step - loss: 0.0016 - val_loss: 0.0108\n",
      "Epoch 18/24\n",
      "3622/3622 [==============================] - 2s 551us/step - loss: 0.0018 - val_loss: 0.0139\n",
      "Epoch 19/24\n",
      "3622/3622 [==============================] - 2s 552us/step - loss: 0.0021 - val_loss: 0.0071\n",
      "Epoch 20/24\n",
      "3622/3622 [==============================] - 2s 550us/step - loss: 0.0016 - val_loss: 0.0024\n",
      "Epoch 21/24\n",
      "3622/3622 [==============================] - 2s 552us/step - loss: 0.0014 - val_loss: 0.0044\n",
      "Epoch 22/24\n",
      "3622/3622 [==============================] - 2s 547us/step - loss: 0.0015 - val_loss: 0.0036\n",
      "Epoch 23/24\n",
      "3622/3622 [==============================] - 2s 555us/step - loss: 0.0013 - val_loss: 0.0095\n",
      "Epoch 24/24\n",
      "3622/3622 [==============================] - 2s 555us/step - loss: 0.0020 - val_loss: 0.0075\n",
      "Train on 3622 samples, validate on 403 samples\n",
      "Epoch 1/24\n",
      "3622/3622 [==============================] - 22s 6ms/step - loss: 0.2901 - val_loss: 0.0042\n",
      "Epoch 2/24\n",
      "3622/3622 [==============================] - 2s 495us/step - loss: 0.0173 - val_loss: 0.0117\n",
      "Epoch 3/24\n",
      "3622/3622 [==============================] - 2s 494us/step - loss: 0.0062 - val_loss: 0.0188\n",
      "Epoch 4/24\n",
      "3622/3622 [==============================] - 2s 497us/step - loss: 0.0041 - val_loss: 0.0049\n",
      "Epoch 5/24\n",
      "3622/3622 [==============================] - 2s 494us/step - loss: 0.0024 - val_loss: 0.0055\n",
      "Epoch 6/24\n",
      "3622/3622 [==============================] - 2s 497us/step - loss: 0.0022 - val_loss: 0.0113\n",
      "Epoch 7/24\n",
      "3622/3622 [==============================] - 2s 498us/step - loss: 0.0027 - val_loss: 0.0055\n",
      "Epoch 8/24\n",
      "3622/3622 [==============================] - 2s 494us/step - loss: 0.0019 - val_loss: 0.0026\n",
      "Epoch 9/24\n",
      "3622/3622 [==============================] - 2s 503us/step - loss: 0.0016 - val_loss: 0.0048\n",
      "Epoch 10/24\n",
      "3622/3622 [==============================] - 2s 503us/step - loss: 0.0017 - val_loss: 0.0020\n",
      "Epoch 11/24\n",
      "3622/3622 [==============================] - 2s 497us/step - loss: 0.0016 - val_loss: 0.0049\n",
      "Epoch 12/24\n",
      "3622/3622 [==============================] - 2s 495us/step - loss: 0.0016 - val_loss: 0.0020\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13/24\n",
      "3622/3622 [==============================] - 2s 494us/step - loss: 0.0015 - val_loss: 0.0034\n",
      "Epoch 14/24\n",
      "3622/3622 [==============================] - 2s 498us/step - loss: 0.0015 - val_loss: 0.0026\n",
      "Epoch 15/24\n",
      "3622/3622 [==============================] - 2s 495us/step - loss: 0.0015 - val_loss: 0.0013\n",
      "Epoch 16/24\n",
      "3622/3622 [==============================] - 2s 495us/step - loss: 0.0015 - val_loss: 0.0028\n",
      "Epoch 17/24\n",
      "3622/3622 [==============================] - 2s 495us/step - loss: 0.0015 - val_loss: 0.0020\n",
      "Epoch 18/24\n",
      "3622/3622 [==============================] - 2s 500us/step - loss: 0.0014 - val_loss: 0.0013\n",
      "Epoch 19/24\n",
      "3622/3622 [==============================] - 2s 492us/step - loss: 0.0016 - val_loss: 0.0011\n",
      "Epoch 20/24\n",
      "3622/3622 [==============================] - 2s 494us/step - loss: 0.0018 - val_loss: 0.0017\n",
      "Epoch 21/24\n",
      "3622/3622 [==============================] - 2s 494us/step - loss: 0.0013 - val_loss: 0.0011\n",
      "Epoch 22/24\n",
      "3622/3622 [==============================] - 2s 495us/step - loss: 0.0014 - val_loss: 0.0024\n",
      "Epoch 23/24\n",
      "3622/3622 [==============================] - 2s 496us/step - loss: 0.0013 - val_loss: 0.0082\n",
      "Epoch 24/24\n",
      "3622/3622 [==============================] - 2s 497us/step - loss: 0.0018 - val_loss: 0.0028\n",
      "\n",
      "GENERATION 15\n",
      "\n",
      "Train on 3622 samples, validate on 403 samples\n",
      "Epoch 1/24\n",
      "3622/3622 [==============================] - 21s 6ms/step - loss: 0.1151 - val_loss: 0.0181\n",
      "Epoch 2/24\n",
      "3622/3622 [==============================] - 1s 218us/step - loss: 0.0245 - val_loss: 0.0020\n",
      "Epoch 3/24\n",
      "3622/3622 [==============================] - 1s 218us/step - loss: 0.0077 - val_loss: 0.0014\n",
      "Epoch 4/24\n",
      "3622/3622 [==============================] - 1s 219us/step - loss: 0.0035 - val_loss: 5.7006e-04\n",
      "Epoch 5/24\n",
      "3622/3622 [==============================] - 1s 219us/step - loss: 0.0018 - val_loss: 0.0046\n",
      "Epoch 6/24\n",
      "3622/3622 [==============================] - 1s 225us/step - loss: 0.0012 - val_loss: 5.2470e-04\n",
      "Epoch 7/24\n",
      "3622/3622 [==============================] - 1s 222us/step - loss: 9.9502e-04 - val_loss: 0.0025\n",
      "Epoch 8/24\n",
      "3622/3622 [==============================] - 1s 218us/step - loss: 8.5413e-04 - val_loss: 6.5399e-04\n",
      "Epoch 9/24\n",
      "3622/3622 [==============================] - 1s 217us/step - loss: 7.9366e-04 - val_loss: 0.0015\n",
      "Epoch 10/24\n",
      "3622/3622 [==============================] - 1s 220us/step - loss: 7.2964e-04 - val_loss: 8.2535e-04\n",
      "Epoch 11/24\n",
      "3622/3622 [==============================] - 1s 216us/step - loss: 6.8113e-04 - val_loss: 0.0011\n",
      "Epoch 12/24\n",
      "3622/3622 [==============================] - 1s 218us/step - loss: 6.5250e-04 - val_loss: 9.1620e-04\n",
      "Epoch 13/24\n",
      "3622/3622 [==============================] - 1s 218us/step - loss: 6.2756e-04 - val_loss: 7.5981e-04\n",
      "Epoch 14/24\n",
      "3622/3622 [==============================] - 1s 218us/step - loss: 6.0065e-04 - val_loss: 9.7517e-04\n",
      "Epoch 15/24\n",
      "3622/3622 [==============================] - 1s 216us/step - loss: 5.8005e-04 - val_loss: 7.8790e-04\n",
      "Epoch 16/24\n",
      "3622/3622 [==============================] - 1s 217us/step - loss: 5.5315e-04 - val_loss: 7.4059e-04\n",
      "Epoch 17/24\n",
      "3622/3622 [==============================] - 1s 219us/step - loss: 5.3496e-04 - val_loss: 6.2999e-04\n",
      "Epoch 18/24\n",
      "3622/3622 [==============================] - 1s 219us/step - loss: 5.2110e-04 - val_loss: 7.3259e-04\n",
      "Epoch 19/24\n",
      "3622/3622 [==============================] - 1s 219us/step - loss: 5.0308e-04 - val_loss: 6.6671e-04\n",
      "Epoch 20/24\n",
      "3622/3622 [==============================] - 1s 219us/step - loss: 4.8780e-04 - val_loss: 6.5525e-04\n",
      "Epoch 21/24\n",
      "3622/3622 [==============================] - 1s 224us/step - loss: 4.7245e-04 - val_loss: 5.8399e-04\n",
      "Epoch 22/24\n",
      "3622/3622 [==============================] - 1s 218us/step - loss: 4.6154e-04 - val_loss: 5.4524e-04\n",
      "Epoch 23/24\n",
      "3622/3622 [==============================] - 1s 218us/step - loss: 4.5149e-04 - val_loss: 5.5545e-04\n",
      "Epoch 24/24\n",
      "3622/3622 [==============================] - 1s 218us/step - loss: 4.3968e-04 - val_loss: 5.6289e-04\n",
      "Train on 3622 samples, validate on 403 samples\n",
      "Epoch 1/24\n",
      "3622/3622 [==============================] - 21s 6ms/step - loss: 0.1124 - val_loss: 0.0497\n",
      "Epoch 2/24\n",
      "3622/3622 [==============================] - 1s 223us/step - loss: 0.0134 - val_loss: 0.0104\n",
      "Epoch 3/24\n",
      "3622/3622 [==============================] - 1s 219us/step - loss: 0.0047 - val_loss: 0.0018\n",
      "Epoch 4/24\n",
      "3622/3622 [==============================] - 1s 221us/step - loss: 0.0019 - val_loss: 7.3737e-04\n",
      "Epoch 5/24\n",
      "3622/3622 [==============================] - 1s 220us/step - loss: 9.0031e-04 - val_loss: 8.3516e-04\n",
      "Epoch 6/24\n",
      "3622/3622 [==============================] - 1s 221us/step - loss: 7.8420e-04 - val_loss: 6.3350e-04\n",
      "Epoch 7/24\n",
      "3622/3622 [==============================] - 1s 220us/step - loss: 6.5821e-04 - val_loss: 7.6180e-04\n",
      "Epoch 8/24\n",
      "3622/3622 [==============================] - 1s 221us/step - loss: 6.0818e-04 - val_loss: 6.9088e-04\n",
      "Epoch 9/24\n",
      "3622/3622 [==============================] - 1s 220us/step - loss: 6.0199e-04 - val_loss: 6.3605e-04\n",
      "Epoch 10/24\n",
      "3622/3622 [==============================] - 1s 221us/step - loss: 5.9989e-04 - val_loss: 5.7792e-04\n",
      "Epoch 11/24\n",
      "3622/3622 [==============================] - 1s 222us/step - loss: 5.7795e-04 - val_loss: 6.0577e-04\n",
      "Epoch 12/24\n",
      "3622/3622 [==============================] - 1s 226us/step - loss: 5.5945e-04 - val_loss: 6.7089e-04\n",
      "Epoch 13/24\n",
      "3622/3622 [==============================] - 1s 219us/step - loss: 5.3186e-04 - val_loss: 6.2054e-04\n",
      "Epoch 14/24\n",
      "3622/3622 [==============================] - 1s 221us/step - loss: 5.0768e-04 - val_loss: 6.1126e-04\n",
      "Epoch 15/24\n",
      "3622/3622 [==============================] - 1s 219us/step - loss: 4.9648e-04 - val_loss: 5.5665e-04\n",
      "Epoch 16/24\n",
      "3622/3622 [==============================] - 1s 221us/step - loss: 4.5504e-04 - val_loss: 5.4224e-04\n",
      "Epoch 17/24\n",
      "3622/3622 [==============================] - 1s 220us/step - loss: 4.4389e-04 - val_loss: 5.1975e-04\n",
      "Epoch 18/24\n",
      "3622/3622 [==============================] - 1s 220us/step - loss: 4.4284e-04 - val_loss: 5.0575e-04\n",
      "Epoch 19/24\n",
      "3622/3622 [==============================] - 1s 220us/step - loss: 4.2644e-04 - val_loss: 4.8414e-04\n",
      "Epoch 20/24\n",
      "3622/3622 [==============================] - 1s 222us/step - loss: 3.9535e-04 - val_loss: 4.3078e-04\n",
      "Epoch 21/24\n",
      "3622/3622 [==============================] - 1s 219us/step - loss: 3.6987e-04 - val_loss: 4.0480e-04\n",
      "Epoch 22/24\n",
      "3622/3622 [==============================] - 1s 224us/step - loss: 3.5331e-04 - val_loss: 4.1734e-04\n",
      "Epoch 23/24\n",
      "3622/3622 [==============================] - 1s 220us/step - loss: 3.4880e-04 - val_loss: 4.9371e-04\n",
      "Epoch 24/24\n",
      "3622/3622 [==============================] - 1s 220us/step - loss: 3.8430e-04 - val_loss: 4.4928e-04\n",
      "Train on 3622 samples, validate on 403 samples\n",
      "Epoch 1/24\n",
      "3622/3622 [==============================] - 22s 6ms/step - loss: 0.1373 - val_loss: 0.0442\n",
      "Epoch 2/24\n",
      "3622/3622 [==============================] - 1s 286us/step - loss: 0.0143 - val_loss: 0.0018\n",
      "Epoch 3/24\n",
      "3622/3622 [==============================] - 1s 282us/step - loss: 0.0045 - val_loss: 0.0053\n",
      "Epoch 4/24\n",
      "3622/3622 [==============================] - 1s 285us/step - loss: 0.0026 - val_loss: 0.0022\n",
      "Epoch 5/24\n",
      "3622/3622 [==============================] - 1s 284us/step - loss: 0.0014 - val_loss: 8.7274e-04\n",
      "Epoch 6/24\n",
      "3622/3622 [==============================] - 1s 284us/step - loss: 9.6962e-04 - val_loss: 0.0012\n",
      "Epoch 7/24\n",
      "3622/3622 [==============================] - 1s 286us/step - loss: 9.2470e-04 - val_loss: 8.6196e-04\n",
      "Epoch 8/24\n",
      "3622/3622 [==============================] - 1s 283us/step - loss: 7.9702e-04 - val_loss: 8.4394e-04\n",
      "Epoch 9/24\n",
      "3622/3622 [==============================] - 1s 283us/step - loss: 7.6125e-04 - val_loss: 7.7734e-04\n",
      "Epoch 10/24\n",
      "3622/3622 [==============================] - 1s 282us/step - loss: 7.3043e-04 - val_loss: 7.4982e-04\n",
      "Epoch 11/24\n",
      "3622/3622 [==============================] - 1s 285us/step - loss: 7.0714e-04 - val_loss: 7.5134e-04\n",
      "Epoch 12/24\n",
      "3622/3622 [==============================] - 1s 289us/step - loss: 6.9378e-04 - val_loss: 7.0286e-04\n",
      "Epoch 13/24\n",
      "3622/3622 [==============================] - 1s 289us/step - loss: 6.5825e-04 - val_loss: 6.8292e-04\n",
      "Epoch 14/24\n",
      "3622/3622 [==============================] - 1s 283us/step - loss: 6.2801e-04 - val_loss: 6.3787e-04\n",
      "Epoch 15/24\n",
      "3622/3622 [==============================] - 1s 284us/step - loss: 6.0191e-04 - val_loss: 6.0845e-04\n",
      "Epoch 16/24\n",
      "3622/3622 [==============================] - 1s 283us/step - loss: 5.7590e-04 - val_loss: 5.8437e-04\n",
      "Epoch 17/24\n",
      "3622/3622 [==============================] - 1s 284us/step - loss: 5.4315e-04 - val_loss: 5.6538e-04\n",
      "Epoch 18/24\n",
      "3622/3622 [==============================] - 1s 284us/step - loss: 5.1983e-04 - val_loss: 5.2427e-04\n",
      "Epoch 19/24\n",
      "3622/3622 [==============================] - 1s 284us/step - loss: 4.9629e-04 - val_loss: 5.7675e-04\n",
      "Epoch 20/24\n",
      "3622/3622 [==============================] - 1s 284us/step - loss: 5.0998e-04 - val_loss: 5.6265e-04\n",
      "Epoch 21/24\n",
      "3622/3622 [==============================] - 1s 283us/step - loss: 4.6442e-04 - val_loss: 4.6497e-04\n",
      "Epoch 22/24\n",
      "3622/3622 [==============================] - 1s 285us/step - loss: 4.3119e-04 - val_loss: 5.4514e-04\n",
      "Epoch 23/24\n",
      "3622/3622 [==============================] - 1s 286us/step - loss: 4.8793e-04 - val_loss: 5.1067e-04\n",
      "Epoch 24/24\n",
      "3622/3622 [==============================] - 1s 283us/step - loss: 4.2176e-04 - val_loss: 4.5919e-04\n",
      "Train on 3622 samples, validate on 403 samples\n",
      "Epoch 1/24\n",
      "3622/3622 [==============================] - 22s 6ms/step - loss: 0.0673 - val_loss: 0.0030\n",
      "Epoch 2/24\n",
      "3622/3622 [==============================] - 1s 243us/step - loss: 0.0036 - val_loss: 0.0251\n",
      "Epoch 3/24\n",
      "3622/3622 [==============================] - 1s 248us/step - loss: 0.0057 - val_loss: 0.0012\n",
      "Epoch 4/24\n",
      "3622/3622 [==============================] - 1s 243us/step - loss: 0.0013 - val_loss: 0.0069\n",
      "Epoch 5/24\n",
      "3622/3622 [==============================] - 1s 243us/step - loss: 0.0023 - val_loss: 0.0016\n",
      "Epoch 6/24\n",
      "3622/3622 [==============================] - 1s 243us/step - loss: 8.5783e-04 - val_loss: 7.0486e-04\n",
      "Epoch 7/24\n",
      "3622/3622 [==============================] - 1s 245us/step - loss: 6.0730e-04 - val_loss: 6.1076e-04\n",
      "Epoch 8/24\n",
      "3622/3622 [==============================] - 1s 244us/step - loss: 0.0019 - val_loss: 0.0164\n",
      "Epoch 9/24\n",
      "3622/3622 [==============================] - 1s 243us/step - loss: 0.0052 - val_loss: 0.0021\n",
      "Epoch 10/24\n",
      "3622/3622 [==============================] - 1s 244us/step - loss: 0.0015 - val_loss: 8.5791e-04\n",
      "Epoch 11/24\n",
      "3622/3622 [==============================] - 1s 243us/step - loss: 0.0011 - val_loss: 0.0050\n",
      "Epoch 12/24\n",
      "3622/3622 [==============================] - 1s 248us/step - loss: 0.0028 - val_loss: 9.0492e-04\n",
      "Epoch 13/24\n",
      "3622/3622 [==============================] - 1s 241us/step - loss: 0.0012 - val_loss: 9.5221e-04\n",
      "Epoch 14/24\n",
      "3622/3622 [==============================] - 1s 242us/step - loss: 0.0013 - val_loss: 0.0041\n",
      "Epoch 15/24\n",
      "3622/3622 [==============================] - 1s 243us/step - loss: 0.0025 - val_loss: 0.0022\n",
      "Epoch 16/24\n",
      "3622/3622 [==============================] - 1s 244us/step - loss: 0.0014 - val_loss: 0.0011\n",
      "Epoch 17/24\n",
      "3622/3622 [==============================] - 1s 243us/step - loss: 0.0014 - val_loss: 0.0011\n",
      "Epoch 18/24\n",
      "3622/3622 [==============================] - 1s 246us/step - loss: 0.0013 - val_loss: 0.0055\n",
      "Epoch 19/24\n",
      "3622/3622 [==============================] - 1s 248us/step - loss: 0.0027 - val_loss: 0.0020\n",
      "Epoch 20/24\n",
      "3622/3622 [==============================] - 1s 242us/step - loss: 0.0016 - val_loss: 0.0013\n",
      "Epoch 21/24\n",
      "3622/3622 [==============================] - 1s 249us/step - loss: 0.0012 - val_loss: 6.0558e-04\n",
      "Epoch 22/24\n",
      "3622/3622 [==============================] - 1s 243us/step - loss: 8.5358e-04 - val_loss: 0.0011\n",
      "Epoch 23/24\n",
      "3622/3622 [==============================] - 1s 243us/step - loss: 0.0014 - val_loss: 0.0027\n",
      "Epoch 24/24\n",
      "3622/3622 [==============================] - 1s 245us/step - loss: 0.0016 - val_loss: 0.0016\n",
      "Train on 3622 samples, validate on 403 samples\n",
      "Epoch 1/24\n",
      "3622/3622 [==============================] - 23s 6ms/step - loss: 0.0814 - val_loss: 0.0224\n",
      "Epoch 2/24\n",
      "3622/3622 [==============================] - 2s 528us/step - loss: 0.0104 - val_loss: 0.0056\n",
      "Epoch 3/24\n",
      "3622/3622 [==============================] - 2s 523us/step - loss: 0.0034 - val_loss: 0.0037\n",
      "Epoch 4/24\n",
      "3622/3622 [==============================] - 2s 522us/step - loss: 0.0022 - val_loss: 0.0018\n",
      "Epoch 5/24\n",
      "3622/3622 [==============================] - 2s 524us/step - loss: 0.0018 - val_loss: 0.0045\n",
      "Epoch 6/24\n",
      "3622/3622 [==============================] - 2s 525us/step - loss: 0.0016 - val_loss: 0.0018\n",
      "Epoch 7/24\n",
      "3622/3622 [==============================] - 2s 527us/step - loss: 0.0015 - val_loss: 0.0023\n",
      "Epoch 8/24\n",
      "3622/3622 [==============================] - 2s 521us/step - loss: 0.0015 - val_loss: 0.0022\n",
      "Epoch 9/24\n",
      "3622/3622 [==============================] - 2s 524us/step - loss: 0.0014 - val_loss: 0.0018\n",
      "Epoch 10/24\n",
      "3622/3622 [==============================] - 2s 525us/step - loss: 0.0014 - val_loss: 0.0020\n",
      "Epoch 11/24\n",
      "3622/3622 [==============================] - 2s 523us/step - loss: 0.0014 - val_loss: 0.0017\n",
      "Epoch 12/24\n",
      "3622/3622 [==============================] - 2s 526us/step - loss: 0.0014 - val_loss: 0.0019\n",
      "Epoch 13/24\n",
      "3622/3622 [==============================] - 2s 524us/step - loss: 0.0013 - val_loss: 0.0018\n",
      "Epoch 14/24\n",
      "3622/3622 [==============================] - 2s 529us/step - loss: 0.0013 - val_loss: 0.0015\n",
      "Epoch 15/24\n",
      "3622/3622 [==============================] - 2s 523us/step - loss: 0.0014 - val_loss: 0.0021\n",
      "Epoch 16/24\n",
      "3622/3622 [==============================] - 2s 525us/step - loss: 0.0013 - val_loss: 0.0015\n",
      "Epoch 17/24\n",
      "3622/3622 [==============================] - 2s 522us/step - loss: 0.0013 - val_loss: 0.0016\n",
      "Epoch 18/24\n",
      "3622/3622 [==============================] - 2s 527us/step - loss: 0.0013 - val_loss: 0.0017\n",
      "Epoch 19/24\n",
      "3622/3622 [==============================] - 2s 523us/step - loss: 0.0013 - val_loss: 0.0014\n",
      "Epoch 20/24\n",
      "3622/3622 [==============================] - 2s 522us/step - loss: 0.0012 - val_loss: 0.0015\n",
      "Epoch 21/24\n",
      "3622/3622 [==============================] - 2s 522us/step - loss: 0.0012 - val_loss: 0.0015\n",
      "Epoch 22/24\n",
      "3622/3622 [==============================] - 2s 526us/step - loss: 0.0012 - val_loss: 0.0015\n",
      "Epoch 23/24\n",
      "3622/3622 [==============================] - 2s 523us/step - loss: 0.0012 - val_loss: 0.0014\n",
      "Epoch 24/24\n",
      "3622/3622 [==============================] - 2s 525us/step - loss: 0.0012 - val_loss: 0.0014\n",
      "Train on 3622 samples, validate on 403 samples\n",
      "Epoch 1/24\n",
      "3622/3622 [==============================] - 23s 6ms/step - loss: 0.7058 - val_loss: 0.0636\n",
      "Epoch 2/24\n",
      "3622/3622 [==============================] - 2s 448us/step - loss: 0.0117 - val_loss: 0.0029\n",
      "Epoch 3/24\n",
      "3622/3622 [==============================] - 2s 447us/step - loss: 0.0048 - val_loss: 0.0099\n",
      "Epoch 4/24\n",
      "3622/3622 [==============================] - 2s 452us/step - loss: 0.0046 - val_loss: 0.0030\n",
      "Epoch 5/24\n",
      "3622/3622 [==============================] - 2s 444us/step - loss: 0.0034 - val_loss: 0.0018\n",
      "Epoch 6/24\n",
      "3622/3622 [==============================] - 2s 448us/step - loss: 0.0027 - val_loss: 0.0016\n",
      "Epoch 7/24\n",
      "3622/3622 [==============================] - 2s 444us/step - loss: 0.0025 - val_loss: 0.0028\n",
      "Epoch 8/24\n",
      "3622/3622 [==============================] - 2s 446us/step - loss: 0.0031 - val_loss: 0.0021\n",
      "Epoch 9/24\n",
      "3622/3622 [==============================] - 2s 446us/step - loss: 0.0026 - val_loss: 0.0026\n",
      "Epoch 10/24\n",
      "3622/3622 [==============================] - 2s 448us/step - loss: 0.0027 - val_loss: 0.0038\n",
      "Epoch 11/24\n",
      "3622/3622 [==============================] - 2s 444us/step - loss: 0.0034 - val_loss: 0.0020\n",
      "Epoch 12/24\n",
      "3622/3622 [==============================] - 2s 445us/step - loss: 0.0026 - val_loss: 0.0016\n",
      "Epoch 13/24\n",
      "3622/3622 [==============================] - 2s 445us/step - loss: 0.0023 - val_loss: 0.0014\n",
      "Epoch 14/24\n",
      "3622/3622 [==============================] - 2s 446us/step - loss: 0.0022 - val_loss: 0.0017\n",
      "Epoch 15/24\n",
      "3622/3622 [==============================] - 2s 451us/step - loss: 0.0025 - val_loss: 0.0023\n",
      "Epoch 16/24\n",
      "3622/3622 [==============================] - 2s 448us/step - loss: 0.0026 - val_loss: 0.0015\n",
      "Epoch 17/24\n",
      "3622/3622 [==============================] - 2s 452us/step - loss: 0.0022 - val_loss: 0.0030\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18/24\n",
      "3622/3622 [==============================] - 2s 445us/step - loss: 0.0023 - val_loss: 0.0014\n",
      "Epoch 19/24\n",
      "3622/3622 [==============================] - 2s 445us/step - loss: 0.0022 - val_loss: 0.0017\n",
      "Epoch 20/24\n",
      "3622/3622 [==============================] - 2s 444us/step - loss: 0.0023 - val_loss: 0.0015\n",
      "Epoch 21/24\n",
      "3622/3622 [==============================] - 2s 446us/step - loss: 0.0022 - val_loss: 0.0014\n",
      "Epoch 22/24\n",
      "3622/3622 [==============================] - 2s 445us/step - loss: 0.0021 - val_loss: 0.0013\n",
      "Epoch 23/24\n",
      "3622/3622 [==============================] - 2s 445us/step - loss: 0.0020 - val_loss: 0.0014\n",
      "Epoch 24/24\n",
      "3622/3622 [==============================] - 2s 448us/step - loss: 0.0020 - val_loss: 0.0019\n",
      "Train on 3622 samples, validate on 403 samples\n",
      "Epoch 1/24\n",
      "3622/3622 [==============================] - 24s 7ms/step - loss: 0.2335 - val_loss: 0.0400\n",
      "Epoch 2/24\n",
      "3622/3622 [==============================] - 2s 500us/step - loss: 0.0698 - val_loss: 0.0330\n",
      "Epoch 3/24\n",
      "3622/3622 [==============================] - 2s 501us/step - loss: 0.0696 - val_loss: 0.0509\n",
      "Epoch 4/24\n",
      "3622/3622 [==============================] - 2s 504us/step - loss: 0.0698 - val_loss: 0.0501\n",
      "Epoch 5/24\n",
      "3622/3622 [==============================] - 2s 502us/step - loss: 0.0697 - val_loss: 0.0352\n",
      "Epoch 6/24\n",
      "3622/3622 [==============================] - 2s 502us/step - loss: 0.0695 - val_loss: 0.0320\n",
      "Epoch 7/24\n",
      "3622/3622 [==============================] - 2s 502us/step - loss: 0.0683 - val_loss: 0.0424\n",
      "Epoch 8/24\n",
      "3622/3622 [==============================] - 2s 504us/step - loss: 0.0673 - val_loss: 0.0521\n",
      "Epoch 9/24\n",
      "3622/3622 [==============================] - 2s 504us/step - loss: 0.0684 - val_loss: 0.0275\n",
      "Epoch 10/24\n",
      "3622/3622 [==============================] - 2s 500us/step - loss: 0.0667 - val_loss: 0.0661\n",
      "Epoch 11/24\n",
      "3622/3622 [==============================] - 2s 503us/step - loss: 0.0617 - val_loss: 0.0372\n",
      "Epoch 12/24\n",
      "3622/3622 [==============================] - 2s 502us/step - loss: 0.0440 - val_loss: 0.1292\n",
      "Epoch 13/24\n",
      "3622/3622 [==============================] - 2s 501us/step - loss: 0.0636 - val_loss: 0.0549\n",
      "Epoch 14/24\n",
      "3622/3622 [==============================] - 2s 506us/step - loss: 0.0647 - val_loss: 0.0119\n",
      "Epoch 15/24\n",
      "3622/3622 [==============================] - 2s 499us/step - loss: 0.0227 - val_loss: 0.0063\n",
      "Epoch 16/24\n",
      "3622/3622 [==============================] - 2s 505us/step - loss: 0.0132 - val_loss: 0.0100\n",
      "Epoch 17/24\n",
      "3622/3622 [==============================] - 2s 506us/step - loss: 0.0090 - val_loss: 0.0087\n",
      "Epoch 18/24\n",
      "3622/3622 [==============================] - 2s 500us/step - loss: 0.0074 - val_loss: 0.0089\n",
      "Epoch 19/24\n",
      "3622/3622 [==============================] - 2s 503us/step - loss: 0.0060 - val_loss: 0.0121\n",
      "Epoch 20/24\n",
      "3622/3622 [==============================] - 2s 504us/step - loss: 0.0052 - val_loss: 0.0035\n",
      "Epoch 21/24\n",
      "3622/3622 [==============================] - 2s 501us/step - loss: 0.0045 - val_loss: 0.0144\n",
      "Epoch 22/24\n",
      "3622/3622 [==============================] - 2s 502us/step - loss: 0.0030 - val_loss: 0.0142\n",
      "Epoch 23/24\n",
      "3622/3622 [==============================] - 2s 501us/step - loss: 0.0059 - val_loss: 0.0176\n",
      "Epoch 24/24\n",
      "3622/3622 [==============================] - 2s 503us/step - loss: 0.0028 - val_loss: 0.0054\n",
      "Train on 3622 samples, validate on 403 samples\n",
      "Epoch 1/24\n",
      "3622/3622 [==============================] - 24s 7ms/step - loss: 0.0396 - val_loss: 0.0140\n",
      "Epoch 2/24\n",
      "3622/3622 [==============================] - 2s 499us/step - loss: 0.0100 - val_loss: 0.0039\n",
      "Epoch 3/24\n",
      "3622/3622 [==============================] - 2s 501us/step - loss: 0.0055 - val_loss: 0.0132\n",
      "Epoch 4/24\n",
      "3622/3622 [==============================] - 2s 500us/step - loss: 0.0040 - val_loss: 0.0011\n",
      "Epoch 5/24\n",
      "3622/3622 [==============================] - 2s 502us/step - loss: 0.0029 - val_loss: 0.0050\n",
      "Epoch 6/24\n",
      "3622/3622 [==============================] - 2s 503us/step - loss: 0.0024 - val_loss: 0.0028\n",
      "Epoch 7/24\n",
      "3622/3622 [==============================] - 2s 505us/step - loss: 0.0019 - val_loss: 0.0013\n",
      "Epoch 8/24\n",
      "3622/3622 [==============================] - 2s 501us/step - loss: 0.0018 - val_loss: 0.0019\n",
      "Epoch 9/24\n",
      "3622/3622 [==============================] - 2s 501us/step - loss: 0.0015 - val_loss: 0.0016\n",
      "Epoch 10/24\n",
      "3622/3622 [==============================] - 2s 508us/step - loss: 0.0015 - val_loss: 9.8439e-04\n",
      "Epoch 11/24\n",
      "3622/3622 [==============================] - 2s 503us/step - loss: 0.0015 - val_loss: 9.8689e-04\n",
      "Epoch 12/24\n",
      "3622/3622 [==============================] - 2s 504us/step - loss: 0.0015 - val_loss: 0.0011\n",
      "Epoch 13/24\n",
      "3622/3622 [==============================] - 2s 500us/step - loss: 0.0015 - val_loss: 9.9707e-04\n",
      "Epoch 14/24\n",
      "3622/3622 [==============================] - 2s 500us/step - loss: 0.0015 - val_loss: 9.6392e-04\n",
      "Epoch 15/24\n",
      "3622/3622 [==============================] - 2s 500us/step - loss: 0.0015 - val_loss: 9.3773e-04\n",
      "Epoch 16/24\n",
      "3622/3622 [==============================] - 2s 506us/step - loss: 0.0014 - val_loss: 0.0011\n",
      "Epoch 17/24\n",
      "3622/3622 [==============================] - 2s 504us/step - loss: 0.0014 - val_loss: 0.0013\n",
      "Epoch 18/24\n",
      "3622/3622 [==============================] - 2s 501us/step - loss: 0.0013 - val_loss: 0.0011\n",
      "Epoch 19/24\n",
      "3622/3622 [==============================] - 2s 499us/step - loss: 0.0012 - val_loss: 0.0014\n",
      "Epoch 20/24\n",
      "3622/3622 [==============================] - 2s 500us/step - loss: 0.0012 - val_loss: 0.0019\n",
      "Epoch 21/24\n",
      "3622/3622 [==============================] - 2s 502us/step - loss: 0.0013 - val_loss: 0.0014\n",
      "Epoch 22/24\n",
      "3622/3622 [==============================] - 2s 500us/step - loss: 0.0012 - val_loss: 0.0011\n",
      "Epoch 23/24\n",
      "3622/3622 [==============================] - 2s 502us/step - loss: 0.0012 - val_loss: 9.8495e-04\n",
      "Epoch 24/24\n",
      "3622/3622 [==============================] - 2s 503us/step - loss: 0.0011 - val_loss: 0.0011\n",
      "Train on 3622 samples, validate on 403 samples\n",
      "Epoch 1/24\n",
      "3622/3622 [==============================] - 24s 7ms/step - loss: 0.0620 - val_loss: 0.0204\n",
      "Epoch 2/24\n",
      "3622/3622 [==============================] - 2s 504us/step - loss: 0.0116 - val_loss: 0.0063\n",
      "Epoch 3/24\n",
      "3622/3622 [==============================] - 2s 507us/step - loss: 0.0053 - val_loss: 0.0085\n",
      "Epoch 4/24\n",
      "3622/3622 [==============================] - 2s 504us/step - loss: 0.0041 - val_loss: 0.0017\n",
      "Epoch 5/24\n",
      "3622/3622 [==============================] - 2s 510us/step - loss: 0.0030 - val_loss: 0.0050\n",
      "Epoch 6/24\n",
      "3622/3622 [==============================] - 2s 507us/step - loss: 0.0026 - val_loss: 0.0021\n",
      "Epoch 7/24\n",
      "3622/3622 [==============================] - 2s 503us/step - loss: 0.0022 - val_loss: 0.0022\n",
      "Epoch 8/24\n",
      "3622/3622 [==============================] - 2s 504us/step - loss: 0.0020 - val_loss: 0.0027\n",
      "Epoch 9/24\n",
      "3622/3622 [==============================] - 2s 504us/step - loss: 0.0018 - val_loss: 0.0026\n",
      "Epoch 10/24\n",
      "3622/3622 [==============================] - 2s 504us/step - loss: 0.0017 - val_loss: 0.0014\n",
      "Epoch 11/24\n",
      "3622/3622 [==============================] - 2s 506us/step - loss: 0.0016 - val_loss: 0.0016\n",
      "Epoch 12/24\n",
      "3622/3622 [==============================] - 2s 502us/step - loss: 0.0016 - val_loss: 0.0017\n",
      "Epoch 13/24\n",
      "3622/3622 [==============================] - 2s 504us/step - loss: 0.0015 - val_loss: 0.0018\n",
      "Epoch 14/24\n",
      "3622/3622 [==============================] - 2s 510us/step - loss: 0.0015 - val_loss: 0.0015\n",
      "Epoch 15/24\n",
      "3622/3622 [==============================] - 2s 503us/step - loss: 0.0014 - val_loss: 0.0016\n",
      "Epoch 16/24\n",
      "3622/3622 [==============================] - 2s 505us/step - loss: 0.0014 - val_loss: 0.0011\n",
      "Epoch 17/24\n",
      "3622/3622 [==============================] - 2s 505us/step - loss: 0.0014 - val_loss: 9.3503e-04\n",
      "Epoch 18/24\n",
      "3622/3622 [==============================] - 2s 507us/step - loss: 0.0015 - val_loss: 0.0010\n",
      "Epoch 19/24\n",
      "3622/3622 [==============================] - 2s 503us/step - loss: 0.0014 - val_loss: 9.4123e-04\n",
      "Epoch 20/24\n",
      "3622/3622 [==============================] - 2s 502us/step - loss: 0.0013 - val_loss: 8.9723e-04\n",
      "Epoch 21/24\n",
      "3622/3622 [==============================] - 2s 504us/step - loss: 0.0013 - val_loss: 9.1309e-04\n",
      "Epoch 22/24\n",
      "3622/3622 [==============================] - 2s 508us/step - loss: 0.0014 - val_loss: 9.9581e-04\n",
      "Epoch 23/24\n",
      "3622/3622 [==============================] - 2s 508us/step - loss: 0.0013 - val_loss: 0.0011\n",
      "Epoch 24/24\n",
      "3622/3622 [==============================] - 2s 505us/step - loss: 0.0012 - val_loss: 8.6575e-04\n",
      "\n",
      "GENERATION 16\n",
      "\n",
      "Train on 3622 samples, validate on 403 samples\n",
      "Epoch 1/24\n",
      "3622/3622 [==============================] - 25s 7ms/step - loss: 0.0833 - val_loss: 0.0316\n",
      "Epoch 2/24\n",
      "3622/3622 [==============================] - 2s 498us/step - loss: 0.0146 - val_loss: 0.0072\n",
      "Epoch 3/24\n",
      "3622/3622 [==============================] - 2s 498us/step - loss: 0.0047 - val_loss: 0.0024\n",
      "Epoch 4/24\n",
      "3622/3622 [==============================] - 2s 501us/step - loss: 0.0025 - val_loss: 0.0023\n",
      "Epoch 5/24\n",
      "3622/3622 [==============================] - 2s 499us/step - loss: 0.0017 - val_loss: 0.0013\n",
      "Epoch 6/24\n",
      "3622/3622 [==============================] - 2s 500us/step - loss: 0.0016 - val_loss: 0.0024\n",
      "Epoch 7/24\n",
      "3622/3622 [==============================] - 2s 494us/step - loss: 0.0013 - val_loss: 0.0022\n",
      "Epoch 8/24\n",
      "3622/3622 [==============================] - 2s 496us/step - loss: 0.0013 - val_loss: 0.0014\n",
      "Epoch 9/24\n",
      "3622/3622 [==============================] - 2s 496us/step - loss: 0.0011 - val_loss: 0.0012\n",
      "Epoch 10/24\n",
      "3622/3622 [==============================] - 2s 499us/step - loss: 0.0011 - val_loss: 0.0012\n",
      "Epoch 11/24\n",
      "3622/3622 [==============================] - 2s 495us/step - loss: 0.0011 - val_loss: 0.0012\n",
      "Epoch 12/24\n",
      "3622/3622 [==============================] - 2s 499us/step - loss: 0.0010 - val_loss: 0.0012\n",
      "Epoch 13/24\n",
      "3622/3622 [==============================] - 2s 499us/step - loss: 0.0010 - val_loss: 0.0011\n",
      "Epoch 14/24\n",
      "3622/3622 [==============================] - 2s 502us/step - loss: 9.6531e-04 - val_loss: 0.0011\n",
      "Epoch 15/24\n",
      "3622/3622 [==============================] - 2s 499us/step - loss: 9.3302e-04 - val_loss: 0.0010\n",
      "Epoch 16/24\n",
      "3622/3622 [==============================] - 2s 500us/step - loss: 9.0610e-04 - val_loss: 9.9273e-04\n",
      "Epoch 17/24\n",
      "3622/3622 [==============================] - 2s 505us/step - loss: 8.8000e-04 - val_loss: 0.0010\n",
      "Epoch 18/24\n",
      "3622/3622 [==============================] - 2s 500us/step - loss: 8.8766e-04 - val_loss: 0.0015\n",
      "Epoch 19/24\n",
      "3622/3622 [==============================] - 2s 496us/step - loss: 0.0010 - val_loss: 0.0016\n",
      "Epoch 20/24\n",
      "3622/3622 [==============================] - 2s 497us/step - loss: 0.0011 - val_loss: 9.1765e-04\n",
      "Epoch 21/24\n",
      "3622/3622 [==============================] - 2s 501us/step - loss: 9.0388e-04 - val_loss: 0.0013\n",
      "Epoch 22/24\n",
      "3622/3622 [==============================] - 2s 501us/step - loss: 8.7896e-04 - val_loss: 9.4981e-04\n",
      "Epoch 23/24\n",
      "3622/3622 [==============================] - 2s 498us/step - loss: 8.1261e-04 - val_loss: 9.3906e-04\n",
      "Epoch 24/24\n",
      "3622/3622 [==============================] - 2s 501us/step - loss: 7.9508e-04 - val_loss: 0.0013\n",
      "Train on 3622 samples, validate on 403 samples\n",
      "Epoch 1/24\n",
      "3622/3622 [==============================] - 24s 7ms/step - loss: 0.0832 - val_loss: 0.0046\n",
      "Epoch 2/24\n",
      "3622/3622 [==============================] - 1s 219us/step - loss: 0.0057 - val_loss: 0.0048\n",
      "Epoch 3/24\n",
      "3622/3622 [==============================] - 1s 221us/step - loss: 0.0022 - val_loss: 8.8020e-04\n",
      "Epoch 4/24\n",
      "3622/3622 [==============================] - 1s 222us/step - loss: 0.0010 - val_loss: 7.6474e-04\n",
      "Epoch 5/24\n",
      "3622/3622 [==============================] - 1s 226us/step - loss: 8.7635e-04 - val_loss: 8.1515e-04\n",
      "Epoch 6/24\n",
      "3622/3622 [==============================] - 1s 222us/step - loss: 7.6118e-04 - val_loss: 6.8457e-04\n",
      "Epoch 7/24\n",
      "3622/3622 [==============================] - 1s 223us/step - loss: 6.7711e-04 - val_loss: 8.9732e-04\n",
      "Epoch 8/24\n",
      "3622/3622 [==============================] - 1s 223us/step - loss: 8.0341e-04 - val_loss: 7.1609e-04\n",
      "Epoch 9/24\n",
      "3622/3622 [==============================] - 1s 222us/step - loss: 6.8444e-04 - val_loss: 6.5188e-04\n",
      "Epoch 10/24\n",
      "3622/3622 [==============================] - 1s 222us/step - loss: 6.1985e-04 - val_loss: 6.1852e-04\n",
      "Epoch 11/24\n",
      "3622/3622 [==============================] - 1s 221us/step - loss: 5.8094e-04 - val_loss: 5.9752e-04\n",
      "Epoch 12/24\n",
      "3622/3622 [==============================] - 1s 223us/step - loss: 5.3128e-04 - val_loss: 8.1209e-04\n",
      "Epoch 13/24\n",
      "3622/3622 [==============================] - 1s 221us/step - loss: 5.3008e-04 - val_loss: 5.3603e-04\n",
      "Epoch 14/24\n",
      "3622/3622 [==============================] - 1s 222us/step - loss: 4.8312e-04 - val_loss: 5.0997e-04\n",
      "Epoch 15/24\n",
      "3622/3622 [==============================] - 1s 223us/step - loss: 4.5381e-04 - val_loss: 5.3658e-04\n",
      "Epoch 16/24\n",
      "3622/3622 [==============================] - 1s 220us/step - loss: 4.5435e-04 - val_loss: 4.5190e-04\n",
      "Epoch 17/24\n",
      "3622/3622 [==============================] - 1s 222us/step - loss: 4.1819e-04 - val_loss: 6.7862e-04\n",
      "Epoch 18/24\n",
      "3622/3622 [==============================] - 1s 222us/step - loss: 5.0380e-04 - val_loss: 6.2144e-04\n",
      "Epoch 19/24\n",
      "3622/3622 [==============================] - 1s 221us/step - loss: 5.1991e-04 - val_loss: 0.0013\n",
      "Epoch 20/24\n",
      "3622/3622 [==============================] - 1s 221us/step - loss: 6.1634e-04 - val_loss: 4.7847e-04\n",
      "Epoch 21/24\n",
      "3622/3622 [==============================] - 1s 222us/step - loss: 4.9880e-04 - val_loss: 4.5720e-04\n",
      "Epoch 22/24\n",
      "3622/3622 [==============================] - 1s 222us/step - loss: 4.3325e-04 - val_loss: 5.4563e-04\n",
      "Epoch 23/24\n",
      "3622/3622 [==============================] - 1s 223us/step - loss: 4.0319e-04 - val_loss: 4.2193e-04\n",
      "Epoch 24/24\n",
      "3622/3622 [==============================] - 1s 226us/step - loss: 3.9710e-04 - val_loss: 3.9746e-04\n",
      "Train on 3622 samples, validate on 403 samples\n",
      "Epoch 1/24\n",
      "3622/3622 [==============================] - 25s 7ms/step - loss: nan - val_loss: nan\n",
      "Epoch 2/24\n",
      "3622/3622 [==============================] - 2s 435us/step - loss: nan - val_loss: nan\n",
      "Epoch 3/24\n",
      "3622/3622 [==============================] - 2s 439us/step - loss: nan - val_loss: nan\n",
      "Epoch 4/24\n",
      "3622/3622 [==============================] - 2s 442us/step - loss: nan - val_loss: nan\n",
      "Epoch 5/24\n",
      "3622/3622 [==============================] - 2s 442us/step - loss: nan - val_loss: nan\n",
      "Epoch 6/24\n",
      "3622/3622 [==============================] - 2s 438us/step - loss: nan - val_loss: nan\n",
      "Epoch 7/24\n",
      "3622/3622 [==============================] - 2s 438us/step - loss: nan - val_loss: nan\n",
      "Epoch 8/24\n",
      "3622/3622 [==============================] - 2s 439us/step - loss: nan - val_loss: nan\n",
      "Epoch 9/24\n",
      "3622/3622 [==============================] - 2s 440us/step - loss: nan - val_loss: nan\n",
      "Epoch 10/24\n",
      "3622/3622 [==============================] - 2s 435us/step - loss: nan - val_loss: nan\n",
      "Epoch 11/24\n",
      "3622/3622 [==============================] - 2s 436us/step - loss: nan - val_loss: nan\n",
      "Epoch 12/24\n",
      "3622/3622 [==============================] - 2s 440us/step - loss: nan - val_loss: nan\n",
      "Epoch 13/24\n",
      "3622/3622 [==============================] - 2s 436us/step - loss: nan - val_loss: nan\n",
      "Epoch 14/24\n",
      "3622/3622 [==============================] - 2s 435us/step - loss: nan - val_loss: nan\n",
      "Epoch 15/24\n",
      "3622/3622 [==============================] - 2s 439us/step - loss: nan - val_loss: nan\n",
      "Epoch 16/24\n",
      "3622/3622 [==============================] - 2s 436us/step - loss: nan - val_loss: nan\n",
      "Epoch 17/24\n",
      "3622/3622 [==============================] - 2s 438us/step - loss: nan - val_loss: nan\n",
      "Epoch 18/24\n",
      "3622/3622 [==============================] - 2s 438us/step - loss: nan - val_loss: nan\n",
      "Epoch 19/24\n",
      "3622/3622 [==============================] - 2s 435us/step - loss: nan - val_loss: nan\n",
      "Epoch 20/24\n",
      "3622/3622 [==============================] - 2s 437us/step - loss: nan - val_loss: nan\n",
      "Epoch 21/24\n",
      "3622/3622 [==============================] - 2s 438us/step - loss: nan - val_loss: nan\n",
      "Epoch 22/24\n",
      "3622/3622 [==============================] - 2s 441us/step - loss: nan - val_loss: nan\n",
      "Epoch 23/24\n",
      "3622/3622 [==============================] - 2s 440us/step - loss: nan - val_loss: nan\n",
      "Epoch 24/24\n",
      "3622/3622 [==============================] - 2s 437us/step - loss: nan - val_loss: nan\n",
      "Train on 3622 samples, validate on 403 samples\n",
      "Epoch 1/24\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3622/3622 [==============================] - 26s 7ms/step - loss: nan - val_loss: nan\n",
      "Epoch 2/24\n",
      "3622/3622 [==============================] - 2s 580us/step - loss: nan - val_loss: nan\n",
      "Epoch 3/24\n",
      "3622/3622 [==============================] - 2s 580us/step - loss: nan - val_loss: nan\n",
      "Epoch 4/24\n",
      "3622/3622 [==============================] - 2s 578us/step - loss: nan - val_loss: nan\n",
      "Epoch 5/24\n",
      "3622/3622 [==============================] - 2s 580us/step - loss: nan - val_loss: nan\n",
      "Epoch 6/24\n",
      "3622/3622 [==============================] - 2s 576us/step - loss: nan - val_loss: nan\n",
      "Epoch 7/24\n",
      "3622/3622 [==============================] - 2s 583us/step - loss: nan - val_loss: nan\n",
      "Epoch 8/24\n",
      "3622/3622 [==============================] - 2s 579us/step - loss: nan - val_loss: nan\n",
      "Epoch 9/24\n",
      "3622/3622 [==============================] - 2s 583us/step - loss: nan - val_loss: nan\n",
      "Epoch 10/24\n",
      "3622/3622 [==============================] - 2s 579us/step - loss: nan - val_loss: nan\n",
      "Epoch 11/24\n",
      "3622/3622 [==============================] - 2s 584us/step - loss: nan - val_loss: nan\n",
      "Epoch 12/24\n",
      "3622/3622 [==============================] - 2s 578us/step - loss: nan - val_loss: nan\n",
      "Epoch 13/24\n",
      "3622/3622 [==============================] - 2s 578us/step - loss: nan - val_loss: nan\n",
      "Epoch 14/24\n",
      "3622/3622 [==============================] - 2s 584us/step - loss: nan - val_loss: nan\n",
      "Epoch 15/24\n",
      "3622/3622 [==============================] - 2s 577us/step - loss: nan - val_loss: nan\n",
      "Epoch 16/24\n",
      "3622/3622 [==============================] - 2s 577us/step - loss: nan - val_loss: nan\n",
      "Epoch 17/24\n",
      "3622/3622 [==============================] - 2s 576us/step - loss: nan - val_loss: nan\n",
      "Epoch 18/24\n",
      "3622/3622 [==============================] - 2s 582us/step - loss: nan - val_loss: nan\n",
      "Epoch 19/24\n",
      "3622/3622 [==============================] - 2s 580us/step - loss: nan - val_loss: nan\n",
      "Epoch 20/24\n",
      "3622/3622 [==============================] - 2s 581us/step - loss: nan - val_loss: nan\n",
      "Epoch 21/24\n",
      "3622/3622 [==============================] - 2s 580us/step - loss: nan - val_loss: nan\n",
      "Epoch 22/24\n",
      "3622/3622 [==============================] - 2s 579us/step - loss: nan - val_loss: nan\n",
      "Epoch 23/24\n",
      "3622/3622 [==============================] - 2s 579us/step - loss: nan - val_loss: nan\n",
      "Epoch 24/24\n",
      "3622/3622 [==============================] - 2s 575us/step - loss: nan - val_loss: nan\n",
      "Train on 3622 samples, validate on 403 samples\n",
      "Epoch 1/24\n",
      "3622/3622 [==============================] - 27s 7ms/step - loss: nan - val_loss: nan\n",
      "Epoch 2/24\n",
      "3622/3622 [==============================] - 2s 579us/step - loss: nan - val_loss: nan\n",
      "Epoch 3/24\n",
      "3622/3622 [==============================] - 2s 578us/step - loss: nan - val_loss: nan\n",
      "Epoch 4/24\n",
      "3622/3622 [==============================] - 2s 578us/step - loss: nan - val_loss: nan\n",
      "Epoch 5/24\n",
      "3622/3622 [==============================] - 2s 591us/step - loss: nan - val_loss: nan\n",
      "Epoch 6/24\n",
      "3622/3622 [==============================] - 2s 579us/step - loss: nan - val_loss: nan\n",
      "Epoch 7/24\n",
      "3622/3622 [==============================] - 2s 573us/step - loss: nan - val_loss: nan\n",
      "Epoch 8/24\n",
      "3622/3622 [==============================] - 2s 585us/step - loss: nan - val_loss: nan\n",
      "Epoch 9/24\n",
      "3622/3622 [==============================] - 2s 580us/step - loss: nan - val_loss: nan\n",
      "Epoch 10/24\n",
      "3622/3622 [==============================] - 2s 577us/step - loss: nan - val_loss: nan\n",
      "Epoch 11/24\n",
      "3622/3622 [==============================] - 2s 580us/step - loss: nan - val_loss: nan\n",
      "Epoch 12/24\n",
      "3622/3622 [==============================] - 2s 582us/step - loss: nan - val_loss: nan\n",
      "Epoch 13/24\n",
      "3622/3622 [==============================] - 2s 580us/step - loss: nan - val_loss: nan\n",
      "Epoch 14/24\n",
      "3622/3622 [==============================] - 2s 578us/step - loss: nan - val_loss: nan\n",
      "Epoch 15/24\n",
      "3622/3622 [==============================] - 2s 575us/step - loss: nan - val_loss: nan\n",
      "Epoch 16/24\n",
      "3622/3622 [==============================] - 2s 579us/step - loss: nan - val_loss: nan\n",
      "Epoch 17/24\n",
      "3622/3622 [==============================] - 2s 576us/step - loss: nan - val_loss: nan\n",
      "Epoch 18/24\n",
      "3622/3622 [==============================] - 2s 577us/step - loss: nan - val_loss: nan\n",
      "Epoch 19/24\n",
      "3622/3622 [==============================] - 2s 573us/step - loss: nan - val_loss: nan\n",
      "Epoch 20/24\n",
      "3622/3622 [==============================] - 2s 581us/step - loss: nan - val_loss: nan\n",
      "Epoch 21/24\n",
      "3622/3622 [==============================] - 2s 582us/step - loss: nan - val_loss: nan\n",
      "Epoch 22/24\n",
      "3622/3622 [==============================] - 2s 575us/step - loss: nan - val_loss: nan\n",
      "Epoch 23/24\n",
      "3622/3622 [==============================] - 2s 574us/step - loss: nan - val_loss: nan\n",
      "Epoch 24/24\n",
      "3622/3622 [==============================] - 2s 576us/step - loss: nan - val_loss: nan\n",
      "Train on 3622 samples, validate on 403 samples\n",
      "Epoch 1/24\n",
      "3622/3622 [==============================] - 27s 7ms/step - loss: nan - val_loss: nan\n",
      "Epoch 2/24\n",
      "3622/3622 [==============================] - 2s 574us/step - loss: nan - val_loss: nan\n",
      "Epoch 3/24\n",
      "3622/3622 [==============================] - 2s 576us/step - loss: nan - val_loss: nan\n",
      "Epoch 4/24\n",
      "3622/3622 [==============================] - 2s 577us/step - loss: nan - val_loss: nan\n",
      "Epoch 5/24\n",
      "3622/3622 [==============================] - 2s 574us/step - loss: nan - val_loss: nan\n",
      "Epoch 6/24\n",
      "3622/3622 [==============================] - 2s 576us/step - loss: nan - val_loss: nan\n",
      "Epoch 7/24\n",
      "3622/3622 [==============================] - 2s 577us/step - loss: nan - val_loss: nan\n",
      "Epoch 8/24\n",
      "3622/3622 [==============================] - 2s 577us/step - loss: nan - val_loss: nan\n",
      "Epoch 9/24\n",
      "3622/3622 [==============================] - 2s 574us/step - loss: nan - val_loss: nan\n",
      "Epoch 10/24\n",
      "3622/3622 [==============================] - 2s 580us/step - loss: nan - val_loss: nan\n",
      "Epoch 11/24\n",
      "3622/3622 [==============================] - 2s 575us/step - loss: nan - val_loss: nan\n",
      "Epoch 12/24\n",
      "3622/3622 [==============================] - 2s 574us/step - loss: nan - val_loss: nan\n",
      "Epoch 13/24\n",
      "3622/3622 [==============================] - 2s 575us/step - loss: nan - val_loss: nan\n",
      "Epoch 14/24\n",
      "3622/3622 [==============================] - 2s 575us/step - loss: nan - val_loss: nan\n",
      "Epoch 15/24\n",
      "3622/3622 [==============================] - 2s 572us/step - loss: nan - val_loss: nan\n",
      "Epoch 16/24\n",
      "3622/3622 [==============================] - 2s 575us/step - loss: nan - val_loss: nan\n",
      "Epoch 17/24\n",
      "3622/3622 [==============================] - 2s 578us/step - loss: nan - val_loss: nan\n",
      "Epoch 18/24\n",
      "3622/3622 [==============================] - 2s 575us/step - loss: nan - val_loss: nan\n",
      "Epoch 19/24\n",
      "3622/3622 [==============================] - 2s 577us/step - loss: nan - val_loss: nan\n",
      "Epoch 20/24\n",
      "3622/3622 [==============================] - 2s 571us/step - loss: nan - val_loss: nan\n",
      "Epoch 21/24\n",
      "3622/3622 [==============================] - 2s 572us/step - loss: nan - val_loss: nan\n",
      "Epoch 22/24\n",
      "3622/3622 [==============================] - 2s 575us/step - loss: nan - val_loss: nan\n",
      "Epoch 23/24\n",
      "3622/3622 [==============================] - 2s 577us/step - loss: nan - val_loss: nan\n",
      "Epoch 24/24\n",
      "3622/3622 [==============================] - 2s 572us/step - loss: nan - val_loss: nan\n",
      "Train on 3622 samples, validate on 403 samples\n",
      "Epoch 1/24\n",
      "3622/3622 [==============================] - 28s 8ms/step - loss: 0.4466 - val_loss: 0.0037\n",
      "Epoch 2/24\n",
      "3622/3622 [==============================] - 2s 575us/step - loss: 0.0061 - val_loss: 0.0181\n",
      "Epoch 3/24\n",
      "3622/3622 [==============================] - 2s 576us/step - loss: 0.0060 - val_loss: 0.0025\n",
      "Epoch 4/24\n",
      "3622/3622 [==============================] - 2s 578us/step - loss: 0.0048 - val_loss: 0.0041\n",
      "Epoch 5/24\n",
      "3622/3622 [==============================] - 2s 580us/step - loss: 0.0043 - val_loss: 0.0028\n",
      "Epoch 6/24\n",
      "3622/3622 [==============================] - 2s 577us/step - loss: 0.0054 - val_loss: 0.0021\n",
      "Epoch 7/24\n",
      "3622/3622 [==============================] - 2s 580us/step - loss: 0.0036 - val_loss: 0.0012\n",
      "Epoch 8/24\n",
      "3622/3622 [==============================] - 2s 576us/step - loss: 0.0035 - val_loss: 0.0011\n",
      "Epoch 9/24\n",
      "3622/3622 [==============================] - 2s 580us/step - loss: 0.0235 - val_loss: 0.0158\n",
      "Epoch 10/24\n",
      "3622/3622 [==============================] - 2s 579us/step - loss: 0.0082 - val_loss: 0.0016\n",
      "Epoch 11/24\n",
      "3622/3622 [==============================] - 2s 575us/step - loss: 0.0026 - val_loss: 0.0010\n",
      "Epoch 12/24\n",
      "3622/3622 [==============================] - 2s 579us/step - loss: 0.0027 - val_loss: 0.0019\n",
      "Epoch 13/24\n",
      "3622/3622 [==============================] - 2s 575us/step - loss: 0.0150 - val_loss: 0.0156\n",
      "Epoch 14/24\n",
      "3622/3622 [==============================] - 2s 580us/step - loss: 0.0327 - val_loss: 0.0030\n",
      "Epoch 15/24\n",
      "3622/3622 [==============================] - 2s 586us/step - loss: 0.0030 - val_loss: 0.0010\n",
      "Epoch 16/24\n",
      "3622/3622 [==============================] - 2s 576us/step - loss: 0.0021 - val_loss: 0.0021\n",
      "Epoch 17/24\n",
      "3622/3622 [==============================] - 2s 577us/step - loss: 0.0019 - val_loss: 0.0012\n",
      "Epoch 18/24\n",
      "3622/3622 [==============================] - 2s 579us/step - loss: 0.0025 - val_loss: 0.0043\n",
      "Epoch 19/24\n",
      "3622/3622 [==============================] - 2s 579us/step - loss: 0.0108 - val_loss: 0.0010\n",
      "Epoch 20/24\n",
      "3622/3622 [==============================] - 2s 581us/step - loss: 0.0030 - val_loss: 0.0012\n",
      "Epoch 21/24\n",
      "3622/3622 [==============================] - 2s 580us/step - loss: 0.0059 - val_loss: 0.0123\n",
      "Epoch 22/24\n",
      "3622/3622 [==============================] - 2s 581us/step - loss: 0.0315 - val_loss: 0.0079\n",
      "Epoch 23/24\n",
      "3622/3622 [==============================] - 2s 580us/step - loss: 0.0084 - val_loss: 0.0015\n",
      "Epoch 24/24\n",
      "3622/3622 [==============================] - 2s 576us/step - loss: 0.0020 - val_loss: 0.0011\n",
      "Train on 3622 samples, validate on 403 samples\n",
      "Epoch 1/24\n",
      "3622/3622 [==============================] - 28s 8ms/step - loss: 1.7727 - val_loss: 0.0110\n",
      "Epoch 2/24\n",
      "3622/3622 [==============================] - 2s 578us/step - loss: 0.0193 - val_loss: 0.0026\n",
      "Epoch 3/24\n",
      "3622/3622 [==============================] - 2s 576us/step - loss: 0.0088 - val_loss: 0.0026\n",
      "Epoch 4/24\n",
      "3622/3622 [==============================] - 2s 580us/step - loss: 0.0056 - val_loss: 0.0019\n",
      "Epoch 5/24\n",
      "3622/3622 [==============================] - 2s 576us/step - loss: 0.0047 - val_loss: 0.0039\n",
      "Epoch 6/24\n",
      "3622/3622 [==============================] - 2s 573us/step - loss: 0.0045 - val_loss: 0.0046\n",
      "Epoch 7/24\n",
      "3622/3622 [==============================] - 2s 580us/step - loss: 0.0045 - val_loss: 0.0035\n",
      "Epoch 8/24\n",
      "3622/3622 [==============================] - 2s 580us/step - loss: 0.0041 - val_loss: 0.0024\n",
      "Epoch 9/24\n",
      "3622/3622 [==============================] - 2s 575us/step - loss: 0.0042 - val_loss: 0.0015\n",
      "Epoch 10/24\n",
      "3622/3622 [==============================] - 2s 578us/step - loss: 0.0040 - val_loss: 0.0021\n",
      "Epoch 11/24\n",
      "3622/3622 [==============================] - 2s 576us/step - loss: 0.0038 - val_loss: 0.0026\n",
      "Epoch 12/24\n",
      "3622/3622 [==============================] - 2s 580us/step - loss: 0.0038 - val_loss: 0.0062\n",
      "Epoch 13/24\n",
      "3622/3622 [==============================] - 2s 575us/step - loss: 0.0038 - val_loss: 0.0024\n",
      "Epoch 14/24\n",
      "3622/3622 [==============================] - 2s 577us/step - loss: 0.0034 - val_loss: 0.0016\n",
      "Epoch 15/24\n",
      "3622/3622 [==============================] - 2s 580us/step - loss: 0.0035 - val_loss: 0.0023\n",
      "Epoch 16/24\n",
      "3622/3622 [==============================] - 2s 574us/step - loss: 0.0035 - val_loss: 0.0016\n",
      "Epoch 17/24\n",
      "3622/3622 [==============================] - 2s 577us/step - loss: 0.0034 - val_loss: 0.0021\n",
      "Epoch 18/24\n",
      "3622/3622 [==============================] - 2s 574us/step - loss: 0.0032 - val_loss: 0.0035\n",
      "Epoch 19/24\n",
      "3622/3622 [==============================] - 2s 579us/step - loss: 0.0033 - val_loss: 0.0027\n",
      "Epoch 20/24\n",
      "3622/3622 [==============================] - 2s 580us/step - loss: 0.0032 - val_loss: 0.0039\n",
      "Epoch 21/24\n",
      "3622/3622 [==============================] - 2s 575us/step - loss: 0.0032 - val_loss: 0.0027\n",
      "Epoch 22/24\n",
      "3622/3622 [==============================] - 2s 581us/step - loss: 0.0032 - val_loss: 0.0034\n",
      "Epoch 23/24\n",
      "3622/3622 [==============================] - 2s 580us/step - loss: 0.0032 - val_loss: 0.0028\n",
      "Epoch 24/24\n",
      "3622/3622 [==============================] - 2s 573us/step - loss: 0.0031 - val_loss: 0.0027\n",
      "Train on 3622 samples, validate on 403 samples\n",
      "Epoch 1/24\n",
      "3622/3622 [==============================] - 27s 7ms/step - loss: 0.1251 - val_loss: 0.0319\n",
      "Epoch 2/24\n",
      "3622/3622 [==============================] - 1s 366us/step - loss: 0.0465 - val_loss: 0.0161\n",
      "Epoch 3/24\n",
      "3622/3622 [==============================] - 1s 365us/step - loss: 0.0376 - val_loss: 0.0113\n",
      "Epoch 4/24\n",
      "3622/3622 [==============================] - 1s 364us/step - loss: 0.0297 - val_loss: 0.0093\n",
      "Epoch 5/24\n",
      "3622/3622 [==============================] - 1s 367us/step - loss: 0.0234 - val_loss: 0.0049\n",
      "Epoch 6/24\n",
      "3622/3622 [==============================] - 1s 366us/step - loss: 0.0178 - val_loss: 0.0038\n",
      "Epoch 7/24\n",
      "3622/3622 [==============================] - 1s 364us/step - loss: 0.0138 - val_loss: 0.0034\n",
      "Epoch 8/24\n",
      "3622/3622 [==============================] - 1s 364us/step - loss: 0.0107 - val_loss: 0.0020\n",
      "Epoch 9/24\n",
      "3622/3622 [==============================] - 1s 367us/step - loss: 0.0082 - val_loss: 0.0018\n",
      "Epoch 10/24\n",
      "3622/3622 [==============================] - 1s 364us/step - loss: 0.0067 - val_loss: 0.0018\n",
      "Epoch 11/24\n",
      "3622/3622 [==============================] - 1s 370us/step - loss: 0.0055 - val_loss: 0.0018\n",
      "Epoch 12/24\n",
      "3622/3622 [==============================] - 1s 368us/step - loss: 0.0050 - val_loss: 0.0018\n",
      "Epoch 13/24\n",
      "3622/3622 [==============================] - 1s 371us/step - loss: 0.0046 - val_loss: 0.0024\n",
      "Epoch 14/24\n",
      "3622/3622 [==============================] - 1s 367us/step - loss: 0.0042 - val_loss: 0.0029\n",
      "Epoch 15/24\n",
      "3622/3622 [==============================] - 1s 365us/step - loss: 0.0039 - val_loss: 0.0031\n",
      "Epoch 16/24\n",
      "3622/3622 [==============================] - 1s 368us/step - loss: 0.0041 - val_loss: 0.0028\n",
      "Epoch 17/24\n",
      "3622/3622 [==============================] - 1s 366us/step - loss: 0.0037 - val_loss: 0.0030\n",
      "Epoch 18/24\n",
      "3622/3622 [==============================] - 1s 365us/step - loss: 0.0038 - val_loss: 0.0032\n",
      "Epoch 19/24\n",
      "3622/3622 [==============================] - 1s 368us/step - loss: 0.0036 - val_loss: 0.0031\n",
      "Epoch 20/24\n",
      "3622/3622 [==============================] - 1s 366us/step - loss: 0.0037 - val_loss: 0.0033\n",
      "Epoch 21/24\n",
      "3622/3622 [==============================] - 1s 365us/step - loss: 0.0039 - val_loss: 0.0039\n",
      "Epoch 22/24\n",
      "3622/3622 [==============================] - 1s 366us/step - loss: 0.0035 - val_loss: 0.0025\n",
      "Epoch 23/24\n",
      "3622/3622 [==============================] - 1s 372us/step - loss: 0.0037 - val_loss: 0.0034\n",
      "Epoch 24/24\n",
      "3622/3622 [==============================] - 1s 365us/step - loss: 0.0037 - val_loss: 0.0034\n",
      "\n",
      "GENERATION 17\n",
      "\n",
      "Train on 3622 samples, validate on 403 samples\n",
      "Epoch 1/24\n",
      "3622/3622 [==============================] - 27s 7ms/step - loss: nan - val_loss: nan\n",
      "Epoch 2/24\n",
      "3622/3622 [==============================] - 1s 273us/step - loss: nan - val_loss: nan\n",
      "Epoch 3/24\n",
      "3622/3622 [==============================] - 1s 270us/step - loss: nan - val_loss: nan\n",
      "Epoch 4/24\n",
      "3622/3622 [==============================] - 1s 270us/step - loss: nan - val_loss: nan\n",
      "Epoch 5/24\n",
      "3622/3622 [==============================] - 1s 271us/step - loss: nan - val_loss: nan\n",
      "Epoch 6/24\n",
      "3622/3622 [==============================] - 1s 271us/step - loss: nan - val_loss: nan\n",
      "Epoch 7/24\n",
      "3622/3622 [==============================] - 1s 271us/step - loss: nan - val_loss: nan\n",
      "Epoch 8/24\n",
      "3622/3622 [==============================] - 1s 270us/step - loss: nan - val_loss: nan\n",
      "Epoch 9/24\n",
      "3622/3622 [==============================] - 1s 271us/step - loss: nan - val_loss: nan\n",
      "Epoch 10/24\n",
      "3622/3622 [==============================] - 1s 273us/step - loss: nan - val_loss: nan\n",
      "Epoch 11/24\n",
      "3622/3622 [==============================] - 1s 271us/step - loss: nan - val_loss: nan\n",
      "Epoch 12/24\n",
      "3622/3622 [==============================] - 1s 271us/step - loss: nan - val_loss: nan\n",
      "Epoch 13/24\n",
      "3622/3622 [==============================] - 1s 273us/step - loss: nan - val_loss: nan\n",
      "Epoch 14/24\n",
      "3622/3622 [==============================] - 1s 274us/step - loss: nan - val_loss: nan\n",
      "Epoch 15/24\n",
      "3622/3622 [==============================] - 1s 270us/step - loss: nan - val_loss: nan\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16/24\n",
      "3622/3622 [==============================] - 1s 271us/step - loss: nan - val_loss: nan\n",
      "Epoch 17/24\n",
      "3622/3622 [==============================] - 1s 273us/step - loss: nan - val_loss: nan\n",
      "Epoch 18/24\n",
      "3622/3622 [==============================] - 1s 276us/step - loss: nan - val_loss: nan\n",
      "Epoch 19/24\n",
      "3622/3622 [==============================] - 1s 272us/step - loss: nan - val_loss: nan\n",
      "Epoch 20/24\n",
      "3622/3622 [==============================] - 1s 273us/step - loss: nan - val_loss: nan\n",
      "Epoch 21/24\n",
      "3622/3622 [==============================] - 1s 271us/step - loss: nan - val_loss: nan\n",
      "Epoch 22/24\n",
      "3622/3622 [==============================] - 1s 271us/step - loss: nan - val_loss: nan\n",
      "Epoch 23/24\n",
      "3622/3622 [==============================] - 1s 271us/step - loss: nan - val_loss: nan\n",
      "Epoch 24/24\n",
      "3622/3622 [==============================] - 1s 271us/step - loss: nan - val_loss: nan\n",
      "Train on 3622 samples, validate on 403 samples\n",
      "Epoch 1/24\n",
      "3622/3622 [==============================] - 27s 7ms/step - loss: 0.0867 - val_loss: 0.0078\n",
      "Epoch 2/24\n",
      "3622/3622 [==============================] - 1s 225us/step - loss: 0.0099 - val_loss: 8.4507e-04\n",
      "Epoch 3/24\n",
      "3622/3622 [==============================] - 1s 224us/step - loss: 0.0042 - val_loss: 0.0028\n",
      "Epoch 4/24\n",
      "3622/3622 [==============================] - 1s 223us/step - loss: 0.0021 - val_loss: 0.0030\n",
      "Epoch 5/24\n",
      "3622/3622 [==============================] - 1s 223us/step - loss: 0.0010 - val_loss: 6.2145e-04\n",
      "Epoch 6/24\n",
      "3622/3622 [==============================] - 1s 224us/step - loss: 7.2248e-04 - val_loss: 6.5527e-04\n",
      "Epoch 7/24\n",
      "3622/3622 [==============================] - 1s 225us/step - loss: 6.2976e-04 - val_loss: 8.4400e-04\n",
      "Epoch 8/24\n",
      "3622/3622 [==============================] - 1s 223us/step - loss: 5.9475e-04 - val_loss: 6.2515e-04\n",
      "Epoch 9/24\n",
      "3622/3622 [==============================] - 1s 225us/step - loss: 5.8492e-04 - val_loss: 6.2062e-04\n",
      "Epoch 10/24\n",
      "3622/3622 [==============================] - 1s 224us/step - loss: 5.7824e-04 - val_loss: 7.4592e-04\n",
      "Epoch 11/24\n",
      "3622/3622 [==============================] - 1s 223us/step - loss: 5.6152e-04 - val_loss: 6.1724e-04\n",
      "Epoch 12/24\n",
      "3622/3622 [==============================] - 1s 223us/step - loss: 5.3611e-04 - val_loss: 6.3138e-04\n",
      "Epoch 13/24\n",
      "3622/3622 [==============================] - 1s 225us/step - loss: 5.1067e-04 - val_loss: 5.8577e-04\n",
      "Epoch 14/24\n",
      "3622/3622 [==============================] - 1s 236us/step - loss: 4.9091e-04 - val_loss: 5.3341e-04\n",
      "Epoch 15/24\n",
      "3622/3622 [==============================] - 1s 222us/step - loss: 4.4883e-04 - val_loss: 5.0363e-04\n",
      "Epoch 16/24\n",
      "3622/3622 [==============================] - 1s 223us/step - loss: 4.2538e-04 - val_loss: 4.6905e-04\n",
      "Epoch 17/24\n",
      "3622/3622 [==============================] - 1s 223us/step - loss: 3.9313e-04 - val_loss: 4.2754e-04\n",
      "Epoch 18/24\n",
      "3622/3622 [==============================] - 1s 224us/step - loss: 3.7436e-04 - val_loss: 4.0488e-04\n",
      "Epoch 19/24\n",
      "3622/3622 [==============================] - 1s 224us/step - loss: 3.7494e-04 - val_loss: 3.8993e-04\n",
      "Epoch 20/24\n",
      "3622/3622 [==============================] - 1s 225us/step - loss: 3.5535e-04 - val_loss: 3.8462e-04\n",
      "Epoch 21/24\n",
      "3622/3622 [==============================] - 1s 229us/step - loss: 3.4924e-04 - val_loss: 4.4223e-04\n",
      "Epoch 22/24\n",
      "3622/3622 [==============================] - 1s 225us/step - loss: 3.5998e-04 - val_loss: 3.9259e-04\n",
      "Epoch 23/24\n",
      "3622/3622 [==============================] - 1s 224us/step - loss: 3.4629e-04 - val_loss: 3.6199e-04\n",
      "Epoch 24/24\n",
      "3622/3622 [==============================] - 1s 224us/step - loss: 3.2845e-04 - val_loss: 3.6091e-04\n",
      "Train on 3622 samples, validate on 403 samples\n",
      "Epoch 1/24\n",
      "3622/3622 [==============================] - 27s 8ms/step - loss: nan - val_loss: nan\n",
      "Epoch 2/24\n",
      "3622/3622 [==============================] - 1s 228us/step - loss: nan - val_loss: nan\n",
      "Epoch 3/24\n",
      "3622/3622 [==============================] - 1s 227us/step - loss: nan - val_loss: nan\n",
      "Epoch 4/24\n",
      "3622/3622 [==============================] - 1s 228us/step - loss: nan - val_loss: nan\n",
      "Epoch 5/24\n",
      "3622/3622 [==============================] - 1s 229us/step - loss: nan - val_loss: nan\n",
      "Epoch 6/24\n",
      "3622/3622 [==============================] - 1s 228us/step - loss: nan - val_loss: nan\n",
      "Epoch 7/24\n",
      "3622/3622 [==============================] - 1s 227us/step - loss: nan - val_loss: nan\n",
      "Epoch 8/24\n",
      "3622/3622 [==============================] - 1s 227us/step - loss: nan - val_loss: nan\n",
      "Epoch 9/24\n",
      "3622/3622 [==============================] - 1s 226us/step - loss: nan - val_loss: nan\n",
      "Epoch 10/24\n",
      "3622/3622 [==============================] - 1s 227us/step - loss: nan - val_loss: nan\n",
      "Epoch 11/24\n",
      "3622/3622 [==============================] - 1s 233us/step - loss: nan - val_loss: nan\n",
      "Epoch 12/24\n",
      "3622/3622 [==============================] - 1s 227us/step - loss: nan - val_loss: nan\n",
      "Epoch 13/24\n",
      "3622/3622 [==============================] - 1s 226us/step - loss: nan - val_loss: nan\n",
      "Epoch 14/24\n",
      "3622/3622 [==============================] - 1s 229us/step - loss: nan - val_loss: nan\n",
      "Epoch 15/24\n",
      "3622/3622 [==============================] - 1s 228us/step - loss: nan - val_loss: nan\n",
      "Epoch 16/24\n",
      "3622/3622 [==============================] - 1s 228us/step - loss: nan - val_loss: nan\n",
      "Epoch 17/24\n",
      "3622/3622 [==============================] - 1s 226us/step - loss: nan - val_loss: nan\n",
      "Epoch 18/24\n",
      "3622/3622 [==============================] - 1s 226us/step - loss: nan - val_loss: nan\n",
      "Epoch 19/24\n",
      "3622/3622 [==============================] - 1s 227us/step - loss: nan - val_loss: nan\n",
      "Epoch 20/24\n",
      "3622/3622 [==============================] - 1s 225us/step - loss: nan - val_loss: nan\n",
      "Epoch 21/24\n",
      "3622/3622 [==============================] - 1s 226us/step - loss: nan - val_loss: nan\n",
      "Epoch 22/24\n",
      "3622/3622 [==============================] - 1s 226us/step - loss: nan - val_loss: nan\n",
      "Epoch 23/24\n",
      "3622/3622 [==============================] - 1s 227us/step - loss: nan - val_loss: nan\n",
      "Epoch 24/24\n",
      "3622/3622 [==============================] - 1s 230us/step - loss: nan - val_loss: nan\n",
      "Train on 3622 samples, validate on 403 samples\n",
      "Epoch 1/24\n",
      "3622/3622 [==============================] - 28s 8ms/step - loss: 0.1338 - val_loss: 0.0148\n",
      "Epoch 2/24\n",
      "3622/3622 [==============================] - 1s 230us/step - loss: 0.1061 - val_loss: 0.0090\n",
      "Epoch 3/24\n",
      "3622/3622 [==============================] - 1s 230us/step - loss: 0.0379 - val_loss: 0.0112\n",
      "Epoch 4/24\n",
      "3622/3622 [==============================] - 1s 229us/step - loss: 0.0144 - val_loss: 0.0275\n",
      "Epoch 5/24\n",
      "3622/3622 [==============================] - 1s 231us/step - loss: 0.0187 - val_loss: 0.0420\n",
      "Epoch 6/24\n",
      "3622/3622 [==============================] - 1s 229us/step - loss: 0.0132 - val_loss: 0.0179\n",
      "Epoch 7/24\n",
      "3622/3622 [==============================] - 1s 229us/step - loss: 0.0034 - val_loss: 0.0262\n",
      "Epoch 8/24\n",
      "3622/3622 [==============================] - 1s 229us/step - loss: 0.0067 - val_loss: 0.0201\n",
      "Epoch 9/24\n",
      "3622/3622 [==============================] - 1s 229us/step - loss: 0.0126 - val_loss: 0.0975\n",
      "Epoch 10/24\n",
      "3622/3622 [==============================] - 1s 229us/step - loss: 0.0247 - val_loss: 0.0156\n",
      "Epoch 11/24\n",
      "3622/3622 [==============================] - 1s 229us/step - loss: 0.0030 - val_loss: 0.0143\n",
      "Epoch 12/24\n",
      "3622/3622 [==============================] - 1s 226us/step - loss: 0.0031 - val_loss: 0.0089\n",
      "Epoch 13/24\n",
      "3622/3622 [==============================] - 1s 229us/step - loss: 0.0025 - val_loss: 0.0143\n",
      "Epoch 14/24\n",
      "3622/3622 [==============================] - 1s 227us/step - loss: 0.0113 - val_loss: 0.0882\n",
      "Epoch 15/24\n",
      "3622/3622 [==============================] - 1s 227us/step - loss: 0.0174 - val_loss: 0.0411\n",
      "Epoch 16/24\n",
      "3622/3622 [==============================] - 1s 228us/step - loss: 0.0048 - val_loss: 0.0121\n",
      "Epoch 17/24\n",
      "3622/3622 [==============================] - 1s 234us/step - loss: 0.0024 - val_loss: 0.0031\n",
      "Epoch 18/24\n",
      "3622/3622 [==============================] - 1s 228us/step - loss: 0.0032 - val_loss: 0.0062\n",
      "Epoch 19/24\n",
      "3622/3622 [==============================] - 1s 234us/step - loss: 0.0120 - val_loss: 0.0071\n",
      "Epoch 20/24\n",
      "3622/3622 [==============================] - 1s 228us/step - loss: 0.0084 - val_loss: 0.0048\n",
      "Epoch 21/24\n",
      "3622/3622 [==============================] - 1s 227us/step - loss: 0.0035 - val_loss: 0.0030\n",
      "Epoch 22/24\n",
      "3622/3622 [==============================] - 1s 229us/step - loss: 0.0049 - val_loss: 0.0040\n",
      "Epoch 23/24\n",
      "3622/3622 [==============================] - 1s 229us/step - loss: 0.0069 - val_loss: 0.0029\n",
      "Epoch 24/24\n",
      "3622/3622 [==============================] - 1s 227us/step - loss: 0.0041 - val_loss: 0.0033\n",
      "Train on 3622 samples, validate on 403 samples\n",
      "Epoch 1/24\n",
      "3622/3622 [==============================] - 28s 8ms/step - loss: 0.2165 - val_loss: 0.0384\n",
      "Epoch 2/24\n",
      "3622/3622 [==============================] - 2s 582us/step - loss: 0.0679 - val_loss: 0.0304\n",
      "Epoch 3/24\n",
      "3622/3622 [==============================] - 2s 581us/step - loss: 0.0679 - val_loss: 0.0242\n",
      "Epoch 4/24\n",
      "3622/3622 [==============================] - 2s 578us/step - loss: 0.0679 - val_loss: 0.0492\n",
      "Epoch 5/24\n",
      "3622/3622 [==============================] - 2s 577us/step - loss: 0.0682 - val_loss: 0.0388\n",
      "Epoch 6/24\n",
      "3622/3622 [==============================] - 2s 583us/step - loss: 0.0676 - val_loss: 0.0323\n",
      "Epoch 7/24\n",
      "3622/3622 [==============================] - 2s 578us/step - loss: 0.0676 - val_loss: 0.0267\n",
      "Epoch 8/24\n",
      "3622/3622 [==============================] - 2s 582us/step - loss: 0.0675 - val_loss: 0.0296\n",
      "Epoch 9/24\n",
      "3622/3622 [==============================] - 2s 580us/step - loss: 0.0674 - val_loss: 0.0244\n",
      "Epoch 10/24\n",
      "3622/3622 [==============================] - 2s 581us/step - loss: 0.0676 - val_loss: 0.0181\n",
      "Epoch 11/24\n",
      "3622/3622 [==============================] - 2s 579us/step - loss: 0.0677 - val_loss: 0.0345\n",
      "Epoch 12/24\n",
      "3622/3622 [==============================] - 2s 581us/step - loss: 0.0671 - val_loss: 0.0437\n",
      "Epoch 13/24\n",
      "3622/3622 [==============================] - 2s 578us/step - loss: 0.0671 - val_loss: 0.0178\n",
      "Epoch 14/24\n",
      "3622/3622 [==============================] - 2s 584us/step - loss: 0.0675 - val_loss: 0.0370\n",
      "Epoch 15/24\n",
      "3622/3622 [==============================] - 2s 579us/step - loss: 0.0668 - val_loss: 0.0454\n",
      "Epoch 16/24\n",
      "3622/3622 [==============================] - 2s 583us/step - loss: 0.0669 - val_loss: 0.0496\n",
      "Epoch 17/24\n",
      "3622/3622 [==============================] - 2s 578us/step - loss: 0.0668 - val_loss: 0.0383\n",
      "Epoch 18/24\n",
      "3622/3622 [==============================] - 2s 582us/step - loss: 0.0666 - val_loss: 0.0395\n",
      "Epoch 19/24\n",
      "3622/3622 [==============================] - 2s 584us/step - loss: 0.0666 - val_loss: 0.0463\n",
      "Epoch 20/24\n",
      "3622/3622 [==============================] - 2s 582us/step - loss: 0.0665 - val_loss: 0.0325\n",
      "Epoch 21/24\n",
      "3622/3622 [==============================] - 2s 584us/step - loss: 0.0663 - val_loss: 0.0337\n",
      "Epoch 22/24\n",
      "3622/3622 [==============================] - 2s 580us/step - loss: 0.0662 - val_loss: 0.0301\n",
      "Epoch 23/24\n",
      "3622/3622 [==============================] - 2s 580us/step - loss: 0.0661 - val_loss: 0.0219\n",
      "Epoch 24/24\n",
      "3622/3622 [==============================] - 2s 578us/step - loss: 0.0662 - val_loss: 0.0396\n",
      "Train on 3622 samples, validate on 403 samples\n",
      "Epoch 1/24\n",
      "3622/3622 [==============================] - 30s 8ms/step - loss: 0.0293 - val_loss: 0.0017\n",
      "Epoch 2/24\n",
      "3622/3622 [==============================] - 2s 579us/step - loss: 0.0080 - val_loss: 0.0015\n",
      "Epoch 3/24\n",
      "3622/3622 [==============================] - 2s 583us/step - loss: 0.0057 - val_loss: 0.0015\n",
      "Epoch 4/24\n",
      "3622/3622 [==============================] - 2s 586us/step - loss: 0.0048 - val_loss: 0.0016\n",
      "Epoch 5/24\n",
      "3622/3622 [==============================] - 2s 585us/step - loss: 0.0045 - val_loss: 0.0034\n",
      "Epoch 6/24\n",
      "3622/3622 [==============================] - 2s 583us/step - loss: 0.0043 - val_loss: 0.0015\n",
      "Epoch 7/24\n",
      "3622/3622 [==============================] - 2s 582us/step - loss: 0.0044 - val_loss: 0.0029\n",
      "Epoch 8/24\n",
      "3622/3622 [==============================] - 2s 582us/step - loss: 0.0039 - val_loss: 0.0018\n",
      "Epoch 9/24\n",
      "3622/3622 [==============================] - 2s 577us/step - loss: 0.0039 - val_loss: 0.0016\n",
      "Epoch 10/24\n",
      "3622/3622 [==============================] - 2s 580us/step - loss: 0.0041 - val_loss: 0.0020\n",
      "Epoch 11/24\n",
      "3622/3622 [==============================] - 2s 579us/step - loss: 0.0040 - val_loss: 0.0017\n",
      "Epoch 12/24\n",
      "3622/3622 [==============================] - 2s 585us/step - loss: 0.0040 - val_loss: 0.0014\n",
      "Epoch 13/24\n",
      "3622/3622 [==============================] - 2s 582us/step - loss: 0.0039 - val_loss: 0.0031\n",
      "Epoch 14/24\n",
      "3622/3622 [==============================] - 2s 583us/step - loss: 0.0039 - val_loss: 0.0022\n",
      "Epoch 15/24\n",
      "3622/3622 [==============================] - 2s 582us/step - loss: 0.0036 - val_loss: 0.0026\n",
      "Epoch 16/24\n",
      "3622/3622 [==============================] - 2s 579us/step - loss: 0.0038 - val_loss: 0.0018\n",
      "Epoch 17/24\n",
      "3622/3622 [==============================] - 2s 582us/step - loss: 0.0036 - val_loss: 0.0019\n",
      "Epoch 18/24\n",
      "3622/3622 [==============================] - 2s 588us/step - loss: 0.0036 - val_loss: 0.0024\n",
      "Epoch 19/24\n",
      "3622/3622 [==============================] - 2s 582us/step - loss: 0.0034 - val_loss: 0.0041\n",
      "Epoch 20/24\n",
      "3622/3622 [==============================] - 2s 583us/step - loss: 0.0035 - val_loss: 0.0020\n",
      "Epoch 21/24\n",
      "3622/3622 [==============================] - 2s 583us/step - loss: 0.0034 - val_loss: 0.0019\n",
      "Epoch 22/24\n",
      "3622/3622 [==============================] - 2s 584us/step - loss: 0.0032 - val_loss: 0.0031\n",
      "Epoch 23/24\n",
      "3622/3622 [==============================] - 2s 580us/step - loss: 0.0035 - val_loss: 0.0016\n",
      "Epoch 24/24\n",
      "3622/3622 [==============================] - 2s 583us/step - loss: 0.0034 - val_loss: 0.0018\n",
      "Train on 3622 samples, validate on 403 samples\n",
      "Epoch 1/24\n",
      "3622/3622 [==============================] - 30s 8ms/step - loss: 0.1218 - val_loss: 0.0013\n",
      "Epoch 2/24\n",
      "3622/3622 [==============================] - 2s 585us/step - loss: 0.0086 - val_loss: 0.0070\n",
      "Epoch 3/24\n",
      "3622/3622 [==============================] - 2s 583us/step - loss: 0.0036 - val_loss: 0.0085\n",
      "Epoch 4/24\n",
      "3622/3622 [==============================] - 2s 580us/step - loss: 0.0046 - val_loss: 0.0144\n",
      "Epoch 5/24\n",
      "3622/3622 [==============================] - 2s 584us/step - loss: 0.0057 - val_loss: 0.0152\n",
      "Epoch 6/24\n",
      "3622/3622 [==============================] - 2s 580us/step - loss: 0.0044 - val_loss: 0.0151\n",
      "Epoch 7/24\n",
      "3622/3622 [==============================] - 2s 581us/step - loss: 0.0043 - val_loss: 0.0070\n",
      "Epoch 8/24\n",
      "3622/3622 [==============================] - 2s 583us/step - loss: 0.0036 - val_loss: 0.0087\n",
      "Epoch 9/24\n",
      "3622/3622 [==============================] - 2s 583us/step - loss: 0.0034 - val_loss: 0.0163\n",
      "Epoch 10/24\n",
      "3622/3622 [==============================] - 2s 584us/step - loss: 0.0058 - val_loss: 0.0027\n",
      "Epoch 11/24\n",
      "3622/3622 [==============================] - 2s 581us/step - loss: 0.0015 - val_loss: 0.0020\n",
      "Epoch 12/24\n",
      "3622/3622 [==============================] - 2s 580us/step - loss: 0.0021 - val_loss: 0.0055\n",
      "Epoch 13/24\n",
      "3622/3622 [==============================] - 2s 578us/step - loss: 0.0048 - val_loss: 0.0175\n",
      "Epoch 14/24\n",
      "3622/3622 [==============================] - 2s 580us/step - loss: 0.0057 - val_loss: 0.0087\n",
      "Epoch 15/24\n",
      "3622/3622 [==============================] - 2s 583us/step - loss: 0.0028 - val_loss: 0.0120\n",
      "Epoch 16/24\n",
      "3622/3622 [==============================] - 2s 582us/step - loss: 0.0037 - val_loss: 0.0100\n",
      "Epoch 17/24\n",
      "3622/3622 [==============================] - 2s 582us/step - loss: 0.0037 - val_loss: 0.0035\n",
      "Epoch 18/24\n",
      "3622/3622 [==============================] - 2s 580us/step - loss: 0.0014 - val_loss: 7.9377e-04\n",
      "Epoch 19/24\n",
      "3622/3622 [==============================] - 2s 588us/step - loss: 0.0016 - val_loss: 7.3537e-04\n",
      "Epoch 20/24\n",
      "3622/3622 [==============================] - 2s 582us/step - loss: 0.0020 - val_loss: 0.0104\n",
      "Epoch 21/24\n",
      "3622/3622 [==============================] - 2s 582us/step - loss: 0.0073 - val_loss: 7.7055e-04\n",
      "Epoch 22/24\n",
      "3622/3622 [==============================] - 2s 583us/step - loss: 0.0014 - val_loss: 8.1347e-04\n",
      "Epoch 23/24\n",
      "3622/3622 [==============================] - 2s 582us/step - loss: 0.0013 - val_loss: 7.3491e-04\n",
      "Epoch 24/24\n",
      "3622/3622 [==============================] - 2s 579us/step - loss: 0.0018 - val_loss: 0.0011\n",
      "Train on 3622 samples, validate on 403 samples\n",
      "Epoch 1/24\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3622/3622 [==============================] - 31s 8ms/step - loss: 0.1352 - val_loss: 0.0012\n",
      "Epoch 2/24\n",
      "3622/3622 [==============================] - 2s 585us/step - loss: 0.0066 - val_loss: 0.0243\n",
      "Epoch 3/24\n",
      "3622/3622 [==============================] - 2s 583us/step - loss: 0.0059 - val_loss: 0.0032\n",
      "Epoch 4/24\n",
      "3622/3622 [==============================] - 2s 584us/step - loss: 0.0022 - val_loss: 0.0014\n",
      "Epoch 5/24\n",
      "3622/3622 [==============================] - 2s 585us/step - loss: 0.0032 - val_loss: 0.0139\n",
      "Epoch 6/24\n",
      "3622/3622 [==============================] - 2s 590us/step - loss: 0.0088 - val_loss: 9.9150e-04\n",
      "Epoch 7/24\n",
      "3622/3622 [==============================] - 2s 588us/step - loss: 0.0023 - val_loss: 9.4703e-04\n",
      "Epoch 8/24\n",
      "3622/3622 [==============================] - 2s 590us/step - loss: 0.0026 - val_loss: 0.0022\n",
      "Epoch 9/24\n",
      "3622/3622 [==============================] - 2s 587us/step - loss: 0.0016 - val_loss: 0.0012\n",
      "Epoch 10/24\n",
      "3622/3622 [==============================] - 2s 589us/step - loss: 0.0023 - val_loss: 0.0194\n",
      "Epoch 11/24\n",
      "3622/3622 [==============================] - 2s 587us/step - loss: 0.0129 - val_loss: 0.0013\n",
      "Epoch 12/24\n",
      "3622/3622 [==============================] - 2s 585us/step - loss: 0.0017 - val_loss: 0.0030\n",
      "Epoch 13/24\n",
      "3622/3622 [==============================] - 2s 587us/step - loss: 0.0016 - val_loss: 0.0021\n",
      "Epoch 14/24\n",
      "3622/3622 [==============================] - 2s 587us/step - loss: 0.0017 - val_loss: 0.0057\n",
      "Epoch 15/24\n",
      "3622/3622 [==============================] - 2s 587us/step - loss: 0.0041 - val_loss: 0.0119\n",
      "Epoch 16/24\n",
      "3622/3622 [==============================] - 2s 587us/step - loss: 0.0047 - val_loss: 0.0137\n",
      "Epoch 17/24\n",
      "3622/3622 [==============================] - 2s 586us/step - loss: 0.0040 - val_loss: 0.0044\n",
      "Epoch 18/24\n",
      "3622/3622 [==============================] - 2s 585us/step - loss: 0.0018 - val_loss: 0.0028\n",
      "Epoch 19/24\n",
      "3622/3622 [==============================] - 2s 585us/step - loss: 0.0017 - val_loss: 0.0060\n",
      "Epoch 20/24\n",
      "3622/3622 [==============================] - 2s 591us/step - loss: 0.0023 - val_loss: 0.0077\n",
      "Epoch 21/24\n",
      "3622/3622 [==============================] - 2s 584us/step - loss: 0.0029 - val_loss: 0.0027\n",
      "Epoch 22/24\n",
      "3622/3622 [==============================] - 2s 591us/step - loss: 0.0014 - val_loss: 0.0024\n",
      "Epoch 23/24\n",
      "3622/3622 [==============================] - 2s 586us/step - loss: 0.0020 - val_loss: 0.0055\n",
      "Epoch 24/24\n",
      "3622/3622 [==============================] - 2s 586us/step - loss: 0.0042 - val_loss: 0.0157\n",
      "Train on 3622 samples, validate on 403 samples\n",
      "Epoch 1/24\n",
      "3622/3622 [==============================] - 30s 8ms/step - loss: 0.2966 - val_loss: 0.0253\n",
      "Epoch 2/24\n",
      "3622/3622 [==============================] - 2s 582us/step - loss: 0.0705 - val_loss: 0.0470\n",
      "Epoch 3/24\n",
      "3622/3622 [==============================] - 2s 578us/step - loss: 0.0697 - val_loss: 0.0406\n",
      "Epoch 4/24\n",
      "3622/3622 [==============================] - 2s 583us/step - loss: 0.0697 - val_loss: 0.0489\n",
      "Epoch 5/24\n",
      "3622/3622 [==============================] - 2s 580us/step - loss: 0.0698 - val_loss: 0.0280\n",
      "Epoch 6/24\n",
      "3622/3622 [==============================] - 2s 581us/step - loss: 0.0697 - val_loss: 0.0399\n",
      "Epoch 7/24\n",
      "3622/3622 [==============================] - 2s 585us/step - loss: 0.0695 - val_loss: 0.0215\n",
      "Epoch 8/24\n",
      "3622/3622 [==============================] - 2s 582us/step - loss: 0.0700 - val_loss: 0.0594\n",
      "Epoch 9/24\n",
      "3622/3622 [==============================] - 2s 585us/step - loss: 0.0700 - val_loss: 0.0456\n",
      "Epoch 10/24\n",
      "3622/3622 [==============================] - 2s 601us/step - loss: 0.0696 - val_loss: 0.0501\n",
      "Epoch 11/24\n",
      "3622/3622 [==============================] - 2s 600us/step - loss: 0.0696 - val_loss: 0.0299\n",
      "Epoch 12/24\n",
      "3622/3622 [==============================] - 2s 641us/step - loss: 0.0695 - val_loss: 0.0318\n",
      "Epoch 13/24\n",
      "3622/3622 [==============================] - 2s 658us/step - loss: 0.0692 - val_loss: 0.0318\n",
      "Epoch 14/24\n",
      "3622/3622 [==============================] - 2s 651us/step - loss: 0.0691 - val_loss: 0.0505\n",
      "Epoch 15/24\n",
      "3622/3622 [==============================] - 2s 659us/step - loss: 0.0694 - val_loss: 0.0494\n",
      "Epoch 16/24\n",
      "3622/3622 [==============================] - 2s 648us/step - loss: 0.0694 - val_loss: 0.0356\n",
      "Epoch 17/24\n",
      "3622/3622 [==============================] - 2s 656us/step - loss: 0.0690 - val_loss: 0.0305\n",
      "Epoch 18/24\n",
      "3622/3622 [==============================] - 2s 659us/step - loss: 0.0690 - val_loss: 0.0499\n",
      "Epoch 19/24\n",
      "3622/3622 [==============================] - 2s 645us/step - loss: 0.0691 - val_loss: 0.0340\n",
      "Epoch 20/24\n",
      "3622/3622 [==============================] - 2s 653us/step - loss: 0.0688 - val_loss: 0.0288\n",
      "Epoch 21/24\n",
      "3622/3622 [==============================] - 2s 638us/step - loss: 0.0688 - val_loss: 0.0336\n",
      "Epoch 22/24\n",
      "3622/3622 [==============================] - 2s 620us/step - loss: 0.0687 - val_loss: 0.0431\n",
      "Epoch 23/24\n",
      "3622/3622 [==============================] - 2s 607us/step - loss: 0.0687 - val_loss: 0.0457\n",
      "Epoch 24/24\n",
      "3622/3622 [==============================] - 2s 598us/step - loss: 0.0687 - val_loss: 0.0448\n",
      "\n",
      "GENERATION 18\n",
      "\n",
      "Train on 3622 samples, validate on 403 samples\n",
      "Epoch 1/24\n",
      "3622/3622 [==============================] - 29s 8ms/step - loss: 0.1088 - val_loss: 0.0294\n",
      "Epoch 2/24\n",
      "3622/3622 [==============================] - 1s 189us/step - loss: 0.0132 - val_loss: 0.0099\n",
      "Epoch 3/24\n",
      "3622/3622 [==============================] - 1s 190us/step - loss: 0.0054 - val_loss: 0.0048\n",
      "Epoch 4/24\n",
      "3622/3622 [==============================] - 1s 191us/step - loss: 0.0023 - val_loss: 0.0015\n",
      "Epoch 5/24\n",
      "3622/3622 [==============================] - 1s 189us/step - loss: 0.0012 - val_loss: 0.0016\n",
      "Epoch 6/24\n",
      "3622/3622 [==============================] - 1s 190us/step - loss: 8.7660e-04 - val_loss: 7.6939e-04\n",
      "Epoch 7/24\n",
      "3622/3622 [==============================] - 1s 188us/step - loss: 8.1640e-04 - val_loss: 8.2428e-04\n",
      "Epoch 8/24\n",
      "3622/3622 [==============================] - 1s 189us/step - loss: 7.5543e-04 - val_loss: 8.7372e-04\n",
      "Epoch 9/24\n",
      "3622/3622 [==============================] - 1s 189us/step - loss: 7.1198e-04 - val_loss: 7.6723e-04\n",
      "Epoch 10/24\n",
      "3622/3622 [==============================] - 1s 188us/step - loss: 6.9577e-04 - val_loss: 8.3567e-04\n",
      "Epoch 11/24\n",
      "3622/3622 [==============================] - 1s 189us/step - loss: 6.7392e-04 - val_loss: 7.6515e-04\n",
      "Epoch 12/24\n",
      "3622/3622 [==============================] - 1s 191us/step - loss: 6.6273e-04 - val_loss: 8.2673e-04\n",
      "Epoch 13/24\n",
      "3622/3622 [==============================] - 1s 190us/step - loss: 6.4869e-04 - val_loss: 7.7430e-04\n",
      "Epoch 14/24\n",
      "3622/3622 [==============================] - 1s 190us/step - loss: 6.4083e-04 - val_loss: 8.0254e-04\n",
      "Epoch 15/24\n",
      "3622/3622 [==============================] - 1s 188us/step - loss: 6.3653e-04 - val_loss: 7.1263e-04\n",
      "Epoch 16/24\n",
      "3622/3622 [==============================] - 1s 190us/step - loss: 6.4309e-04 - val_loss: 8.9151e-04\n",
      "Epoch 17/24\n",
      "3622/3622 [==============================] - 1s 195us/step - loss: 6.2733e-04 - val_loss: 6.8127e-04\n",
      "Epoch 18/24\n",
      "3622/3622 [==============================] - 1s 192us/step - loss: 6.1504e-04 - val_loss: 8.2995e-04\n",
      "Epoch 19/24\n",
      "3622/3622 [==============================] - 1s 187us/step - loss: 6.0390e-04 - val_loss: 6.9204e-04\n",
      "Epoch 20/24\n",
      "3622/3622 [==============================] - 1s 193us/step - loss: 5.8693e-04 - val_loss: 6.7502e-04\n",
      "Epoch 21/24\n",
      "3622/3622 [==============================] - 1s 192us/step - loss: 5.7445e-04 - val_loss: 6.7232e-04\n",
      "Epoch 22/24\n",
      "3622/3622 [==============================] - 1s 190us/step - loss: 5.6466e-04 - val_loss: 6.3853e-04\n",
      "Epoch 23/24\n",
      "3622/3622 [==============================] - 1s 187us/step - loss: 5.5385e-04 - val_loss: 5.9951e-04\n",
      "Epoch 24/24\n",
      "3622/3622 [==============================] - 1s 189us/step - loss: 5.4691e-04 - val_loss: 6.6813e-04\n",
      "Train on 3622 samples, validate on 403 samples\n",
      "Epoch 1/24\n",
      "3622/3622 [==============================] - 30s 8ms/step - loss: 0.1212 - val_loss: 0.0520\n",
      "Epoch 2/24\n",
      "3622/3622 [==============================] - 1s 226us/step - loss: 0.0166 - val_loss: 0.0115\n",
      "Epoch 3/24\n",
      "3622/3622 [==============================] - 1s 225us/step - loss: 0.0060 - val_loss: 0.0020\n",
      "Epoch 4/24\n",
      "3622/3622 [==============================] - 1s 227us/step - loss: 0.0024 - val_loss: 0.0014\n",
      "Epoch 5/24\n",
      "3622/3622 [==============================] - 1s 228us/step - loss: 0.0013 - val_loss: 0.0015\n",
      "Epoch 6/24\n",
      "3622/3622 [==============================] - 1s 226us/step - loss: 0.0011 - val_loss: 9.3061e-04\n",
      "Epoch 7/24\n",
      "3622/3622 [==============================] - 1s 225us/step - loss: 8.3931e-04 - val_loss: 0.0011\n",
      "Epoch 8/24\n",
      "3622/3622 [==============================] - 1s 225us/step - loss: 7.7659e-04 - val_loss: 8.5686e-04\n",
      "Epoch 9/24\n",
      "3622/3622 [==============================] - 1s 227us/step - loss: 7.4774e-04 - val_loss: 7.7488e-04\n",
      "Epoch 10/24\n",
      "3622/3622 [==============================] - 1s 226us/step - loss: 7.0628e-04 - val_loss: 8.0694e-04\n",
      "Epoch 11/24\n",
      "3622/3622 [==============================] - 1s 229us/step - loss: 6.8595e-04 - val_loss: 7.3227e-04\n",
      "Epoch 12/24\n",
      "3622/3622 [==============================] - 1s 230us/step - loss: 6.7317e-04 - val_loss: 7.8142e-04\n",
      "Epoch 13/24\n",
      "3622/3622 [==============================] - 1s 226us/step - loss: 6.5349e-04 - val_loss: 7.4622e-04\n",
      "Epoch 14/24\n",
      "3622/3622 [==============================] - 1s 227us/step - loss: 6.3843e-04 - val_loss: 6.9447e-04\n",
      "Epoch 15/24\n",
      "3622/3622 [==============================] - 1s 227us/step - loss: 6.2221e-04 - val_loss: 6.7380e-04\n",
      "Epoch 16/24\n",
      "3622/3622 [==============================] - 1s 226us/step - loss: 6.1484e-04 - val_loss: 6.5240e-04\n",
      "Epoch 17/24\n",
      "3622/3622 [==============================] - 1s 227us/step - loss: 6.1368e-04 - val_loss: 6.2833e-04\n",
      "Epoch 18/24\n",
      "3622/3622 [==============================] - 1s 228us/step - loss: 5.7271e-04 - val_loss: 6.0458e-04\n",
      "Epoch 19/24\n",
      "3622/3622 [==============================] - 1s 227us/step - loss: 5.3587e-04 - val_loss: 5.8316e-04\n",
      "Epoch 20/24\n",
      "3622/3622 [==============================] - 1s 227us/step - loss: 5.1384e-04 - val_loss: 5.4712e-04\n",
      "Epoch 21/24\n",
      "3622/3622 [==============================] - 1s 233us/step - loss: 4.8135e-04 - val_loss: 5.7490e-04\n",
      "Epoch 22/24\n",
      "3622/3622 [==============================] - 1s 232us/step - loss: 4.6837e-04 - val_loss: 4.9642e-04\n",
      "Epoch 23/24\n",
      "3622/3622 [==============================] - 1s 229us/step - loss: 4.3533e-04 - val_loss: 4.7906e-04\n",
      "Epoch 24/24\n",
      "3622/3622 [==============================] - 1s 229us/step - loss: 4.3637e-04 - val_loss: 5.1536e-04\n",
      "Train on 3622 samples, validate on 403 samples\n",
      "Epoch 1/24\n",
      "3622/3622 [==============================] - 30s 8ms/step - loss: 0.0929 - val_loss: 0.0223\n",
      "Epoch 2/24\n",
      "3622/3622 [==============================] - 1s 227us/step - loss: 0.0101 - val_loss: 0.0074\n",
      "Epoch 3/24\n",
      "3622/3622 [==============================] - 1s 226us/step - loss: 0.0045 - val_loss: 0.0032\n",
      "Epoch 4/24\n",
      "3622/3622 [==============================] - 1s 224us/step - loss: 0.0023 - val_loss: 9.1085e-04\n",
      "Epoch 5/24\n",
      "3622/3622 [==============================] - 1s 227us/step - loss: 0.0011 - val_loss: 8.7220e-04\n",
      "Epoch 6/24\n",
      "3622/3622 [==============================] - 1s 227us/step - loss: 8.7644e-04 - val_loss: 9.8145e-04\n",
      "Epoch 7/24\n",
      "3622/3622 [==============================] - 1s 225us/step - loss: 7.4425e-04 - val_loss: 6.2521e-04\n",
      "Epoch 8/24\n",
      "3622/3622 [==============================] - 1s 226us/step - loss: 6.3938e-04 - val_loss: 8.7763e-04\n",
      "Epoch 9/24\n",
      "3622/3622 [==============================] - 1s 226us/step - loss: 6.0587e-04 - val_loss: 6.4051e-04\n",
      "Epoch 10/24\n",
      "3622/3622 [==============================] - 1s 226us/step - loss: 5.8592e-04 - val_loss: 6.1360e-04\n",
      "Epoch 11/24\n",
      "3622/3622 [==============================] - 1s 227us/step - loss: 5.6350e-04 - val_loss: 5.9120e-04\n",
      "Epoch 12/24\n",
      "3622/3622 [==============================] - 1s 226us/step - loss: 5.3631e-04 - val_loss: 6.2114e-04\n",
      "Epoch 13/24\n",
      "3622/3622 [==============================] - 1s 225us/step - loss: 5.0162e-04 - val_loss: 5.8172e-04\n",
      "Epoch 14/24\n",
      "3622/3622 [==============================] - 1s 227us/step - loss: 4.7353e-04 - val_loss: 6.4300e-04\n",
      "Epoch 15/24\n",
      "3622/3622 [==============================] - 1s 227us/step - loss: 4.6494e-04 - val_loss: 4.6041e-04\n",
      "Epoch 16/24\n",
      "3622/3622 [==============================] - 1s 227us/step - loss: 4.2584e-04 - val_loss: 4.7777e-04\n",
      "Epoch 17/24\n",
      "3622/3622 [==============================] - 1s 227us/step - loss: 4.0641e-04 - val_loss: 4.2984e-04\n",
      "Epoch 18/24\n",
      "3622/3622 [==============================] - 1s 226us/step - loss: 3.8553e-04 - val_loss: 4.0920e-04\n",
      "Epoch 19/24\n",
      "3622/3622 [==============================] - 1s 227us/step - loss: 3.6868e-04 - val_loss: 4.0431e-04\n",
      "Epoch 20/24\n",
      "3622/3622 [==============================] - 1s 230us/step - loss: 3.6256e-04 - val_loss: 5.0604e-04\n",
      "Epoch 21/24\n",
      "3622/3622 [==============================] - 1s 227us/step - loss: 3.9123e-04 - val_loss: 3.7961e-04\n",
      "Epoch 22/24\n",
      "3622/3622 [==============================] - 1s 234us/step - loss: 4.0013e-04 - val_loss: 4.7996e-04\n",
      "Epoch 23/24\n",
      "3622/3622 [==============================] - 1s 226us/step - loss: 3.5835e-04 - val_loss: 3.8033e-04\n",
      "Epoch 24/24\n",
      "3622/3622 [==============================] - 1s 227us/step - loss: 3.4418e-04 - val_loss: 3.9523e-04\n",
      "Train on 3622 samples, validate on 403 samples\n",
      "Epoch 1/24\n",
      "3622/3622 [==============================] - 30s 8ms/step - loss: 0.0927 - val_loss: 0.0051\n",
      "Epoch 2/24\n",
      "3622/3622 [==============================] - 1s 226us/step - loss: 0.0046 - val_loss: 0.0052\n",
      "Epoch 3/24\n",
      "3622/3622 [==============================] - 1s 227us/step - loss: 0.0087 - val_loss: 0.0102\n",
      "Epoch 4/24\n",
      "3622/3622 [==============================] - 1s 228us/step - loss: 0.0082 - val_loss: 0.0085\n",
      "Epoch 5/24\n",
      "3622/3622 [==============================] - 1s 227us/step - loss: 0.0085 - val_loss: 0.0070\n",
      "Epoch 6/24\n",
      "3622/3622 [==============================] - 1s 226us/step - loss: 0.0077 - val_loss: 0.0212\n",
      "Epoch 7/24\n",
      "3622/3622 [==============================] - 1s 227us/step - loss: 0.0058 - val_loss: 0.0081\n",
      "Epoch 8/24\n",
      "3622/3622 [==============================] - 1s 230us/step - loss: 0.0073 - val_loss: 0.0061\n",
      "Epoch 9/24\n",
      "3622/3622 [==============================] - 1s 228us/step - loss: 0.0080 - val_loss: 0.0060\n",
      "Epoch 10/24\n",
      "3622/3622 [==============================] - 1s 228us/step - loss: 0.0048 - val_loss: 0.0249\n",
      "Epoch 11/24\n",
      "3622/3622 [==============================] - 1s 227us/step - loss: 0.0053 - val_loss: 0.0126\n",
      "Epoch 12/24\n",
      "3622/3622 [==============================] - 1s 226us/step - loss: 0.0051 - val_loss: 0.0125\n",
      "Epoch 13/24\n",
      "3622/3622 [==============================] - 1s 226us/step - loss: 0.0050 - val_loss: 0.0152\n",
      "Epoch 14/24\n",
      "3622/3622 [==============================] - 1s 225us/step - loss: 0.0051 - val_loss: 0.0090\n",
      "Epoch 15/24\n",
      "3622/3622 [==============================] - 1s 225us/step - loss: 0.0043 - val_loss: 0.0240\n",
      "Epoch 16/24\n",
      "3622/3622 [==============================] - 1s 226us/step - loss: 0.0045 - val_loss: 0.0081\n",
      "Epoch 17/24\n",
      "3622/3622 [==============================] - 1s 227us/step - loss: 0.0061 - val_loss: 0.0089\n",
      "Epoch 18/24\n",
      "3622/3622 [==============================] - 1s 227us/step - loss: 0.0028 - val_loss: 0.0107\n",
      "Epoch 19/24\n",
      "3622/3622 [==============================] - 1s 226us/step - loss: 0.0048 - val_loss: 0.0082\n",
      "Epoch 20/24\n",
      "3622/3622 [==============================] - 1s 227us/step - loss: 0.0040 - val_loss: 0.0073\n",
      "Epoch 21/24\n",
      "3622/3622 [==============================] - 1s 229us/step - loss: 0.0040 - val_loss: 0.0136\n",
      "Epoch 22/24\n",
      "3622/3622 [==============================] - 1s 229us/step - loss: 0.0041 - val_loss: 0.0067\n",
      "Epoch 23/24\n",
      "3622/3622 [==============================] - 1s 233us/step - loss: 0.0041 - val_loss: 0.0072\n",
      "Epoch 24/24\n",
      "3622/3622 [==============================] - 1s 241us/step - loss: 0.0032 - val_loss: 0.0109\n",
      "Train on 3622 samples, validate on 403 samples\n",
      "Epoch 1/24\n",
      "3622/3622 [==============================] - 31s 9ms/step - loss: 0.0910 - val_loss: 0.0034\n",
      "Epoch 2/24\n",
      "3622/3622 [==============================] - 1s 227us/step - loss: 0.0053 - val_loss: 0.0070\n",
      "Epoch 3/24\n",
      "3622/3622 [==============================] - 1s 227us/step - loss: 0.0069 - val_loss: 0.0021\n",
      "Epoch 4/24\n",
      "3622/3622 [==============================] - 1s 227us/step - loss: 0.0095 - val_loss: 0.0062\n",
      "Epoch 5/24\n",
      "3622/3622 [==============================] - 1s 227us/step - loss: 0.0088 - val_loss: 0.0105\n",
      "Epoch 6/24\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3622/3622 [==============================] - 1s 228us/step - loss: 0.0072 - val_loss: 0.0027\n",
      "Epoch 7/24\n",
      "3622/3622 [==============================] - 1s 228us/step - loss: 0.0082 - val_loss: 0.0065\n",
      "Epoch 8/24\n",
      "3622/3622 [==============================] - 1s 227us/step - loss: 0.0083 - val_loss: 0.0025\n",
      "Epoch 9/24\n",
      "3622/3622 [==============================] - 1s 234us/step - loss: 0.0049 - val_loss: 0.0448\n",
      "Epoch 10/24\n",
      "3622/3622 [==============================] - 1s 226us/step - loss: 0.0084 - val_loss: 0.0131\n",
      "Epoch 11/24\n",
      "3622/3622 [==============================] - 1s 227us/step - loss: 0.0065 - val_loss: 0.0099\n",
      "Epoch 12/24\n",
      "3622/3622 [==============================] - 1s 228us/step - loss: 0.0055 - val_loss: 0.0129\n",
      "Epoch 13/24\n",
      "3622/3622 [==============================] - 1s 227us/step - loss: 0.0067 - val_loss: 0.0076\n",
      "Epoch 14/24\n",
      "3622/3622 [==============================] - 1s 227us/step - loss: 0.0047 - val_loss: 0.0270\n",
      "Epoch 15/24\n",
      "3622/3622 [==============================] - 1s 232us/step - loss: 0.0059 - val_loss: 0.0070\n",
      "Epoch 16/24\n",
      "3622/3622 [==============================] - 1s 229us/step - loss: 0.0055 - val_loss: 0.0036\n",
      "Epoch 17/24\n",
      "3622/3622 [==============================] - 1s 227us/step - loss: 0.0045 - val_loss: 0.0094\n",
      "Epoch 18/24\n",
      "3622/3622 [==============================] - 1s 227us/step - loss: 0.0043 - val_loss: 0.0029\n",
      "Epoch 19/24\n",
      "3622/3622 [==============================] - 1s 226us/step - loss: 0.0060 - val_loss: 0.0084\n",
      "Epoch 20/24\n",
      "3622/3622 [==============================] - 1s 228us/step - loss: 0.0033 - val_loss: 0.0096\n",
      "Epoch 21/24\n",
      "3622/3622 [==============================] - 1s 227us/step - loss: 0.0047 - val_loss: 0.0090\n",
      "Epoch 22/24\n",
      "3622/3622 [==============================] - 1s 231us/step - loss: 0.0045 - val_loss: 0.0073\n",
      "Epoch 23/24\n",
      "3622/3622 [==============================] - 1s 230us/step - loss: 0.0043 - val_loss: 0.0020\n",
      "Epoch 24/24\n",
      "3622/3622 [==============================] - 1s 227us/step - loss: 0.0040 - val_loss: 0.0098\n",
      "Train on 3622 samples, validate on 403 samples\n",
      "Epoch 1/24\n",
      "3622/3622 [==============================] - 31s 8ms/step - loss: 0.1564 - val_loss: 0.1286\n",
      "Epoch 2/24\n",
      "3622/3622 [==============================] - 1s 232us/step - loss: 0.0737 - val_loss: 0.0554\n",
      "Epoch 3/24\n",
      "3622/3622 [==============================] - 1s 230us/step - loss: 0.0457 - val_loss: 0.0253\n",
      "Epoch 4/24\n",
      "3622/3622 [==============================] - 1s 227us/step - loss: 0.0349 - val_loss: 0.0169\n",
      "Epoch 5/24\n",
      "3622/3622 [==============================] - 1s 229us/step - loss: 0.0312 - val_loss: 0.0120\n",
      "Epoch 6/24\n",
      "3622/3622 [==============================] - 1s 226us/step - loss: 0.0285 - val_loss: 0.0095\n",
      "Epoch 7/24\n",
      "3622/3622 [==============================] - 1s 227us/step - loss: 0.0258 - val_loss: 0.0074\n",
      "Epoch 8/24\n",
      "3622/3622 [==============================] - 1s 228us/step - loss: 0.0237 - val_loss: 0.0072\n",
      "Epoch 9/24\n",
      "3622/3622 [==============================] - 1s 227us/step - loss: 0.0214 - val_loss: 0.0069\n",
      "Epoch 10/24\n",
      "3622/3622 [==============================] - 1s 227us/step - loss: 0.0201 - val_loss: 0.0057\n",
      "Epoch 11/24\n",
      "3622/3622 [==============================] - 1s 228us/step - loss: 0.0179 - val_loss: 0.0046\n",
      "Epoch 12/24\n",
      "3622/3622 [==============================] - 1s 228us/step - loss: 0.0165 - val_loss: 0.0040\n",
      "Epoch 13/24\n",
      "3622/3622 [==============================] - 1s 224us/step - loss: 0.0147 - val_loss: 0.0038\n",
      "Epoch 14/24\n",
      "3622/3622 [==============================] - 1s 226us/step - loss: 0.0135 - val_loss: 0.0034\n",
      "Epoch 15/24\n",
      "3622/3622 [==============================] - 1s 230us/step - loss: 0.0123 - val_loss: 0.0028\n",
      "Epoch 16/24\n",
      "3622/3622 [==============================] - 1s 227us/step - loss: 0.0111 - val_loss: 0.0022\n",
      "Epoch 17/24\n",
      "3622/3622 [==============================] - 1s 226us/step - loss: 0.0097 - val_loss: 0.0023\n",
      "Epoch 18/24\n",
      "3622/3622 [==============================] - 1s 227us/step - loss: 0.0091 - val_loss: 0.0019\n",
      "Epoch 19/24\n",
      "3622/3622 [==============================] - 1s 228us/step - loss: 0.0082 - val_loss: 0.0016\n",
      "Epoch 20/24\n",
      "3622/3622 [==============================] - 1s 228us/step - loss: 0.0073 - val_loss: 0.0015\n",
      "Epoch 21/24\n",
      "3622/3622 [==============================] - 1s 230us/step - loss: 0.0069 - val_loss: 0.0014\n",
      "Epoch 22/24\n",
      "3622/3622 [==============================] - 1s 240us/step - loss: 0.0062 - val_loss: 0.0012\n",
      "Epoch 23/24\n",
      "3622/3622 [==============================] - 1s 230us/step - loss: 0.0056 - val_loss: 0.0011\n",
      "Epoch 24/24\n",
      "3622/3622 [==============================] - 1s 227us/step - loss: 0.0052 - val_loss: 9.4550e-04\n",
      "Train on 3622 samples, validate on 403 samples\n",
      "Epoch 1/24\n",
      "3622/3622 [==============================] - 31s 9ms/step - loss: 0.2458 - val_loss: 0.1342\n",
      "Epoch 2/24\n",
      "3622/3622 [==============================] - 1s 227us/step - loss: 0.0287 - val_loss: 0.0060\n",
      "Epoch 3/24\n",
      "3622/3622 [==============================] - 1s 233us/step - loss: 0.0062 - val_loss: 0.0025\n",
      "Epoch 4/24\n",
      "3622/3622 [==============================] - 1s 227us/step - loss: 0.0042 - val_loss: 0.0017\n",
      "Epoch 5/24\n",
      "3622/3622 [==============================] - 1s 227us/step - loss: 0.0024 - val_loss: 0.0026\n",
      "Epoch 6/24\n",
      "3622/3622 [==============================] - 1s 227us/step - loss: 0.0023 - val_loss: 9.4668e-04\n",
      "Epoch 7/24\n",
      "3622/3622 [==============================] - 1s 227us/step - loss: 0.0021 - val_loss: 7.4583e-04\n",
      "Epoch 8/24\n",
      "3622/3622 [==============================] - 1s 230us/step - loss: 0.0019 - val_loss: 9.9693e-04\n",
      "Epoch 9/24\n",
      "3622/3622 [==============================] - 1s 230us/step - loss: 0.0020 - val_loss: 0.0030\n",
      "Epoch 10/24\n",
      "3622/3622 [==============================] - 1s 228us/step - loss: 0.0027 - val_loss: 0.0012\n",
      "Epoch 11/24\n",
      "3622/3622 [==============================] - 1s 228us/step - loss: 0.0020 - val_loss: 9.3943e-04\n",
      "Epoch 12/24\n",
      "3622/3622 [==============================] - 1s 228us/step - loss: 0.0018 - val_loss: 7.3004e-04\n",
      "Epoch 13/24\n",
      "3622/3622 [==============================] - 1s 227us/step - loss: 0.0018 - val_loss: 7.2420e-04\n",
      "Epoch 14/24\n",
      "3622/3622 [==============================] - 1s 227us/step - loss: 0.0017 - val_loss: 9.4601e-04\n",
      "Epoch 15/24\n",
      "3622/3622 [==============================] - 1s 228us/step - loss: 0.0022 - val_loss: 0.0011\n",
      "Epoch 16/24\n",
      "3622/3622 [==============================] - 1s 230us/step - loss: 0.0017 - val_loss: 7.7739e-04\n",
      "Epoch 17/24\n",
      "3622/3622 [==============================] - 1s 228us/step - loss: 0.0016 - val_loss: 7.7886e-04\n",
      "Epoch 18/24\n",
      "3622/3622 [==============================] - 1s 227us/step - loss: 0.0016 - val_loss: 9.2193e-04\n",
      "Epoch 19/24\n",
      "3622/3622 [==============================] - 1s 227us/step - loss: 0.0015 - val_loss: 7.0278e-04\n",
      "Epoch 20/24\n",
      "3622/3622 [==============================] - 1s 233us/step - loss: 0.0015 - val_loss: 7.5736e-04\n",
      "Epoch 21/24\n",
      "3622/3622 [==============================] - 1s 228us/step - loss: 0.0020 - val_loss: 6.9085e-04\n",
      "Epoch 22/24\n",
      "3622/3622 [==============================] - 1s 228us/step - loss: 0.0016 - val_loss: 8.2309e-04\n",
      "Epoch 23/24\n",
      "3622/3622 [==============================] - 1s 227us/step - loss: 0.0015 - val_loss: 9.4309e-04\n",
      "Epoch 24/24\n",
      "3622/3622 [==============================] - 1s 229us/step - loss: 0.0014 - val_loss: 0.0010\n",
      "Train on 3622 samples, validate on 403 samples\n",
      "Epoch 1/24\n",
      "3622/3622 [==============================] - 31s 9ms/step - loss: 0.1546 - val_loss: 0.1037\n",
      "Epoch 2/24\n",
      "3622/3622 [==============================] - 1s 221us/step - loss: 0.0214 - val_loss: 0.0011\n",
      "Epoch 3/24\n",
      "3622/3622 [==============================] - 1s 223us/step - loss: 0.0040 - val_loss: 0.0034\n",
      "Epoch 4/24\n",
      "3622/3622 [==============================] - 1s 221us/step - loss: 0.0039 - val_loss: 7.2138e-04\n",
      "Epoch 5/24\n",
      "3622/3622 [==============================] - 1s 223us/step - loss: 0.0028 - val_loss: 9.9269e-04\n",
      "Epoch 6/24\n",
      "3622/3622 [==============================] - 1s 222us/step - loss: 0.0025 - val_loss: 0.0033\n",
      "Epoch 7/24\n",
      "3622/3622 [==============================] - 1s 222us/step - loss: 0.0030 - val_loss: 7.2493e-04\n",
      "Epoch 8/24\n",
      "3622/3622 [==============================] - 1s 222us/step - loss: 0.0023 - val_loss: 0.0046\n",
      "Epoch 9/24\n",
      "3622/3622 [==============================] - 1s 223us/step - loss: 0.0031 - val_loss: 8.2356e-04\n",
      "Epoch 10/24\n",
      "3622/3622 [==============================] - 1s 225us/step - loss: 0.0020 - val_loss: 0.0023\n",
      "Epoch 11/24\n",
      "3622/3622 [==============================] - 1s 222us/step - loss: 0.0024 - val_loss: 6.6905e-04\n",
      "Epoch 12/24\n",
      "3622/3622 [==============================] - 1s 222us/step - loss: 0.0020 - val_loss: 6.5805e-04\n",
      "Epoch 13/24\n",
      "3622/3622 [==============================] - 1s 222us/step - loss: 0.0020 - val_loss: 0.0014\n",
      "Epoch 14/24\n",
      "3622/3622 [==============================] - 1s 227us/step - loss: 0.0022 - val_loss: 6.6404e-04\n",
      "Epoch 15/24\n",
      "3622/3622 [==============================] - 1s 222us/step - loss: 0.0019 - val_loss: 0.0012\n",
      "Epoch 16/24\n",
      "3622/3622 [==============================] - 1s 221us/step - loss: 0.0018 - val_loss: 0.0017\n",
      "Epoch 17/24\n",
      "3622/3622 [==============================] - 1s 222us/step - loss: 0.0022 - val_loss: 0.0020\n",
      "Epoch 18/24\n",
      "3622/3622 [==============================] - 1s 227us/step - loss: 0.0019 - val_loss: 0.0019\n",
      "Epoch 19/24\n",
      "3622/3622 [==============================] - 1s 227us/step - loss: 0.0019 - val_loss: 0.0016\n",
      "Epoch 20/24\n",
      "3622/3622 [==============================] - 1s 221us/step - loss: 0.0019 - val_loss: 0.0016\n",
      "Epoch 21/24\n",
      "3622/3622 [==============================] - 1s 222us/step - loss: 0.0017 - val_loss: 6.9965e-04\n",
      "Epoch 22/24\n",
      "3622/3622 [==============================] - 1s 223us/step - loss: 0.0017 - val_loss: 0.0015\n",
      "Epoch 23/24\n",
      "3622/3622 [==============================] - 1s 224us/step - loss: 0.0016 - val_loss: 8.4972e-04\n",
      "Epoch 24/24\n",
      "3622/3622 [==============================] - 1s 222us/step - loss: 0.0015 - val_loss: 7.7579e-04\n",
      "Train on 3622 samples, validate on 403 samples\n",
      "Epoch 1/24\n",
      "3622/3622 [==============================] - 33s 9ms/step - loss: 0.2425 - val_loss: 0.1899\n",
      "Epoch 2/24\n",
      "3622/3622 [==============================] - 2s 435us/step - loss: 0.0496 - val_loss: 0.0055\n",
      "Epoch 3/24\n",
      "3622/3622 [==============================] - 2s 438us/step - loss: 0.0111 - val_loss: 0.0028\n",
      "Epoch 4/24\n",
      "3622/3622 [==============================] - 2s 438us/step - loss: 0.0048 - val_loss: 0.0015\n",
      "Epoch 5/24\n",
      "3622/3622 [==============================] - 2s 438us/step - loss: 0.0034 - val_loss: 0.0015\n",
      "Epoch 6/24\n",
      "3622/3622 [==============================] - 2s 437us/step - loss: 0.0026 - val_loss: 0.0014\n",
      "Epoch 7/24\n",
      "3622/3622 [==============================] - 2s 435us/step - loss: 0.0022 - val_loss: 0.0023\n",
      "Epoch 8/24\n",
      "3622/3622 [==============================] - 2s 440us/step - loss: 0.0020 - val_loss: 0.0051\n",
      "Epoch 9/24\n",
      "3622/3622 [==============================] - 2s 440us/step - loss: 0.0023 - val_loss: 0.0048\n",
      "Epoch 10/24\n",
      "3622/3622 [==============================] - 2s 437us/step - loss: 0.0023 - val_loss: 0.0049\n",
      "Epoch 11/24\n",
      "3622/3622 [==============================] - 2s 438us/step - loss: 0.0028 - val_loss: 0.0024\n",
      "Epoch 12/24\n",
      "3622/3622 [==============================] - 2s 441us/step - loss: 0.0020 - val_loss: 0.0021\n",
      "Epoch 13/24\n",
      "3622/3622 [==============================] - 2s 436us/step - loss: 0.0019 - val_loss: 0.0013\n",
      "Epoch 14/24\n",
      "3622/3622 [==============================] - 2s 438us/step - loss: 0.0019 - val_loss: 0.0021\n",
      "Epoch 15/24\n",
      "3622/3622 [==============================] - 2s 438us/step - loss: 0.0017 - val_loss: 0.0016\n",
      "Epoch 16/24\n",
      "3622/3622 [==============================] - 2s 436us/step - loss: 0.0018 - val_loss: 0.0016\n",
      "Epoch 17/24\n",
      "3622/3622 [==============================] - 2s 435us/step - loss: 0.0019 - val_loss: 0.0024\n",
      "Epoch 18/24\n",
      "3622/3622 [==============================] - 2s 435us/step - loss: 0.0021 - val_loss: 0.0040\n",
      "Epoch 19/24\n",
      "3622/3622 [==============================] - 2s 438us/step - loss: 0.0030 - val_loss: 0.0012\n",
      "Epoch 20/24\n",
      "3622/3622 [==============================] - 2s 435us/step - loss: 0.0017 - val_loss: 0.0018\n",
      "Epoch 21/24\n",
      "3622/3622 [==============================] - 2s 435us/step - loss: 0.0017 - val_loss: 0.0024\n",
      "Epoch 22/24\n",
      "3622/3622 [==============================] - 2s 434us/step - loss: 0.0017 - val_loss: 0.0020\n",
      "Epoch 23/24\n",
      "3622/3622 [==============================] - 2s 435us/step - loss: 0.0019 - val_loss: 0.0012\n",
      "Epoch 24/24\n",
      "3622/3622 [==============================] - 2s 437us/step - loss: 0.0017 - val_loss: 0.0029\n",
      "\n",
      "GENERATION 19\n",
      "\n",
      "Train on 3622 samples, validate on 403 samples\n",
      "Epoch 1/24\n",
      "3622/3622 [==============================] - 32s 9ms/step - loss: 0.2314 - val_loss: 0.1952\n",
      "Epoch 2/24\n",
      "3622/3622 [==============================] - 1s 231us/step - loss: 0.0601 - val_loss: 0.0086\n",
      "Epoch 3/24\n",
      "3622/3622 [==============================] - 1s 229us/step - loss: 0.0137 - val_loss: 0.0072\n",
      "Epoch 4/24\n",
      "3622/3622 [==============================] - 1s 230us/step - loss: 0.0060 - val_loss: 0.0042\n",
      "Epoch 5/24\n",
      "3622/3622 [==============================] - 1s 230us/step - loss: 0.0033 - val_loss: 9.5063e-04\n",
      "Epoch 6/24\n",
      "3622/3622 [==============================] - 1s 230us/step - loss: 0.0021 - val_loss: 9.5212e-04\n",
      "Epoch 7/24\n",
      "3622/3622 [==============================] - 1s 230us/step - loss: 0.0019 - val_loss: 0.0015\n",
      "Epoch 8/24\n",
      "3622/3622 [==============================] - 1s 229us/step - loss: 0.0017 - val_loss: 0.0012\n",
      "Epoch 9/24\n",
      "3622/3622 [==============================] - 1s 230us/step - loss: 0.0015 - val_loss: 9.2647e-04\n",
      "Epoch 10/24\n",
      "3622/3622 [==============================] - 1s 230us/step - loss: 0.0015 - val_loss: 8.7891e-04\n",
      "Epoch 11/24\n",
      "3622/3622 [==============================] - 1s 228us/step - loss: 0.0014 - val_loss: 9.8848e-04\n",
      "Epoch 12/24\n",
      "3622/3622 [==============================] - 1s 230us/step - loss: 0.0013 - val_loss: 9.7827e-04\n",
      "Epoch 13/24\n",
      "3622/3622 [==============================] - 1s 228us/step - loss: 0.0013 - val_loss: 9.8953e-04\n",
      "Epoch 14/24\n",
      "3622/3622 [==============================] - 1s 230us/step - loss: 0.0013 - val_loss: 8.7440e-04\n",
      "Epoch 15/24\n",
      "3622/3622 [==============================] - 1s 234us/step - loss: 0.0012 - val_loss: 8.1977e-04\n",
      "Epoch 16/24\n",
      "3622/3622 [==============================] - 1s 229us/step - loss: 0.0012 - val_loss: 8.9268e-04\n",
      "Epoch 17/24\n",
      "3622/3622 [==============================] - 1s 231us/step - loss: 0.0011 - val_loss: 6.8173e-04\n",
      "Epoch 18/24\n",
      "3622/3622 [==============================] - 1s 230us/step - loss: 0.0011 - val_loss: 6.6366e-04\n",
      "Epoch 19/24\n",
      "3622/3622 [==============================] - 1s 233us/step - loss: 0.0012 - val_loss: 6.3622e-04\n",
      "Epoch 20/24\n",
      "3622/3622 [==============================] - 1s 229us/step - loss: 0.0011 - val_loss: 6.5483e-04\n",
      "Epoch 21/24\n",
      "3622/3622 [==============================] - 1s 231us/step - loss: 0.0011 - val_loss: 6.6689e-04\n",
      "Epoch 22/24\n",
      "3622/3622 [==============================] - 1s 235us/step - loss: 9.9784e-04 - val_loss: 8.7958e-04\n",
      "Epoch 23/24\n",
      "3622/3622 [==============================] - 1s 233us/step - loss: 9.2086e-04 - val_loss: 8.2535e-04\n",
      "Epoch 24/24\n",
      "3622/3622 [==============================] - 1s 230us/step - loss: 8.8380e-04 - val_loss: 0.0012\n",
      "Train on 3622 samples, validate on 403 samples\n",
      "Epoch 1/24\n",
      "3622/3622 [==============================] - 33s 9ms/step - loss: 0.1772 - val_loss: 0.0023\n",
      "Epoch 2/24\n",
      "3622/3622 [==============================] - 1s 230us/step - loss: 0.0202 - val_loss: 0.0106\n",
      "Epoch 3/24\n",
      "3622/3622 [==============================] - 1s 230us/step - loss: 0.0053 - val_loss: 0.0024\n",
      "Epoch 4/24\n",
      "3622/3622 [==============================] - 1s 230us/step - loss: 0.0021 - val_loss: 8.3120e-04\n",
      "Epoch 5/24\n",
      "3622/3622 [==============================] - 1s 228us/step - loss: 9.6591e-04 - val_loss: 9.6926e-04\n",
      "Epoch 6/24\n",
      "3622/3622 [==============================] - 1s 229us/step - loss: 9.2389e-04 - val_loss: 8.1948e-04\n",
      "Epoch 7/24\n",
      "3622/3622 [==============================] - 1s 231us/step - loss: 8.1123e-04 - val_loss: 8.2809e-04\n",
      "Epoch 8/24\n",
      "3622/3622 [==============================] - 1s 228us/step - loss: 7.4051e-04 - val_loss: 8.1421e-04\n",
      "Epoch 9/24\n",
      "3622/3622 [==============================] - 1s 231us/step - loss: 7.2931e-04 - val_loss: 7.3394e-04\n",
      "Epoch 10/24\n",
      "3622/3622 [==============================] - 1s 232us/step - loss: 7.0933e-04 - val_loss: 7.4681e-04\n",
      "Epoch 11/24\n",
      "3622/3622 [==============================] - 1s 230us/step - loss: 6.8941e-04 - val_loss: 6.8295e-04\n",
      "Epoch 12/24\n",
      "3622/3622 [==============================] - 1s 231us/step - loss: 6.4431e-04 - val_loss: 6.6772e-04\n",
      "Epoch 13/24\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3622/3622 [==============================] - 1s 230us/step - loss: 6.3139e-04 - val_loss: 6.3805e-04\n",
      "Epoch 14/24\n",
      "3622/3622 [==============================] - 1s 230us/step - loss: 5.8897e-04 - val_loss: 5.9682e-04\n",
      "Epoch 15/24\n",
      "3622/3622 [==============================] - 1s 243us/step - loss: 5.4361e-04 - val_loss: 5.6071e-04\n",
      "Epoch 16/24\n",
      "3622/3622 [==============================] - 1s 230us/step - loss: 5.0674e-04 - val_loss: 5.5268e-04\n",
      "Epoch 17/24\n",
      "3622/3622 [==============================] - 1s 229us/step - loss: 4.8719e-04 - val_loss: 5.4780e-04\n",
      "Epoch 18/24\n",
      "3622/3622 [==============================] - 1s 228us/step - loss: 4.7875e-04 - val_loss: 5.5457e-04\n",
      "Epoch 19/24\n",
      "3622/3622 [==============================] - 1s 230us/step - loss: 4.5393e-04 - val_loss: 4.8080e-04\n",
      "Epoch 20/24\n",
      "3622/3622 [==============================] - 1s 230us/step - loss: 4.2615e-04 - val_loss: 4.6546e-04\n",
      "Epoch 21/24\n",
      "3622/3622 [==============================] - 1s 230us/step - loss: 4.1024e-04 - val_loss: 4.1613e-04\n",
      "Epoch 22/24\n",
      "3622/3622 [==============================] - 1s 229us/step - loss: 3.9984e-04 - val_loss: 4.4601e-04\n",
      "Epoch 23/24\n",
      "3622/3622 [==============================] - 1s 229us/step - loss: 4.2356e-04 - val_loss: 4.0560e-04\n",
      "Epoch 24/24\n",
      "3622/3622 [==============================] - 1s 230us/step - loss: 4.1195e-04 - val_loss: 4.2531e-04\n",
      "Train on 3622 samples, validate on 403 samples\n",
      "Epoch 1/24\n",
      "3622/3622 [==============================] - 35s 10ms/step - loss: 0.1134 - val_loss: 0.0145\n",
      "Epoch 2/24\n",
      "3622/3622 [==============================] - 2s 549us/step - loss: 0.0121 - val_loss: 0.0014\n",
      "Epoch 3/24\n",
      "3622/3622 [==============================] - 2s 542us/step - loss: 0.0037 - val_loss: 0.0069\n",
      "Epoch 4/24\n",
      "3622/3622 [==============================] - 2s 546us/step - loss: 0.0022 - val_loss: 0.0033\n",
      "Epoch 5/24\n",
      "3622/3622 [==============================] - 2s 545us/step - loss: 0.0017 - val_loss: 0.0012\n",
      "Epoch 6/24\n",
      "3622/3622 [==============================] - 2s 542us/step - loss: 0.0013 - val_loss: 0.0012\n",
      "Epoch 7/24\n",
      "3622/3622 [==============================] - 2s 540us/step - loss: 0.0012 - val_loss: 0.0012\n",
      "Epoch 8/24\n",
      "3622/3622 [==============================] - 2s 541us/step - loss: 0.0012 - val_loss: 0.0014\n",
      "Epoch 9/24\n",
      "3622/3622 [==============================] - 2s 543us/step - loss: 0.0011 - val_loss: 0.0013\n",
      "Epoch 10/24\n",
      "3622/3622 [==============================] - 2s 542us/step - loss: 0.0011 - val_loss: 0.0012\n",
      "Epoch 11/24\n",
      "3622/3622 [==============================] - 2s 545us/step - loss: 0.0010 - val_loss: 0.0012\n",
      "Epoch 12/24\n",
      "3622/3622 [==============================] - 2s 544us/step - loss: 0.0010 - val_loss: 0.0011\n",
      "Epoch 13/24\n",
      "3622/3622 [==============================] - 2s 547us/step - loss: 0.0010 - val_loss: 0.0011\n",
      "Epoch 14/24\n",
      "3622/3622 [==============================] - 2s 542us/step - loss: 9.8561e-04 - val_loss: 0.0011\n",
      "Epoch 15/24\n",
      "3622/3622 [==============================] - 2s 546us/step - loss: 0.0010 - val_loss: 0.0010\n",
      "Epoch 16/24\n",
      "3622/3622 [==============================] - 2s 544us/step - loss: 9.8096e-04 - val_loss: 9.7883e-04\n",
      "Epoch 17/24\n",
      "3622/3622 [==============================] - 2s 553us/step - loss: 9.2624e-04 - val_loss: 9.7459e-04\n",
      "Epoch 18/24\n",
      "3622/3622 [==============================] - 2s 542us/step - loss: 9.0482e-04 - val_loss: 0.0010\n",
      "Epoch 19/24\n",
      "3622/3622 [==============================] - 2s 541us/step - loss: 8.9156e-04 - val_loss: 0.0011\n",
      "Epoch 20/24\n",
      "3622/3622 [==============================] - 2s 544us/step - loss: 8.7582e-04 - val_loss: 8.8750e-04\n",
      "Epoch 21/24\n",
      "3622/3622 [==============================] - 2s 545us/step - loss: 8.4211e-04 - val_loss: 8.6266e-04\n",
      "Epoch 22/24\n",
      "3622/3622 [==============================] - 2s 543us/step - loss: 8.3784e-04 - val_loss: 9.8405e-04\n",
      "Epoch 23/24\n",
      "3622/3622 [==============================] - 2s 544us/step - loss: 8.2091e-04 - val_loss: 0.0011\n",
      "Epoch 24/24\n",
      "3622/3622 [==============================] - 2s 541us/step - loss: 8.2601e-04 - val_loss: 8.9262e-04\n",
      "Train on 3622 samples, validate on 403 samples\n",
      "Epoch 1/24\n",
      "3622/3622 [==============================] - 35s 10ms/step - loss: 0.1156 - val_loss: 0.0142\n",
      "Epoch 2/24\n",
      "3622/3622 [==============================] - 2s 571us/step - loss: 0.0207 - val_loss: 0.0105\n",
      "Epoch 3/24\n",
      "3622/3622 [==============================] - 2s 569us/step - loss: 0.0079 - val_loss: 0.0061\n",
      "Epoch 4/24\n",
      "3622/3622 [==============================] - 2s 568us/step - loss: 0.0046 - val_loss: 0.0055\n",
      "Epoch 5/24\n",
      "3622/3622 [==============================] - 2s 570us/step - loss: 0.0031 - val_loss: 0.0015\n",
      "Epoch 6/24\n",
      "3622/3622 [==============================] - 2s 567us/step - loss: 0.0026 - val_loss: 0.0014\n",
      "Epoch 7/24\n",
      "3622/3622 [==============================] - 2s 573us/step - loss: 0.0022 - val_loss: 0.0015\n",
      "Epoch 8/24\n",
      "3622/3622 [==============================] - 2s 566us/step - loss: 0.0021 - val_loss: 0.0017\n",
      "Epoch 9/24\n",
      "3622/3622 [==============================] - 2s 567us/step - loss: 0.0020 - val_loss: 0.0013\n",
      "Epoch 10/24\n",
      "3622/3622 [==============================] - 2s 569us/step - loss: 0.0020 - val_loss: 0.0013\n",
      "Epoch 11/24\n",
      "3622/3622 [==============================] - 2s 566us/step - loss: 0.0019 - val_loss: 0.0012\n",
      "Epoch 12/24\n",
      "3622/3622 [==============================] - 2s 568us/step - loss: 0.0019 - val_loss: 0.0012\n",
      "Epoch 13/24\n",
      "3622/3622 [==============================] - 2s 575us/step - loss: 0.0018 - val_loss: 0.0012\n",
      "Epoch 14/24\n",
      "3622/3622 [==============================] - 2s 567us/step - loss: 0.0018 - val_loss: 0.0012\n",
      "Epoch 15/24\n",
      "3622/3622 [==============================] - 2s 565us/step - loss: 0.0018 - val_loss: 0.0012\n",
      "Epoch 16/24\n",
      "3622/3622 [==============================] - 2s 571us/step - loss: 0.0017 - val_loss: 0.0012\n",
      "Epoch 17/24\n",
      "3622/3622 [==============================] - 2s 567us/step - loss: 0.0017 - val_loss: 0.0013\n",
      "Epoch 18/24\n",
      "3622/3622 [==============================] - 2s 568us/step - loss: 0.0016 - val_loss: 0.0012\n",
      "Epoch 19/24\n",
      "3622/3622 [==============================] - 2s 566us/step - loss: 0.0016 - val_loss: 0.0011\n",
      "Epoch 20/24\n",
      "3622/3622 [==============================] - 2s 566us/step - loss: 0.0017 - val_loss: 0.0013\n",
      "Epoch 21/24\n",
      "3622/3622 [==============================] - 2s 571us/step - loss: 0.0016 - val_loss: 0.0023\n",
      "Epoch 22/24\n",
      "3622/3622 [==============================] - 2s 565us/step - loss: 0.0016 - val_loss: 0.0018\n",
      "Epoch 23/24\n",
      "3622/3622 [==============================] - 2s 568us/step - loss: 0.0015 - val_loss: 0.0018\n",
      "Epoch 24/24\n",
      "3622/3622 [==============================] - 2s 570us/step - loss: 0.0015 - val_loss: 0.0016\n",
      "Train on 3622 samples, validate on 403 samples\n",
      "Epoch 1/24\n",
      "3622/3622 [==============================] - 36s 10ms/step - loss: 0.0765 - val_loss: 0.0273\n",
      "Epoch 2/24\n",
      "3622/3622 [==============================] - 2s 572us/step - loss: 0.0124 - val_loss: 0.0022\n",
      "Epoch 3/24\n",
      "3622/3622 [==============================] - 2s 577us/step - loss: 0.0045 - val_loss: 0.0049\n",
      "Epoch 4/24\n",
      "3622/3622 [==============================] - 2s 590us/step - loss: 0.0031 - val_loss: 0.0017\n",
      "Epoch 5/24\n",
      "3622/3622 [==============================] - 2s 564us/step - loss: 0.0025 - val_loss: 0.0014\n",
      "Epoch 6/24\n",
      "3622/3622 [==============================] - 2s 570us/step - loss: 0.0023 - val_loss: 0.0013\n",
      "Epoch 7/24\n",
      "3622/3622 [==============================] - 2s 564us/step - loss: 0.0021 - val_loss: 0.0013\n",
      "Epoch 8/24\n",
      "3622/3622 [==============================] - 2s 567us/step - loss: 0.0021 - val_loss: 0.0013\n",
      "Epoch 9/24\n",
      "3622/3622 [==============================] - 2s 568us/step - loss: 0.0020 - val_loss: 0.0012\n",
      "Epoch 10/24\n",
      "3622/3622 [==============================] - 2s 563us/step - loss: 0.0018 - val_loss: 0.0011\n",
      "Epoch 11/24\n",
      "3622/3622 [==============================] - 2s 569us/step - loss: 0.0019 - val_loss: 0.0012\n",
      "Epoch 12/24\n",
      "3622/3622 [==============================] - 2s 567us/step - loss: 0.0018 - val_loss: 0.0012\n",
      "Epoch 13/24\n",
      "3622/3622 [==============================] - 2s 575us/step - loss: 0.0017 - val_loss: 0.0011\n",
      "Epoch 14/24\n",
      "3622/3622 [==============================] - 2s 575us/step - loss: 0.0017 - val_loss: 0.0012\n",
      "Epoch 15/24\n",
      "3622/3622 [==============================] - 2s 585us/step - loss: 0.0018 - val_loss: 0.0015\n",
      "Epoch 16/24\n",
      "3622/3622 [==============================] - 2s 571us/step - loss: 0.0017 - val_loss: 0.0013\n",
      "Epoch 17/24\n",
      "3622/3622 [==============================] - 2s 562us/step - loss: 0.0016 - val_loss: 0.0012\n",
      "Epoch 18/24\n",
      "3622/3622 [==============================] - 2s 586us/step - loss: 0.0015 - val_loss: 0.0014\n",
      "Epoch 19/24\n",
      "3622/3622 [==============================] - 2s 576us/step - loss: 0.0015 - val_loss: 0.0010\n",
      "Epoch 20/24\n",
      "3622/3622 [==============================] - 2s 577us/step - loss: 0.0015 - val_loss: 0.0026\n",
      "Epoch 21/24\n",
      "3622/3622 [==============================] - 2s 565us/step - loss: 0.0018 - val_loss: 0.0014\n",
      "Epoch 22/24\n",
      "3622/3622 [==============================] - 2s 590us/step - loss: 0.0016 - val_loss: 0.0010\n",
      "Epoch 23/24\n",
      "3622/3622 [==============================] - 2s 568us/step - loss: 0.0015 - val_loss: 9.1498e-04\n",
      "Epoch 24/24\n",
      "3622/3622 [==============================] - 2s 566us/step - loss: 0.0015 - val_loss: 0.0015\n",
      "Train on 3622 samples, validate on 403 samples\n",
      "Epoch 1/24\n",
      "3622/3622 [==============================] - 36s 10ms/step - loss: 0.0671 - val_loss: 0.0206\n",
      "Epoch 2/24\n",
      "3622/3622 [==============================] - 2s 564us/step - loss: 0.0102 - val_loss: 0.0036\n",
      "Epoch 3/24\n",
      "3622/3622 [==============================] - 2s 575us/step - loss: 0.0054 - val_loss: 0.0056\n",
      "Epoch 4/24\n",
      "3622/3622 [==============================] - 2s 561us/step - loss: 0.0033 - val_loss: 0.0031\n",
      "Epoch 5/24\n",
      "3622/3622 [==============================] - 2s 562us/step - loss: 0.0026 - val_loss: 0.0012\n",
      "Epoch 6/24\n",
      "3622/3622 [==============================] - 2s 560us/step - loss: 0.0022 - val_loss: 0.0021\n",
      "Epoch 7/24\n",
      "3622/3622 [==============================] - 2s 562us/step - loss: 0.0021 - val_loss: 0.0017\n",
      "Epoch 8/24\n",
      "3622/3622 [==============================] - 2s 561us/step - loss: 0.0019 - val_loss: 0.0012\n",
      "Epoch 9/24\n",
      "3622/3622 [==============================] - 2s 562us/step - loss: 0.0018 - val_loss: 0.0017\n",
      "Epoch 10/24\n",
      "3622/3622 [==============================] - 2s 558us/step - loss: 0.0019 - val_loss: 0.0018\n",
      "Epoch 11/24\n",
      "3622/3622 [==============================] - 2s 566us/step - loss: 0.0016 - val_loss: 0.0019\n",
      "Epoch 12/24\n",
      "3622/3622 [==============================] - 2s 562us/step - loss: 0.0016 - val_loss: 0.0019\n",
      "Epoch 13/24\n",
      "3622/3622 [==============================] - 2s 565us/step - loss: 0.0016 - val_loss: 0.0028\n",
      "Epoch 14/24\n",
      "3622/3622 [==============================] - 2s 560us/step - loss: 0.0016 - val_loss: 0.0025\n",
      "Epoch 15/24\n",
      "3622/3622 [==============================] - 2s 565us/step - loss: 0.0015 - val_loss: 9.4925e-04\n",
      "Epoch 16/24\n",
      "3622/3622 [==============================] - 2s 563us/step - loss: 0.0016 - val_loss: 0.0012\n",
      "Epoch 17/24\n",
      "3622/3622 [==============================] - 2s 561us/step - loss: 0.0014 - val_loss: 0.0017\n",
      "Epoch 18/24\n",
      "3622/3622 [==============================] - 2s 566us/step - loss: 0.0013 - val_loss: 0.0029\n",
      "Epoch 19/24\n",
      "3622/3622 [==============================] - 2s 563us/step - loss: 0.0013 - val_loss: 0.0018\n",
      "Epoch 20/24\n",
      "3622/3622 [==============================] - 2s 565us/step - loss: 0.0013 - val_loss: 0.0012\n",
      "Epoch 21/24\n",
      "3622/3622 [==============================] - 2s 565us/step - loss: 0.0012 - val_loss: 0.0024\n",
      "Epoch 22/24\n",
      "3622/3622 [==============================] - 2s 563us/step - loss: 0.0012 - val_loss: 0.0033\n",
      "Epoch 23/24\n",
      "3622/3622 [==============================] - 2s 565us/step - loss: 0.0013 - val_loss: 0.0013\n",
      "Epoch 24/24\n",
      "3622/3622 [==============================] - 2s 564us/step - loss: 0.0012 - val_loss: 0.0030\n",
      "Train on 3622 samples, validate on 403 samples\n",
      "Epoch 1/24\n",
      "3622/3622 [==============================] - 36s 10ms/step - loss: 0.1113 - val_loss: 0.0240\n",
      "Epoch 2/24\n",
      "3622/3622 [==============================] - 2s 562us/step - loss: 0.0193 - val_loss: 0.0202\n",
      "Epoch 3/24\n",
      "3622/3622 [==============================] - 2s 562us/step - loss: 0.0074 - val_loss: 0.0018\n",
      "Epoch 4/24\n",
      "3622/3622 [==============================] - 2s 561us/step - loss: 0.0038 - val_loss: 0.0019\n",
      "Epoch 5/24\n",
      "3622/3622 [==============================] - 2s 561us/step - loss: 0.0030 - val_loss: 0.0046\n",
      "Epoch 6/24\n",
      "3622/3622 [==============================] - 2s 558us/step - loss: 0.0026 - val_loss: 0.0014\n",
      "Epoch 7/24\n",
      "3622/3622 [==============================] - 2s 558us/step - loss: 0.0022 - val_loss: 0.0014\n",
      "Epoch 8/24\n",
      "3622/3622 [==============================] - 2s 560us/step - loss: 0.0022 - val_loss: 0.0015\n",
      "Epoch 9/24\n",
      "3622/3622 [==============================] - 2s 563us/step - loss: 0.0019 - val_loss: 0.0014\n",
      "Epoch 10/24\n",
      "3622/3622 [==============================] - 2s 565us/step - loss: 0.0019 - val_loss: 0.0014\n",
      "Epoch 11/24\n",
      "3622/3622 [==============================] - 2s 561us/step - loss: 0.0018 - val_loss: 0.0012\n",
      "Epoch 12/24\n",
      "3622/3622 [==============================] - 2s 562us/step - loss: 0.0018 - val_loss: 0.0012\n",
      "Epoch 13/24\n",
      "3622/3622 [==============================] - 2s 565us/step - loss: 0.0017 - val_loss: 0.0012\n",
      "Epoch 14/24\n",
      "3622/3622 [==============================] - 2s 560us/step - loss: 0.0017 - val_loss: 0.0012\n",
      "Epoch 15/24\n",
      "3622/3622 [==============================] - 2s 562us/step - loss: 0.0017 - val_loss: 0.0012\n",
      "Epoch 16/24\n",
      "3622/3622 [==============================] - 2s 561us/step - loss: 0.0017 - val_loss: 0.0011\n",
      "Epoch 17/24\n",
      "3622/3622 [==============================] - 2s 562us/step - loss: 0.0016 - val_loss: 0.0012\n",
      "Epoch 18/24\n",
      "3622/3622 [==============================] - 2s 563us/step - loss: 0.0016 - val_loss: 0.0011\n",
      "Epoch 19/24\n",
      "3622/3622 [==============================] - 2s 561us/step - loss: 0.0016 - val_loss: 0.0016\n",
      "Epoch 20/24\n",
      "3622/3622 [==============================] - 2s 561us/step - loss: 0.0016 - val_loss: 0.0020\n",
      "Epoch 21/24\n",
      "3622/3622 [==============================] - 2s 558us/step - loss: 0.0016 - val_loss: 0.0014\n",
      "Epoch 22/24\n",
      "3622/3622 [==============================] - 2s 563us/step - loss: 0.0015 - val_loss: 0.0016\n",
      "Epoch 23/24\n",
      "3622/3622 [==============================] - 2s 563us/step - loss: 0.0014 - val_loss: 0.0013\n",
      "Epoch 24/24\n",
      "3622/3622 [==============================] - 2s 557us/step - loss: 0.0015 - val_loss: 0.0011\n",
      "Train on 3622 samples, validate on 403 samples\n",
      "Epoch 1/24\n",
      "3622/3622 [==============================] - 37s 10ms/step - loss: 0.1671 - val_loss: 0.1111\n",
      "Epoch 2/24\n",
      "3622/3622 [==============================] - 2s 519us/step - loss: 0.0322 - val_loss: 0.0153\n",
      "Epoch 3/24\n",
      "3622/3622 [==============================] - 2s 518us/step - loss: 0.0102 - val_loss: 0.0031\n",
      "Epoch 4/24\n",
      "3622/3622 [==============================] - 2s 523us/step - loss: 0.0049 - val_loss: 0.0025\n",
      "Epoch 5/24\n",
      "3622/3622 [==============================] - 2s 522us/step - loss: 0.0033 - val_loss: 0.0028\n",
      "Epoch 6/24\n",
      "3622/3622 [==============================] - 2s 522us/step - loss: 0.0028 - val_loss: 0.0018\n",
      "Epoch 7/24\n",
      "3622/3622 [==============================] - 2s 520us/step - loss: 0.0024 - val_loss: 0.0016\n",
      "Epoch 8/24\n",
      "3622/3622 [==============================] - 2s 524us/step - loss: 0.0021 - val_loss: 0.0015\n",
      "Epoch 9/24\n",
      "3622/3622 [==============================] - 2s 524us/step - loss: 0.0020 - val_loss: 0.0015\n",
      "Epoch 10/24\n",
      "3622/3622 [==============================] - 2s 519us/step - loss: 0.0019 - val_loss: 0.0013\n",
      "Epoch 11/24\n",
      "3622/3622 [==============================] - 2s 519us/step - loss: 0.0018 - val_loss: 0.0013\n",
      "Epoch 12/24\n",
      "3622/3622 [==============================] - 2s 519us/step - loss: 0.0018 - val_loss: 0.0013\n",
      "Epoch 13/24\n",
      "3622/3622 [==============================] - 2s 519us/step - loss: 0.0018 - val_loss: 0.0014\n",
      "Epoch 14/24\n",
      "3622/3622 [==============================] - 2s 521us/step - loss: 0.0017 - val_loss: 0.0017\n",
      "Epoch 15/24\n",
      "3622/3622 [==============================] - 2s 520us/step - loss: 0.0016 - val_loss: 0.0014\n",
      "Epoch 16/24\n",
      "3622/3622 [==============================] - 2s 521us/step - loss: 0.0017 - val_loss: 0.0014\n",
      "Epoch 17/24\n",
      "3622/3622 [==============================] - 2s 519us/step - loss: 0.0016 - val_loss: 0.0016\n",
      "Epoch 18/24\n",
      "3622/3622 [==============================] - 2s 525us/step - loss: 0.0015 - val_loss: 0.0011\n",
      "Epoch 19/24\n",
      "3622/3622 [==============================] - 2s 522us/step - loss: 0.0017 - val_loss: 0.0012\n",
      "Epoch 20/24\n",
      "3622/3622 [==============================] - 2s 519us/step - loss: 0.0016 - val_loss: 0.0013\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21/24\n",
      "3622/3622 [==============================] - 2s 523us/step - loss: 0.0016 - val_loss: 0.0017\n",
      "Epoch 22/24\n",
      "3622/3622 [==============================] - 2s 521us/step - loss: 0.0015 - val_loss: 0.0026\n",
      "Epoch 23/24\n",
      "3622/3622 [==============================] - 2s 523us/step - loss: 0.0015 - val_loss: 0.0020\n",
      "Epoch 24/24\n",
      "3622/3622 [==============================] - 2s 526us/step - loss: 0.0014 - val_loss: 0.0023\n",
      "Train on 3622 samples, validate on 403 samples\n",
      "Epoch 1/24\n",
      "3622/3622 [==============================] - 37s 10ms/step - loss: 0.1316 - val_loss: 0.0026\n",
      "Epoch 2/24\n",
      "3622/3622 [==============================] - 2s 522us/step - loss: 0.0174 - val_loss: 0.0189\n",
      "Epoch 3/24\n",
      "3622/3622 [==============================] - 2s 523us/step - loss: 0.0080 - val_loss: 0.0065\n",
      "Epoch 4/24\n",
      "3622/3622 [==============================] - 2s 526us/step - loss: 0.0041 - val_loss: 0.0017\n",
      "Epoch 5/24\n",
      "3622/3622 [==============================] - 2s 523us/step - loss: 0.0027 - val_loss: 0.0039\n",
      "Epoch 6/24\n",
      "3622/3622 [==============================] - 2s 522us/step - loss: 0.0025 - val_loss: 0.0017\n",
      "Epoch 7/24\n",
      "3622/3622 [==============================] - 2s 524us/step - loss: 0.0022 - val_loss: 0.0014\n",
      "Epoch 8/24\n",
      "3622/3622 [==============================] - 2s 522us/step - loss: 0.0020 - val_loss: 0.0014\n",
      "Epoch 9/24\n",
      "3622/3622 [==============================] - 2s 526us/step - loss: 0.0020 - val_loss: 0.0014\n",
      "Epoch 10/24\n",
      "3622/3622 [==============================] - 2s 524us/step - loss: 0.0018 - val_loss: 0.0015\n",
      "Epoch 11/24\n",
      "3622/3622 [==============================] - 2s 524us/step - loss: 0.0020 - val_loss: 0.0018\n",
      "Epoch 12/24\n",
      "3622/3622 [==============================] - 2s 522us/step - loss: 0.0019 - val_loss: 0.0028\n",
      "Epoch 13/24\n",
      "3622/3622 [==============================] - 2s 526us/step - loss: 0.0021 - val_loss: 0.0019\n",
      "Epoch 14/24\n",
      "3622/3622 [==============================] - 2s 525us/step - loss: 0.0017 - val_loss: 0.0017\n",
      "Epoch 15/24\n",
      "3622/3622 [==============================] - 2s 521us/step - loss: 0.0017 - val_loss: 0.0021\n",
      "Epoch 16/24\n",
      "3622/3622 [==============================] - 2s 526us/step - loss: 0.0016 - val_loss: 0.0018\n",
      "Epoch 17/24\n",
      "3622/3622 [==============================] - 2s 526us/step - loss: 0.0016 - val_loss: 0.0020\n",
      "Epoch 18/24\n",
      "3622/3622 [==============================] - 2s 523us/step - loss: 0.0017 - val_loss: 0.0032\n",
      "Epoch 19/24\n",
      "3622/3622 [==============================] - 2s 531us/step - loss: 0.0016 - val_loss: 0.0017\n",
      "Epoch 20/24\n",
      "3622/3622 [==============================] - 2s 526us/step - loss: 0.0017 - val_loss: 0.0013\n",
      "Epoch 21/24\n",
      "3622/3622 [==============================] - 2s 527us/step - loss: 0.0015 - val_loss: 0.0012\n",
      "Epoch 22/24\n",
      "3622/3622 [==============================] - 2s 522us/step - loss: 0.0016 - val_loss: 0.0014\n",
      "Epoch 23/24\n",
      "3622/3622 [==============================] - 2s 524us/step - loss: 0.0014 - val_loss: 0.0026\n",
      "Epoch 24/24\n",
      "3622/3622 [==============================] - 2s 523us/step - loss: 0.0014 - val_loss: 0.0046\n"
     ]
    }
   ],
   "source": [
    "best_models = breed_population(X_train, Y_train, generations=20, population_size=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.007785015739500523,\n",
       " 0.0008450688910670578,\n",
       " 0.002771921455860138,\n",
       " 0.0030496995896101,\n",
       " 0.0006214528111740947,\n",
       " 0.0006552666309289634,\n",
       " 0.0008440040401183069,\n",
       " 0.0006251477170735598,\n",
       " 0.000620624516159296,\n",
       " 0.0007459184271283448,\n",
       " 0.000617238343693316,\n",
       " 0.0006313800113275647,\n",
       " 0.0005857678479515016,\n",
       " 0.0005334086017683148,\n",
       " 0.0005036257789470255,\n",
       " 0.00046905167982913554,\n",
       " 0.000427537685027346,\n",
       " 0.0004048815171699971,\n",
       " 0.0003899291914422065,\n",
       " 0.0003846220497507602,\n",
       " 0.0004422336060088128,\n",
       " 0.00039258724427782,\n",
       " 0.00036199085297994316,\n",
       " 0.0003609123232308775]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_models[0][1].history['val_loss']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "optimal parameters\n",
      "_______________________\n",
      "lstmsize: 126\n",
      "density: 154\n",
      "optimizer: adam\n",
      "full_density: True\n",
      "activation: relu\n",
      "shuffle: True\n",
      "twice: False\n"
     ]
    }
   ],
   "source": [
    "print('optimal parameters')\n",
    "print('_______________________')\n",
    "for key,val in best_models[0][2].items():\n",
    "    if key != 'x' and key != 'y': print(f'{key}: {val}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_156\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_231 (LSTM)              (None, 126)               66528     \n",
      "_________________________________________________________________\n",
      "dense_783 (Dense)            (None, 154)               19558     \n",
      "_________________________________________________________________\n",
      "dense_784 (Dense)            (None, 77)                11935     \n",
      "_________________________________________________________________\n",
      "dense_785 (Dense)            (None, 38)                2964      \n",
      "_________________________________________________________________\n",
      "dense_786 (Dense)            (None, 19)                741       \n",
      "_________________________________________________________________\n",
      "dense_787 (Dense)            (None, 1)                 20        \n",
      "=================================================================\n",
      "Total params: 101,746\n",
      "Trainable params: 101,746\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "best_models[0][0].summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "ibm_checkpoint = ModelCheckpoint(\n",
    "    filepath='./checkpoints/IBM.{epoch}-{val_loss}.hdf5',\n",
    "    save_weights_only=True,\n",
    "    monitor='val_loss',\n",
    "    mode='min',\n",
    "    save_best_only=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = best_models[0][2]\n",
    "params['x'] = X_train\n",
    "params['y'] = Y_train\n",
    "params['epochs'] = 2000\n",
    "params['callbacks'] = [ibm_checkpoint]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 3622 samples, validate on 403 samples\n",
      "Epoch 1/2000\n",
      "3622/3622 [==============================] - 36s 10ms/step - loss: 0.1159 - val_loss: 0.0725\n",
      "Epoch 2/2000\n",
      "3622/3622 [==============================] - 1s 237us/step - loss: 0.0179 - val_loss: 0.0161\n",
      "Epoch 3/2000\n",
      "3622/3622 [==============================] - 1s 230us/step - loss: 0.0069 - val_loss: 0.0069\n",
      "Epoch 4/2000\n",
      "3622/3622 [==============================] - 1s 230us/step - loss: 0.0033 - val_loss: 0.0017\n",
      "Epoch 5/2000\n",
      "3622/3622 [==============================] - 1s 229us/step - loss: 0.0013 - val_loss: 0.0012\n",
      "Epoch 6/2000\n",
      "3622/3622 [==============================] - 1s 230us/step - loss: 8.4852e-04 - val_loss: 0.0011\n",
      "Epoch 7/2000\n",
      "3622/3622 [==============================] - 1s 229us/step - loss: 7.9021e-04 - val_loss: 6.9089e-04\n",
      "Epoch 8/2000\n",
      "3622/3622 [==============================] - 1s 229us/step - loss: 6.9736e-04 - val_loss: 7.4725e-04\n",
      "Epoch 9/2000\n",
      "3622/3622 [==============================] - 1s 230us/step - loss: 6.5826e-04 - val_loss: 8.2220e-04\n",
      "Epoch 10/2000\n",
      "3622/3622 [==============================] - 1s 229us/step - loss: 6.1262e-04 - val_loss: 7.0847e-04\n",
      "Epoch 11/2000\n",
      "3622/3622 [==============================] - 1s 232us/step - loss: 5.8184e-04 - val_loss: 6.5178e-04\n",
      "Epoch 12/2000\n",
      "3622/3622 [==============================] - 1s 230us/step - loss: 5.4511e-04 - val_loss: 6.4862e-04\n",
      "Epoch 13/2000\n",
      "3622/3622 [==============================] - 1s 229us/step - loss: 5.2292e-04 - val_loss: 6.1554e-04\n",
      "Epoch 14/2000\n",
      "3622/3622 [==============================] - 1s 230us/step - loss: 4.9580e-04 - val_loss: 5.0205e-04\n",
      "Epoch 15/2000\n",
      "3622/3622 [==============================] - 1s 231us/step - loss: 4.5144e-04 - val_loss: 5.3991e-04\n",
      "Epoch 16/2000\n",
      "3622/3622 [==============================] - 1s 230us/step - loss: 4.5179e-04 - val_loss: 5.5826e-04\n",
      "Epoch 17/2000\n",
      "3622/3622 [==============================] - 1s 230us/step - loss: 4.3283e-04 - val_loss: 5.4144e-04\n",
      "Epoch 18/2000\n",
      "3622/3622 [==============================] - 1s 230us/step - loss: 4.3949e-04 - val_loss: 4.8367e-04\n",
      "Epoch 19/2000\n",
      "3622/3622 [==============================] - 1s 230us/step - loss: 3.9702e-04 - val_loss: 4.3458e-04\n",
      "Epoch 20/2000\n",
      "3622/3622 [==============================] - 1s 235us/step - loss: 3.9312e-04 - val_loss: 4.0873e-04\n",
      "Epoch 21/2000\n",
      "3622/3622 [==============================] - 1s 230us/step - loss: 3.8690e-04 - val_loss: 4.2195e-04\n",
      "Epoch 22/2000\n",
      "3622/3622 [==============================] - 1s 229us/step - loss: 3.8498e-04 - val_loss: 3.9490e-04\n",
      "Epoch 23/2000\n",
      "3622/3622 [==============================] - 1s 228us/step - loss: 3.6239e-04 - val_loss: 4.3088e-04\n",
      "Epoch 24/2000\n",
      "3622/3622 [==============================] - 1s 228us/step - loss: 3.5335e-04 - val_loss: 3.8561e-04\n",
      "Epoch 25/2000\n",
      "3622/3622 [==============================] - 1s 228us/step - loss: 3.4800e-04 - val_loss: 3.7404e-04\n",
      "Epoch 26/2000\n",
      "3622/3622 [==============================] - 1s 227us/step - loss: 3.4306e-04 - val_loss: 3.6893e-04\n",
      "Epoch 27/2000\n",
      "3622/3622 [==============================] - 1s 230us/step - loss: 3.3079e-04 - val_loss: 3.8233e-04\n",
      "Epoch 28/2000\n",
      "3622/3622 [==============================] - 1s 229us/step - loss: 3.3738e-04 - val_loss: 4.5551e-04\n",
      "Epoch 29/2000\n",
      "3622/3622 [==============================] - 1s 230us/step - loss: 3.4398e-04 - val_loss: 3.6216e-04\n",
      "Epoch 30/2000\n",
      "3622/3622 [==============================] - 1s 234us/step - loss: 3.2473e-04 - val_loss: 6.6131e-04\n",
      "Epoch 31/2000\n",
      "3622/3622 [==============================] - 1s 228us/step - loss: 4.2360e-04 - val_loss: 4.0315e-04\n",
      "Epoch 32/2000\n",
      "3622/3622 [==============================] - 1s 229us/step - loss: 3.4874e-04 - val_loss: 3.5318e-04\n",
      "Epoch 33/2000\n",
      "3622/3622 [==============================] - 1s 229us/step - loss: 3.3575e-04 - val_loss: 4.9266e-04\n",
      "Epoch 34/2000\n",
      "3622/3622 [==============================] - 1s 230us/step - loss: 3.3108e-04 - val_loss: 3.4208e-04\n",
      "Epoch 35/2000\n",
      "3622/3622 [==============================] - 1s 229us/step - loss: 3.2251e-04 - val_loss: 3.6565e-04\n",
      "Epoch 36/2000\n",
      "3622/3622 [==============================] - 1s 229us/step - loss: 3.0914e-04 - val_loss: 3.3769e-04\n",
      "Epoch 37/2000\n",
      "3622/3622 [==============================] - 1s 230us/step - loss: 2.9653e-04 - val_loss: 3.3239e-04\n",
      "Epoch 38/2000\n",
      "3622/3622 [==============================] - 1s 234us/step - loss: 2.9582e-04 - val_loss: 3.3801e-04\n",
      "Epoch 39/2000\n",
      "3622/3622 [==============================] - 1s 229us/step - loss: 3.0431e-04 - val_loss: 3.2301e-04\n",
      "Epoch 40/2000\n",
      "3622/3622 [==============================] - 1s 231us/step - loss: 2.8526e-04 - val_loss: 5.8917e-04\n",
      "Epoch 41/2000\n",
      "3622/3622 [==============================] - 1s 229us/step - loss: 3.8837e-04 - val_loss: 4.9310e-04\n",
      "Epoch 42/2000\n",
      "3622/3622 [==============================] - 1s 227us/step - loss: 3.3836e-04 - val_loss: 3.7455e-04\n",
      "Epoch 43/2000\n",
      "3622/3622 [==============================] - 1s 231us/step - loss: 3.1709e-04 - val_loss: 3.1996e-04\n",
      "Epoch 44/2000\n",
      "3622/3622 [==============================] - 1s 229us/step - loss: 3.1753e-04 - val_loss: 5.1616e-04\n",
      "Epoch 45/2000\n",
      "3622/3622 [==============================] - 1s 229us/step - loss: 3.7384e-04 - val_loss: 3.8558e-04\n",
      "Epoch 46/2000\n",
      "3622/3622 [==============================] - 1s 229us/step - loss: 2.9359e-04 - val_loss: 3.2049e-04\n",
      "Epoch 47/2000\n",
      "3622/3622 [==============================] - 1s 230us/step - loss: 2.9626e-04 - val_loss: 3.0805e-04\n",
      "Epoch 48/2000\n",
      "3622/3622 [==============================] - 1s 231us/step - loss: 2.8213e-04 - val_loss: 3.5197e-04\n",
      "Epoch 49/2000\n",
      "3622/3622 [==============================] - 1s 234us/step - loss: 2.9976e-04 - val_loss: 3.1449e-04\n",
      "Epoch 50/2000\n",
      "3622/3622 [==============================] - 1s 229us/step - loss: 2.9929e-04 - val_loss: 3.2223e-04\n",
      "Epoch 51/2000\n",
      "3622/3622 [==============================] - 1s 228us/step - loss: 2.9428e-04 - val_loss: 4.4521e-04\n",
      "Epoch 52/2000\n",
      "3622/3622 [==============================] - 1s 230us/step - loss: 2.8343e-04 - val_loss: 3.4104e-04\n",
      "Epoch 53/2000\n",
      "3622/3622 [==============================] - 1s 230us/step - loss: 2.8322e-04 - val_loss: 6.2216e-04\n",
      "Epoch 54/2000\n",
      "3622/3622 [==============================] - 1s 228us/step - loss: 3.6839e-04 - val_loss: 3.1909e-04\n",
      "Epoch 55/2000\n",
      "3622/3622 [==============================] - 1s 233us/step - loss: 3.0743e-04 - val_loss: 5.6555e-04\n",
      "Epoch 56/2000\n",
      "3622/3622 [==============================] - 1s 233us/step - loss: 3.5732e-04 - val_loss: 3.7680e-04\n",
      "Epoch 57/2000\n",
      "3622/3622 [==============================] - 1s 229us/step - loss: 3.3170e-04 - val_loss: 3.2646e-04\n",
      "Epoch 58/2000\n",
      "3622/3622 [==============================] - 1s 230us/step - loss: 2.9620e-04 - val_loss: 5.0389e-04\n",
      "Epoch 59/2000\n",
      "3622/3622 [==============================] - 1s 229us/step - loss: 3.0565e-04 - val_loss: 3.0361e-04\n",
      "Epoch 60/2000\n",
      "3622/3622 [==============================] - 1s 230us/step - loss: 2.7976e-04 - val_loss: 3.8332e-04\n",
      "Epoch 61/2000\n",
      "3622/3622 [==============================] - 1s 230us/step - loss: 2.7037e-04 - val_loss: 3.3611e-04\n",
      "Epoch 62/2000\n",
      "3622/3622 [==============================] - 1s 230us/step - loss: 2.8849e-04 - val_loss: 3.6039e-04\n",
      "Epoch 63/2000\n",
      "3622/3622 [==============================] - 1s 229us/step - loss: 2.8686e-04 - val_loss: 0.0014\n",
      "Epoch 64/2000\n",
      "3622/3622 [==============================] - 1s 230us/step - loss: 5.3679e-04 - val_loss: 5.8826e-04\n",
      "Epoch 65/2000\n",
      "3622/3622 [==============================] - 1s 230us/step - loss: 4.1886e-04 - val_loss: 4.0141e-04\n",
      "Epoch 66/2000\n",
      "3622/3622 [==============================] - 1s 229us/step - loss: 3.8012e-04 - val_loss: 4.6354e-04\n",
      "Epoch 67/2000\n",
      "3622/3622 [==============================] - 1s 231us/step - loss: 3.1513e-04 - val_loss: 3.2182e-04\n",
      "Epoch 68/2000\n",
      "3622/3622 [==============================] - 1s 230us/step - loss: 2.7661e-04 - val_loss: 3.0228e-04\n",
      "Epoch 69/2000\n",
      "3622/3622 [==============================] - 1s 230us/step - loss: 2.5892e-04 - val_loss: 3.2725e-04\n",
      "Epoch 70/2000\n",
      "3622/3622 [==============================] - 1s 231us/step - loss: 2.7608e-04 - val_loss: 4.4247e-04\n",
      "Epoch 71/2000\n",
      "3622/3622 [==============================] - 1s 231us/step - loss: 2.8437e-04 - val_loss: 2.9619e-04\n",
      "Epoch 72/2000\n",
      "3622/3622 [==============================] - 1s 229us/step - loss: 2.8385e-04 - val_loss: 3.0527e-04\n",
      "Epoch 73/2000\n",
      "3622/3622 [==============================] - 1s 230us/step - loss: 2.6567e-04 - val_loss: 4.7431e-04\n",
      "Epoch 74/2000\n",
      "3622/3622 [==============================] - 1s 228us/step - loss: 3.3757e-04 - val_loss: 3.2201e-04\n",
      "Epoch 75/2000\n",
      "3622/3622 [==============================] - 1s 229us/step - loss: 2.9722e-04 - val_loss: 2.9027e-04\n",
      "Epoch 76/2000\n",
      "3622/3622 [==============================] - 1s 229us/step - loss: 2.7287e-04 - val_loss: 6.1064e-04\n",
      "Epoch 77/2000\n",
      "3622/3622 [==============================] - 1s 230us/step - loss: 3.0428e-04 - val_loss: 3.4894e-04\n",
      "Epoch 78/2000\n",
      "3622/3622 [==============================] - 1s 230us/step - loss: 2.9299e-04 - val_loss: 2.9120e-04\n",
      "Epoch 79/2000\n",
      "3622/3622 [==============================] - 1s 230us/step - loss: 2.7356e-04 - val_loss: 4.2068e-04\n",
      "Epoch 80/2000\n",
      "3622/3622 [==============================] - 1s 231us/step - loss: 2.7018e-04 - val_loss: 3.1434e-04\n",
      "Epoch 81/2000\n",
      "3622/3622 [==============================] - 1s 229us/step - loss: 2.5277e-04 - val_loss: 3.6028e-04\n",
      "Epoch 82/2000\n",
      "3622/3622 [==============================] - 1s 230us/step - loss: 2.5885e-04 - val_loss: 2.8340e-04\n",
      "Epoch 83/2000\n",
      "3622/3622 [==============================] - 1s 229us/step - loss: 2.4189e-04 - val_loss: 3.0441e-04\n",
      "Epoch 84/2000\n",
      "3622/3622 [==============================] - 1s 229us/step - loss: 2.5856e-04 - val_loss: 2.9433e-04\n",
      "Epoch 85/2000\n",
      "3622/3622 [==============================] - 1s 231us/step - loss: 2.5435e-04 - val_loss: 3.2824e-04\n",
      "Epoch 86/2000\n",
      "3622/3622 [==============================] - 1s 231us/step - loss: 2.4652e-04 - val_loss: 2.7867e-04\n",
      "Epoch 87/2000\n",
      "3622/3622 [==============================] - 1s 231us/step - loss: 2.3815e-04 - val_loss: 2.9828e-04\n",
      "Epoch 88/2000\n",
      "3622/3622 [==============================] - 1s 230us/step - loss: 2.5431e-04 - val_loss: 2.8018e-04\n",
      "Epoch 89/2000\n",
      "3622/3622 [==============================] - 1s 229us/step - loss: 2.5202e-04 - val_loss: 3.4463e-04\n",
      "Epoch 90/2000\n",
      "3622/3622 [==============================] - 1s 232us/step - loss: 2.4512e-04 - val_loss: 5.0234e-04\n",
      "Epoch 91/2000\n",
      "3622/3622 [==============================] - 1s 241us/step - loss: 2.9099e-04 - val_loss: 3.5922e-04\n",
      "Epoch 92/2000\n",
      "3622/3622 [==============================] - 1s 235us/step - loss: 2.4481e-04 - val_loss: 3.1440e-04\n",
      "Epoch 93/2000\n",
      "3622/3622 [==============================] - 1s 228us/step - loss: 2.4235e-04 - val_loss: 2.7786e-04\n",
      "Epoch 94/2000\n",
      "3622/3622 [==============================] - 1s 229us/step - loss: 2.3614e-04 - val_loss: 2.9745e-04\n",
      "Epoch 95/2000\n",
      "3622/3622 [==============================] - 1s 228us/step - loss: 2.3768e-04 - val_loss: 2.7938e-04\n",
      "Epoch 96/2000\n",
      "3622/3622 [==============================] - 1s 229us/step - loss: 2.3970e-04 - val_loss: 3.0130e-04\n",
      "Epoch 97/2000\n",
      "3622/3622 [==============================] - 1s 230us/step - loss: 2.3601e-04 - val_loss: 2.6765e-04\n",
      "Epoch 98/2000\n",
      "3622/3622 [==============================] - 1s 228us/step - loss: 2.3461e-04 - val_loss: 2.7623e-04\n",
      "Epoch 99/2000\n",
      "3622/3622 [==============================] - 1s 230us/step - loss: 2.3020e-04 - val_loss: 2.8478e-04\n",
      "Epoch 100/2000\n",
      "3622/3622 [==============================] - 1s 228us/step - loss: 2.3316e-04 - val_loss: 3.5639e-04\n",
      "Epoch 101/2000\n",
      "3622/3622 [==============================] - 1s 230us/step - loss: 2.4454e-04 - val_loss: 3.7518e-04\n",
      "Epoch 102/2000\n",
      "3622/3622 [==============================] - 1s 231us/step - loss: 2.5366e-04 - val_loss: 4.6145e-04\n",
      "Epoch 103/2000\n",
      "3622/3622 [==============================] - 1s 229us/step - loss: 2.8839e-04 - val_loss: 2.6801e-04\n",
      "Epoch 104/2000\n",
      "3622/3622 [==============================] - 1s 228us/step - loss: 2.4929e-04 - val_loss: 5.8315e-04\n",
      "Epoch 105/2000\n",
      "3622/3622 [==============================] - 1s 234us/step - loss: 2.8048e-04 - val_loss: 4.2589e-04\n",
      "Epoch 106/2000\n",
      "3622/3622 [==============================] - 1s 230us/step - loss: 2.4532e-04 - val_loss: 2.9385e-04\n",
      "Epoch 107/2000\n",
      "3622/3622 [==============================] - 1s 229us/step - loss: 2.2757e-04 - val_loss: 2.7577e-04\n",
      "Epoch 108/2000\n",
      "3622/3622 [==============================] - 1s 229us/step - loss: 2.2273e-04 - val_loss: 2.6717e-04\n",
      "Epoch 109/2000\n",
      "3622/3622 [==============================] - 1s 227us/step - loss: 2.2379e-04 - val_loss: 2.6932e-04\n",
      "Epoch 110/2000\n",
      "3622/3622 [==============================] - 1s 230us/step - loss: 2.2875e-04 - val_loss: 2.6373e-04\n",
      "Epoch 111/2000\n",
      "3622/3622 [==============================] - 1s 229us/step - loss: 2.3493e-04 - val_loss: 2.6636e-04\n",
      "Epoch 112/2000\n",
      "3622/3622 [==============================] - 1s 228us/step - loss: 2.2454e-04 - val_loss: 2.9205e-04\n",
      "Epoch 113/2000\n",
      "3622/3622 [==============================] - 1s 229us/step - loss: 2.3525e-04 - val_loss: 2.7363e-04\n",
      "Epoch 114/2000\n",
      "3622/3622 [==============================] - 1s 231us/step - loss: 2.4189e-04 - val_loss: 2.6750e-04\n",
      "Epoch 115/2000\n",
      "3622/3622 [==============================] - 1s 230us/step - loss: 2.3236e-04 - val_loss: 2.7486e-04\n",
      "Epoch 116/2000\n",
      "3622/3622 [==============================] - 1s 229us/step - loss: 2.3779e-04 - val_loss: 2.6317e-04\n",
      "Epoch 117/2000\n",
      "3622/3622 [==============================] - 1s 230us/step - loss: 2.5269e-04 - val_loss: 2.7497e-04\n",
      "Epoch 118/2000\n",
      "3622/3622 [==============================] - 1s 230us/step - loss: 2.2247e-04 - val_loss: 2.5960e-04\n",
      "Epoch 119/2000\n",
      "3622/3622 [==============================] - 1s 228us/step - loss: 2.2401e-04 - val_loss: 3.3300e-04\n",
      "Epoch 120/2000\n",
      "3622/3622 [==============================] - 1s 230us/step - loss: 2.2613e-04 - val_loss: 2.7429e-04\n",
      "Epoch 121/2000\n",
      "3622/3622 [==============================] - 1s 228us/step - loss: 2.1559e-04 - val_loss: 3.3563e-04\n",
      "Epoch 122/2000\n",
      "3622/3622 [==============================] - 1s 230us/step - loss: 2.4045e-04 - val_loss: 2.6397e-04\n",
      "Epoch 123/2000\n",
      "3622/3622 [==============================] - 1s 229us/step - loss: 2.3972e-04 - val_loss: 2.7925e-04\n",
      "Epoch 124/2000\n",
      "3622/3622 [==============================] - 1s 235us/step - loss: 2.4537e-04 - val_loss: 4.2054e-04\n",
      "Epoch 125/2000\n",
      "3622/3622 [==============================] - 1s 232us/step - loss: 2.4788e-04 - val_loss: 4.1989e-04\n",
      "Epoch 126/2000\n",
      "3622/3622 [==============================] - 1s 233us/step - loss: 2.6956e-04 - val_loss: 3.3682e-04\n",
      "Epoch 127/2000\n",
      "3622/3622 [==============================] - 1s 232us/step - loss: 2.5619e-04 - val_loss: 3.1994e-04\n",
      "Epoch 128/2000\n",
      "3622/3622 [==============================] - 1s 229us/step - loss: 2.4310e-04 - val_loss: 2.5692e-04\n",
      "Epoch 129/2000\n",
      "3622/3622 [==============================] - 1s 227us/step - loss: 2.1320e-04 - val_loss: 2.5508e-04\n",
      "Epoch 130/2000\n",
      "3622/3622 [==============================] - 1s 230us/step - loss: 2.1094e-04 - val_loss: 4.9651e-04\n",
      "Epoch 131/2000\n",
      "3622/3622 [==============================] - 1s 228us/step - loss: 2.6568e-04 - val_loss: 3.8285e-04\n",
      "Epoch 132/2000\n",
      "3622/3622 [==============================] - 1s 228us/step - loss: 2.4222e-04 - val_loss: 2.6605e-04\n",
      "Epoch 133/2000\n",
      "3622/3622 [==============================] - 1s 228us/step - loss: 2.2128e-04 - val_loss: 2.6965e-04\n",
      "Epoch 134/2000\n",
      "3622/3622 [==============================] - 1s 231us/step - loss: 2.1782e-04 - val_loss: 2.5360e-04\n",
      "Epoch 135/2000\n",
      "3622/3622 [==============================] - 1s 230us/step - loss: 2.1891e-04 - val_loss: 3.5946e-04\n",
      "Epoch 136/2000\n",
      "3622/3622 [==============================] - 1s 228us/step - loss: 2.4537e-04 - val_loss: 3.5980e-04\n",
      "Epoch 137/2000\n",
      "3622/3622 [==============================] - 1s 228us/step - loss: 2.3325e-04 - val_loss: 2.5746e-04\n",
      "Epoch 138/2000\n",
      "3622/3622 [==============================] - 1s 231us/step - loss: 2.4880e-04 - val_loss: 3.6481e-04\n",
      "Epoch 139/2000\n",
      "3622/3622 [==============================] - 1s 230us/step - loss: 2.5826e-04 - val_loss: 2.5716e-04\n",
      "Epoch 140/2000\n",
      "3622/3622 [==============================] - 1s 230us/step - loss: 2.1942e-04 - val_loss: 2.8301e-04\n",
      "Epoch 141/2000\n",
      "3622/3622 [==============================] - 1s 229us/step - loss: 2.1339e-04 - val_loss: 2.5413e-04\n",
      "Epoch 142/2000\n",
      "3622/3622 [==============================] - 1s 230us/step - loss: 2.1760e-04 - val_loss: 2.8200e-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 143/2000\n",
      "3622/3622 [==============================] - 1s 235us/step - loss: 2.1871e-04 - val_loss: 2.5930e-04\n",
      "Epoch 144/2000\n",
      "3622/3622 [==============================] - 1s 231us/step - loss: 2.1505e-04 - val_loss: 2.6815e-04\n",
      "Epoch 145/2000\n",
      "3622/3622 [==============================] - 1s 228us/step - loss: 2.1192e-04 - val_loss: 3.7447e-04\n",
      "Epoch 146/2000\n",
      "3622/3622 [==============================] - 1s 229us/step - loss: 2.2546e-04 - val_loss: 2.5513e-04\n",
      "Epoch 147/2000\n",
      "3622/3622 [==============================] - 1s 230us/step - loss: 2.3781e-04 - val_loss: 2.5517e-04\n",
      "Epoch 148/2000\n",
      "3622/3622 [==============================] - 1s 230us/step - loss: 2.1664e-04 - val_loss: 2.5154e-04\n",
      "Epoch 149/2000\n",
      "3622/3622 [==============================] - 1s 231us/step - loss: 2.3873e-04 - val_loss: 2.5409e-04\n",
      "Epoch 150/2000\n",
      "3622/3622 [==============================] - 1s 229us/step - loss: 2.1776e-04 - val_loss: 2.6148e-04\n",
      "Epoch 151/2000\n",
      "3622/3622 [==============================] - 1s 229us/step - loss: 2.1474e-04 - val_loss: 4.1546e-04\n",
      "Epoch 152/2000\n",
      "3622/3622 [==============================] - 1s 229us/step - loss: 2.5281e-04 - val_loss: 2.5108e-04\n",
      "Epoch 153/2000\n",
      "3622/3622 [==============================] - 1s 230us/step - loss: 2.1961e-04 - val_loss: 2.9057e-04\n",
      "Epoch 154/2000\n",
      "3622/3622 [==============================] - 1s 229us/step - loss: 2.2613e-04 - val_loss: 2.4669e-04\n",
      "Epoch 155/2000\n",
      "3622/3622 [==============================] - 1s 230us/step - loss: 2.0125e-04 - val_loss: 2.4602e-04\n",
      "Epoch 156/2000\n",
      "3622/3622 [==============================] - 1s 232us/step - loss: 2.0660e-04 - val_loss: 2.4829e-04\n",
      "Epoch 157/2000\n",
      "3622/3622 [==============================] - 1s 232us/step - loss: 2.0465e-04 - val_loss: 2.9008e-04\n",
      "Epoch 158/2000\n",
      "3622/3622 [==============================] - 1s 230us/step - loss: 2.0499e-04 - val_loss: 3.3388e-04\n",
      "Epoch 159/2000\n",
      "3622/3622 [==============================] - 1s 229us/step - loss: 2.3113e-04 - val_loss: 3.1345e-04\n",
      "Epoch 160/2000\n",
      "3622/3622 [==============================] - 1s 228us/step - loss: 2.4566e-04 - val_loss: 3.3195e-04\n",
      "Epoch 161/2000\n",
      "3622/3622 [==============================] - 1s 230us/step - loss: 2.6391e-04 - val_loss: 2.9666e-04\n",
      "Epoch 162/2000\n",
      "3622/3622 [==============================] - 1s 243us/step - loss: 2.4443e-04 - val_loss: 4.2713e-04\n",
      "Epoch 163/2000\n",
      "3622/3622 [==============================] - 1s 230us/step - loss: 2.8354e-04 - val_loss: 3.0101e-04\n",
      "Epoch 164/2000\n",
      "3622/3622 [==============================] - 1s 229us/step - loss: 2.3631e-04 - val_loss: 2.8951e-04\n",
      "Epoch 165/2000\n",
      "3622/3622 [==============================] - 1s 230us/step - loss: 2.1390e-04 - val_loss: 2.7036e-04\n",
      "Epoch 166/2000\n",
      "3622/3622 [==============================] - 1s 229us/step - loss: 2.0543e-04 - val_loss: 2.4547e-04\n",
      "Epoch 167/2000\n",
      "3622/3622 [==============================] - 1s 229us/step - loss: 2.0321e-04 - val_loss: 2.4756e-04\n",
      "Epoch 168/2000\n",
      "3622/3622 [==============================] - 1s 230us/step - loss: 2.0932e-04 - val_loss: 2.4202e-04\n",
      "Epoch 169/2000\n",
      "3622/3622 [==============================] - 1s 230us/step - loss: 1.9924e-04 - val_loss: 2.4170e-04\n",
      "Epoch 170/2000\n",
      "3622/3622 [==============================] - 1s 229us/step - loss: 2.0541e-04 - val_loss: 2.7068e-04\n",
      "Epoch 171/2000\n",
      "3622/3622 [==============================] - 1s 229us/step - loss: 2.1685e-04 - val_loss: 2.5000e-04\n",
      "Epoch 172/2000\n",
      "3622/3622 [==============================] - 1s 229us/step - loss: 2.0575e-04 - val_loss: 2.7825e-04\n",
      "Epoch 173/2000\n",
      "3622/3622 [==============================] - 1s 230us/step - loss: 2.1546e-04 - val_loss: 2.9178e-04\n",
      "Epoch 174/2000\n",
      "3622/3622 [==============================] - 1s 229us/step - loss: 2.3827e-04 - val_loss: 6.4356e-04\n",
      "Epoch 175/2000\n",
      "3622/3622 [==============================] - 1s 230us/step - loss: 3.3687e-04 - val_loss: 4.4177e-04\n",
      "Epoch 176/2000\n",
      "3622/3622 [==============================] - 1s 230us/step - loss: 2.8461e-04 - val_loss: 3.8054e-04\n",
      "Epoch 177/2000\n",
      "3622/3622 [==============================] - 1s 231us/step - loss: 2.4641e-04 - val_loss: 2.6730e-04\n",
      "Epoch 178/2000\n",
      "3622/3622 [==============================] - 1s 230us/step - loss: 2.1988e-04 - val_loss: 4.0295e-04\n",
      "Epoch 179/2000\n",
      "3622/3622 [==============================] - 1s 231us/step - loss: 2.3396e-04 - val_loss: 2.4286e-04\n",
      "Epoch 180/2000\n",
      "3622/3622 [==============================] - 1s 234us/step - loss: 2.0003e-04 - val_loss: 2.4162e-04\n",
      "Epoch 181/2000\n",
      "3622/3622 [==============================] - 1s 234us/step - loss: 2.0793e-04 - val_loss: 3.3738e-04\n",
      "Epoch 182/2000\n",
      "3622/3622 [==============================] - 1s 230us/step - loss: 2.3482e-04 - val_loss: 7.5680e-04\n",
      "Epoch 183/2000\n",
      "3622/3622 [==============================] - 1s 229us/step - loss: 3.8691e-04 - val_loss: 4.8034e-04\n",
      "Epoch 184/2000\n",
      "3622/3622 [==============================] - 1s 229us/step - loss: 2.9727e-04 - val_loss: 3.8073e-04\n",
      "Epoch 185/2000\n",
      "3622/3622 [==============================] - 1s 231us/step - loss: 2.4684e-04 - val_loss: 3.6635e-04\n",
      "Epoch 186/2000\n",
      "3622/3622 [==============================] - 1s 230us/step - loss: 2.2129e-04 - val_loss: 2.4359e-04\n",
      "Epoch 187/2000\n",
      "3622/3622 [==============================] - 1s 230us/step - loss: 2.1025e-04 - val_loss: 2.8632e-04\n",
      "Epoch 188/2000\n",
      "3622/3622 [==============================] - 1s 229us/step - loss: 2.1376e-04 - val_loss: 2.6892e-04\n",
      "Epoch 189/2000\n",
      "3622/3622 [==============================] - 1s 228us/step - loss: 2.0790e-04 - val_loss: 2.6721e-04\n",
      "Epoch 190/2000\n",
      "3622/3622 [==============================] - 1s 233us/step - loss: 1.9943e-04 - val_loss: 2.4139e-04\n",
      "Epoch 191/2000\n",
      "3622/3622 [==============================] - 1s 228us/step - loss: 1.9487e-04 - val_loss: 2.6124e-04\n",
      "Epoch 192/2000\n",
      "3622/3622 [==============================] - 1s 229us/step - loss: 1.9549e-04 - val_loss: 2.5357e-04\n",
      "Epoch 193/2000\n",
      "3622/3622 [==============================] - 1s 230us/step - loss: 2.0446e-04 - val_loss: 2.4373e-04\n",
      "Epoch 194/2000\n",
      "3622/3622 [==============================] - 1s 229us/step - loss: 1.9870e-04 - val_loss: 2.9210e-04\n",
      "Epoch 195/2000\n",
      "3622/3622 [==============================] - 1s 229us/step - loss: 2.1036e-04 - val_loss: 2.6192e-04\n",
      "Epoch 196/2000\n",
      "3622/3622 [==============================] - 1s 230us/step - loss: 2.3048e-04 - val_loss: 3.1824e-04\n",
      "Epoch 197/2000\n",
      "3622/3622 [==============================] - 1s 231us/step - loss: 2.4584e-04 - val_loss: 2.5322e-04\n",
      "Epoch 198/2000\n",
      "3622/3622 [==============================] - 1s 233us/step - loss: 2.1479e-04 - val_loss: 2.4888e-04\n",
      "Epoch 199/2000\n",
      "3622/3622 [==============================] - 1s 230us/step - loss: 2.0775e-04 - val_loss: 2.3970e-04\n",
      "Epoch 200/2000\n",
      "3622/3622 [==============================] - 1s 235us/step - loss: 1.9146e-04 - val_loss: 3.9508e-04\n",
      "Epoch 201/2000\n",
      "3622/3622 [==============================] - 1s 229us/step - loss: 2.3037e-04 - val_loss: 2.3705e-04\n",
      "Epoch 202/2000\n",
      "3622/3622 [==============================] - 1s 230us/step - loss: 1.9912e-04 - val_loss: 2.3890e-04\n",
      "Epoch 203/2000\n",
      "3622/3622 [==============================] - 1s 229us/step - loss: 1.9726e-04 - val_loss: 2.8839e-04\n",
      "Epoch 204/2000\n",
      "3622/3622 [==============================] - 1s 230us/step - loss: 2.0736e-04 - val_loss: 5.1574e-04\n",
      "Epoch 205/2000\n",
      "3622/3622 [==============================] - 1s 229us/step - loss: 2.7359e-04 - val_loss: 6.2837e-04\n",
      "Epoch 206/2000\n",
      "3622/3622 [==============================] - 1s 229us/step - loss: 2.7553e-04 - val_loss: 2.5037e-04\n",
      "Epoch 207/2000\n",
      "3622/3622 [==============================] - 1s 229us/step - loss: 2.3031e-04 - val_loss: 2.3999e-04\n",
      "Epoch 208/2000\n",
      "3622/3622 [==============================] - 1s 230us/step - loss: 1.9808e-04 - val_loss: 2.5402e-04\n",
      "Epoch 209/2000\n",
      "3622/3622 [==============================] - 1s 230us/step - loss: 1.8995e-04 - val_loss: 2.3985e-04\n",
      "Epoch 210/2000\n",
      "3622/3622 [==============================] - 1s 229us/step - loss: 1.9120e-04 - val_loss: 2.4241e-04\n",
      "Epoch 211/2000\n",
      "3622/3622 [==============================] - 1s 229us/step - loss: 1.9386e-04 - val_loss: 3.2574e-04\n",
      "Epoch 212/2000\n",
      "3622/3622 [==============================] - 1s 230us/step - loss: 2.1041e-04 - val_loss: 2.6995e-04\n",
      "Epoch 213/2000\n",
      "3622/3622 [==============================] - 1s 229us/step - loss: 1.9754e-04 - val_loss: 2.4980e-04\n",
      "Epoch 214/2000\n",
      "3622/3622 [==============================] - 1s 230us/step - loss: 1.8965e-04 - val_loss: 2.3391e-04\n",
      "Epoch 215/2000\n",
      "3622/3622 [==============================] - 1s 229us/step - loss: 1.8757e-04 - val_loss: 2.7259e-04\n",
      "Epoch 216/2000\n",
      "3622/3622 [==============================] - 1s 229us/step - loss: 1.9398e-04 - val_loss: 2.3293e-04\n",
      "Epoch 217/2000\n",
      "3622/3622 [==============================] - 1s 228us/step - loss: 1.8660e-04 - val_loss: 2.5042e-04\n",
      "Epoch 218/2000\n",
      "3622/3622 [==============================] - 1s 230us/step - loss: 1.8981e-04 - val_loss: 2.4347e-04\n",
      "Epoch 219/2000\n",
      "3622/3622 [==============================] - 1s 235us/step - loss: 1.9084e-04 - val_loss: 2.3496e-04\n",
      "Epoch 220/2000\n",
      "3622/3622 [==============================] - 1s 229us/step - loss: 1.9036e-04 - val_loss: 2.3293e-04\n",
      "Epoch 221/2000\n",
      "3622/3622 [==============================] - 1s 228us/step - loss: 1.8334e-04 - val_loss: 2.3357e-04\n",
      "Epoch 222/2000\n",
      "3622/3622 [==============================] - 1s 230us/step - loss: 1.9176e-04 - val_loss: 3.0032e-04\n",
      "Epoch 223/2000\n",
      "3622/3622 [==============================] - 1s 227us/step - loss: 1.9700e-04 - val_loss: 2.8091e-04\n",
      "Epoch 224/2000\n",
      "3622/3622 [==============================] - 1s 227us/step - loss: 2.1795e-04 - val_loss: 2.4925e-04\n",
      "Epoch 225/2000\n",
      "3622/3622 [==============================] - 1s 230us/step - loss: 2.0508e-04 - val_loss: 2.3255e-04\n",
      "Epoch 226/2000\n",
      "3622/3622 [==============================] - 1s 227us/step - loss: 1.9475e-04 - val_loss: 3.0880e-04\n",
      "Epoch 227/2000\n",
      "3622/3622 [==============================] - 1s 229us/step - loss: 2.0860e-04 - val_loss: 2.4761e-04\n",
      "Epoch 228/2000\n",
      "3622/3622 [==============================] - 1s 227us/step - loss: 1.8569e-04 - val_loss: 2.3512e-04\n",
      "Epoch 229/2000\n",
      "3622/3622 [==============================] - 1s 230us/step - loss: 1.8694e-04 - val_loss: 2.6365e-04\n",
      "Epoch 230/2000\n",
      "3622/3622 [==============================] - 1s 229us/step - loss: 1.8691e-04 - val_loss: 2.8444e-04\n",
      "Epoch 231/2000\n",
      "3622/3622 [==============================] - 1s 228us/step - loss: 2.0662e-04 - val_loss: 3.0054e-04\n",
      "Epoch 232/2000\n",
      "3622/3622 [==============================] - 1s 230us/step - loss: 2.1812e-04 - val_loss: 2.5019e-04\n",
      "Epoch 233/2000\n",
      "3622/3622 [==============================] - 1s 236us/step - loss: 2.0363e-04 - val_loss: 2.3082e-04\n",
      "Epoch 234/2000\n",
      "3622/3622 [==============================] - 1s 230us/step - loss: 1.8578e-04 - val_loss: 2.6368e-04\n",
      "Epoch 235/2000\n",
      "3622/3622 [==============================] - 1s 228us/step - loss: 1.9933e-04 - val_loss: 2.3509e-04\n",
      "Epoch 236/2000\n",
      "3622/3622 [==============================] - 1s 230us/step - loss: 1.8520e-04 - val_loss: 2.2970e-04\n",
      "Epoch 237/2000\n",
      "3622/3622 [==============================] - 1s 229us/step - loss: 1.8326e-04 - val_loss: 2.6208e-04\n",
      "Epoch 238/2000\n",
      "3622/3622 [==============================] - 1s 233us/step - loss: 1.9935e-04 - val_loss: 2.8078e-04\n",
      "Epoch 239/2000\n",
      "3622/3622 [==============================] - 1s 228us/step - loss: 1.9778e-04 - val_loss: 2.3735e-04\n",
      "Epoch 240/2000\n",
      "3622/3622 [==============================] - 1s 229us/step - loss: 1.8413e-04 - val_loss: 3.0463e-04\n",
      "Epoch 241/2000\n",
      "3622/3622 [==============================] - 1s 228us/step - loss: 1.9633e-04 - val_loss: 3.6549e-04\n",
      "Epoch 242/2000\n",
      "3622/3622 [==============================] - 1s 230us/step - loss: 2.3459e-04 - val_loss: 2.9489e-04\n",
      "Epoch 243/2000\n",
      "3622/3622 [==============================] - 1s 230us/step - loss: 2.0350e-04 - val_loss: 4.1273e-04\n",
      "Epoch 244/2000\n",
      "3622/3622 [==============================] - 1s 230us/step - loss: 2.2013e-04 - val_loss: 5.3126e-04\n",
      "Epoch 245/2000\n",
      "3622/3622 [==============================] - 1s 230us/step - loss: 2.8572e-04 - val_loss: 5.3509e-04\n",
      "Epoch 246/2000\n",
      "3622/3622 [==============================] - 1s 228us/step - loss: 2.5137e-04 - val_loss: 3.0649e-04\n",
      "Epoch 247/2000\n",
      "3622/3622 [==============================] - 1s 228us/step - loss: 2.0338e-04 - val_loss: 2.5134e-04\n",
      "Epoch 248/2000\n",
      "3622/3622 [==============================] - 1s 229us/step - loss: 1.9310e-04 - val_loss: 2.4217e-04\n",
      "Epoch 249/2000\n",
      "3622/3622 [==============================] - 1s 229us/step - loss: 1.8571e-04 - val_loss: 2.3996e-04\n",
      "Epoch 250/2000\n",
      "3622/3622 [==============================] - 1s 228us/step - loss: 1.8209e-04 - val_loss: 2.3575e-04\n",
      "Epoch 251/2000\n",
      "3622/3622 [==============================] - 1s 230us/step - loss: 1.8047e-04 - val_loss: 2.2847e-04\n",
      "Epoch 252/2000\n",
      "3622/3622 [==============================] - 1s 229us/step - loss: 1.8367e-04 - val_loss: 2.9428e-04\n",
      "Epoch 253/2000\n",
      "3622/3622 [==============================] - 1s 228us/step - loss: 2.0292e-04 - val_loss: 2.5472e-04\n",
      "Epoch 254/2000\n",
      "3622/3622 [==============================] - 1s 228us/step - loss: 1.9882e-04 - val_loss: 2.3835e-04\n",
      "Epoch 255/2000\n",
      "3622/3622 [==============================] - 1s 229us/step - loss: 1.8406e-04 - val_loss: 2.3010e-04\n",
      "Epoch 256/2000\n",
      "3622/3622 [==============================] - 1s 230us/step - loss: 1.7935e-04 - val_loss: 3.5411e-04\n",
      "Epoch 257/2000\n",
      "3622/3622 [==============================] - 1s 233us/step - loss: 2.2650e-04 - val_loss: 4.0595e-04\n",
      "Epoch 258/2000\n",
      "3622/3622 [==============================] - 1s 228us/step - loss: 2.3973e-04 - val_loss: 2.4690e-04\n",
      "Epoch 259/2000\n",
      "3622/3622 [==============================] - 1s 230us/step - loss: 2.1863e-04 - val_loss: 2.8184e-04\n",
      "Epoch 260/2000\n",
      "3622/3622 [==============================] - 1s 229us/step - loss: 1.8588e-04 - val_loss: 2.7051e-04\n",
      "Epoch 261/2000\n",
      "3622/3622 [==============================] - 1s 230us/step - loss: 1.9070e-04 - val_loss: 3.0115e-04\n",
      "Epoch 262/2000\n",
      "3622/3622 [==============================] - 1s 229us/step - loss: 1.9589e-04 - val_loss: 2.7790e-04\n",
      "Epoch 263/2000\n",
      "3622/3622 [==============================] - 1s 230us/step - loss: 1.8624e-04 - val_loss: 2.2877e-04\n",
      "Epoch 264/2000\n",
      "3622/3622 [==============================] - 1s 228us/step - loss: 1.8066e-04 - val_loss: 2.2861e-04\n",
      "Epoch 265/2000\n",
      "3622/3622 [==============================] - 1s 229us/step - loss: 1.8717e-04 - val_loss: 2.3530e-04\n",
      "Epoch 266/2000\n",
      "3622/3622 [==============================] - 1s 230us/step - loss: 1.8238e-04 - val_loss: 2.2791e-04\n",
      "Epoch 267/2000\n",
      "3622/3622 [==============================] - 1s 229us/step - loss: 1.7853e-04 - val_loss: 2.6376e-04\n",
      "Epoch 268/2000\n",
      "3622/3622 [==============================] - 1s 230us/step - loss: 1.8069e-04 - val_loss: 2.7835e-04\n",
      "Epoch 269/2000\n",
      "3622/3622 [==============================] - 1s 239us/step - loss: 2.0508e-04 - val_loss: 2.2605e-04\n",
      "Epoch 270/2000\n",
      "3622/3622 [==============================] - 1s 230us/step - loss: 1.7555e-04 - val_loss: 2.3490e-04\n",
      "Epoch 271/2000\n",
      "3622/3622 [==============================] - 1s 228us/step - loss: 1.8212e-04 - val_loss: 2.3605e-04\n",
      "Epoch 272/2000\n",
      "3622/3622 [==============================] - 1s 227us/step - loss: 1.8088e-04 - val_loss: 3.2657e-04\n",
      "Epoch 273/2000\n",
      "3622/3622 [==============================] - 1s 231us/step - loss: 2.2262e-04 - val_loss: 2.2562e-04\n",
      "Epoch 274/2000\n",
      "3622/3622 [==============================] - 1s 230us/step - loss: 1.8121e-04 - val_loss: 3.0984e-04\n",
      "Epoch 275/2000\n",
      "3622/3622 [==============================] - 1s 229us/step - loss: 2.0219e-04 - val_loss: 2.7140e-04\n",
      "Epoch 276/2000\n",
      "3622/3622 [==============================] - 1s 234us/step - loss: 1.8218e-04 - val_loss: 2.2890e-04\n",
      "Epoch 277/2000\n",
      "3622/3622 [==============================] - 1s 229us/step - loss: 1.7806e-04 - val_loss: 2.2498e-04\n",
      "Epoch 278/2000\n",
      "3622/3622 [==============================] - 1s 229us/step - loss: 1.7862e-04 - val_loss: 2.6213e-04\n",
      "Epoch 279/2000\n",
      "3622/3622 [==============================] - 1s 227us/step - loss: 1.7893e-04 - val_loss: 2.2745e-04\n",
      "Epoch 280/2000\n",
      "3622/3622 [==============================] - 1s 229us/step - loss: 1.7698e-04 - val_loss: 2.2660e-04\n",
      "Epoch 281/2000\n",
      "3622/3622 [==============================] - 1s 230us/step - loss: 1.7615e-04 - val_loss: 2.2518e-04\n",
      "Epoch 282/2000\n",
      "3622/3622 [==============================] - 1s 229us/step - loss: 1.7431e-04 - val_loss: 2.3063e-04\n",
      "Epoch 283/2000\n",
      "3622/3622 [==============================] - 1s 230us/step - loss: 1.7434e-04 - val_loss: 3.8150e-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 284/2000\n",
      "3622/3622 [==============================] - 1s 228us/step - loss: 2.2208e-04 - val_loss: 2.4333e-04\n",
      "Epoch 285/2000\n",
      "3622/3622 [==============================] - 1s 230us/step - loss: 1.9356e-04 - val_loss: 2.2403e-04\n",
      "Epoch 286/2000\n",
      "3622/3622 [==============================] - 1s 234us/step - loss: 1.7789e-04 - val_loss: 2.2875e-04\n",
      "Epoch 287/2000\n",
      "3622/3622 [==============================] - 1s 229us/step - loss: 1.7354e-04 - val_loss: 2.4183e-04\n",
      "Epoch 288/2000\n",
      "3622/3622 [==============================] - 1s 229us/step - loss: 1.8321e-04 - val_loss: 2.3673e-04\n",
      "Epoch 289/2000\n",
      "3622/3622 [==============================] - 1s 229us/step - loss: 1.8482e-04 - val_loss: 2.9960e-04\n",
      "Epoch 290/2000\n",
      "3622/3622 [==============================] - 1s 229us/step - loss: 2.0022e-04 - val_loss: 2.8780e-04\n",
      "Epoch 291/2000\n",
      "3622/3622 [==============================] - 1s 229us/step - loss: 2.0979e-04 - val_loss: 2.2634e-04\n",
      "Epoch 292/2000\n",
      "3622/3622 [==============================] - 1s 227us/step - loss: 1.7600e-04 - val_loss: 2.5247e-04\n",
      "Epoch 293/2000\n",
      "3622/3622 [==============================] - 1s 230us/step - loss: 1.8076e-04 - val_loss: 2.4338e-04\n",
      "Epoch 294/2000\n",
      "3622/3622 [==============================] - 1s 228us/step - loss: 1.7960e-04 - val_loss: 2.2770e-04\n",
      "Epoch 295/2000\n",
      "3622/3622 [==============================] - 1s 234us/step - loss: 1.8301e-04 - val_loss: 2.2456e-04\n",
      "Epoch 296/2000\n",
      "3622/3622 [==============================] - 1s 229us/step - loss: 1.7669e-04 - val_loss: 2.8161e-04\n",
      "Epoch 297/2000\n",
      "3622/3622 [==============================] - 1s 229us/step - loss: 1.8477e-04 - val_loss: 2.2513e-04\n",
      "Epoch 298/2000\n",
      "3622/3622 [==============================] - 1s 232us/step - loss: 1.7476e-04 - val_loss: 3.4609e-04\n",
      "Epoch 299/2000\n",
      "3622/3622 [==============================] - 1s 229us/step - loss: 2.2686e-04 - val_loss: 3.2259e-04\n",
      "Epoch 300/2000\n",
      "3622/3622 [==============================] - 1s 230us/step - loss: 1.9689e-04 - val_loss: 2.5385e-04\n",
      "Epoch 301/2000\n",
      "3622/3622 [==============================] - 1s 229us/step - loss: 1.7984e-04 - val_loss: 2.5521e-04\n",
      "Epoch 302/2000\n",
      "3622/3622 [==============================] - 1s 227us/step - loss: 1.8988e-04 - val_loss: 2.5890e-04\n",
      "Epoch 303/2000\n",
      "3622/3622 [==============================] - 1s 228us/step - loss: 1.8623e-04 - val_loss: 2.4222e-04\n",
      "Epoch 304/2000\n",
      "3622/3622 [==============================] - 1s 232us/step - loss: 1.8211e-04 - val_loss: 3.1792e-04\n",
      "Epoch 305/2000\n",
      "3622/3622 [==============================] - 1s 233us/step - loss: 2.0593e-04 - val_loss: 4.0248e-04\n",
      "Epoch 306/2000\n",
      "3622/3622 [==============================] - 1s 230us/step - loss: 2.4947e-04 - val_loss: 2.9717e-04\n",
      "Epoch 307/2000\n",
      "3622/3622 [==============================] - 1s 228us/step - loss: 2.0524e-04 - val_loss: 2.3594e-04\n",
      "Epoch 308/2000\n",
      "3622/3622 [==============================] - 1s 229us/step - loss: 1.7527e-04 - val_loss: 2.2778e-04\n",
      "Epoch 309/2000\n",
      "3622/3622 [==============================] - 1s 229us/step - loss: 1.7575e-04 - val_loss: 2.2766e-04\n",
      "Epoch 310/2000\n",
      "3622/3622 [==============================] - 1s 229us/step - loss: 1.7674e-04 - val_loss: 2.3808e-04\n",
      "Epoch 311/2000\n",
      "3622/3622 [==============================] - 1s 228us/step - loss: 1.8373e-04 - val_loss: 3.4619e-04\n",
      "Epoch 312/2000\n",
      "3622/3622 [==============================] - 1s 229us/step - loss: 2.2730e-04 - val_loss: 2.2480e-04\n",
      "Epoch 313/2000\n",
      "3622/3622 [==============================] - 1s 230us/step - loss: 2.0311e-04 - val_loss: 2.3442e-04\n",
      "Epoch 314/2000\n",
      "3622/3622 [==============================] - 1s 234us/step - loss: 2.1265e-04 - val_loss: 3.2694e-04\n",
      "Epoch 315/2000\n",
      "3622/3622 [==============================] - 1s 232us/step - loss: 2.3475e-04 - val_loss: 3.8886e-04\n",
      "Epoch 316/2000\n",
      "3622/3622 [==============================] - 1s 229us/step - loss: 2.3213e-04 - val_loss: 2.4102e-04\n",
      "Epoch 317/2000\n",
      "3622/3622 [==============================] - 1s 229us/step - loss: 1.8546e-04 - val_loss: 3.7394e-04\n",
      "Epoch 318/2000\n",
      "3622/3622 [==============================] - 1s 234us/step - loss: 2.2846e-04 - val_loss: 4.9663e-04\n",
      "Epoch 319/2000\n",
      "3622/3622 [==============================] - 1s 231us/step - loss: 2.3314e-04 - val_loss: 3.7662e-04\n",
      "Epoch 320/2000\n",
      "3622/3622 [==============================] - 1s 232us/step - loss: 2.2649e-04 - val_loss: 2.2652e-04\n",
      "Epoch 321/2000\n",
      "3622/3622 [==============================] - 1s 232us/step - loss: 1.9716e-04 - val_loss: 2.2594e-04\n",
      "Epoch 322/2000\n",
      "3622/3622 [==============================] - 1s 229us/step - loss: 1.7445e-04 - val_loss: 2.3841e-04\n",
      "Epoch 323/2000\n",
      "3622/3622 [==============================] - 1s 233us/step - loss: 1.7770e-04 - val_loss: 2.3456e-04\n",
      "Epoch 324/2000\n",
      "3622/3622 [==============================] - 1s 235us/step - loss: 1.7638e-04 - val_loss: 2.8324e-04\n",
      "Epoch 325/2000\n",
      "3622/3622 [==============================] - 1s 228us/step - loss: 2.0731e-04 - val_loss: 2.9118e-04\n",
      "Epoch 326/2000\n",
      "3622/3622 [==============================] - 1s 230us/step - loss: 1.8712e-04 - val_loss: 2.2867e-04\n",
      "Epoch 327/2000\n",
      "3622/3622 [==============================] - 1s 230us/step - loss: 1.7282e-04 - val_loss: 3.2644e-04\n",
      "Epoch 328/2000\n",
      "3622/3622 [==============================] - 1s 230us/step - loss: 1.8475e-04 - val_loss: 2.2451e-04\n",
      "Epoch 329/2000\n",
      "3622/3622 [==============================] - 1s 231us/step - loss: 1.7421e-04 - val_loss: 2.9965e-04\n",
      "Epoch 330/2000\n",
      "3622/3622 [==============================] - 1s 229us/step - loss: 1.9423e-04 - val_loss: 2.3995e-04\n",
      "Epoch 331/2000\n",
      "3622/3622 [==============================] - 1s 228us/step - loss: 1.8466e-04 - val_loss: 2.2310e-04\n",
      "Epoch 332/2000\n",
      "3622/3622 [==============================] - 1s 230us/step - loss: 1.7728e-04 - val_loss: 2.6751e-04\n",
      "Epoch 333/2000\n",
      "3622/3622 [==============================] - 1s 231us/step - loss: 1.9782e-04 - val_loss: 2.8253e-04\n",
      "Epoch 334/2000\n",
      "3622/3622 [==============================] - 1s 232us/step - loss: 1.8753e-04 - val_loss: 2.5538e-04\n",
      "Epoch 335/2000\n",
      "3622/3622 [==============================] - 1s 227us/step - loss: 1.8369e-04 - val_loss: 3.2990e-04\n",
      "Epoch 336/2000\n",
      "3622/3622 [==============================] - 1s 227us/step - loss: 2.0499e-04 - val_loss: 2.6394e-04\n",
      "Epoch 337/2000\n",
      "3622/3622 [==============================] - 1s 227us/step - loss: 1.8183e-04 - val_loss: 2.2319e-04\n",
      "Epoch 338/2000\n",
      "3622/3622 [==============================] - 1s 229us/step - loss: 1.7501e-04 - val_loss: 2.2419e-04\n",
      "Epoch 339/2000\n",
      "3622/3622 [==============================] - 1s 228us/step - loss: 1.7918e-04 - val_loss: 2.2738e-04\n",
      "Epoch 340/2000\n",
      "3622/3622 [==============================] - 1s 232us/step - loss: 1.9097e-04 - val_loss: 2.3597e-04\n",
      "Epoch 341/2000\n",
      "3622/3622 [==============================] - 1s 232us/step - loss: 1.7447e-04 - val_loss: 2.2702e-04\n",
      "Epoch 342/2000\n",
      "3622/3622 [==============================] - 1s 228us/step - loss: 1.7451e-04 - val_loss: 2.4800e-04\n",
      "Epoch 343/2000\n",
      "3622/3622 [==============================] - 1s 229us/step - loss: 1.8670e-04 - val_loss: 2.4847e-04\n",
      "Epoch 344/2000\n",
      "3622/3622 [==============================] - 1s 229us/step - loss: 1.7732e-04 - val_loss: 2.4001e-04\n",
      "Epoch 345/2000\n",
      "3622/3622 [==============================] - 1s 229us/step - loss: 1.7686e-04 - val_loss: 2.7009e-04\n",
      "Epoch 346/2000\n",
      "3622/3622 [==============================] - 1s 229us/step - loss: 1.8488e-04 - val_loss: 2.4851e-04\n",
      "Epoch 347/2000\n",
      "3622/3622 [==============================] - 1s 230us/step - loss: 1.9103e-04 - val_loss: 2.7513e-04\n",
      "Epoch 348/2000\n",
      "3622/3622 [==============================] - 1s 229us/step - loss: 1.9468e-04 - val_loss: 2.3606e-04\n",
      "Epoch 349/2000\n",
      "3622/3622 [==============================] - 1s 228us/step - loss: 1.8633e-04 - val_loss: 2.2510e-04\n",
      "Epoch 350/2000\n",
      "3622/3622 [==============================] - 1s 230us/step - loss: 1.8896e-04 - val_loss: 2.6381e-04\n",
      "Epoch 351/2000\n",
      "3622/3622 [==============================] - 1s 230us/step - loss: 1.8222e-04 - val_loss: 2.5123e-04\n",
      "Epoch 352/2000\n",
      "3622/3622 [==============================] - 1s 232us/step - loss: 1.9233e-04 - val_loss: 2.9208e-04\n",
      "Epoch 353/2000\n",
      "3622/3622 [==============================] - 1s 231us/step - loss: 1.9312e-04 - val_loss: 2.4617e-04\n",
      "Epoch 354/2000\n",
      "3622/3622 [==============================] - 1s 228us/step - loss: 1.7274e-04 - val_loss: 2.2513e-04\n",
      "Epoch 355/2000\n",
      "3622/3622 [==============================] - 1s 230us/step - loss: 1.7208e-04 - val_loss: 2.6687e-04\n",
      "Epoch 356/2000\n",
      "3622/3622 [==============================] - 1s 230us/step - loss: 1.7851e-04 - val_loss: 2.2990e-04\n",
      "Epoch 357/2000\n",
      "3622/3622 [==============================] - 1s 229us/step - loss: 1.7782e-04 - val_loss: 2.6107e-04\n",
      "Epoch 358/2000\n",
      "3622/3622 [==============================] - 1s 229us/step - loss: 1.9881e-04 - val_loss: 2.2473e-04\n",
      "Epoch 359/2000\n",
      "3622/3622 [==============================] - 1s 230us/step - loss: 1.7796e-04 - val_loss: 2.2297e-04\n",
      "Epoch 360/2000\n",
      "3622/3622 [==============================] - 1s 229us/step - loss: 1.8058e-04 - val_loss: 2.3132e-04\n",
      "Epoch 361/2000\n",
      "3622/3622 [==============================] - 1s 230us/step - loss: 1.8302e-04 - val_loss: 2.8385e-04\n",
      "Epoch 362/2000\n",
      "3622/3622 [==============================] - 1s 228us/step - loss: 1.9183e-04 - val_loss: 2.4143e-04\n",
      "Epoch 363/2000\n",
      "3622/3622 [==============================] - 1s 228us/step - loss: 1.8617e-04 - val_loss: 2.5044e-04\n",
      "Epoch 364/2000\n",
      "3622/3622 [==============================] - 1s 228us/step - loss: 1.7845e-04 - val_loss: 2.2519e-04\n",
      "Epoch 365/2000\n",
      "3622/3622 [==============================] - 1s 229us/step - loss: 1.9413e-04 - val_loss: 4.6329e-04\n",
      "Epoch 366/2000\n",
      "3622/3622 [==============================] - 1s 229us/step - loss: 2.5408e-04 - val_loss: 3.2265e-04\n",
      "Epoch 367/2000\n",
      "3622/3622 [==============================] - 1s 229us/step - loss: 2.3822e-04 - val_loss: 2.5666e-04\n",
      "Epoch 368/2000\n",
      "3622/3622 [==============================] - 1s 228us/step - loss: 1.9108e-04 - val_loss: 3.0462e-04\n",
      "Epoch 369/2000\n",
      "3622/3622 [==============================] - 1s 230us/step - loss: 1.9610e-04 - val_loss: 2.2877e-04\n",
      "Epoch 370/2000\n",
      "3622/3622 [==============================] - 1s 228us/step - loss: 1.8278e-04 - val_loss: 2.2674e-04\n",
      "Epoch 371/2000\n",
      "3622/3622 [==============================] - 1s 231us/step - loss: 1.7453e-04 - val_loss: 2.2657e-04\n",
      "Epoch 372/2000\n",
      "3622/3622 [==============================] - 1s 234us/step - loss: 1.7683e-04 - val_loss: 2.8423e-04\n",
      "Epoch 373/2000\n",
      "3622/3622 [==============================] - 1s 228us/step - loss: 1.9505e-04 - val_loss: 2.6546e-04\n",
      "Epoch 374/2000\n",
      "3622/3622 [==============================] - 1s 229us/step - loss: 1.8284e-04 - val_loss: 2.4153e-04\n",
      "Epoch 375/2000\n",
      "3622/3622 [==============================] - 1s 230us/step - loss: 1.7948e-04 - val_loss: 3.7024e-04\n",
      "Epoch 376/2000\n",
      "3622/3622 [==============================] - 1s 233us/step - loss: 2.2019e-04 - val_loss: 2.9892e-04\n",
      "Epoch 377/2000\n",
      "3622/3622 [==============================] - 1s 232us/step - loss: 2.1446e-04 - val_loss: 0.0011\n",
      "Epoch 378/2000\n",
      "3622/3622 [==============================] - 1s 229us/step - loss: 4.1367e-04 - val_loss: 2.4551e-04\n",
      "Epoch 379/2000\n",
      "3622/3622 [==============================] - 1s 228us/step - loss: 2.9560e-04 - val_loss: 3.4646e-04\n",
      "Epoch 380/2000\n",
      "3622/3622 [==============================] - 1s 229us/step - loss: 2.0590e-04 - val_loss: 2.9012e-04\n",
      "Epoch 381/2000\n",
      "3622/3622 [==============================] - 1s 231us/step - loss: 2.0251e-04 - val_loss: 2.4135e-04\n",
      "Epoch 382/2000\n",
      "3622/3622 [==============================] - 1s 229us/step - loss: 1.9562e-04 - val_loss: 4.0226e-04\n",
      "Epoch 383/2000\n",
      "3622/3622 [==============================] - 1s 230us/step - loss: 2.3188e-04 - val_loss: 3.2085e-04\n",
      "Epoch 384/2000\n",
      "3622/3622 [==============================] - 1s 231us/step - loss: 2.2178e-04 - val_loss: 2.2475e-04\n",
      "Epoch 385/2000\n",
      "3622/3622 [==============================] - 1s 228us/step - loss: 1.8210e-04 - val_loss: 2.2581e-04\n",
      "Epoch 386/2000\n",
      "3622/3622 [==============================] - 1s 228us/step - loss: 1.7559e-04 - val_loss: 2.5757e-04\n",
      "Epoch 387/2000\n",
      "3622/3622 [==============================] - 1s 227us/step - loss: 1.9115e-04 - val_loss: 2.2764e-04\n",
      "Epoch 388/2000\n",
      "3622/3622 [==============================] - 1s 230us/step - loss: 1.7576e-04 - val_loss: 2.4195e-04\n",
      "Epoch 389/2000\n",
      "3622/3622 [==============================] - 1s 230us/step - loss: 1.8110e-04 - val_loss: 2.3809e-04\n",
      "Epoch 390/2000\n",
      "3622/3622 [==============================] - 1s 229us/step - loss: 1.7301e-04 - val_loss: 2.9523e-04\n",
      "Epoch 391/2000\n",
      "3622/3622 [==============================] - 1s 235us/step - loss: 1.8443e-04 - val_loss: 2.2441e-04\n",
      "Epoch 392/2000\n",
      "3622/3622 [==============================] - 1s 227us/step - loss: 1.7254e-04 - val_loss: 2.4612e-04\n",
      "Epoch 393/2000\n",
      "3622/3622 [==============================] - 1s 229us/step - loss: 1.7523e-04 - val_loss: 2.5265e-04\n",
      "Epoch 394/2000\n",
      "3622/3622 [==============================] - 1s 229us/step - loss: 1.8446e-04 - val_loss: 2.4033e-04\n",
      "Epoch 395/2000\n",
      "3622/3622 [==============================] - 1s 229us/step - loss: 1.8375e-04 - val_loss: 2.5226e-04\n",
      "Epoch 396/2000\n",
      "3622/3622 [==============================] - 1s 229us/step - loss: 1.8252e-04 - val_loss: 2.2489e-04\n",
      "Epoch 397/2000\n",
      "3622/3622 [==============================] - 1s 229us/step - loss: 1.7351e-04 - val_loss: 2.2673e-04\n",
      "Epoch 398/2000\n",
      "3622/3622 [==============================] - 1s 229us/step - loss: 1.7164e-04 - val_loss: 2.6419e-04\n",
      "Epoch 399/2000\n",
      "3622/3622 [==============================] - 1s 230us/step - loss: 1.7811e-04 - val_loss: 2.3224e-04\n",
      "Epoch 400/2000\n",
      "3622/3622 [==============================] - 1s 229us/step - loss: 1.7736e-04 - val_loss: 3.3373e-04\n",
      "Epoch 401/2000\n",
      "3622/3622 [==============================] - 1s 230us/step - loss: 1.9793e-04 - val_loss: 3.1286e-04\n",
      "Epoch 402/2000\n",
      "3622/3622 [==============================] - 1s 230us/step - loss: 2.0025e-04 - val_loss: 2.6088e-04\n",
      "Epoch 403/2000\n",
      "3622/3622 [==============================] - 1s 228us/step - loss: 1.8063e-04 - val_loss: 2.2859e-04\n",
      "Epoch 404/2000\n",
      "3622/3622 [==============================] - 1s 229us/step - loss: 1.8235e-04 - val_loss: 2.2503e-04\n",
      "Epoch 405/2000\n",
      "3622/3622 [==============================] - 1s 229us/step - loss: 1.7292e-04 - val_loss: 2.2480e-04\n",
      "Epoch 406/2000\n",
      "3622/3622 [==============================] - 1s 227us/step - loss: 1.7320e-04 - val_loss: 2.2679e-04\n",
      "Epoch 407/2000\n",
      "3622/3622 [==============================] - 1s 229us/step - loss: 1.7436e-04 - val_loss: 2.5658e-04\n",
      "Epoch 408/2000\n",
      "3622/3622 [==============================] - 1s 229us/step - loss: 1.8591e-04 - val_loss: 2.2305e-04\n",
      "Epoch 409/2000\n",
      "3622/3622 [==============================] - 1s 230us/step - loss: 1.7418e-04 - val_loss: 2.5468e-04\n",
      "Epoch 410/2000\n",
      "3622/3622 [==============================] - 1s 235us/step - loss: 1.7572e-04 - val_loss: 2.2729e-04\n",
      "Epoch 411/2000\n",
      "3622/3622 [==============================] - 1s 228us/step - loss: 1.7340e-04 - val_loss: 2.2744e-04\n",
      "Epoch 412/2000\n",
      "3622/3622 [==============================] - 1s 233us/step - loss: 1.7385e-04 - val_loss: 2.2263e-04\n",
      "Epoch 413/2000\n",
      "3622/3622 [==============================] - 1s 232us/step - loss: 1.8229e-04 - val_loss: 2.9684e-04\n",
      "Epoch 414/2000\n",
      "3622/3622 [==============================] - 1s 229us/step - loss: 1.8761e-04 - val_loss: 2.5425e-04\n",
      "Epoch 415/2000\n",
      "3622/3622 [==============================] - 1s 229us/step - loss: 1.8378e-04 - val_loss: 2.4295e-04\n",
      "Epoch 416/2000\n",
      "3622/3622 [==============================] - 1s 229us/step - loss: 1.9147e-04 - val_loss: 2.6397e-04\n",
      "Epoch 417/2000\n",
      "3622/3622 [==============================] - 1s 231us/step - loss: 1.9248e-04 - val_loss: 2.3097e-04\n",
      "Epoch 418/2000\n",
      "3622/3622 [==============================] - 1s 228us/step - loss: 1.8130e-04 - val_loss: 2.4658e-04\n",
      "Epoch 419/2000\n",
      "3622/3622 [==============================] - 1s 230us/step - loss: 1.8311e-04 - val_loss: 2.7393e-04\n",
      "Epoch 420/2000\n",
      "3622/3622 [==============================] - 1s 227us/step - loss: 1.8344e-04 - val_loss: 2.6753e-04\n",
      "Epoch 421/2000\n",
      "3622/3622 [==============================] - 1s 229us/step - loss: 1.9429e-04 - val_loss: 2.2372e-04\n",
      "Epoch 422/2000\n",
      "3622/3622 [==============================] - 1s 229us/step - loss: 1.7005e-04 - val_loss: 3.7700e-04\n",
      "Epoch 423/2000\n",
      "3622/3622 [==============================] - 1s 231us/step - loss: 2.5692e-04 - val_loss: 4.1464e-04\n",
      "Epoch 424/2000\n",
      "3622/3622 [==============================] - 1s 229us/step - loss: 2.4380e-04 - val_loss: 2.4132e-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 425/2000\n",
      "3622/3622 [==============================] - 1s 229us/step - loss: 2.0247e-04 - val_loss: 2.5351e-04\n",
      "Epoch 426/2000\n",
      "3622/3622 [==============================] - 1s 229us/step - loss: 1.8832e-04 - val_loss: 3.5129e-04\n",
      "Epoch 427/2000\n",
      "3622/3622 [==============================] - 1s 230us/step - loss: 1.9583e-04 - val_loss: 2.7731e-04\n",
      "Epoch 428/2000\n",
      "3622/3622 [==============================] - 1s 231us/step - loss: 2.0135e-04 - val_loss: 3.7567e-04\n",
      "Epoch 429/2000\n",
      "3622/3622 [==============================] - 1s 233us/step - loss: 2.2948e-04 - val_loss: 2.2370e-04\n",
      "Epoch 430/2000\n",
      "3622/3622 [==============================] - 1s 229us/step - loss: 1.8737e-04 - val_loss: 2.2556e-04\n",
      "Epoch 431/2000\n",
      "3622/3622 [==============================] - 1s 229us/step - loss: 1.7105e-04 - val_loss: 2.2452e-04\n",
      "Epoch 432/2000\n",
      "3622/3622 [==============================] - 1s 228us/step - loss: 1.7683e-04 - val_loss: 2.5400e-04\n",
      "Epoch 433/2000\n",
      "3622/3622 [==============================] - 1s 229us/step - loss: 1.8472e-04 - val_loss: 2.2533e-04\n",
      "Epoch 434/2000\n",
      "3622/3622 [==============================] - 1s 229us/step - loss: 1.9343e-04 - val_loss: 3.0380e-04\n",
      "Epoch 435/2000\n",
      "3622/3622 [==============================] - 1s 227us/step - loss: 2.1891e-04 - val_loss: 2.6638e-04\n",
      "Epoch 436/2000\n",
      "3622/3622 [==============================] - 1s 230us/step - loss: 1.7918e-04 - val_loss: 2.2307e-04\n",
      "Epoch 437/2000\n",
      "3622/3622 [==============================] - 1s 229us/step - loss: 1.7160e-04 - val_loss: 2.2486e-04\n",
      "Epoch 438/2000\n",
      "3622/3622 [==============================] - 1s 227us/step - loss: 1.7012e-04 - val_loss: 2.4607e-04\n",
      "Epoch 439/2000\n",
      "3622/3622 [==============================] - 1s 229us/step - loss: 1.7826e-04 - val_loss: 2.3218e-04\n",
      "Epoch 440/2000\n",
      "3622/3622 [==============================] - 1s 228us/step - loss: 1.7517e-04 - val_loss: 2.3454e-04\n",
      "Epoch 441/2000\n",
      "3622/3622 [==============================] - 1s 227us/step - loss: 1.6953e-04 - val_loss: 2.3006e-04\n",
      "Epoch 442/2000\n",
      "3622/3622 [==============================] - 1s 229us/step - loss: 1.7102e-04 - val_loss: 2.2968e-04\n",
      "Epoch 443/2000\n",
      "3622/3622 [==============================] - 1s 230us/step - loss: 1.6941e-04 - val_loss: 2.3475e-04\n",
      "Epoch 444/2000\n",
      "3622/3622 [==============================] - 1s 230us/step - loss: 1.7395e-04 - val_loss: 2.2518e-04\n",
      "Epoch 445/2000\n",
      "3622/3622 [==============================] - 1s 231us/step - loss: 1.7157e-04 - val_loss: 2.3657e-04\n",
      "Epoch 446/2000\n",
      "3622/3622 [==============================] - 1s 230us/step - loss: 1.7486e-04 - val_loss: 2.2374e-04\n",
      "Epoch 447/2000\n",
      "3622/3622 [==============================] - 1s 230us/step - loss: 1.8307e-04 - val_loss: 2.2703e-04\n",
      "Epoch 448/2000\n",
      "3622/3622 [==============================] - 1s 240us/step - loss: 1.7512e-04 - val_loss: 2.2339e-04\n",
      "Epoch 449/2000\n",
      "3622/3622 [==============================] - 1s 232us/step - loss: 1.8195e-04 - val_loss: 2.2751e-04\n",
      "Epoch 450/2000\n",
      "3622/3622 [==============================] - 1s 230us/step - loss: 1.7313e-04 - val_loss: 2.2481e-04\n",
      "Epoch 451/2000\n",
      "3622/3622 [==============================] - 1s 229us/step - loss: 1.7158e-04 - val_loss: 2.2534e-04\n",
      "Epoch 452/2000\n",
      "3622/3622 [==============================] - 1s 231us/step - loss: 1.7160e-04 - val_loss: 2.6012e-04\n",
      "Epoch 453/2000\n",
      "3622/3622 [==============================] - 1s 230us/step - loss: 1.8305e-04 - val_loss: 3.7377e-04\n",
      "Epoch 454/2000\n",
      "3622/3622 [==============================] - 1s 227us/step - loss: 2.0164e-04 - val_loss: 2.7658e-04\n",
      "Epoch 455/2000\n",
      "3622/3622 [==============================] - 1s 235us/step - loss: 1.8719e-04 - val_loss: 2.2667e-04\n",
      "Epoch 456/2000\n",
      "3622/3622 [==============================] - 1s 230us/step - loss: 1.7161e-04 - val_loss: 2.2532e-04\n",
      "Epoch 457/2000\n",
      "3622/3622 [==============================] - 1s 229us/step - loss: 1.6993e-04 - val_loss: 2.2552e-04\n",
      "Epoch 458/2000\n",
      "3622/3622 [==============================] - 1s 227us/step - loss: 1.7654e-04 - val_loss: 2.2281e-04\n",
      "Epoch 459/2000\n",
      "3622/3622 [==============================] - 1s 231us/step - loss: 1.6957e-04 - val_loss: 3.2014e-04\n",
      "Epoch 460/2000\n",
      "3622/3622 [==============================] - 1s 229us/step - loss: 2.0491e-04 - val_loss: 3.9435e-04\n",
      "Epoch 461/2000\n",
      "3622/3622 [==============================] - 1s 229us/step - loss: 2.4418e-04 - val_loss: 2.5728e-04\n",
      "Epoch 462/2000\n",
      "3622/3622 [==============================] - 1s 230us/step - loss: 2.4192e-04 - val_loss: 2.8409e-04\n",
      "Epoch 463/2000\n",
      "3622/3622 [==============================] - 1s 229us/step - loss: 2.0495e-04 - val_loss: 3.5110e-04\n",
      "Epoch 464/2000\n",
      "3622/3622 [==============================] - 1s 231us/step - loss: 2.2249e-04 - val_loss: 2.5739e-04\n",
      "Epoch 465/2000\n",
      "3622/3622 [==============================] - 1s 232us/step - loss: 1.8362e-04 - val_loss: 2.7422e-04\n",
      "Epoch 466/2000\n",
      "3622/3622 [==============================] - 1s 230us/step - loss: 1.7837e-04 - val_loss: 2.3611e-04\n",
      "Epoch 467/2000\n",
      "3622/3622 [==============================] - 1s 233us/step - loss: 1.7332e-04 - val_loss: 2.5162e-04\n",
      "Epoch 468/2000\n",
      "3622/3622 [==============================] - 1s 231us/step - loss: 1.7762e-04 - val_loss: 2.8541e-04\n",
      "Epoch 469/2000\n",
      "3622/3622 [==============================] - 1s 231us/step - loss: 1.9041e-04 - val_loss: 4.8599e-04\n",
      "Epoch 470/2000\n",
      "3622/3622 [==============================] - 1s 229us/step - loss: 2.7203e-04 - val_loss: 2.4801e-04\n",
      "Epoch 471/2000\n",
      "3622/3622 [==============================] - 1s 228us/step - loss: 1.8338e-04 - val_loss: 2.2479e-04\n",
      "Epoch 472/2000\n",
      "3622/3622 [==============================] - 1s 231us/step - loss: 1.7052e-04 - val_loss: 2.4822e-04\n",
      "Epoch 473/2000\n",
      "3622/3622 [==============================] - 1s 230us/step - loss: 1.7333e-04 - val_loss: 2.3457e-04\n",
      "Epoch 474/2000\n",
      "3622/3622 [==============================] - 1s 229us/step - loss: 1.8067e-04 - val_loss: 3.2296e-04\n",
      "Epoch 475/2000\n",
      "3622/3622 [==============================] - 1s 229us/step - loss: 1.9828e-04 - val_loss: 2.5848e-04\n",
      "Epoch 476/2000\n",
      "3622/3622 [==============================] - 1s 230us/step - loss: 1.7713e-04 - val_loss: 2.2407e-04\n",
      "Epoch 477/2000\n",
      "3622/3622 [==============================] - 1s 227us/step - loss: 1.8409e-04 - val_loss: 2.2881e-04\n",
      "Epoch 478/2000\n",
      "3622/3622 [==============================] - 1s 231us/step - loss: 1.7078e-04 - val_loss: 2.4719e-04\n",
      "Epoch 479/2000\n",
      "3622/3622 [==============================] - 1s 230us/step - loss: 1.7814e-04 - val_loss: 2.2943e-04\n",
      "Epoch 480/2000\n",
      "3622/3622 [==============================] - 1s 228us/step - loss: 1.6981e-04 - val_loss: 3.4500e-04\n",
      "Epoch 481/2000\n",
      "3622/3622 [==============================] - 1s 230us/step - loss: 2.0147e-04 - val_loss: 2.2212e-04\n",
      "Epoch 482/2000\n",
      "3622/3622 [==============================] - 1s 229us/step - loss: 1.6943e-04 - val_loss: 3.0984e-04\n",
      "Epoch 483/2000\n",
      "3622/3622 [==============================] - 1s 230us/step - loss: 1.9725e-04 - val_loss: 2.2561e-04\n",
      "Epoch 484/2000\n",
      "3622/3622 [==============================] - 1s 236us/step - loss: 1.7594e-04 - val_loss: 2.9627e-04\n",
      "Epoch 485/2000\n",
      "3622/3622 [==============================] - 1s 230us/step - loss: 1.9426e-04 - val_loss: 2.2670e-04\n",
      "Epoch 486/2000\n",
      "3622/3622 [==============================] - 1s 229us/step - loss: 1.7644e-04 - val_loss: 2.2554e-04\n",
      "Epoch 487/2000\n",
      "3622/3622 [==============================] - 1s 233us/step - loss: 1.7754e-04 - val_loss: 2.3005e-04\n",
      "Epoch 488/2000\n",
      "3622/3622 [==============================] - 1s 229us/step - loss: 1.9892e-04 - val_loss: 2.5371e-04\n",
      "Epoch 489/2000\n",
      "3622/3622 [==============================] - 1s 228us/step - loss: 1.9273e-04 - val_loss: 2.8792e-04\n",
      "Epoch 490/2000\n",
      "3622/3622 [==============================] - 1s 229us/step - loss: 1.8006e-04 - val_loss: 2.3228e-04\n",
      "Epoch 491/2000\n",
      "3622/3622 [==============================] - 1s 227us/step - loss: 1.7217e-04 - val_loss: 2.2884e-04\n",
      "Epoch 492/2000\n",
      "3622/3622 [==============================] - 1s 229us/step - loss: 1.7270e-04 - val_loss: 2.2417e-04\n",
      "Epoch 493/2000\n",
      "3622/3622 [==============================] - 1s 231us/step - loss: 1.6941e-04 - val_loss: 2.3075e-04\n",
      "Epoch 494/2000\n",
      "3622/3622 [==============================] - 1s 228us/step - loss: 1.6721e-04 - val_loss: 2.5771e-04\n",
      "Epoch 495/2000\n",
      "3622/3622 [==============================] - 1s 230us/step - loss: 1.7537e-04 - val_loss: 2.3117e-04\n",
      "Epoch 496/2000\n",
      "3622/3622 [==============================] - 1s 230us/step - loss: 1.7750e-04 - val_loss: 2.5467e-04\n",
      "Epoch 497/2000\n",
      "3622/3622 [==============================] - 1s 227us/step - loss: 1.7690e-04 - val_loss: 3.0247e-04\n",
      "Epoch 498/2000\n",
      "3622/3622 [==============================] - 1s 227us/step - loss: 1.8993e-04 - val_loss: 2.4835e-04\n",
      "Epoch 499/2000\n",
      "3622/3622 [==============================] - 1s 227us/step - loss: 1.8703e-04 - val_loss: 2.2271e-04\n",
      "Epoch 500/2000\n",
      "3622/3622 [==============================] - 1s 228us/step - loss: 1.8938e-04 - val_loss: 2.8663e-04\n",
      "Epoch 501/2000\n",
      "3622/3622 [==============================] - 1s 229us/step - loss: 1.9303e-04 - val_loss: 2.7527e-04\n",
      "Epoch 502/2000\n",
      "3622/3622 [==============================] - 1s 228us/step - loss: 1.8782e-04 - val_loss: 2.7402e-04\n",
      "Epoch 503/2000\n",
      "3622/3622 [==============================] - 1s 228us/step - loss: 1.9818e-04 - val_loss: 3.0661e-04\n",
      "Epoch 504/2000\n",
      "3622/3622 [==============================] - 1s 231us/step - loss: 2.0021e-04 - val_loss: 3.1501e-04\n",
      "Epoch 505/2000\n",
      "3622/3622 [==============================] - 1s 228us/step - loss: 2.0548e-04 - val_loss: 3.4366e-04\n",
      "Epoch 506/2000\n",
      "3622/3622 [==============================] - 1s 234us/step - loss: 2.1938e-04 - val_loss: 2.4982e-04\n",
      "Epoch 507/2000\n",
      "3622/3622 [==============================] - 1s 229us/step - loss: 1.8122e-04 - val_loss: 2.5056e-04\n",
      "Epoch 508/2000\n",
      "3622/3622 [==============================] - 1s 227us/step - loss: 1.7714e-04 - val_loss: 2.4332e-04\n",
      "Epoch 509/2000\n",
      "3622/3622 [==============================] - 1s 230us/step - loss: 1.8222e-04 - val_loss: 3.1337e-04\n",
      "Epoch 510/2000\n",
      "3622/3622 [==============================] - 1s 228us/step - loss: 2.0837e-04 - val_loss: 2.9063e-04\n",
      "Epoch 511/2000\n",
      "3622/3622 [==============================] - 1s 228us/step - loss: 1.9636e-04 - val_loss: 2.8772e-04\n",
      "Epoch 512/2000\n",
      "3622/3622 [==============================] - 1s 229us/step - loss: 1.8006e-04 - val_loss: 3.0351e-04\n",
      "Epoch 513/2000\n",
      "3622/3622 [==============================] - 1s 228us/step - loss: 1.9371e-04 - val_loss: 2.4949e-04\n",
      "Epoch 514/2000\n",
      "3622/3622 [==============================] - 1s 230us/step - loss: 1.7972e-04 - val_loss: 2.5257e-04\n",
      "Epoch 515/2000\n",
      "3622/3622 [==============================] - 1s 230us/step - loss: 1.8291e-04 - val_loss: 2.2806e-04\n",
      "Epoch 516/2000\n",
      "3622/3622 [==============================] - 1s 232us/step - loss: 1.7947e-04 - val_loss: 2.6720e-04\n",
      "Epoch 517/2000\n",
      "3622/3622 [==============================] - 1s 230us/step - loss: 1.9250e-04 - val_loss: 2.3976e-04\n",
      "Epoch 518/2000\n",
      "3622/3622 [==============================] - 1s 230us/step - loss: 1.7557e-04 - val_loss: 2.4858e-04\n",
      "Epoch 519/2000\n",
      "3622/3622 [==============================] - 1s 229us/step - loss: 1.8474e-04 - val_loss: 2.3238e-04\n",
      "Epoch 520/2000\n",
      "3622/3622 [==============================] - 1s 236us/step - loss: 1.7332e-04 - val_loss: 2.3030e-04\n",
      "Epoch 521/2000\n",
      "3622/3622 [==============================] - 1s 229us/step - loss: 1.7617e-04 - val_loss: 3.2078e-04\n",
      "Epoch 522/2000\n",
      "3622/3622 [==============================] - 1s 231us/step - loss: 1.9585e-04 - val_loss: 2.3416e-04\n",
      "Epoch 523/2000\n",
      "3622/3622 [==============================] - 1s 229us/step - loss: 1.7077e-04 - val_loss: 2.2775e-04\n",
      "Epoch 524/2000\n",
      "3622/3622 [==============================] - 1s 228us/step - loss: 1.7264e-04 - val_loss: 2.6560e-04\n",
      "Epoch 525/2000\n",
      "3622/3622 [==============================] - 1s 232us/step - loss: 1.8570e-04 - val_loss: 2.9546e-04\n",
      "Epoch 526/2000\n",
      "3622/3622 [==============================] - 1s 229us/step - loss: 1.8722e-04 - val_loss: 2.6387e-04\n",
      "Epoch 527/2000\n",
      "3622/3622 [==============================] - 1s 228us/step - loss: 1.8666e-04 - val_loss: 3.0746e-04\n",
      "Epoch 528/2000\n",
      "3622/3622 [==============================] - 1s 229us/step - loss: 1.9004e-04 - val_loss: 3.2552e-04\n",
      "Epoch 529/2000\n",
      "3622/3622 [==============================] - 1s 230us/step - loss: 1.9926e-04 - val_loss: 2.2562e-04\n",
      "Epoch 530/2000\n",
      "3622/3622 [==============================] - 1s 230us/step - loss: 1.7596e-04 - val_loss: 3.2494e-04\n",
      "Epoch 531/2000\n",
      "3622/3622 [==============================] - 1s 228us/step - loss: 2.0712e-04 - val_loss: 3.1819e-04\n",
      "Epoch 532/2000\n",
      "3622/3622 [==============================] - 1s 230us/step - loss: 2.2330e-04 - val_loss: 2.7552e-04\n",
      "Epoch 533/2000\n",
      "3622/3622 [==============================] - 1s 229us/step - loss: 2.1324e-04 - val_loss: 2.2348e-04\n",
      "Epoch 534/2000\n",
      "3622/3622 [==============================] - 1s 229us/step - loss: 2.0284e-04 - val_loss: 3.2847e-04\n",
      "Epoch 535/2000\n",
      "3622/3622 [==============================] - 1s 230us/step - loss: 2.0535e-04 - val_loss: 2.5015e-04\n",
      "Epoch 536/2000\n",
      "3622/3622 [==============================] - 1s 228us/step - loss: 1.8949e-04 - val_loss: 4.3101e-04\n",
      "Epoch 537/2000\n",
      "3622/3622 [==============================] - 1s 229us/step - loss: 2.2568e-04 - val_loss: 2.4277e-04\n",
      "Epoch 538/2000\n",
      "3622/3622 [==============================] - 1s 228us/step - loss: 1.8230e-04 - val_loss: 2.3582e-04\n",
      "Epoch 539/2000\n",
      "3622/3622 [==============================] - 1s 227us/step - loss: 1.8421e-04 - val_loss: 2.3646e-04\n",
      "Epoch 540/2000\n",
      "3622/3622 [==============================] - 1s 228us/step - loss: 1.7729e-04 - val_loss: 2.2186e-04\n",
      "Epoch 541/2000\n",
      "3622/3622 [==============================] - 1s 230us/step - loss: 1.8456e-04 - val_loss: 2.7336e-04\n",
      "Epoch 542/2000\n",
      "3622/3622 [==============================] - 1s 228us/step - loss: 1.8785e-04 - val_loss: 4.0052e-04\n",
      "Epoch 543/2000\n",
      "3622/3622 [==============================] - 1s 230us/step - loss: 2.1397e-04 - val_loss: 2.2410e-04\n",
      "Epoch 544/2000\n",
      "3622/3622 [==============================] - 1s 235us/step - loss: 1.8545e-04 - val_loss: 3.3598e-04\n",
      "Epoch 545/2000\n",
      "3622/3622 [==============================] - 1s 230us/step - loss: 2.1105e-04 - val_loss: 3.1482e-04\n",
      "Epoch 546/2000\n",
      "3622/3622 [==============================] - 1s 230us/step - loss: 2.0317e-04 - val_loss: 2.9995e-04\n",
      "Epoch 547/2000\n",
      "3622/3622 [==============================] - 1s 230us/step - loss: 1.9922e-04 - val_loss: 3.3194e-04\n",
      "Epoch 548/2000\n",
      "3622/3622 [==============================] - 1s 228us/step - loss: 2.4400e-04 - val_loss: 3.3162e-04\n",
      "Epoch 549/2000\n",
      "3622/3622 [==============================] - 1s 229us/step - loss: 2.1509e-04 - val_loss: 2.9141e-04\n",
      "Epoch 550/2000\n",
      "3622/3622 [==============================] - 1s 230us/step - loss: 1.9709e-04 - val_loss: 2.6619e-04\n",
      "Epoch 551/2000\n",
      "3622/3622 [==============================] - 1s 230us/step - loss: 1.7636e-04 - val_loss: 2.3445e-04\n",
      "Epoch 552/2000\n",
      "3622/3622 [==============================] - 1s 230us/step - loss: 1.7426e-04 - val_loss: 2.4546e-04\n",
      "Epoch 553/2000\n",
      "3622/3622 [==============================] - 1s 229us/step - loss: 1.7935e-04 - val_loss: 2.8454e-04\n",
      "Epoch 554/2000\n",
      "3622/3622 [==============================] - 1s 229us/step - loss: 1.8294e-04 - val_loss: 2.2076e-04\n",
      "Epoch 555/2000\n",
      "3622/3622 [==============================] - 1s 229us/step - loss: 1.7047e-04 - val_loss: 2.2204e-04\n",
      "Epoch 556/2000\n",
      "3622/3622 [==============================] - 1s 236us/step - loss: 1.7038e-04 - val_loss: 2.2161e-04\n",
      "Epoch 557/2000\n",
      "3622/3622 [==============================] - 1s 230us/step - loss: 1.6837e-04 - val_loss: 2.4683e-04\n",
      "Epoch 558/2000\n",
      "3622/3622 [==============================] - 1s 228us/step - loss: 1.7641e-04 - val_loss: 2.2373e-04\n",
      "Epoch 559/2000\n",
      "3622/3622 [==============================] - 1s 230us/step - loss: 1.7411e-04 - val_loss: 2.5170e-04\n",
      "Epoch 560/2000\n",
      "3622/3622 [==============================] - 1s 229us/step - loss: 1.7797e-04 - val_loss: 2.2357e-04\n",
      "Epoch 561/2000\n",
      "3622/3622 [==============================] - 1s 230us/step - loss: 1.6927e-04 - val_loss: 2.6306e-04\n",
      "Epoch 562/2000\n",
      "3622/3622 [==============================] - 1s 230us/step - loss: 1.8485e-04 - val_loss: 2.5364e-04\n",
      "Epoch 563/2000\n",
      "3622/3622 [==============================] - 1s 236us/step - loss: 1.7767e-04 - val_loss: 2.6815e-04\n",
      "Epoch 564/2000\n",
      "3622/3622 [==============================] - 1s 228us/step - loss: 1.8297e-04 - val_loss: 2.3855e-04\n",
      "Epoch 565/2000\n",
      "3622/3622 [==============================] - 1s 231us/step - loss: 1.7250e-04 - val_loss: 3.4292e-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 566/2000\n",
      "3622/3622 [==============================] - 1s 229us/step - loss: 1.9970e-04 - val_loss: 2.2261e-04\n",
      "Epoch 567/2000\n",
      "3622/3622 [==============================] - 1s 229us/step - loss: 1.7866e-04 - val_loss: 2.8052e-04\n",
      "Epoch 568/2000\n",
      "3622/3622 [==============================] - 1s 229us/step - loss: 1.8130e-04 - val_loss: 2.8983e-04\n",
      "Epoch 569/2000\n",
      "3622/3622 [==============================] - 1s 230us/step - loss: 2.0081e-04 - val_loss: 2.4828e-04\n",
      "Epoch 570/2000\n",
      "3622/3622 [==============================] - 1s 229us/step - loss: 1.8591e-04 - val_loss: 2.2434e-04\n",
      "Epoch 571/2000\n",
      "3622/3622 [==============================] - 1s 229us/step - loss: 1.7287e-04 - val_loss: 2.9923e-04\n",
      "Epoch 572/2000\n",
      "3622/3622 [==============================] - 1s 229us/step - loss: 1.8342e-04 - val_loss: 3.0829e-04\n",
      "Epoch 573/2000\n",
      "3622/3622 [==============================] - 1s 227us/step - loss: 1.8562e-04 - val_loss: 2.2450e-04\n",
      "Epoch 574/2000\n",
      "3622/3622 [==============================] - 1s 229us/step - loss: 1.7095e-04 - val_loss: 2.2383e-04\n",
      "Epoch 575/2000\n",
      "3622/3622 [==============================] - 1s 227us/step - loss: 1.7023e-04 - val_loss: 2.4270e-04\n",
      "Epoch 576/2000\n",
      "3622/3622 [==============================] - 1s 229us/step - loss: 1.7595e-04 - val_loss: 3.2009e-04\n",
      "Epoch 577/2000\n",
      "3622/3622 [==============================] - 1s 230us/step - loss: 1.9330e-04 - val_loss: 2.4483e-04\n",
      "Epoch 578/2000\n",
      "3622/3622 [==============================] - 1s 228us/step - loss: 1.7096e-04 - val_loss: 2.2474e-04\n",
      "Epoch 579/2000\n",
      "3622/3622 [==============================] - 1s 229us/step - loss: 1.6779e-04 - val_loss: 2.7560e-04\n",
      "Epoch 580/2000\n",
      "3622/3622 [==============================] - 1s 231us/step - loss: 1.9253e-04 - val_loss: 2.3223e-04\n",
      "Epoch 581/2000\n",
      "3622/3622 [==============================] - 1s 229us/step - loss: 1.7612e-04 - val_loss: 2.2193e-04\n",
      "Epoch 582/2000\n",
      "3622/3622 [==============================] - 1s 235us/step - loss: 1.7068e-04 - val_loss: 4.0128e-04\n",
      "Epoch 583/2000\n",
      "3622/3622 [==============================] - 1s 230us/step - loss: 2.3054e-04 - val_loss: 3.6931e-04\n",
      "Epoch 584/2000\n",
      "3622/3622 [==============================] - 1s 228us/step - loss: 2.0811e-04 - val_loss: 6.6849e-04\n",
      "Epoch 585/2000\n",
      "3622/3622 [==============================] - 1s 230us/step - loss: 2.8182e-04 - val_loss: 2.2283e-04\n",
      "Epoch 586/2000\n",
      "3622/3622 [==============================] - 1s 230us/step - loss: 1.8782e-04 - val_loss: 2.2759e-04\n",
      "Epoch 587/2000\n",
      "3622/3622 [==============================] - 1s 230us/step - loss: 1.7628e-04 - val_loss: 2.2383e-04\n",
      "Epoch 588/2000\n",
      "3622/3622 [==============================] - 1s 229us/step - loss: 1.7223e-04 - val_loss: 2.9260e-04\n",
      "Epoch 589/2000\n",
      "3622/3622 [==============================] - 1s 227us/step - loss: 1.8315e-04 - val_loss: 3.2125e-04\n",
      "Epoch 590/2000\n",
      "3622/3622 [==============================] - 1s 229us/step - loss: 2.1248e-04 - val_loss: 2.6847e-04\n",
      "Epoch 591/2000\n",
      "3622/3622 [==============================] - 1s 230us/step - loss: 2.0129e-04 - val_loss: 2.2387e-04\n",
      "Epoch 592/2000\n",
      "3622/3622 [==============================] - 1s 236us/step - loss: 1.7528e-04 - val_loss: 2.8942e-04\n",
      "Epoch 593/2000\n",
      "3622/3622 [==============================] - 1s 230us/step - loss: 2.1544e-04 - val_loss: 2.2835e-04\n",
      "Epoch 594/2000\n",
      "3622/3622 [==============================] - 1s 227us/step - loss: 1.9363e-04 - val_loss: 3.4532e-04\n",
      "Epoch 595/2000\n",
      "3622/3622 [==============================] - 1s 227us/step - loss: 2.1081e-04 - val_loss: 3.5899e-04\n",
      "Epoch 596/2000\n",
      "3622/3622 [==============================] - 1s 227us/step - loss: 2.0787e-04 - val_loss: 2.7355e-04\n",
      "Epoch 597/2000\n",
      "3622/3622 [==============================] - 1s 230us/step - loss: 1.9699e-04 - val_loss: 3.6531e-04\n",
      "Epoch 598/2000\n",
      "3622/3622 [==============================] - 1s 229us/step - loss: 2.1171e-04 - val_loss: 2.6934e-04\n",
      "Epoch 599/2000\n",
      "3622/3622 [==============================] - 1s 228us/step - loss: 1.7577e-04 - val_loss: 2.5243e-04\n",
      "Epoch 600/2000\n",
      "3622/3622 [==============================] - 1s 227us/step - loss: 1.8282e-04 - val_loss: 2.3283e-04\n",
      "Epoch 601/2000\n",
      "3622/3622 [==============================] - 1s 231us/step - loss: 1.8446e-04 - val_loss: 2.2723e-04\n",
      "Epoch 602/2000\n",
      "3622/3622 [==============================] - 1s 232us/step - loss: 1.6890e-04 - val_loss: 2.2638e-04\n",
      "Epoch 603/2000\n",
      "3622/3622 [==============================] - 1s 232us/step - loss: 1.6661e-04 - val_loss: 2.2617e-04\n",
      "Epoch 604/2000\n",
      "3622/3622 [==============================] - 1s 228us/step - loss: 1.7129e-04 - val_loss: 2.7122e-04\n",
      "Epoch 605/2000\n",
      "3622/3622 [==============================] - 1s 229us/step - loss: 1.7314e-04 - val_loss: 2.2211e-04\n",
      "Epoch 606/2000\n",
      "3622/3622 [==============================] - 1s 230us/step - loss: 1.7481e-04 - val_loss: 3.4123e-04\n",
      "Epoch 607/2000\n",
      "3622/3622 [==============================] - 1s 229us/step - loss: 2.0617e-04 - val_loss: 2.3634e-04\n",
      "Epoch 608/2000\n",
      "3622/3622 [==============================] - 1s 229us/step - loss: 1.7772e-04 - val_loss: 2.9179e-04\n",
      "Epoch 609/2000\n",
      "3622/3622 [==============================] - 1s 230us/step - loss: 1.8660e-04 - val_loss: 4.6544e-04\n",
      "Epoch 610/2000\n",
      "3622/3622 [==============================] - 1s 228us/step - loss: 2.3833e-04 - val_loss: 2.6681e-04\n",
      "Epoch 611/2000\n",
      "3622/3622 [==============================] - 1s 228us/step - loss: 2.0447e-04 - val_loss: 2.4540e-04\n",
      "Epoch 612/2000\n",
      "3622/3622 [==============================] - 1s 229us/step - loss: 1.8045e-04 - val_loss: 2.2394e-04\n",
      "Epoch 613/2000\n",
      "3622/3622 [==============================] - 1s 229us/step - loss: 1.7131e-04 - val_loss: 2.4913e-04\n",
      "Epoch 614/2000\n",
      "3622/3622 [==============================] - 1s 229us/step - loss: 1.7419e-04 - val_loss: 2.8638e-04\n",
      "Epoch 615/2000\n",
      "3622/3622 [==============================] - 1s 229us/step - loss: 1.9188e-04 - val_loss: 2.3154e-04\n",
      "Epoch 616/2000\n",
      "3622/3622 [==============================] - 1s 230us/step - loss: 1.7557e-04 - val_loss: 3.6105e-04\n",
      "Epoch 617/2000\n",
      "3622/3622 [==============================] - 1s 230us/step - loss: 1.9998e-04 - val_loss: 2.2320e-04\n",
      "Epoch 618/2000\n",
      "3622/3622 [==============================] - 1s 230us/step - loss: 1.7944e-04 - val_loss: 3.7804e-04\n",
      "Epoch 619/2000\n",
      "3622/3622 [==============================] - 1s 227us/step - loss: 2.3916e-04 - val_loss: 2.4315e-04\n",
      "Epoch 620/2000\n",
      "3622/3622 [==============================] - 1s 229us/step - loss: 1.9341e-04 - val_loss: 2.4899e-04\n",
      "Epoch 621/2000\n",
      "3622/3622 [==============================] - 1s 233us/step - loss: 1.7523e-04 - val_loss: 2.8505e-04\n",
      "Epoch 622/2000\n",
      "3622/3622 [==============================] - 1s 231us/step - loss: 2.1066e-04 - val_loss: 2.2984e-04\n",
      "Epoch 623/2000\n",
      "3622/3622 [==============================] - 1s 230us/step - loss: 1.7235e-04 - val_loss: 3.4065e-04\n",
      "Epoch 624/2000\n",
      "3622/3622 [==============================] - 1s 228us/step - loss: 1.9841e-04 - val_loss: 2.7849e-04\n",
      "Epoch 625/2000\n",
      "3622/3622 [==============================] - 1s 229us/step - loss: 2.0106e-04 - val_loss: 2.2498e-04\n",
      "Epoch 626/2000\n",
      "3622/3622 [==============================] - 1s 230us/step - loss: 1.7690e-04 - val_loss: 3.7135e-04\n",
      "Epoch 627/2000\n",
      "3622/3622 [==============================] - 1s 230us/step - loss: 2.2020e-04 - val_loss: 2.2227e-04\n",
      "Epoch 628/2000\n",
      "3622/3622 [==============================] - 1s 236us/step - loss: 1.9449e-04 - val_loss: 2.4919e-04\n",
      "Epoch 629/2000\n",
      "3622/3622 [==============================] - 1s 229us/step - loss: 1.7580e-04 - val_loss: 2.7973e-04\n",
      "Epoch 630/2000\n",
      "3622/3622 [==============================] - 1s 230us/step - loss: 1.7513e-04 - val_loss: 2.2347e-04\n",
      "Epoch 631/2000\n",
      "3622/3622 [==============================] - 1s 228us/step - loss: 1.7268e-04 - val_loss: 3.2234e-04\n",
      "Epoch 632/2000\n",
      "3622/3622 [==============================] - 1s 230us/step - loss: 1.9112e-04 - val_loss: 2.2330e-04\n",
      "Epoch 633/2000\n",
      "3622/3622 [==============================] - 1s 229us/step - loss: 1.6873e-04 - val_loss: 3.1353e-04\n",
      "Epoch 634/2000\n",
      "3622/3622 [==============================] - 1s 231us/step - loss: 1.8854e-04 - val_loss: 2.2348e-04\n",
      "Epoch 635/2000\n",
      "3622/3622 [==============================] - 1s 228us/step - loss: 1.7604e-04 - val_loss: 3.0087e-04\n",
      "Epoch 636/2000\n",
      "3622/3622 [==============================] - 1s 229us/step - loss: 1.8809e-04 - val_loss: 2.7113e-04\n",
      "Epoch 637/2000\n",
      "3622/3622 [==============================] - 1s 230us/step - loss: 1.8050e-04 - val_loss: 2.3056e-04\n",
      "Epoch 638/2000\n",
      "3622/3622 [==============================] - 1s 228us/step - loss: 1.9118e-04 - val_loss: 3.2397e-04\n",
      "Epoch 639/2000\n",
      "3622/3622 [==============================] - 1s 230us/step - loss: 1.9522e-04 - val_loss: 2.2334e-04\n",
      "Epoch 640/2000\n",
      "3622/3622 [==============================] - 1s 234us/step - loss: 1.7977e-04 - val_loss: 2.2743e-04\n",
      "Epoch 641/2000\n",
      "3622/3622 [==============================] - 1s 228us/step - loss: 1.7722e-04 - val_loss: 2.2629e-04\n",
      "Epoch 642/2000\n",
      "3622/3622 [==============================] - 1s 229us/step - loss: 1.8367e-04 - val_loss: 2.8105e-04\n",
      "Epoch 643/2000\n",
      "3622/3622 [==============================] - 1s 228us/step - loss: 1.8950e-04 - val_loss: 2.2401e-04\n",
      "Epoch 644/2000\n",
      "3622/3622 [==============================] - 1s 228us/step - loss: 1.6688e-04 - val_loss: 2.8465e-04\n",
      "Epoch 645/2000\n",
      "3622/3622 [==============================] - 1s 228us/step - loss: 1.7852e-04 - val_loss: 2.4369e-04\n",
      "Epoch 646/2000\n",
      "3622/3622 [==============================] - 1s 227us/step - loss: 1.7528e-04 - val_loss: 2.4207e-04\n",
      "Epoch 647/2000\n",
      "3622/3622 [==============================] - 1s 228us/step - loss: 1.7634e-04 - val_loss: 2.2343e-04\n",
      "Epoch 648/2000\n",
      "3622/3622 [==============================] - 1s 228us/step - loss: 1.6834e-04 - val_loss: 2.6509e-04\n",
      "Epoch 649/2000\n",
      "3622/3622 [==============================] - 1s 228us/step - loss: 1.6913e-04 - val_loss: 2.2380e-04\n",
      "Epoch 650/2000\n",
      "3622/3622 [==============================] - 1s 229us/step - loss: 1.7202e-04 - val_loss: 2.2989e-04\n",
      "Epoch 651/2000\n",
      "3622/3622 [==============================] - 1s 229us/step - loss: 1.7096e-04 - val_loss: 2.8172e-04\n",
      "Epoch 652/2000\n",
      "3622/3622 [==============================] - 1s 230us/step - loss: 1.8434e-04 - val_loss: 3.4681e-04\n",
      "Epoch 653/2000\n",
      "3622/3622 [==============================] - 1s 228us/step - loss: 2.0287e-04 - val_loss: 2.3211e-04\n",
      "Epoch 654/2000\n",
      "3622/3622 [==============================] - 1s 229us/step - loss: 1.8682e-04 - val_loss: 2.2231e-04\n",
      "Epoch 655/2000\n",
      "3622/3622 [==============================] - 1s 229us/step - loss: 1.6699e-04 - val_loss: 2.4025e-04\n",
      "Epoch 656/2000\n",
      "3622/3622 [==============================] - 1s 229us/step - loss: 1.8075e-04 - val_loss: 2.4120e-04\n",
      "Epoch 657/2000\n",
      "3622/3622 [==============================] - 1s 228us/step - loss: 1.7041e-04 - val_loss: 2.3087e-04\n",
      "Epoch 658/2000\n",
      "3622/3622 [==============================] - 1s 229us/step - loss: 1.7810e-04 - val_loss: 2.6621e-04\n",
      "Epoch 659/2000\n",
      "3622/3622 [==============================] - 1s 234us/step - loss: 1.8781e-04 - val_loss: 2.3242e-04\n",
      "Epoch 660/2000\n",
      "3622/3622 [==============================] - 1s 228us/step - loss: 1.8353e-04 - val_loss: 3.0665e-04\n",
      "Epoch 661/2000\n",
      "3622/3622 [==============================] - 1s 228us/step - loss: 1.8669e-04 - val_loss: 2.6304e-04\n",
      "Epoch 662/2000\n",
      "3622/3622 [==============================] - 1s 227us/step - loss: 1.7586e-04 - val_loss: 2.2163e-04\n",
      "Epoch 663/2000\n",
      "3622/3622 [==============================] - 1s 233us/step - loss: 1.6582e-04 - val_loss: 2.8535e-04\n",
      "Epoch 664/2000\n",
      "3622/3622 [==============================] - 1s 235us/step - loss: 1.7127e-04 - val_loss: 2.2140e-04\n",
      "Epoch 665/2000\n",
      "3622/3622 [==============================] - 1s 229us/step - loss: 1.7191e-04 - val_loss: 2.2212e-04\n",
      "Epoch 666/2000\n",
      "3622/3622 [==============================] - 1s 229us/step - loss: 1.7156e-04 - val_loss: 2.4602e-04\n",
      "Epoch 667/2000\n",
      "3622/3622 [==============================] - 1s 230us/step - loss: 1.7809e-04 - val_loss: 2.6588e-04\n",
      "Epoch 668/2000\n",
      "3622/3622 [==============================] - 1s 229us/step - loss: 1.8610e-04 - val_loss: 2.3804e-04\n",
      "Epoch 669/2000\n",
      "3622/3622 [==============================] - 1s 230us/step - loss: 1.7317e-04 - val_loss: 2.5357e-04\n",
      "Epoch 670/2000\n",
      "3622/3622 [==============================] - 1s 228us/step - loss: 1.8492e-04 - val_loss: 3.3371e-04\n",
      "Epoch 671/2000\n",
      "3622/3622 [==============================] - 1s 228us/step - loss: 2.1001e-04 - val_loss: 2.3201e-04\n",
      "Epoch 672/2000\n",
      "3622/3622 [==============================] - 1s 232us/step - loss: 2.0737e-04 - val_loss: 2.4952e-04\n",
      "Epoch 673/2000\n",
      "3622/3622 [==============================] - 1s 229us/step - loss: 1.8169e-04 - val_loss: 2.2770e-04\n",
      "Epoch 674/2000\n",
      "3622/3622 [==============================] - 1s 228us/step - loss: 1.6717e-04 - val_loss: 2.2442e-04\n",
      "Epoch 675/2000\n",
      "3622/3622 [==============================] - 1s 229us/step - loss: 1.6758e-04 - val_loss: 2.2247e-04\n",
      "Epoch 676/2000\n",
      "3622/3622 [==============================] - 1s 229us/step - loss: 1.6843e-04 - val_loss: 2.4353e-04\n",
      "Epoch 677/2000\n",
      "3622/3622 [==============================] - 1s 228us/step - loss: 1.7508e-04 - val_loss: 2.9797e-04\n",
      "Epoch 678/2000\n",
      "3622/3622 [==============================] - 1s 234us/step - loss: 1.7452e-04 - val_loss: 2.6652e-04\n",
      "Epoch 679/2000\n",
      "3622/3622 [==============================] - 1s 230us/step - loss: 1.7869e-04 - val_loss: 3.0324e-04\n",
      "Epoch 680/2000\n",
      "3622/3622 [==============================] - 1s 230us/step - loss: 1.9481e-04 - val_loss: 2.2203e-04\n",
      "Epoch 681/2000\n",
      "3622/3622 [==============================] - 1s 229us/step - loss: 1.6500e-04 - val_loss: 2.2282e-04\n",
      "Epoch 682/2000\n",
      "3622/3622 [==============================] - 1s 234us/step - loss: 1.6583e-04 - val_loss: 2.4578e-04\n",
      "Epoch 683/2000\n",
      "3622/3622 [==============================] - 1s 230us/step - loss: 1.7848e-04 - val_loss: 3.1773e-04\n",
      "Epoch 684/2000\n",
      "3622/3622 [==============================] - 1s 230us/step - loss: 1.9034e-04 - val_loss: 2.3458e-04\n",
      "Epoch 685/2000\n",
      "3622/3622 [==============================] - 1s 229us/step - loss: 1.7538e-04 - val_loss: 2.3429e-04\n",
      "Epoch 686/2000\n",
      "3622/3622 [==============================] - 1s 229us/step - loss: 1.7028e-04 - val_loss: 3.3726e-04\n",
      "Epoch 687/2000\n",
      "3622/3622 [==============================] - 1s 227us/step - loss: 2.1039e-04 - val_loss: 2.9548e-04\n",
      "Epoch 688/2000\n",
      "3622/3622 [==============================] - 1s 232us/step - loss: 2.1165e-04 - val_loss: 2.2310e-04\n",
      "Epoch 689/2000\n",
      "3622/3622 [==============================] - 1s 230us/step - loss: 1.9036e-04 - val_loss: 2.8981e-04\n",
      "Epoch 690/2000\n",
      "3622/3622 [==============================] - 1s 230us/step - loss: 2.1069e-04 - val_loss: 2.6189e-04\n",
      "Epoch 691/2000\n",
      "3622/3622 [==============================] - 1s 229us/step - loss: 1.9547e-04 - val_loss: 4.2783e-04\n",
      "Epoch 692/2000\n",
      "3622/3622 [==============================] - 1s 235us/step - loss: 2.2655e-04 - val_loss: 2.8928e-04\n",
      "Epoch 693/2000\n",
      "3622/3622 [==============================] - 1s 229us/step - loss: 1.8819e-04 - val_loss: 2.2067e-04\n",
      "Epoch 694/2000\n",
      "3622/3622 [==============================] - 1s 229us/step - loss: 1.7677e-04 - val_loss: 3.6244e-04\n",
      "Epoch 695/2000\n",
      "3622/3622 [==============================] - 1s 230us/step - loss: 2.1934e-04 - val_loss: 2.2102e-04\n",
      "Epoch 696/2000\n",
      "3622/3622 [==============================] - 1s 229us/step - loss: 1.7767e-04 - val_loss: 2.2164e-04\n",
      "Epoch 697/2000\n",
      "3622/3622 [==============================] - 1s 234us/step - loss: 1.6964e-04 - val_loss: 2.2421e-04\n",
      "Epoch 698/2000\n",
      "3622/3622 [==============================] - 1s 229us/step - loss: 1.7179e-04 - val_loss: 2.7457e-04\n",
      "Epoch 699/2000\n",
      "3622/3622 [==============================] - 1s 235us/step - loss: 1.8920e-04 - val_loss: 2.4831e-04\n",
      "Epoch 700/2000\n",
      "3622/3622 [==============================] - 1s 230us/step - loss: 1.8803e-04 - val_loss: 2.3477e-04\n",
      "Epoch 701/2000\n",
      "3622/3622 [==============================] - 1s 228us/step - loss: 1.8166e-04 - val_loss: 2.8154e-04\n",
      "Epoch 702/2000\n",
      "3622/3622 [==============================] - 1s 229us/step - loss: 2.0448e-04 - val_loss: 2.8805e-04\n",
      "Epoch 703/2000\n",
      "3622/3622 [==============================] - 1s 230us/step - loss: 1.9097e-04 - val_loss: 3.5689e-04\n",
      "Epoch 704/2000\n",
      "3622/3622 [==============================] - 1s 228us/step - loss: 2.1070e-04 - val_loss: 2.5303e-04\n",
      "Epoch 705/2000\n",
      "3622/3622 [==============================] - 1s 229us/step - loss: 1.9071e-04 - val_loss: 2.4945e-04\n",
      "Epoch 706/2000\n",
      "3622/3622 [==============================] - 1s 228us/step - loss: 1.7544e-04 - val_loss: 2.7408e-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 707/2000\n",
      "3622/3622 [==============================] - 1s 229us/step - loss: 1.7780e-04 - val_loss: 2.2324e-04\n",
      "Epoch 708/2000\n",
      "3622/3622 [==============================] - 1s 228us/step - loss: 1.7036e-04 - val_loss: 2.2388e-04\n",
      "Epoch 709/2000\n",
      "3622/3622 [==============================] - 1s 229us/step - loss: 1.7263e-04 - val_loss: 2.4312e-04\n",
      "Epoch 710/2000\n",
      "3622/3622 [==============================] - 1s 228us/step - loss: 1.6845e-04 - val_loss: 2.2453e-04\n",
      "Epoch 711/2000\n",
      "3622/3622 [==============================] - 1s 228us/step - loss: 1.6677e-04 - val_loss: 2.2824e-04\n",
      "Epoch 712/2000\n",
      "3622/3622 [==============================] - 1s 231us/step - loss: 1.6761e-04 - val_loss: 2.5288e-04\n",
      "Epoch 713/2000\n",
      "3622/3622 [==============================] - 1s 230us/step - loss: 1.8180e-04 - val_loss: 2.2095e-04\n",
      "Epoch 714/2000\n",
      "3622/3622 [==============================] - 1s 229us/step - loss: 1.7401e-04 - val_loss: 3.5263e-04\n",
      "Epoch 715/2000\n",
      "3622/3622 [==============================] - 1s 228us/step - loss: 2.0069e-04 - val_loss: 2.2516e-04\n",
      "Epoch 716/2000\n",
      "3622/3622 [==============================] - 1s 231us/step - loss: 1.9093e-04 - val_loss: 2.3930e-04\n",
      "Epoch 717/2000\n",
      "3622/3622 [==============================] - 1s 229us/step - loss: 1.7575e-04 - val_loss: 2.2229e-04\n",
      "Epoch 718/2000\n",
      "3622/3622 [==============================] - 1s 228us/step - loss: 1.6714e-04 - val_loss: 2.2106e-04\n",
      "Epoch 719/2000\n",
      "3622/3622 [==============================] - 1s 230us/step - loss: 1.7013e-04 - val_loss: 2.6611e-04\n",
      "Epoch 720/2000\n",
      "3622/3622 [==============================] - 1s 227us/step - loss: 1.7335e-04 - val_loss: 2.8624e-04\n",
      "Epoch 721/2000\n",
      "3622/3622 [==============================] - 1s 229us/step - loss: 1.7405e-04 - val_loss: 2.4564e-04\n",
      "Epoch 722/2000\n",
      "3622/3622 [==============================] - 1s 228us/step - loss: 1.7098e-04 - val_loss: 2.2990e-04\n",
      "Epoch 723/2000\n",
      "3622/3622 [==============================] - 1s 229us/step - loss: 1.7014e-04 - val_loss: 2.3429e-04\n",
      "Epoch 724/2000\n",
      "3622/3622 [==============================] - 1s 229us/step - loss: 1.6933e-04 - val_loss: 2.4130e-04\n",
      "Epoch 725/2000\n",
      "3622/3622 [==============================] - 1s 230us/step - loss: 1.7665e-04 - val_loss: 3.7155e-04\n",
      "Epoch 726/2000\n",
      "3622/3622 [==============================] - 1s 229us/step - loss: 1.9792e-04 - val_loss: 2.3844e-04\n",
      "Epoch 727/2000\n",
      "3622/3622 [==============================] - 1s 230us/step - loss: 1.7555e-04 - val_loss: 2.2714e-04\n",
      "Epoch 728/2000\n",
      "3622/3622 [==============================] - 1s 227us/step - loss: 1.6991e-04 - val_loss: 2.2317e-04\n",
      "Epoch 729/2000\n",
      "3622/3622 [==============================] - 1s 232us/step - loss: 1.6970e-04 - val_loss: 2.2188e-04\n",
      "Epoch 730/2000\n",
      "3622/3622 [==============================] - 1s 229us/step - loss: 1.7091e-04 - val_loss: 2.4651e-04\n",
      "Epoch 731/2000\n",
      "3622/3622 [==============================] - 1s 229us/step - loss: 1.8309e-04 - val_loss: 2.2406e-04\n",
      "Epoch 732/2000\n",
      "3622/3622 [==============================] - 1s 228us/step - loss: 1.6575e-04 - val_loss: 2.7192e-04\n",
      "Epoch 733/2000\n",
      "3622/3622 [==============================] - 1s 230us/step - loss: 1.7660e-04 - val_loss: 2.3070e-04\n",
      "Epoch 734/2000\n",
      "3622/3622 [==============================] - 1s 229us/step - loss: 1.7760e-04 - val_loss: 2.8861e-04\n",
      "Epoch 735/2000\n",
      "3622/3622 [==============================] - 1s 236us/step - loss: 1.9657e-04 - val_loss: 2.2301e-04\n",
      "Epoch 736/2000\n",
      "3622/3622 [==============================] - 1s 236us/step - loss: 1.6870e-04 - val_loss: 2.9119e-04\n",
      "Epoch 737/2000\n",
      "3622/3622 [==============================] - 1s 229us/step - loss: 1.7781e-04 - val_loss: 2.2829e-04\n",
      "Epoch 738/2000\n",
      "3622/3622 [==============================] - 1s 229us/step - loss: 1.7236e-04 - val_loss: 2.3099e-04\n",
      "Epoch 739/2000\n",
      "3622/3622 [==============================] - 1s 229us/step - loss: 1.6889e-04 - val_loss: 2.5942e-04\n",
      "Epoch 740/2000\n",
      "3622/3622 [==============================] - 1s 228us/step - loss: 1.7133e-04 - val_loss: 2.2957e-04\n",
      "Epoch 741/2000\n",
      "3622/3622 [==============================] - 1s 227us/step - loss: 1.7468e-04 - val_loss: 2.3312e-04\n",
      "Epoch 742/2000\n",
      "3622/3622 [==============================] - 1s 229us/step - loss: 1.6451e-04 - val_loss: 2.3154e-04\n",
      "Epoch 743/2000\n",
      "3622/3622 [==============================] - 1s 228us/step - loss: 1.6764e-04 - val_loss: 2.2605e-04\n",
      "Epoch 744/2000\n",
      "3622/3622 [==============================] - 1s 228us/step - loss: 1.6548e-04 - val_loss: 2.5077e-04\n",
      "Epoch 745/2000\n",
      "3622/3622 [==============================] - 1s 230us/step - loss: 1.8734e-04 - val_loss: 2.2710e-04\n",
      "Epoch 746/2000\n",
      "3622/3622 [==============================] - 1s 229us/step - loss: 1.6820e-04 - val_loss: 2.3699e-04\n",
      "Epoch 747/2000\n",
      "3622/3622 [==============================] - 1s 228us/step - loss: 1.6857e-04 - val_loss: 2.2138e-04\n",
      "Epoch 748/2000\n",
      "3622/3622 [==============================] - 1s 231us/step - loss: 1.7049e-04 - val_loss: 2.2944e-04\n",
      "Epoch 749/2000\n",
      "3622/3622 [==============================] - 1s 228us/step - loss: 1.8029e-04 - val_loss: 4.3986e-04\n",
      "Epoch 750/2000\n",
      "3622/3622 [==============================] - 1s 227us/step - loss: 2.2244e-04 - val_loss: 2.8306e-04\n",
      "Epoch 751/2000\n",
      "3622/3622 [==============================] - 1s 229us/step - loss: 2.1260e-04 - val_loss: 2.2078e-04\n",
      "Epoch 752/2000\n",
      "3622/3622 [==============================] - 1s 229us/step - loss: 1.8788e-04 - val_loss: 2.6666e-04\n",
      "Epoch 753/2000\n",
      "3622/3622 [==============================] - 1s 229us/step - loss: 1.8627e-04 - val_loss: 2.9703e-04\n",
      "Epoch 754/2000\n",
      "3622/3622 [==============================] - 1s 230us/step - loss: 2.0115e-04 - val_loss: 2.5811e-04\n",
      "Epoch 755/2000\n",
      "3622/3622 [==============================] - 1s 234us/step - loss: 2.1369e-04 - val_loss: 2.5193e-04\n",
      "Epoch 756/2000\n",
      "3622/3622 [==============================] - 1s 229us/step - loss: 1.9864e-04 - val_loss: 2.7235e-04\n",
      "Epoch 757/2000\n",
      "3622/3622 [==============================] - 1s 228us/step - loss: 1.7619e-04 - val_loss: 2.2471e-04\n",
      "Epoch 758/2000\n",
      "3622/3622 [==============================] - 1s 227us/step - loss: 1.7063e-04 - val_loss: 2.3120e-04\n",
      "Epoch 759/2000\n",
      "3622/3622 [==============================] - 1s 228us/step - loss: 1.7164e-04 - val_loss: 2.3371e-04\n",
      "Epoch 760/2000\n",
      "3622/3622 [==============================] - 1s 227us/step - loss: 1.7334e-04 - val_loss: 2.2517e-04\n",
      "Epoch 761/2000\n",
      "3622/3622 [==============================] - 1s 228us/step - loss: 1.6677e-04 - val_loss: 2.2255e-04\n",
      "Epoch 762/2000\n",
      "3622/3622 [==============================] - 1s 228us/step - loss: 1.7031e-04 - val_loss: 2.3904e-04\n",
      "Epoch 763/2000\n",
      "3622/3622 [==============================] - 1s 229us/step - loss: 1.7474e-04 - val_loss: 2.3480e-04\n",
      "Epoch 764/2000\n",
      "3622/3622 [==============================] - 1s 229us/step - loss: 1.9235e-04 - val_loss: 2.7021e-04\n",
      "Epoch 765/2000\n",
      "3622/3622 [==============================] - 1s 228us/step - loss: 1.8075e-04 - val_loss: 2.2366e-04\n",
      "Epoch 766/2000\n",
      "3622/3622 [==============================] - 1s 229us/step - loss: 1.6904e-04 - val_loss: 2.6618e-04\n",
      "Epoch 767/2000\n",
      "3622/3622 [==============================] - 1s 229us/step - loss: 1.9089e-04 - val_loss: 2.2415e-04\n",
      "Epoch 768/2000\n",
      "3622/3622 [==============================] - 1s 230us/step - loss: 1.6948e-04 - val_loss: 2.7208e-04\n",
      "Epoch 769/2000\n",
      "3622/3622 [==============================] - 1s 230us/step - loss: 1.7557e-04 - val_loss: 2.2430e-04\n",
      "Epoch 770/2000\n",
      "3622/3622 [==============================] - 1s 228us/step - loss: 1.7524e-04 - val_loss: 3.0618e-04\n",
      "Epoch 771/2000\n",
      "3622/3622 [==============================] - 1s 232us/step - loss: 1.9523e-04 - val_loss: 2.2384e-04\n",
      "Epoch 772/2000\n",
      "3622/3622 [==============================] - 1s 232us/step - loss: 1.7265e-04 - val_loss: 2.3736e-04\n",
      "Epoch 773/2000\n",
      "3622/3622 [==============================] - 1s 230us/step - loss: 1.7898e-04 - val_loss: 4.2647e-04\n",
      "Epoch 774/2000\n",
      "3622/3622 [==============================] - 1s 233us/step - loss: 2.2005e-04 - val_loss: 6.5041e-04\n",
      "Epoch 775/2000\n",
      "3622/3622 [==============================] - 1s 229us/step - loss: 2.8128e-04 - val_loss: 2.2267e-04\n",
      "Epoch 776/2000\n",
      "3622/3622 [==============================] - 1s 229us/step - loss: 1.9840e-04 - val_loss: 2.2160e-04\n",
      "Epoch 777/2000\n",
      "3622/3622 [==============================] - 1s 227us/step - loss: 1.7652e-04 - val_loss: 2.3383e-04\n",
      "Epoch 778/2000\n",
      "3622/3622 [==============================] - 1s 230us/step - loss: 1.7456e-04 - val_loss: 2.6677e-04\n",
      "Epoch 779/2000\n",
      "3622/3622 [==============================] - 1s 228us/step - loss: 1.9951e-04 - val_loss: 3.7898e-04\n",
      "Epoch 780/2000\n",
      "3622/3622 [==============================] - 1s 229us/step - loss: 1.9784e-04 - val_loss: 2.6945e-04\n",
      "Epoch 781/2000\n",
      "3622/3622 [==============================] - 1s 228us/step - loss: 1.7812e-04 - val_loss: 2.5820e-04\n",
      "Epoch 782/2000\n",
      "3622/3622 [==============================] - 1s 228us/step - loss: 1.8411e-04 - val_loss: 2.2050e-04\n",
      "Epoch 783/2000\n",
      "3622/3622 [==============================] - 1s 230us/step - loss: 1.7690e-04 - val_loss: 2.7420e-04\n",
      "Epoch 784/2000\n",
      "3622/3622 [==============================] - 1s 230us/step - loss: 1.8140e-04 - val_loss: 2.7591e-04\n",
      "Epoch 785/2000\n",
      "3622/3622 [==============================] - 1s 230us/step - loss: 1.8742e-04 - val_loss: 2.3339e-04\n",
      "Epoch 786/2000\n",
      "3622/3622 [==============================] - 1s 229us/step - loss: 1.7599e-04 - val_loss: 2.2393e-04\n",
      "Epoch 787/2000\n",
      "3622/3622 [==============================] - 1s 229us/step - loss: 1.6474e-04 - val_loss: 4.4190e-04\n",
      "Epoch 788/2000\n",
      "3622/3622 [==============================] - 1s 228us/step - loss: 2.3823e-04 - val_loss: 2.5799e-04\n",
      "Epoch 789/2000\n",
      "3622/3622 [==============================] - 1s 229us/step - loss: 2.0151e-04 - val_loss: 2.2401e-04\n",
      "Epoch 790/2000\n",
      "3622/3622 [==============================] - 1s 230us/step - loss: 1.7671e-04 - val_loss: 2.7657e-04\n",
      "Epoch 791/2000\n",
      "3622/3622 [==============================] - 1s 228us/step - loss: 1.8020e-04 - val_loss: 2.9400e-04\n",
      "Epoch 792/2000\n",
      "3622/3622 [==============================] - 1s 229us/step - loss: 1.8161e-04 - val_loss: 2.5044e-04\n",
      "Epoch 793/2000\n",
      "3622/3622 [==============================] - 1s 233us/step - loss: 1.6859e-04 - val_loss: 2.3478e-04\n",
      "Epoch 794/2000\n",
      "3622/3622 [==============================] - 1s 230us/step - loss: 1.6441e-04 - val_loss: 2.2379e-04\n",
      "Epoch 795/2000\n",
      "3622/3622 [==============================] - 1s 229us/step - loss: 1.6845e-04 - val_loss: 3.0707e-04\n",
      "Epoch 796/2000\n",
      "3622/3622 [==============================] - 1s 230us/step - loss: 1.9452e-04 - val_loss: 3.1937e-04\n",
      "Epoch 797/2000\n",
      "3622/3622 [==============================] - 1s 229us/step - loss: 2.1634e-04 - val_loss: 2.5391e-04\n",
      "Epoch 798/2000\n",
      "3622/3622 [==============================] - 1s 229us/step - loss: 1.7512e-04 - val_loss: 2.8416e-04\n",
      "Epoch 799/2000\n",
      "3622/3622 [==============================] - 1s 229us/step - loss: 1.8458e-04 - val_loss: 2.6743e-04\n",
      "Epoch 800/2000\n",
      "3622/3622 [==============================] - 1s 229us/step - loss: 1.9752e-04 - val_loss: 2.8988e-04\n",
      "Epoch 801/2000\n",
      "3622/3622 [==============================] - 1s 228us/step - loss: 1.8520e-04 - val_loss: 6.3541e-04\n",
      "Epoch 802/2000\n",
      "3622/3622 [==============================] - 1s 231us/step - loss: 2.8336e-04 - val_loss: 2.3933e-04\n",
      "Epoch 803/2000\n",
      "3622/3622 [==============================] - 1s 229us/step - loss: 2.1766e-04 - val_loss: 7.4399e-04\n",
      "Epoch 804/2000\n",
      "3622/3622 [==============================] - 1s 227us/step - loss: 3.4388e-04 - val_loss: 5.5651e-04\n",
      "Epoch 805/2000\n",
      "3622/3622 [==============================] - 1s 229us/step - loss: 2.7155e-04 - val_loss: 3.4484e-04\n",
      "Epoch 806/2000\n",
      "3622/3622 [==============================] - 1s 231us/step - loss: 2.3012e-04 - val_loss: 2.2917e-04\n",
      "Epoch 807/2000\n",
      "3622/3622 [==============================] - 1s 236us/step - loss: 1.8017e-04 - val_loss: 2.5567e-04\n",
      "Epoch 808/2000\n",
      "3622/3622 [==============================] - 1s 230us/step - loss: 1.8040e-04 - val_loss: 2.4846e-04\n",
      "Epoch 809/2000\n",
      "3622/3622 [==============================] - 1s 230us/step - loss: 1.7023e-04 - val_loss: 2.3111e-04\n",
      "Epoch 810/2000\n",
      "3622/3622 [==============================] - 1s 229us/step - loss: 1.7441e-04 - val_loss: 2.2040e-04\n",
      "Epoch 811/2000\n",
      "3622/3622 [==============================] - 1s 227us/step - loss: 1.6467e-04 - val_loss: 2.2106e-04\n",
      "Epoch 812/2000\n",
      "3622/3622 [==============================] - 1s 234us/step - loss: 1.6717e-04 - val_loss: 2.5899e-04\n",
      "Epoch 813/2000\n",
      "3622/3622 [==============================] - 1s 230us/step - loss: 1.7314e-04 - val_loss: 2.4636e-04\n",
      "Epoch 814/2000\n",
      "3622/3622 [==============================] - 1s 232us/step - loss: 1.7126e-04 - val_loss: 2.3189e-04\n",
      "Epoch 815/2000\n",
      "3622/3622 [==============================] - 1s 231us/step - loss: 1.6823e-04 - val_loss: 3.9742e-04\n",
      "Epoch 816/2000\n",
      "3622/3622 [==============================] - 1s 231us/step - loss: 1.9951e-04 - val_loss: 2.1952e-04\n",
      "Epoch 817/2000\n",
      "3622/3622 [==============================] - 1s 228us/step - loss: 1.6583e-04 - val_loss: 2.2289e-04\n",
      "Epoch 818/2000\n",
      "3622/3622 [==============================] - 1s 230us/step - loss: 1.6220e-04 - val_loss: 2.3227e-04\n",
      "Epoch 819/2000\n",
      "3622/3622 [==============================] - 1s 229us/step - loss: 1.6499e-04 - val_loss: 2.2165e-04\n",
      "Epoch 820/2000\n",
      "3622/3622 [==============================] - 1s 230us/step - loss: 1.7095e-04 - val_loss: 2.2681e-04\n",
      "Epoch 821/2000\n",
      "3622/3622 [==============================] - 1s 229us/step - loss: 1.6935e-04 - val_loss: 2.2147e-04\n",
      "Epoch 822/2000\n",
      "3622/3622 [==============================] - 1s 228us/step - loss: 1.6320e-04 - val_loss: 2.2226e-04\n",
      "Epoch 823/2000\n",
      "3622/3622 [==============================] - 1s 227us/step - loss: 1.6746e-04 - val_loss: 2.2247e-04\n",
      "Epoch 824/2000\n",
      "3622/3622 [==============================] - 1s 231us/step - loss: 1.6751e-04 - val_loss: 2.5040e-04\n",
      "Epoch 825/2000\n",
      "3622/3622 [==============================] - 1s 229us/step - loss: 1.7336e-04 - val_loss: 2.3041e-04\n",
      "Epoch 826/2000\n",
      "3622/3622 [==============================] - 1s 227us/step - loss: 1.7020e-04 - val_loss: 2.5637e-04\n",
      "Epoch 827/2000\n",
      "3622/3622 [==============================] - 1s 230us/step - loss: 1.8578e-04 - val_loss: 2.2188e-04\n",
      "Epoch 828/2000\n",
      "3622/3622 [==============================] - 1s 230us/step - loss: 1.6858e-04 - val_loss: 2.2376e-04\n",
      "Epoch 829/2000\n",
      "3622/3622 [==============================] - 1s 230us/step - loss: 1.6728e-04 - val_loss: 2.4727e-04\n",
      "Epoch 830/2000\n",
      "3622/3622 [==============================] - 1s 230us/step - loss: 1.7806e-04 - val_loss: 2.6381e-04\n",
      "Epoch 831/2000\n",
      "3622/3622 [==============================] - 1s 233us/step - loss: 1.7922e-04 - val_loss: 2.8688e-04\n",
      "Epoch 832/2000\n",
      "3622/3622 [==============================] - 1s 230us/step - loss: 1.7739e-04 - val_loss: 2.4696e-04\n",
      "Epoch 833/2000\n",
      "3622/3622 [==============================] - 1s 229us/step - loss: 1.7942e-04 - val_loss: 2.2395e-04\n",
      "Epoch 834/2000\n",
      "3622/3622 [==============================] - 1s 229us/step - loss: 1.6979e-04 - val_loss: 2.6958e-04\n",
      "Epoch 835/2000\n",
      "3622/3622 [==============================] - 1s 232us/step - loss: 1.7255e-04 - val_loss: 2.3129e-04\n",
      "Epoch 836/2000\n",
      "3622/3622 [==============================] - 1s 229us/step - loss: 1.7034e-04 - val_loss: 2.2078e-04\n",
      "Epoch 837/2000\n",
      "3622/3622 [==============================] - 1s 229us/step - loss: 1.6687e-04 - val_loss: 2.2299e-04\n",
      "Epoch 838/2000\n",
      "3622/3622 [==============================] - 1s 229us/step - loss: 1.6479e-04 - val_loss: 2.3300e-04\n",
      "Epoch 839/2000\n",
      "3622/3622 [==============================] - 1s 231us/step - loss: 1.8141e-04 - val_loss: 2.2446e-04\n",
      "Epoch 840/2000\n",
      "3622/3622 [==============================] - 1s 227us/step - loss: 1.7427e-04 - val_loss: 2.8093e-04\n",
      "Epoch 841/2000\n",
      "3622/3622 [==============================] - 1s 230us/step - loss: 1.7517e-04 - val_loss: 2.2869e-04\n",
      "Epoch 842/2000\n",
      "3622/3622 [==============================] - 1s 230us/step - loss: 1.6704e-04 - val_loss: 2.2707e-04\n",
      "Epoch 843/2000\n",
      "3622/3622 [==============================] - 1s 235us/step - loss: 1.6408e-04 - val_loss: 2.2007e-04\n",
      "Epoch 844/2000\n",
      "3622/3622 [==============================] - 1s 231us/step - loss: 1.6220e-04 - val_loss: 2.4348e-04\n",
      "Epoch 845/2000\n",
      "3622/3622 [==============================] - 1s 227us/step - loss: 1.7523e-04 - val_loss: 2.3731e-04\n",
      "Epoch 846/2000\n",
      "3622/3622 [==============================] - 1s 229us/step - loss: 1.7106e-04 - val_loss: 2.8030e-04\n",
      "Epoch 847/2000\n",
      "3622/3622 [==============================] - 1s 228us/step - loss: 1.7551e-04 - val_loss: 2.4855e-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 848/2000\n",
      "3622/3622 [==============================] - 1s 228us/step - loss: 1.8222e-04 - val_loss: 2.2109e-04\n",
      "Epoch 849/2000\n",
      "3622/3622 [==============================] - 1s 228us/step - loss: 1.6794e-04 - val_loss: 2.3416e-04\n",
      "Epoch 850/2000\n",
      "3622/3622 [==============================] - 1s 234us/step - loss: 1.6235e-04 - val_loss: 2.2574e-04\n",
      "Epoch 851/2000\n",
      "3622/3622 [==============================] - 1s 230us/step - loss: 1.6462e-04 - val_loss: 2.4169e-04\n",
      "Epoch 852/2000\n",
      "3622/3622 [==============================] - 1s 229us/step - loss: 1.6557e-04 - val_loss: 2.3644e-04\n",
      "Epoch 853/2000\n",
      "3622/3622 [==============================] - 1s 227us/step - loss: 1.8706e-04 - val_loss: 2.4509e-04\n",
      "Epoch 854/2000\n",
      "3622/3622 [==============================] - 1s 228us/step - loss: 1.6933e-04 - val_loss: 2.3468e-04\n",
      "Epoch 855/2000\n",
      "3622/3622 [==============================] - 1s 229us/step - loss: 1.6633e-04 - val_loss: 2.3164e-04\n",
      "Epoch 856/2000\n",
      "3622/3622 [==============================] - 1s 227us/step - loss: 1.7837e-04 - val_loss: 2.2328e-04\n",
      "Epoch 857/2000\n",
      "3622/3622 [==============================] - 1s 227us/step - loss: 1.6879e-04 - val_loss: 2.2884e-04\n",
      "Epoch 858/2000\n",
      "3622/3622 [==============================] - 1s 229us/step - loss: 1.6317e-04 - val_loss: 2.2912e-04\n",
      "Epoch 859/2000\n",
      "3622/3622 [==============================] - 1s 229us/step - loss: 1.6808e-04 - val_loss: 2.3756e-04\n",
      "Epoch 860/2000\n",
      "3622/3622 [==============================] - 1s 230us/step - loss: 1.7378e-04 - val_loss: 2.1972e-04\n",
      "Epoch 861/2000\n",
      "3622/3622 [==============================] - 1s 229us/step - loss: 1.6442e-04 - val_loss: 2.4062e-04\n",
      "Epoch 862/2000\n",
      "3622/3622 [==============================] - 1s 229us/step - loss: 1.8728e-04 - val_loss: 3.0951e-04\n",
      "Epoch 863/2000\n",
      "3622/3622 [==============================] - 1s 230us/step - loss: 2.0786e-04 - val_loss: 4.1788e-04\n",
      "Epoch 864/2000\n",
      "3622/3622 [==============================] - 1s 227us/step - loss: 2.3904e-04 - val_loss: 3.4151e-04\n",
      "Epoch 865/2000\n",
      "3622/3622 [==============================] - 1s 231us/step - loss: 1.9712e-04 - val_loss: 2.7952e-04\n",
      "Epoch 866/2000\n",
      "3622/3622 [==============================] - 1s 228us/step - loss: 1.7743e-04 - val_loss: 2.1950e-04\n",
      "Epoch 867/2000\n",
      "3622/3622 [==============================] - 1s 229us/step - loss: 1.6596e-04 - val_loss: 2.8268e-04\n",
      "Epoch 868/2000\n",
      "3622/3622 [==============================] - 1s 229us/step - loss: 1.9032e-04 - val_loss: 2.2885e-04\n",
      "Epoch 869/2000\n",
      "3622/3622 [==============================] - 1s 230us/step - loss: 1.6666e-04 - val_loss: 3.3411e-04\n",
      "Epoch 870/2000\n",
      "3622/3622 [==============================] - 1s 232us/step - loss: 1.8133e-04 - val_loss: 3.9730e-04\n",
      "Epoch 871/2000\n",
      "3622/3622 [==============================] - 1s 231us/step - loss: 2.1585e-04 - val_loss: 2.2587e-04\n",
      "Epoch 872/2000\n",
      "3622/3622 [==============================] - 1s 229us/step - loss: 1.8863e-04 - val_loss: 3.5809e-04\n",
      "Epoch 873/2000\n",
      "3622/3622 [==============================] - 1s 230us/step - loss: 1.9640e-04 - val_loss: 2.2876e-04\n",
      "Epoch 874/2000\n",
      "3622/3622 [==============================] - 1s 232us/step - loss: 1.8164e-04 - val_loss: 2.8354e-04\n",
      "Epoch 875/2000\n",
      "3622/3622 [==============================] - 1s 229us/step - loss: 1.8581e-04 - val_loss: 2.2609e-04\n",
      "Epoch 876/2000\n",
      "3622/3622 [==============================] - 1s 229us/step - loss: 1.7187e-04 - val_loss: 4.5895e-04\n",
      "Epoch 877/2000\n",
      "3622/3622 [==============================] - 1s 230us/step - loss: 2.1627e-04 - val_loss: 3.2481e-04\n",
      "Epoch 878/2000\n",
      "3622/3622 [==============================] - 1s 230us/step - loss: 2.1626e-04 - val_loss: 2.2123e-04\n",
      "Epoch 879/2000\n",
      "3622/3622 [==============================] - 1s 235us/step - loss: 1.8635e-04 - val_loss: 2.2640e-04\n",
      "Epoch 880/2000\n",
      "3622/3622 [==============================] - 1s 230us/step - loss: 1.7433e-04 - val_loss: 2.2948e-04\n",
      "Epoch 881/2000\n",
      "3622/3622 [==============================] - 1s 228us/step - loss: 1.7089e-04 - val_loss: 2.2225e-04\n",
      "Epoch 882/2000\n",
      "3622/3622 [==============================] - 1s 229us/step - loss: 1.8421e-04 - val_loss: 2.3991e-04\n",
      "Epoch 883/2000\n",
      "3622/3622 [==============================] - 1s 231us/step - loss: 1.7302e-04 - val_loss: 2.7126e-04\n",
      "Epoch 884/2000\n",
      "3622/3622 [==============================] - 1s 228us/step - loss: 1.8696e-04 - val_loss: 2.2702e-04\n",
      "Epoch 885/2000\n",
      "3622/3622 [==============================] - 1s 228us/step - loss: 1.7464e-04 - val_loss: 2.4927e-04\n",
      "Epoch 886/2000\n",
      "3622/3622 [==============================] - 1s 228us/step - loss: 1.7209e-04 - val_loss: 2.5099e-04\n",
      "Epoch 887/2000\n",
      "3622/3622 [==============================] - 1s 231us/step - loss: 1.7384e-04 - val_loss: 2.1988e-04\n",
      "Epoch 888/2000\n",
      "3622/3622 [==============================] - 1s 227us/step - loss: 1.7027e-04 - val_loss: 2.2054e-04\n",
      "Epoch 889/2000\n",
      "3622/3622 [==============================] - 1s 233us/step - loss: 1.6399e-04 - val_loss: 3.3731e-04\n",
      "Epoch 890/2000\n",
      "3622/3622 [==============================] - 1s 231us/step - loss: 1.9598e-04 - val_loss: 4.2769e-04\n",
      "Epoch 891/2000\n",
      "3622/3622 [==============================] - 1s 229us/step - loss: 2.1964e-04 - val_loss: 2.2350e-04\n",
      "Epoch 892/2000\n",
      "3622/3622 [==============================] - 1s 229us/step - loss: 1.7958e-04 - val_loss: 2.2187e-04\n",
      "Epoch 893/2000\n",
      "3622/3622 [==============================] - 1s 229us/step - loss: 1.6450e-04 - val_loss: 2.6994e-04\n",
      "Epoch 894/2000\n",
      "3622/3622 [==============================] - 1s 232us/step - loss: 1.7299e-04 - val_loss: 2.7504e-04\n",
      "Epoch 895/2000\n",
      "3622/3622 [==============================] - 1s 229us/step - loss: 1.7444e-04 - val_loss: 2.2193e-04\n",
      "Epoch 896/2000\n",
      "3622/3622 [==============================] - 1s 228us/step - loss: 1.6702e-04 - val_loss: 2.2184e-04\n",
      "Epoch 897/2000\n",
      "3622/3622 [==============================] - 1s 228us/step - loss: 1.6131e-04 - val_loss: 2.2501e-04\n",
      "Epoch 898/2000\n",
      "3622/3622 [==============================] - 1s 229us/step - loss: 1.6407e-04 - val_loss: 2.2705e-04\n",
      "Epoch 899/2000\n",
      "3622/3622 [==============================] - 1s 229us/step - loss: 1.6339e-04 - val_loss: 2.2861e-04\n",
      "Epoch 900/2000\n",
      "3622/3622 [==============================] - 1s 228us/step - loss: 1.6177e-04 - val_loss: 2.2813e-04\n",
      "Epoch 901/2000\n",
      "3622/3622 [==============================] - 1s 229us/step - loss: 1.6023e-04 - val_loss: 2.2195e-04\n",
      "Epoch 902/2000\n",
      "3622/3622 [==============================] - 1s 227us/step - loss: 1.6915e-04 - val_loss: 2.9995e-04\n",
      "Epoch 903/2000\n",
      "3622/3622 [==============================] - 1s 228us/step - loss: 2.0448e-04 - val_loss: 2.5916e-04\n",
      "Epoch 904/2000\n",
      "3622/3622 [==============================] - 1s 228us/step - loss: 1.8290e-04 - val_loss: 3.5999e-04\n",
      "Epoch 905/2000\n",
      "3622/3622 [==============================] - 1s 230us/step - loss: 2.1138e-04 - val_loss: 3.6839e-04\n",
      "Epoch 906/2000\n",
      "3622/3622 [==============================] - 1s 229us/step - loss: 2.3923e-04 - val_loss: 2.7414e-04\n",
      "Epoch 907/2000\n",
      "3622/3622 [==============================] - 1s 230us/step - loss: 2.1570e-04 - val_loss: 2.8957e-04\n",
      "Epoch 908/2000\n",
      "3622/3622 [==============================] - 1s 234us/step - loss: 1.8069e-04 - val_loss: 2.4965e-04\n",
      "Epoch 909/2000\n",
      "3622/3622 [==============================] - 1s 231us/step - loss: 1.7811e-04 - val_loss: 2.2023e-04\n",
      "Epoch 910/2000\n",
      "3622/3622 [==============================] - 1s 229us/step - loss: 1.6865e-04 - val_loss: 2.1950e-04\n",
      "Epoch 911/2000\n",
      "3622/3622 [==============================] - 1s 229us/step - loss: 1.6172e-04 - val_loss: 2.2279e-04\n",
      "Epoch 912/2000\n",
      "3622/3622 [==============================] - 1s 229us/step - loss: 1.6294e-04 - val_loss: 2.2376e-04\n",
      "Epoch 913/2000\n",
      "3622/3622 [==============================] - 1s 231us/step - loss: 1.6485e-04 - val_loss: 2.9652e-04\n",
      "Epoch 914/2000\n",
      "3622/3622 [==============================] - 1s 230us/step - loss: 1.7315e-04 - val_loss: 2.2010e-04\n",
      "Epoch 915/2000\n",
      "3622/3622 [==============================] - 1s 235us/step - loss: 1.6189e-04 - val_loss: 2.2183e-04\n",
      "Epoch 916/2000\n",
      "3622/3622 [==============================] - 1s 228us/step - loss: 1.6936e-04 - val_loss: 2.5285e-04\n",
      "Epoch 917/2000\n",
      "3622/3622 [==============================] - 1s 229us/step - loss: 1.7582e-04 - val_loss: 2.1938e-04\n",
      "Epoch 918/2000\n",
      "3622/3622 [==============================] - 1s 229us/step - loss: 1.6685e-04 - val_loss: 2.2090e-04\n",
      "Epoch 919/2000\n",
      "3622/3622 [==============================] - 1s 229us/step - loss: 1.6877e-04 - val_loss: 2.3213e-04\n",
      "Epoch 920/2000\n",
      "3622/3622 [==============================] - 1s 230us/step - loss: 1.6448e-04 - val_loss: 2.8533e-04\n",
      "Epoch 921/2000\n",
      "3622/3622 [==============================] - 1s 230us/step - loss: 1.7866e-04 - val_loss: 2.6669e-04\n",
      "Epoch 922/2000\n",
      "3622/3622 [==============================] - 1s 228us/step - loss: 1.7543e-04 - val_loss: 2.3783e-04\n",
      "Epoch 923/2000\n",
      "3622/3622 [==============================] - 1s 228us/step - loss: 1.7249e-04 - val_loss: 2.3542e-04\n",
      "Epoch 924/2000\n",
      "3622/3622 [==============================] - 1s 228us/step - loss: 1.6585e-04 - val_loss: 3.3988e-04\n",
      "Epoch 925/2000\n",
      "3622/3622 [==============================] - 1s 230us/step - loss: 1.8683e-04 - val_loss: 2.5598e-04\n",
      "Epoch 926/2000\n",
      "3622/3622 [==============================] - 1s 228us/step - loss: 1.7203e-04 - val_loss: 2.5566e-04\n",
      "Epoch 927/2000\n",
      "3622/3622 [==============================] - 1s 235us/step - loss: 1.8440e-04 - val_loss: 2.2337e-04\n",
      "Epoch 928/2000\n",
      "3622/3622 [==============================] - 1s 227us/step - loss: 1.9227e-04 - val_loss: 2.8071e-04\n",
      "Epoch 929/2000\n",
      "3622/3622 [==============================] - 1s 229us/step - loss: 1.8149e-04 - val_loss: 2.6689e-04\n",
      "Epoch 930/2000\n",
      "3622/3622 [==============================] - 1s 230us/step - loss: 1.7913e-04 - val_loss: 2.1905e-04\n",
      "Epoch 931/2000\n",
      "3622/3622 [==============================] - 1s 229us/step - loss: 1.6620e-04 - val_loss: 2.4593e-04\n",
      "Epoch 932/2000\n",
      "3622/3622 [==============================] - 1s 231us/step - loss: 1.6853e-04 - val_loss: 2.4985e-04\n",
      "Epoch 933/2000\n",
      "3622/3622 [==============================] - 1s 231us/step - loss: 1.6438e-04 - val_loss: 2.2418e-04\n",
      "Epoch 934/2000\n",
      "3622/3622 [==============================] - 1s 229us/step - loss: 1.6700e-04 - val_loss: 2.3764e-04\n",
      "Epoch 935/2000\n",
      "3622/3622 [==============================] - 1s 230us/step - loss: 1.6648e-04 - val_loss: 3.5069e-04\n",
      "Epoch 936/2000\n",
      "3622/3622 [==============================] - 1s 229us/step - loss: 1.9638e-04 - val_loss: 2.9251e-04\n",
      "Epoch 937/2000\n",
      "3622/3622 [==============================] - 1s 229us/step - loss: 1.9622e-04 - val_loss: 2.2217e-04\n",
      "Epoch 938/2000\n",
      "3622/3622 [==============================] - 1s 233us/step - loss: 1.7469e-04 - val_loss: 2.4318e-04\n",
      "Epoch 939/2000\n",
      "3622/3622 [==============================] - 1s 228us/step - loss: 1.6729e-04 - val_loss: 2.4602e-04\n",
      "Epoch 940/2000\n",
      "3622/3622 [==============================] - 1s 230us/step - loss: 1.6883e-04 - val_loss: 2.2330e-04\n",
      "Epoch 941/2000\n",
      "3622/3622 [==============================] - 1s 230us/step - loss: 1.7112e-04 - val_loss: 2.4868e-04\n",
      "Epoch 942/2000\n",
      "3622/3622 [==============================] - 1s 228us/step - loss: 1.8465e-04 - val_loss: 2.7855e-04\n",
      "Epoch 943/2000\n",
      "3622/3622 [==============================] - 1s 229us/step - loss: 1.8112e-04 - val_loss: 2.5806e-04\n",
      "Epoch 944/2000\n",
      "3622/3622 [==============================] - 1s 229us/step - loss: 1.7650e-04 - val_loss: 2.4090e-04\n",
      "Epoch 945/2000\n",
      "3622/3622 [==============================] - 1s 231us/step - loss: 1.6233e-04 - val_loss: 2.2454e-04\n",
      "Epoch 946/2000\n",
      "3622/3622 [==============================] - 1s 234us/step - loss: 1.6040e-04 - val_loss: 2.3299e-04\n",
      "Epoch 947/2000\n",
      "3622/3622 [==============================] - 1s 230us/step - loss: 1.6027e-04 - val_loss: 2.2330e-04\n",
      "Epoch 948/2000\n",
      "3622/3622 [==============================] - 1s 228us/step - loss: 1.6873e-04 - val_loss: 2.4564e-04\n",
      "Epoch 949/2000\n",
      "3622/3622 [==============================] - 1s 229us/step - loss: 1.7281e-04 - val_loss: 3.4210e-04\n",
      "Epoch 950/2000\n",
      "3622/3622 [==============================] - 1s 230us/step - loss: 2.0642e-04 - val_loss: 2.4003e-04\n",
      "Epoch 951/2000\n",
      "3622/3622 [==============================] - 1s 239us/step - loss: 1.9812e-04 - val_loss: 3.3178e-04\n",
      "Epoch 952/2000\n",
      "3622/3622 [==============================] - 1s 236us/step - loss: 2.0560e-04 - val_loss: 2.7975e-04\n",
      "Epoch 953/2000\n",
      "3622/3622 [==============================] - 1s 230us/step - loss: 1.8702e-04 - val_loss: 2.2157e-04\n",
      "Epoch 954/2000\n",
      "3622/3622 [==============================] - 1s 227us/step - loss: 1.6134e-04 - val_loss: 2.2151e-04\n",
      "Epoch 955/2000\n",
      "3622/3622 [==============================] - 1s 230us/step - loss: 1.5901e-04 - val_loss: 2.2234e-04\n",
      "Epoch 956/2000\n",
      "3622/3622 [==============================] - 1s 231us/step - loss: 1.6005e-04 - val_loss: 2.2207e-04\n",
      "Epoch 957/2000\n",
      "3622/3622 [==============================] - 1s 229us/step - loss: 1.6009e-04 - val_loss: 2.5622e-04\n",
      "Epoch 958/2000\n",
      "3622/3622 [==============================] - 1s 231us/step - loss: 1.6482e-04 - val_loss: 2.2781e-04\n",
      "Epoch 959/2000\n",
      "3622/3622 [==============================] - 1s 228us/step - loss: 1.6139e-04 - val_loss: 2.2397e-04\n",
      "Epoch 960/2000\n",
      "3622/3622 [==============================] - 1s 231us/step - loss: 1.6074e-04 - val_loss: 3.3767e-04\n",
      "Epoch 961/2000\n",
      "3622/3622 [==============================] - 1s 230us/step - loss: 2.0565e-04 - val_loss: 2.2307e-04\n",
      "Epoch 962/2000\n",
      "3622/3622 [==============================] - 1s 229us/step - loss: 1.6926e-04 - val_loss: 3.2210e-04\n",
      "Epoch 963/2000\n",
      "3622/3622 [==============================] - 1s 229us/step - loss: 1.8956e-04 - val_loss: 2.2283e-04\n",
      "Epoch 964/2000\n",
      "3622/3622 [==============================] - 1s 229us/step - loss: 1.8942e-04 - val_loss: 2.3477e-04\n",
      "Epoch 965/2000\n",
      "3622/3622 [==============================] - 1s 235us/step - loss: 1.7524e-04 - val_loss: 3.2752e-04\n",
      "Epoch 966/2000\n",
      "3622/3622 [==============================] - 1s 230us/step - loss: 2.0241e-04 - val_loss: 4.0697e-04\n",
      "Epoch 967/2000\n",
      "3622/3622 [==============================] - 1s 229us/step - loss: 2.2978e-04 - val_loss: 2.5353e-04\n",
      "Epoch 968/2000\n",
      "3622/3622 [==============================] - 1s 229us/step - loss: 1.9221e-04 - val_loss: 2.4383e-04\n",
      "Epoch 969/2000\n",
      "3622/3622 [==============================] - 1s 230us/step - loss: 1.7476e-04 - val_loss: 2.2346e-04\n",
      "Epoch 970/2000\n",
      "3622/3622 [==============================] - 1s 228us/step - loss: 1.6683e-04 - val_loss: 2.4158e-04\n",
      "Epoch 971/2000\n",
      "3622/3622 [==============================] - 1s 230us/step - loss: 1.6421e-04 - val_loss: 2.2728e-04\n",
      "Epoch 972/2000\n",
      "3622/3622 [==============================] - 1s 229us/step - loss: 1.6318e-04 - val_loss: 2.7570e-04\n",
      "Epoch 973/2000\n",
      "3622/3622 [==============================] - 1s 229us/step - loss: 1.7406e-04 - val_loss: 3.2390e-04\n",
      "Epoch 974/2000\n",
      "3622/3622 [==============================] - 1s 228us/step - loss: 1.8465e-04 - val_loss: 2.2368e-04\n",
      "Epoch 975/2000\n",
      "3622/3622 [==============================] - 1s 231us/step - loss: 1.8796e-04 - val_loss: 2.2354e-04\n",
      "Epoch 976/2000\n",
      "3622/3622 [==============================] - 1s 230us/step - loss: 1.6399e-04 - val_loss: 2.6447e-04\n",
      "Epoch 977/2000\n",
      "3622/3622 [==============================] - 1s 230us/step - loss: 1.6985e-04 - val_loss: 2.2268e-04\n",
      "Epoch 978/2000\n",
      "3622/3622 [==============================] - 1s 229us/step - loss: 1.6878e-04 - val_loss: 2.2666e-04\n",
      "Epoch 979/2000\n",
      "3622/3622 [==============================] - 1s 228us/step - loss: 1.6618e-04 - val_loss: 2.7315e-04\n",
      "Epoch 980/2000\n",
      "3622/3622 [==============================] - 1s 229us/step - loss: 1.7181e-04 - val_loss: 2.3279e-04\n",
      "Epoch 981/2000\n",
      "3622/3622 [==============================] - 1s 230us/step - loss: 1.7837e-04 - val_loss: 2.2317e-04\n",
      "Epoch 982/2000\n",
      "3622/3622 [==============================] - 1s 230us/step - loss: 1.6361e-04 - val_loss: 2.3916e-04\n",
      "Epoch 983/2000\n",
      "3622/3622 [==============================] - 1s 230us/step - loss: 1.7194e-04 - val_loss: 2.3120e-04\n",
      "Epoch 984/2000\n",
      "3622/3622 [==============================] - 1s 232us/step - loss: 1.6174e-04 - val_loss: 2.8303e-04\n",
      "Epoch 985/2000\n",
      "3622/3622 [==============================] - 1s 232us/step - loss: 1.7932e-04 - val_loss: 2.6578e-04\n",
      "Epoch 986/2000\n",
      "3622/3622 [==============================] - 1s 232us/step - loss: 1.6779e-04 - val_loss: 2.2126e-04\n",
      "Epoch 987/2000\n",
      "3622/3622 [==============================] - 1s 234us/step - loss: 1.6098e-04 - val_loss: 2.2362e-04\n",
      "Epoch 988/2000\n",
      "3622/3622 [==============================] - 1s 227us/step - loss: 1.5808e-04 - val_loss: 2.8681e-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 989/2000\n",
      "3622/3622 [==============================] - 1s 229us/step - loss: 1.8295e-04 - val_loss: 2.2136e-04\n",
      "Epoch 990/2000\n",
      "3622/3622 [==============================] - 1s 229us/step - loss: 1.6533e-04 - val_loss: 2.2791e-04\n",
      "Epoch 991/2000\n",
      "3622/3622 [==============================] - 1s 227us/step - loss: 1.6254e-04 - val_loss: 2.7385e-04\n",
      "Epoch 992/2000\n",
      "3622/3622 [==============================] - 1s 229us/step - loss: 1.8873e-04 - val_loss: 2.3167e-04\n",
      "Epoch 993/2000\n",
      "3622/3622 [==============================] - 1s 229us/step - loss: 1.7265e-04 - val_loss: 2.9495e-04\n",
      "Epoch 994/2000\n",
      "3622/3622 [==============================] - 1s 230us/step - loss: 1.8523e-04 - val_loss: 2.2781e-04\n",
      "Epoch 995/2000\n",
      "3622/3622 [==============================] - 1s 227us/step - loss: 1.6202e-04 - val_loss: 2.7834e-04\n",
      "Epoch 996/2000\n",
      "3622/3622 [==============================] - 1s 229us/step - loss: 1.8323e-04 - val_loss: 4.0791e-04\n",
      "Epoch 997/2000\n",
      "3622/3622 [==============================] - 1s 231us/step - loss: 2.2711e-04 - val_loss: 2.3964e-04\n",
      "Epoch 998/2000\n",
      "3622/3622 [==============================] - 1s 227us/step - loss: 1.8144e-04 - val_loss: 2.2876e-04\n",
      "Epoch 999/2000\n",
      "3622/3622 [==============================] - 1s 228us/step - loss: 1.6554e-04 - val_loss: 2.8985e-04\n",
      "Epoch 1000/2000\n",
      "3622/3622 [==============================] - 1s 230us/step - loss: 1.8353e-04 - val_loss: 2.3207e-04\n",
      "Epoch 1001/2000\n",
      "3622/3622 [==============================] - 1s 227us/step - loss: 1.8227e-04 - val_loss: 2.5510e-04\n",
      "Epoch 1002/2000\n",
      "3622/3622 [==============================] - 1s 227us/step - loss: 1.6574e-04 - val_loss: 2.5817e-04\n",
      "Epoch 1003/2000\n",
      "3622/3622 [==============================] - 1s 232us/step - loss: 1.6572e-04 - val_loss: 2.2426e-04\n",
      "Epoch 1004/2000\n",
      "3622/3622 [==============================] - 1s 232us/step - loss: 1.6026e-04 - val_loss: 2.2104e-04\n",
      "Epoch 1005/2000\n",
      "3622/3622 [==============================] - 1s 231us/step - loss: 1.5977e-04 - val_loss: 2.2173e-04\n",
      "Epoch 1006/2000\n",
      "3622/3622 [==============================] - 1s 231us/step - loss: 1.5861e-04 - val_loss: 2.2549e-04\n",
      "Epoch 1007/2000\n",
      "3622/3622 [==============================] - 1s 230us/step - loss: 1.6111e-04 - val_loss: 2.3888e-04\n",
      "Epoch 1008/2000\n",
      "3622/3622 [==============================] - 1s 228us/step - loss: 1.5703e-04 - val_loss: 2.2417e-04\n",
      "Epoch 1009/2000\n",
      "3622/3622 [==============================] - 1s 228us/step - loss: 1.5825e-04 - val_loss: 2.3070e-04\n",
      "Epoch 1010/2000\n",
      "3622/3622 [==============================] - 1s 229us/step - loss: 1.6008e-04 - val_loss: 2.7057e-04\n",
      "Epoch 1011/2000\n",
      "3622/3622 [==============================] - 1s 231us/step - loss: 1.7151e-04 - val_loss: 2.4850e-04\n",
      "Epoch 1012/2000\n",
      "3622/3622 [==============================] - 1s 230us/step - loss: 1.7189e-04 - val_loss: 2.4967e-04\n",
      "Epoch 1013/2000\n",
      "3622/3622 [==============================] - 1s 230us/step - loss: 1.6944e-04 - val_loss: 2.3240e-04\n",
      "Epoch 1014/2000\n",
      "3622/3622 [==============================] - 1s 229us/step - loss: 1.6155e-04 - val_loss: 2.2007e-04\n",
      "Epoch 1015/2000\n",
      "3622/3622 [==============================] - 1s 228us/step - loss: 1.5806e-04 - val_loss: 2.2220e-04\n",
      "Epoch 1016/2000\n",
      "3622/3622 [==============================] - 1s 229us/step - loss: 1.6446e-04 - val_loss: 2.5676e-04\n",
      "Epoch 1017/2000\n",
      "3622/3622 [==============================] - 1s 229us/step - loss: 1.7977e-04 - val_loss: 2.7489e-04\n",
      "Epoch 1018/2000\n",
      "3622/3622 [==============================] - 1s 229us/step - loss: 1.7823e-04 - val_loss: 2.9308e-04\n",
      "Epoch 1019/2000\n",
      "3622/3622 [==============================] - 1s 229us/step - loss: 1.7399e-04 - val_loss: 2.2495e-04\n",
      "Epoch 1020/2000\n",
      "3622/3622 [==============================] - 1s 226us/step - loss: 1.6738e-04 - val_loss: 3.1264e-04\n",
      "Epoch 1021/2000\n",
      "3622/3622 [==============================] - 1s 228us/step - loss: 1.9485e-04 - val_loss: 2.2077e-04\n",
      "Epoch 1022/2000\n",
      "3622/3622 [==============================] - 1s 235us/step - loss: 1.6113e-04 - val_loss: 5.1908e-04\n",
      "Epoch 1023/2000\n",
      "3622/3622 [==============================] - 1s 241us/step - loss: 2.5219e-04 - val_loss: 2.5788e-04\n",
      "Epoch 1024/2000\n",
      "3622/3622 [==============================] - 1s 230us/step - loss: 2.0859e-04 - val_loss: 2.6981e-04\n",
      "Epoch 1025/2000\n",
      "3622/3622 [==============================] - 1s 229us/step - loss: 1.8460e-04 - val_loss: 2.5588e-04\n",
      "Epoch 1026/2000\n",
      "3622/3622 [==============================] - 1s 229us/step - loss: 1.7103e-04 - val_loss: 2.2805e-04\n",
      "Epoch 1027/2000\n",
      "3622/3622 [==============================] - 1s 228us/step - loss: 1.6142e-04 - val_loss: 2.3356e-04\n",
      "Epoch 1028/2000\n",
      "3622/3622 [==============================] - 1s 230us/step - loss: 1.6188e-04 - val_loss: 2.4365e-04\n",
      "Epoch 1029/2000\n",
      "3622/3622 [==============================] - 1s 228us/step - loss: 1.6492e-04 - val_loss: 2.2548e-04\n",
      "Epoch 1030/2000\n",
      "3622/3622 [==============================] - 1s 230us/step - loss: 1.6022e-04 - val_loss: 2.3553e-04\n",
      "Epoch 1031/2000\n",
      "3622/3622 [==============================] - 1s 228us/step - loss: 1.6300e-04 - val_loss: 2.4324e-04\n",
      "Epoch 1032/2000\n",
      "3622/3622 [==============================] - 1s 229us/step - loss: 1.6547e-04 - val_loss: 2.4160e-04\n",
      "Epoch 1033/2000\n",
      "3622/3622 [==============================] - 1s 229us/step - loss: 1.6377e-04 - val_loss: 2.3184e-04\n",
      "Epoch 1034/2000\n",
      "3622/3622 [==============================] - 1s 230us/step - loss: 1.6412e-04 - val_loss: 2.7345e-04\n",
      "Epoch 1035/2000\n",
      "3622/3622 [==============================] - 1s 229us/step - loss: 1.7482e-04 - val_loss: 2.6197e-04\n",
      "Epoch 1036/2000\n",
      "3622/3622 [==============================] - 1s 230us/step - loss: 1.6356e-04 - val_loss: 2.2865e-04\n",
      "Epoch 1037/2000\n",
      "3622/3622 [==============================] - 1s 228us/step - loss: 1.5915e-04 - val_loss: 2.7838e-04\n",
      "Epoch 1038/2000\n",
      "3622/3622 [==============================] - 1s 229us/step - loss: 1.7231e-04 - val_loss: 2.2187e-04\n",
      "Epoch 1039/2000\n",
      "3622/3622 [==============================] - 1s 229us/step - loss: 1.5716e-04 - val_loss: 2.3518e-04\n",
      "Epoch 1040/2000\n",
      "3622/3622 [==============================] - 1s 232us/step - loss: 1.5857e-04 - val_loss: 3.1675e-04\n",
      "Epoch 1041/2000\n",
      "3622/3622 [==============================] - 1s 229us/step - loss: 1.7470e-04 - val_loss: 2.8359e-04\n",
      "Epoch 1042/2000\n",
      "3622/3622 [==============================] - 1s 231us/step - loss: 1.7972e-04 - val_loss: 2.2064e-04\n",
      "Epoch 1043/2000\n",
      "3622/3622 [==============================] - 1s 229us/step - loss: 1.6397e-04 - val_loss: 2.3590e-04\n",
      "Epoch 1044/2000\n",
      "3622/3622 [==============================] - 1s 227us/step - loss: 1.6213e-04 - val_loss: 2.2320e-04\n",
      "Epoch 1045/2000\n",
      "3622/3622 [==============================] - 1s 228us/step - loss: 1.5882e-04 - val_loss: 2.2556e-04\n",
      "Epoch 1046/2000\n",
      "3622/3622 [==============================] - 1s 228us/step - loss: 1.6814e-04 - val_loss: 2.2242e-04\n",
      "Epoch 1047/2000\n",
      "3622/3622 [==============================] - 1s 231us/step - loss: 1.6698e-04 - val_loss: 2.9323e-04\n",
      "Epoch 1048/2000\n",
      "3622/3622 [==============================] - 1s 228us/step - loss: 1.7926e-04 - val_loss: 2.6053e-04\n",
      "Epoch 1049/2000\n",
      "3622/3622 [==============================] - 1s 228us/step - loss: 1.7249e-04 - val_loss: 2.2184e-04\n",
      "Epoch 1050/2000\n",
      "3622/3622 [==============================] - 1s 229us/step - loss: 1.6670e-04 - val_loss: 2.6125e-04\n",
      "Epoch 1051/2000\n",
      "3622/3622 [==============================] - 1s 230us/step - loss: 1.7103e-04 - val_loss: 2.6405e-04\n",
      "Epoch 1052/2000\n",
      "3622/3622 [==============================] - 1s 229us/step - loss: 1.8598e-04 - val_loss: 2.3099e-04\n",
      "Epoch 1053/2000\n",
      "3622/3622 [==============================] - 1s 228us/step - loss: 1.9519e-04 - val_loss: 4.0115e-04\n",
      "Epoch 1054/2000\n",
      "3622/3622 [==============================] - 1s 229us/step - loss: 2.0606e-04 - val_loss: 2.7547e-04\n",
      "Epoch 1055/2000\n",
      "3622/3622 [==============================] - 1s 228us/step - loss: 1.8282e-04 - val_loss: 3.2990e-04\n",
      "Epoch 1056/2000\n",
      "3622/3622 [==============================] - 1s 227us/step - loss: 1.8340e-04 - val_loss: 2.3067e-04\n",
      "Epoch 1057/2000\n",
      "3622/3622 [==============================] - 1s 229us/step - loss: 1.7548e-04 - val_loss: 2.4674e-04\n",
      "Epoch 1058/2000\n",
      "3622/3622 [==============================] - 1s 233us/step - loss: 1.7119e-04 - val_loss: 2.1821e-04\n",
      "Epoch 1059/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3622/3622 [==============================] - 1s 233us/step - loss: 1.6068e-04 - val_loss: 2.3563e-04\n",
      "Epoch 1060/2000\n",
      "3622/3622 [==============================] - 1s 230us/step - loss: 1.6264e-04 - val_loss: 2.5864e-04\n",
      "Epoch 1061/2000\n",
      "3622/3622 [==============================] - 1s 234us/step - loss: 1.6060e-04 - val_loss: 2.2934e-04\n",
      "Epoch 1062/2000\n",
      "3622/3622 [==============================] - 1s 229us/step - loss: 1.6689e-04 - val_loss: 2.2290e-04\n",
      "Epoch 1063/2000\n",
      "3622/3622 [==============================] - 1s 229us/step - loss: 1.5970e-04 - val_loss: 2.3788e-04\n",
      "Epoch 1064/2000\n",
      "3622/3622 [==============================] - 1s 230us/step - loss: 1.6855e-04 - val_loss: 2.3126e-04\n",
      "Epoch 1065/2000\n",
      "3622/3622 [==============================] - 1s 230us/step - loss: 1.6585e-04 - val_loss: 2.2509e-04\n",
      "Epoch 1066/2000\n",
      "3622/3622 [==============================] - 1s 231us/step - loss: 1.6025e-04 - val_loss: 2.2352e-04\n",
      "Epoch 1067/2000\n",
      "3622/3622 [==============================] - 1s 230us/step - loss: 1.5703e-04 - val_loss: 2.2337e-04\n",
      "Epoch 1068/2000\n",
      "3622/3622 [==============================] - 1s 230us/step - loss: 1.5822e-04 - val_loss: 2.2091e-04\n",
      "Epoch 1069/2000\n",
      "3622/3622 [==============================] - 1s 227us/step - loss: 1.5612e-04 - val_loss: 2.2718e-04\n",
      "Epoch 1070/2000\n",
      "3622/3622 [==============================] - 1s 229us/step - loss: 1.5517e-04 - val_loss: 2.2864e-04\n",
      "Epoch 1071/2000\n",
      "3622/3622 [==============================] - 1s 230us/step - loss: 1.6125e-04 - val_loss: 2.4839e-04\n",
      "Epoch 1072/2000\n",
      "3622/3622 [==============================] - 1s 227us/step - loss: 1.6224e-04 - val_loss: 3.0328e-04\n",
      "Epoch 1073/2000\n",
      "3622/3622 [==============================] - 1s 229us/step - loss: 1.7924e-04 - val_loss: 2.5151e-04\n",
      "Epoch 1074/2000\n",
      "3622/3622 [==============================] - 1s 230us/step - loss: 1.7767e-04 - val_loss: 2.6281e-04\n",
      "Epoch 1075/2000\n",
      "3622/3622 [==============================] - 1s 231us/step - loss: 1.8294e-04 - val_loss: 2.3046e-04\n",
      "Epoch 1076/2000\n",
      "3622/3622 [==============================] - 1s 229us/step - loss: 1.5936e-04 - val_loss: 2.3676e-04\n",
      "Epoch 1077/2000\n",
      "3622/3622 [==============================] - 1s 228us/step - loss: 1.6679e-04 - val_loss: 3.1941e-04\n",
      "Epoch 1078/2000\n",
      "3622/3622 [==============================] - 1s 229us/step - loss: 2.0576e-04 - val_loss: 2.6114e-04\n",
      "Epoch 1079/2000\n",
      "3622/3622 [==============================] - 1s 230us/step - loss: 1.7209e-04 - val_loss: 2.4629e-04\n",
      "Epoch 1080/2000\n",
      "3622/3622 [==============================] - 1s 235us/step - loss: 1.7353e-04 - val_loss: 3.2948e-04\n",
      "Epoch 1081/2000\n",
      "3622/3622 [==============================] - 1s 230us/step - loss: 1.7969e-04 - val_loss: 2.2359e-04\n",
      "Epoch 1082/2000\n",
      "3622/3622 [==============================] - 1s 229us/step - loss: 1.5741e-04 - val_loss: 2.2523e-04\n",
      "Epoch 1083/2000\n",
      "3622/3622 [==============================] - 1s 229us/step - loss: 1.5815e-04 - val_loss: 2.2768e-04\n",
      "Epoch 1084/2000\n",
      "3622/3622 [==============================] - 1s 229us/step - loss: 1.6497e-04 - val_loss: 2.2166e-04\n",
      "Epoch 1085/2000\n",
      "3622/3622 [==============================] - 1s 229us/step - loss: 1.5799e-04 - val_loss: 2.7859e-04\n",
      "Epoch 1086/2000\n",
      "3622/3622 [==============================] - 1s 227us/step - loss: 1.7175e-04 - val_loss: 2.3590e-04\n",
      "Epoch 1087/2000\n",
      "3622/3622 [==============================] - 1s 231us/step - loss: 1.7605e-04 - val_loss: 2.3329e-04\n",
      "Epoch 1088/2000\n",
      "3622/3622 [==============================] - 1s 228us/step - loss: 1.6919e-04 - val_loss: 2.2211e-04\n",
      "Epoch 1089/2000\n",
      "3622/3622 [==============================] - 1s 232us/step - loss: 1.5760e-04 - val_loss: 2.3587e-04\n",
      "Epoch 1090/2000\n",
      "3622/3622 [==============================] - 1s 228us/step - loss: 1.6200e-04 - val_loss: 2.3230e-04\n",
      "Epoch 1091/2000\n",
      "3622/3622 [==============================] - 1s 228us/step - loss: 1.6415e-04 - val_loss: 2.7752e-04\n",
      "Epoch 1092/2000\n",
      "3622/3622 [==============================] - 1s 230us/step - loss: 1.6882e-04 - val_loss: 2.2466e-04\n",
      "Epoch 1093/2000\n",
      "3622/3622 [==============================] - 1s 228us/step - loss: 1.6509e-04 - val_loss: 2.7501e-04\n",
      "Epoch 1094/2000\n",
      "3622/3622 [==============================] - 1s 233us/step - loss: 1.9181e-04 - val_loss: 2.2588e-04\n",
      "Epoch 1095/2000\n",
      "3622/3622 [==============================] - 1s 233us/step - loss: 1.8942e-04 - val_loss: 3.7013e-04\n",
      "Epoch 1096/2000\n",
      "3622/3622 [==============================] - 1s 230us/step - loss: 1.9793e-04 - val_loss: 2.3143e-04\n",
      "Epoch 1097/2000\n",
      "3622/3622 [==============================] - 1s 230us/step - loss: 1.9217e-04 - val_loss: 3.1659e-04\n",
      "Epoch 1098/2000\n",
      "3622/3622 [==============================] - 1s 229us/step - loss: 1.9437e-04 - val_loss: 2.2340e-04\n",
      "Epoch 1099/2000\n",
      "3622/3622 [==============================] - 1s 233us/step - loss: 1.7476e-04 - val_loss: 2.2293e-04\n",
      "Epoch 1100/2000\n",
      "3622/3622 [==============================] - 1s 231us/step - loss: 1.7026e-04 - val_loss: 2.2157e-04\n",
      "Epoch 1101/2000\n",
      "3622/3622 [==============================] - 1s 230us/step - loss: 1.6405e-04 - val_loss: 2.2453e-04\n",
      "Epoch 1102/2000\n",
      "3622/3622 [==============================] - 1s 228us/step - loss: 1.5684e-04 - val_loss: 2.2682e-04\n",
      "Epoch 1103/2000\n",
      "3622/3622 [==============================] - 1s 227us/step - loss: 1.6158e-04 - val_loss: 2.4954e-04\n",
      "Epoch 1104/2000\n",
      "3622/3622 [==============================] - 1s 228us/step - loss: 1.7134e-04 - val_loss: 2.2773e-04\n",
      "Epoch 1105/2000\n",
      "3622/3622 [==============================] - 1s 228us/step - loss: 1.7196e-04 - val_loss: 2.7702e-04\n",
      "Epoch 1106/2000\n",
      "3622/3622 [==============================] - 1s 231us/step - loss: 1.9079e-04 - val_loss: 4.3025e-04\n",
      "Epoch 1107/2000\n",
      "3622/3622 [==============================] - 1s 228us/step - loss: 2.0422e-04 - val_loss: 2.3220e-04\n",
      "Epoch 1108/2000\n",
      "3622/3622 [==============================] - 1s 229us/step - loss: 1.7196e-04 - val_loss: 2.2106e-04\n",
      "Epoch 1109/2000\n",
      "3622/3622 [==============================] - 1s 230us/step - loss: 1.5902e-04 - val_loss: 2.1986e-04\n",
      "Epoch 1110/2000\n",
      "3622/3622 [==============================] - 1s 229us/step - loss: 1.5467e-04 - val_loss: 2.2674e-04\n",
      "Epoch 1111/2000\n",
      "3622/3622 [==============================] - 1s 230us/step - loss: 1.5379e-04 - val_loss: 2.2115e-04\n",
      "Epoch 1112/2000\n",
      "3622/3622 [==============================] - 1s 231us/step - loss: 1.5556e-04 - val_loss: 2.2127e-04\n",
      "Epoch 1113/2000\n",
      "3622/3622 [==============================] - 1s 228us/step - loss: 1.5728e-04 - val_loss: 2.8396e-04\n",
      "Epoch 1114/2000\n",
      "3622/3622 [==============================] - 1s 231us/step - loss: 1.8143e-04 - val_loss: 2.2140e-04\n",
      "Epoch 1115/2000\n",
      "3622/3622 [==============================] - 1s 230us/step - loss: 1.7255e-04 - val_loss: 2.1960e-04\n",
      "Epoch 1116/2000\n",
      "3622/3622 [==============================] - 1s 230us/step - loss: 1.6387e-04 - val_loss: 2.4936e-04\n",
      "Epoch 1117/2000\n",
      "3622/3622 [==============================] - 1s 228us/step - loss: 1.6319e-04 - val_loss: 2.2004e-04\n",
      "Epoch 1118/2000\n",
      "3622/3622 [==============================] - 1s 230us/step - loss: 1.5770e-04 - val_loss: 2.1757e-04\n",
      "Epoch 1119/2000\n",
      "3622/3622 [==============================] - 1s 231us/step - loss: 1.5397e-04 - val_loss: 2.2071e-04\n",
      "Epoch 1120/2000\n",
      "3622/3622 [==============================] - 1s 228us/step - loss: 1.5585e-04 - val_loss: 2.3110e-04\n",
      "Epoch 1121/2000\n",
      "3622/3622 [==============================] - 1s 228us/step - loss: 1.6360e-04 - val_loss: 2.2009e-04\n",
      "Epoch 1122/2000\n",
      "3622/3622 [==============================] - 1s 227us/step - loss: 1.5836e-04 - val_loss: 2.3137e-04\n",
      "Epoch 1123/2000\n",
      "3622/3622 [==============================] - 1s 230us/step - loss: 1.5557e-04 - val_loss: 2.4359e-04\n",
      "Epoch 1124/2000\n",
      "3622/3622 [==============================] - 1s 229us/step - loss: 1.6227e-04 - val_loss: 2.2671e-04\n",
      "Epoch 1125/2000\n",
      "3622/3622 [==============================] - 1s 230us/step - loss: 1.5652e-04 - val_loss: 2.2434e-04\n",
      "Epoch 1126/2000\n",
      "3622/3622 [==============================] - 1s 230us/step - loss: 1.5736e-04 - val_loss: 3.1181e-04\n",
      "Epoch 1127/2000\n",
      "3622/3622 [==============================] - 1s 230us/step - loss: 1.7621e-04 - val_loss: 4.6934e-04\n",
      "Epoch 1128/2000\n",
      "3622/3622 [==============================] - 1s 229us/step - loss: 2.2542e-04 - val_loss: 2.8267e-04\n",
      "Epoch 1129/2000\n",
      "3622/3622 [==============================] - 1s 229us/step - loss: 2.1814e-04 - val_loss: 2.3598e-04\n",
      "Epoch 1130/2000\n",
      "3622/3622 [==============================] - 1s 235us/step - loss: 1.5951e-04 - val_loss: 2.9791e-04\n",
      "Epoch 1131/2000\n",
      "3622/3622 [==============================] - 1s 232us/step - loss: 1.6441e-04 - val_loss: 2.4669e-04\n",
      "Epoch 1132/2000\n",
      "3622/3622 [==============================] - 1s 229us/step - loss: 1.9119e-04 - val_loss: 3.2584e-04\n",
      "Epoch 1133/2000\n",
      "3622/3622 [==============================] - 1s 230us/step - loss: 2.0739e-04 - val_loss: 2.3118e-04\n",
      "Epoch 1134/2000\n",
      "3622/3622 [==============================] - 1s 230us/step - loss: 1.6767e-04 - val_loss: 2.1880e-04\n",
      "Epoch 1135/2000\n",
      "3622/3622 [==============================] - 1s 230us/step - loss: 1.5300e-04 - val_loss: 2.4472e-04\n",
      "Epoch 1136/2000\n",
      "3622/3622 [==============================] - 1s 229us/step - loss: 1.5883e-04 - val_loss: 2.2738e-04\n",
      "Epoch 1137/2000\n",
      "3622/3622 [==============================] - 1s 230us/step - loss: 1.5299e-04 - val_loss: 3.0943e-04\n",
      "Epoch 1138/2000\n",
      "3622/3622 [==============================] - 1s 233us/step - loss: 1.8529e-04 - val_loss: 2.5860e-04\n",
      "Epoch 1139/2000\n",
      "3622/3622 [==============================] - 1s 230us/step - loss: 1.6000e-04 - val_loss: 2.2239e-04\n",
      "Epoch 1140/2000\n",
      "3622/3622 [==============================] - 1s 230us/step - loss: 1.5965e-04 - val_loss: 2.7897e-04\n",
      "Epoch 1141/2000\n",
      "3622/3622 [==============================] - 1s 230us/step - loss: 1.6803e-04 - val_loss: 2.2066e-04\n",
      "Epoch 1142/2000\n",
      "3622/3622 [==============================] - 1s 229us/step - loss: 1.5896e-04 - val_loss: 2.2124e-04\n",
      "Epoch 1143/2000\n",
      "3622/3622 [==============================] - 1s 228us/step - loss: 1.5703e-04 - val_loss: 2.2898e-04\n",
      "Epoch 1144/2000\n",
      "3622/3622 [==============================] - 1s 230us/step - loss: 1.6389e-04 - val_loss: 2.9165e-04\n",
      "Epoch 1145/2000\n",
      "3622/3622 [==============================] - 1s 230us/step - loss: 1.8005e-04 - val_loss: 2.2075e-04\n",
      "Epoch 1146/2000\n",
      "3622/3622 [==============================] - 1s 229us/step - loss: 1.6066e-04 - val_loss: 2.6749e-04\n",
      "Epoch 1147/2000\n",
      "3622/3622 [==============================] - 1s 229us/step - loss: 1.6143e-04 - val_loss: 2.2071e-04\n",
      "Epoch 1148/2000\n",
      "3622/3622 [==============================] - 1s 228us/step - loss: 1.5513e-04 - val_loss: 2.2687e-04\n",
      "Epoch 1149/2000\n",
      "3622/3622 [==============================] - 1s 228us/step - loss: 1.6853e-04 - val_loss: 3.1930e-04\n",
      "Epoch 1150/2000\n",
      "3622/3622 [==============================] - 1s 229us/step - loss: 1.8032e-04 - val_loss: 2.2977e-04\n",
      "Epoch 1151/2000\n",
      "3622/3622 [==============================] - 1s 229us/step - loss: 1.7314e-04 - val_loss: 2.4709e-04\n",
      "Epoch 1152/2000\n",
      "3622/3622 [==============================] - 1s 228us/step - loss: 1.7019e-04 - val_loss: 2.5075e-04\n",
      "Epoch 1153/2000\n",
      "3622/3622 [==============================] - 1s 229us/step - loss: 1.7443e-04 - val_loss: 2.3379e-04\n",
      "Epoch 1154/2000\n",
      "3622/3622 [==============================] - 1s 230us/step - loss: 1.5684e-04 - val_loss: 2.8458e-04\n",
      "Epoch 1155/2000\n",
      "3622/3622 [==============================] - 1s 229us/step - loss: 1.7811e-04 - val_loss: 2.2952e-04\n",
      "Epoch 1156/2000\n",
      "3622/3622 [==============================] - 1s 228us/step - loss: 1.5974e-04 - val_loss: 3.1831e-04\n",
      "Epoch 1157/2000\n",
      "3622/3622 [==============================] - 1s 233us/step - loss: 1.7565e-04 - val_loss: 2.3267e-04\n",
      "Epoch 1158/2000\n",
      "3622/3622 [==============================] - 1s 230us/step - loss: 1.6895e-04 - val_loss: 2.8079e-04\n",
      "Epoch 1159/2000\n",
      "3622/3622 [==============================] - 1s 230us/step - loss: 1.7794e-04 - val_loss: 2.2137e-04\n",
      "Epoch 1160/2000\n",
      "3622/3622 [==============================] - 1s 227us/step - loss: 1.6627e-04 - val_loss: 2.6837e-04\n",
      "Epoch 1161/2000\n",
      "3622/3622 [==============================] - 1s 231us/step - loss: 1.5577e-04 - val_loss: 2.2888e-04\n",
      "Epoch 1162/2000\n",
      "3622/3622 [==============================] - 1s 229us/step - loss: 1.5209e-04 - val_loss: 2.3026e-04\n",
      "Epoch 1163/2000\n",
      "3622/3622 [==============================] - 1s 229us/step - loss: 1.5432e-04 - val_loss: 2.4941e-04\n",
      "Epoch 1164/2000\n",
      "3622/3622 [==============================] - 1s 229us/step - loss: 1.6001e-04 - val_loss: 2.2302e-04\n",
      "Epoch 1165/2000\n",
      "3622/3622 [==============================] - 1s 230us/step - loss: 1.5443e-04 - val_loss: 2.2496e-04\n",
      "Epoch 1166/2000\n",
      "3622/3622 [==============================] - 1s 236us/step - loss: 1.5598e-04 - val_loss: 2.2397e-04\n",
      "Epoch 1167/2000\n",
      "3622/3622 [==============================] - 1s 247us/step - loss: 1.6377e-04 - val_loss: 2.4431e-04\n",
      "Epoch 1168/2000\n",
      "3622/3622 [==============================] - 1s 238us/step - loss: 1.7596e-04 - val_loss: 2.3185e-04\n",
      "Epoch 1169/2000\n",
      "3622/3622 [==============================] - 1s 235us/step - loss: 1.7147e-04 - val_loss: 3.0670e-04\n",
      "Epoch 1170/2000\n",
      "3622/3622 [==============================] - 1s 229us/step - loss: 1.7108e-04 - val_loss: 2.2143e-04\n",
      "Epoch 1171/2000\n",
      "3622/3622 [==============================] - 1s 231us/step - loss: 1.5773e-04 - val_loss: 2.3057e-04\n",
      "Epoch 1172/2000\n",
      "3622/3622 [==============================] - 1s 226us/step - loss: 1.5680e-04 - val_loss: 2.2026e-04\n",
      "Epoch 1173/2000\n",
      "3622/3622 [==============================] - 1s 229us/step - loss: 1.5782e-04 - val_loss: 2.2330e-04\n",
      "Epoch 1174/2000\n",
      "3622/3622 [==============================] - 1s 230us/step - loss: 1.5532e-04 - val_loss: 2.2996e-04\n",
      "Epoch 1175/2000\n",
      "3622/3622 [==============================] - 1s 231us/step - loss: 1.5629e-04 - val_loss: 2.2220e-04\n",
      "Epoch 1176/2000\n",
      "3622/3622 [==============================] - 1s 234us/step - loss: 1.5162e-04 - val_loss: 2.2071e-04\n",
      "Epoch 1177/2000\n",
      "3622/3622 [==============================] - 1s 232us/step - loss: 1.5487e-04 - val_loss: 2.5234e-04\n",
      "Epoch 1178/2000\n",
      "3622/3622 [==============================] - 1s 230us/step - loss: 1.7250e-04 - val_loss: 3.0068e-04\n",
      "Epoch 1179/2000\n",
      "3622/3622 [==============================] - 1s 229us/step - loss: 1.6473e-04 - val_loss: 2.2881e-04\n",
      "Epoch 1180/2000\n",
      "3622/3622 [==============================] - 1s 228us/step - loss: 1.6098e-04 - val_loss: 2.2346e-04\n",
      "Epoch 1181/2000\n",
      "3622/3622 [==============================] - 1s 229us/step - loss: 1.5317e-04 - val_loss: 2.2838e-04\n",
      "Epoch 1182/2000\n",
      "3622/3622 [==============================] - 1s 231us/step - loss: 1.5840e-04 - val_loss: 2.1950e-04\n",
      "Epoch 1183/2000\n",
      "3622/3622 [==============================] - 1s 229us/step - loss: 1.5270e-04 - val_loss: 2.7748e-04\n",
      "Epoch 1184/2000\n",
      "3622/3622 [==============================] - 1s 229us/step - loss: 1.8055e-04 - val_loss: 2.2439e-04\n",
      "Epoch 1185/2000\n",
      "3622/3622 [==============================] - 1s 227us/step - loss: 1.8450e-04 - val_loss: 2.4379e-04\n",
      "Epoch 1186/2000\n",
      "3622/3622 [==============================] - 1s 230us/step - loss: 1.6629e-04 - val_loss: 2.4248e-04\n",
      "Epoch 1187/2000\n",
      "3622/3622 [==============================] - 1s 229us/step - loss: 1.6524e-04 - val_loss: 2.5227e-04\n",
      "Epoch 1188/2000\n",
      "3622/3622 [==============================] - 1s 229us/step - loss: 1.5723e-04 - val_loss: 2.1942e-04\n",
      "Epoch 1189/2000\n",
      "3622/3622 [==============================] - 1s 227us/step - loss: 1.6078e-04 - val_loss: 2.2041e-04\n",
      "Epoch 1190/2000\n",
      "3622/3622 [==============================] - 1s 232us/step - loss: 1.5692e-04 - val_loss: 2.2162e-04\n",
      "Epoch 1191/2000\n",
      "3622/3622 [==============================] - 1s 229us/step - loss: 1.5399e-04 - val_loss: 2.2984e-04\n",
      "Epoch 1192/2000\n",
      "3622/3622 [==============================] - 1s 230us/step - loss: 1.5375e-04 - val_loss: 2.2298e-04\n",
      "Epoch 1193/2000\n",
      "3622/3622 [==============================] - 1s 231us/step - loss: 1.5217e-04 - val_loss: 2.5483e-04\n",
      "Epoch 1194/2000\n",
      "3622/3622 [==============================] - 1s 229us/step - loss: 1.6107e-04 - val_loss: 2.2496e-04\n",
      "Epoch 1195/2000\n",
      "3622/3622 [==============================] - 1s 233us/step - loss: 1.5509e-04 - val_loss: 2.2174e-04\n",
      "Epoch 1196/2000\n",
      "3622/3622 [==============================] - 1s 232us/step - loss: 1.5082e-04 - val_loss: 2.2105e-04\n",
      "Epoch 1197/2000\n",
      "3622/3622 [==============================] - 1s 229us/step - loss: 1.4948e-04 - val_loss: 2.2473e-04\n",
      "Epoch 1198/2000\n",
      "3622/3622 [==============================] - 1s 231us/step - loss: 1.5462e-04 - val_loss: 2.5170e-04\n",
      "Epoch 1199/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3622/3622 [==============================] - 1s 229us/step - loss: 1.5560e-04 - val_loss: 3.3862e-04\n",
      "Epoch 1200/2000\n",
      "3622/3622 [==============================] - 1s 228us/step - loss: 1.8675e-04 - val_loss: 2.1820e-04\n",
      "Epoch 1201/2000\n",
      "3622/3622 [==============================] - 1s 229us/step - loss: 1.7349e-04 - val_loss: 2.5731e-04\n",
      "Epoch 1202/2000\n",
      "3622/3622 [==============================] - 1s 236us/step - loss: 1.5920e-04 - val_loss: 2.2872e-04\n",
      "Epoch 1203/2000\n",
      "3622/3622 [==============================] - 1s 228us/step - loss: 1.6020e-04 - val_loss: 2.3506e-04\n",
      "Epoch 1204/2000\n",
      "3622/3622 [==============================] - 1s 230us/step - loss: 1.5755e-04 - val_loss: 3.3113e-04\n",
      "Epoch 1205/2000\n",
      "3622/3622 [==============================] - 1s 229us/step - loss: 1.7864e-04 - val_loss: 2.2561e-04\n",
      "Epoch 1206/2000\n",
      "3622/3622 [==============================] - 1s 230us/step - loss: 1.6558e-04 - val_loss: 2.4067e-04\n",
      "Epoch 1207/2000\n",
      "3622/3622 [==============================] - 1s 229us/step - loss: 1.6506e-04 - val_loss: 2.2731e-04\n",
      "Epoch 1208/2000\n",
      "3622/3622 [==============================] - 1s 229us/step - loss: 1.5862e-04 - val_loss: 2.3303e-04\n",
      "Epoch 1209/2000\n",
      "3622/3622 [==============================] - 1s 230us/step - loss: 1.5421e-04 - val_loss: 2.2284e-04\n",
      "Epoch 1210/2000\n",
      "3622/3622 [==============================] - 1s 230us/step - loss: 1.5061e-04 - val_loss: 2.8367e-04\n",
      "Epoch 1211/2000\n",
      "3622/3622 [==============================] - 1s 229us/step - loss: 1.7798e-04 - val_loss: 2.3337e-04\n",
      "Epoch 1212/2000\n",
      "3622/3622 [==============================] - 1s 229us/step - loss: 1.5907e-04 - val_loss: 2.5000e-04\n",
      "Epoch 1213/2000\n",
      "3622/3622 [==============================] - 1s 229us/step - loss: 1.5983e-04 - val_loss: 2.9606e-04\n",
      "Epoch 1214/2000\n",
      "3622/3622 [==============================] - 1s 237us/step - loss: 1.5740e-04 - val_loss: 2.7992e-04\n",
      "Epoch 1215/2000\n",
      "3622/3622 [==============================] - 1s 229us/step - loss: 1.6693e-04 - val_loss: 2.1903e-04\n",
      "Epoch 1216/2000\n",
      "3622/3622 [==============================] - 1s 230us/step - loss: 1.5775e-04 - val_loss: 2.2626e-04\n",
      "Epoch 1217/2000\n",
      "3622/3622 [==============================] - 1s 230us/step - loss: 1.5743e-04 - val_loss: 2.6719e-04\n",
      "Epoch 1218/2000\n",
      "3622/3622 [==============================] - 1s 230us/step - loss: 1.6289e-04 - val_loss: 2.4647e-04\n",
      "Epoch 1219/2000\n",
      "3622/3622 [==============================] - 1s 229us/step - loss: 1.5483e-04 - val_loss: 2.6514e-04\n",
      "Epoch 1220/2000\n",
      "3622/3622 [==============================] - 1s 228us/step - loss: 1.6743e-04 - val_loss: 3.4621e-04\n",
      "Epoch 1221/2000\n",
      "3622/3622 [==============================] - 1s 229us/step - loss: 1.6586e-04 - val_loss: 2.2210e-04\n",
      "Epoch 1222/2000\n",
      "3622/3622 [==============================] - 1s 229us/step - loss: 1.5341e-04 - val_loss: 2.3394e-04\n",
      "Epoch 1223/2000\n",
      "3622/3622 [==============================] - 1s 231us/step - loss: 1.5932e-04 - val_loss: 2.4886e-04\n",
      "Epoch 1224/2000\n",
      "3622/3622 [==============================] - 1s 229us/step - loss: 1.6053e-04 - val_loss: 2.8240e-04\n",
      "Epoch 1225/2000\n",
      "3622/3622 [==============================] - 1s 229us/step - loss: 1.7294e-04 - val_loss: 3.2052e-04\n",
      "Epoch 1226/2000\n",
      "3622/3622 [==============================] - 1s 230us/step - loss: 1.8959e-04 - val_loss: 2.8339e-04\n",
      "Epoch 1227/2000\n",
      "3622/3622 [==============================] - 1s 229us/step - loss: 1.9733e-04 - val_loss: 2.2222e-04\n",
      "Epoch 1228/2000\n",
      "3622/3622 [==============================] - 1s 229us/step - loss: 1.7438e-04 - val_loss: 2.9612e-04\n",
      "Epoch 1229/2000\n",
      "3622/3622 [==============================] - 1s 229us/step - loss: 1.8018e-04 - val_loss: 2.8934e-04\n",
      "Epoch 1230/2000\n",
      "3622/3622 [==============================] - 1s 229us/step - loss: 1.7003e-04 - val_loss: 2.2012e-04\n",
      "Epoch 1231/2000\n",
      "3622/3622 [==============================] - 1s 231us/step - loss: 1.5729e-04 - val_loss: 2.3406e-04\n",
      "Epoch 1232/2000\n",
      "3622/3622 [==============================] - 1s 230us/step - loss: 1.5735e-04 - val_loss: 2.4484e-04\n",
      "Epoch 1233/2000\n",
      "3622/3622 [==============================] - 1s 234us/step - loss: 1.5577e-04 - val_loss: 2.2165e-04\n",
      "Epoch 1234/2000\n",
      "3622/3622 [==============================] - 1s 230us/step - loss: 1.5473e-04 - val_loss: 2.6634e-04\n",
      "Epoch 1235/2000\n",
      "3622/3622 [==============================] - 1s 229us/step - loss: 1.6642e-04 - val_loss: 2.6039e-04\n",
      "Epoch 1236/2000\n",
      "3622/3622 [==============================] - 1s 229us/step - loss: 1.5587e-04 - val_loss: 2.3575e-04\n",
      "Epoch 1237/2000\n",
      "3622/3622 [==============================] - 1s 230us/step - loss: 1.6441e-04 - val_loss: 2.1906e-04\n",
      "Epoch 1238/2000\n",
      "3622/3622 [==============================] - 1s 239us/step - loss: 1.5890e-04 - val_loss: 2.1870e-04\n",
      "Epoch 1239/2000\n",
      "3622/3622 [==============================] - 1s 228us/step - loss: 1.5561e-04 - val_loss: 2.3225e-04\n",
      "Epoch 1240/2000\n",
      "3622/3622 [==============================] - 1s 230us/step - loss: 1.5773e-04 - val_loss: 2.2146e-04\n",
      "Epoch 1241/2000\n",
      "3622/3622 [==============================] - 1s 229us/step - loss: 1.5988e-04 - val_loss: 2.7813e-04\n",
      "Epoch 1242/2000\n",
      "3622/3622 [==============================] - 1s 230us/step - loss: 1.6594e-04 - val_loss: 2.1912e-04\n",
      "Epoch 1243/2000\n",
      "3622/3622 [==============================] - 1s 230us/step - loss: 1.5114e-04 - val_loss: 2.7975e-04\n",
      "Epoch 1244/2000\n",
      "3622/3622 [==============================] - 1s 229us/step - loss: 1.6660e-04 - val_loss: 2.1711e-04\n",
      "Epoch 1245/2000\n",
      "3622/3622 [==============================] - 1s 229us/step - loss: 1.5363e-04 - val_loss: 3.1428e-04\n",
      "Epoch 1246/2000\n",
      "3622/3622 [==============================] - 1s 232us/step - loss: 1.7536e-04 - val_loss: 2.4982e-04\n",
      "Epoch 1247/2000\n",
      "3622/3622 [==============================] - 1s 230us/step - loss: 1.6578e-04 - val_loss: 3.7869e-04\n",
      "Epoch 1248/2000\n",
      "3622/3622 [==============================] - 1s 231us/step - loss: 2.0790e-04 - val_loss: 2.7774e-04\n",
      "Epoch 1249/2000\n",
      "3622/3622 [==============================] - 1s 230us/step - loss: 1.6467e-04 - val_loss: 3.1742e-04\n",
      "Epoch 1250/2000\n",
      "3622/3622 [==============================] - 1s 229us/step - loss: 1.6311e-04 - val_loss: 2.4189e-04\n",
      "Epoch 1251/2000\n",
      "3622/3622 [==============================] - 1s 231us/step - loss: 1.5308e-04 - val_loss: 2.1899e-04\n",
      "Epoch 1252/2000\n",
      "3622/3622 [==============================] - 1s 236us/step - loss: 1.5499e-04 - val_loss: 2.3174e-04\n",
      "Epoch 1253/2000\n",
      "3622/3622 [==============================] - 1s 237us/step - loss: 1.5603e-04 - val_loss: 2.3039e-04\n",
      "Epoch 1254/2000\n",
      "3622/3622 [==============================] - 1s 230us/step - loss: 1.5093e-04 - val_loss: 2.2261e-04\n",
      "Epoch 1255/2000\n",
      "3622/3622 [==============================] - 1s 229us/step - loss: 1.5152e-04 - val_loss: 2.2434e-04\n",
      "Epoch 1256/2000\n",
      "3622/3622 [==============================] - 1s 230us/step - loss: 1.4795e-04 - val_loss: 2.1950e-04\n",
      "Epoch 1257/2000\n",
      "3622/3622 [==============================] - 1s 231us/step - loss: 1.5105e-04 - val_loss: 2.8182e-04\n",
      "Epoch 1258/2000\n",
      "3622/3622 [==============================] - 1s 230us/step - loss: 1.6602e-04 - val_loss: 2.1959e-04\n",
      "Epoch 1259/2000\n",
      "3622/3622 [==============================] - 1s 228us/step - loss: 1.5370e-04 - val_loss: 2.2662e-04\n",
      "Epoch 1260/2000\n",
      "3622/3622 [==============================] - 1s 231us/step - loss: 1.5127e-04 - val_loss: 2.2749e-04\n",
      "Epoch 1261/2000\n",
      "3622/3622 [==============================] - 1s 229us/step - loss: 1.5007e-04 - val_loss: 2.7075e-04\n",
      "Epoch 1262/2000\n",
      "3622/3622 [==============================] - 1s 229us/step - loss: 1.6958e-04 - val_loss: 2.2709e-04\n",
      "Epoch 1263/2000\n",
      "3622/3622 [==============================] - 1s 228us/step - loss: 1.4715e-04 - val_loss: 2.8842e-04\n",
      "Epoch 1264/2000\n",
      "3622/3622 [==============================] - 1s 230us/step - loss: 1.6745e-04 - val_loss: 2.5139e-04\n",
      "Epoch 1265/2000\n",
      "3622/3622 [==============================] - 1s 229us/step - loss: 1.6259e-04 - val_loss: 2.7642e-04\n",
      "Epoch 1266/2000\n",
      "3622/3622 [==============================] - 1s 231us/step - loss: 1.8717e-04 - val_loss: 2.2098e-04\n",
      "Epoch 1267/2000\n",
      "3622/3622 [==============================] - 1s 230us/step - loss: 1.8356e-04 - val_loss: 3.9800e-04\n",
      "Epoch 1268/2000\n",
      "3622/3622 [==============================] - 1s 229us/step - loss: 2.0275e-04 - val_loss: 2.3789e-04\n",
      "Epoch 1269/2000\n",
      "3622/3622 [==============================] - 1s 229us/step - loss: 1.5968e-04 - val_loss: 2.2343e-04\n",
      "Epoch 1270/2000\n",
      "3622/3622 [==============================] - 1s 228us/step - loss: 1.5436e-04 - val_loss: 2.4045e-04\n",
      "Epoch 1271/2000\n",
      "3622/3622 [==============================] - 1s 230us/step - loss: 1.5046e-04 - val_loss: 2.2268e-04\n",
      "Epoch 1272/2000\n",
      "3622/3622 [==============================] - 1s 230us/step - loss: 1.5261e-04 - val_loss: 2.1724e-04\n",
      "Epoch 1273/2000\n",
      "3622/3622 [==============================] - 1s 233us/step - loss: 1.5394e-04 - val_loss: 2.4540e-04\n",
      "Epoch 1274/2000\n",
      "3622/3622 [==============================] - 1s 234us/step - loss: 1.6237e-04 - val_loss: 2.3339e-04\n",
      "Epoch 1275/2000\n",
      "3622/3622 [==============================] - 1s 228us/step - loss: 1.5729e-04 - val_loss: 3.9557e-04\n",
      "Epoch 1276/2000\n",
      "3622/3622 [==============================] - 1s 229us/step - loss: 1.7593e-04 - val_loss: 2.2116e-04\n",
      "Epoch 1277/2000\n",
      "3622/3622 [==============================] - 1s 230us/step - loss: 1.4847e-04 - val_loss: 2.3859e-04\n",
      "Epoch 1278/2000\n",
      "3622/3622 [==============================] - 1s 230us/step - loss: 1.4963e-04 - val_loss: 2.1967e-04\n",
      "Epoch 1279/2000\n",
      "3622/3622 [==============================] - 1s 227us/step - loss: 1.5199e-04 - val_loss: 2.5759e-04\n",
      "Epoch 1280/2000\n",
      "3622/3622 [==============================] - 1s 228us/step - loss: 1.5889e-04 - val_loss: 2.2046e-04\n",
      "Epoch 1281/2000\n",
      "3622/3622 [==============================] - 1s 231us/step - loss: 1.5371e-04 - val_loss: 2.5546e-04\n",
      "Epoch 1282/2000\n",
      "3622/3622 [==============================] - 1s 230us/step - loss: 1.6487e-04 - val_loss: 2.2318e-04\n",
      "Epoch 1283/2000\n",
      "3622/3622 [==============================] - 1s 228us/step - loss: 1.5201e-04 - val_loss: 2.2498e-04\n",
      "Epoch 1284/2000\n",
      "3622/3622 [==============================] - 1s 230us/step - loss: 1.6028e-04 - val_loss: 2.6752e-04\n",
      "Epoch 1285/2000\n",
      "3622/3622 [==============================] - 1s 227us/step - loss: 1.6403e-04 - val_loss: 3.1387e-04\n",
      "Epoch 1286/2000\n",
      "3622/3622 [==============================] - 1s 230us/step - loss: 1.6750e-04 - val_loss: 2.3414e-04\n",
      "Epoch 1287/2000\n",
      "3622/3622 [==============================] - 1s 229us/step - loss: 1.5564e-04 - val_loss: 2.1967e-04\n",
      "Epoch 1288/2000\n",
      "3622/3622 [==============================] - 1s 229us/step - loss: 1.5383e-04 - val_loss: 3.0831e-04\n",
      "Epoch 1289/2000\n",
      "3622/3622 [==============================] - 1s 229us/step - loss: 1.9193e-04 - val_loss: 2.2394e-04\n",
      "Epoch 1290/2000\n",
      "3622/3622 [==============================] - 1s 233us/step - loss: 1.7011e-04 - val_loss: 3.7894e-04\n",
      "Epoch 1291/2000\n",
      "3622/3622 [==============================] - 1s 232us/step - loss: 1.9199e-04 - val_loss: 2.4580e-04\n",
      "Epoch 1292/2000\n",
      "3622/3622 [==============================] - 1s 231us/step - loss: 1.6134e-04 - val_loss: 2.2275e-04\n",
      "Epoch 1293/2000\n",
      "3622/3622 [==============================] - 1s 229us/step - loss: 1.5338e-04 - val_loss: 2.4831e-04\n",
      "Epoch 1294/2000\n",
      "3622/3622 [==============================] - 1s 228us/step - loss: 1.7222e-04 - val_loss: 2.2019e-04\n",
      "Epoch 1295/2000\n",
      "3622/3622 [==============================] - 1s 229us/step - loss: 1.6502e-04 - val_loss: 4.1200e-04\n",
      "Epoch 1296/2000\n",
      "3622/3622 [==============================] - 1s 228us/step - loss: 2.1341e-04 - val_loss: 2.3061e-04\n",
      "Epoch 1297/2000\n",
      "3622/3622 [==============================] - 1s 228us/step - loss: 1.7034e-04 - val_loss: 2.3111e-04\n",
      "Epoch 1298/2000\n",
      "3622/3622 [==============================] - 1s 228us/step - loss: 1.5441e-04 - val_loss: 3.9736e-04\n",
      "Epoch 1299/2000\n",
      "3622/3622 [==============================] - 1s 229us/step - loss: 2.1898e-04 - val_loss: 2.3446e-04\n",
      "Epoch 1300/2000\n",
      "3622/3622 [==============================] - 1s 229us/step - loss: 1.8134e-04 - val_loss: 3.0097e-04\n",
      "Epoch 1301/2000\n",
      "3622/3622 [==============================] - 1s 227us/step - loss: 1.6611e-04 - val_loss: 2.4188e-04\n",
      "Epoch 1302/2000\n",
      "3622/3622 [==============================] - 1s 230us/step - loss: 1.6037e-04 - val_loss: 2.3110e-04\n",
      "Epoch 1303/2000\n",
      "3622/3622 [==============================] - 1s 229us/step - loss: 1.5395e-04 - val_loss: 2.3378e-04\n",
      "Epoch 1304/2000\n",
      "3622/3622 [==============================] - 1s 229us/step - loss: 1.5703e-04 - val_loss: 2.2942e-04\n",
      "Epoch 1305/2000\n",
      "3622/3622 [==============================] - 1s 229us/step - loss: 1.4971e-04 - val_loss: 2.2475e-04\n",
      "Epoch 1306/2000\n",
      "3622/3622 [==============================] - 1s 228us/step - loss: 1.5729e-04 - val_loss: 2.2325e-04\n",
      "Epoch 1307/2000\n",
      "3622/3622 [==============================] - 1s 229us/step - loss: 1.5661e-04 - val_loss: 3.5517e-04\n",
      "Epoch 1308/2000\n",
      "3622/3622 [==============================] - 1s 229us/step - loss: 1.6485e-04 - val_loss: 2.7998e-04\n",
      "Epoch 1309/2000\n",
      "3622/3622 [==============================] - 1s 235us/step - loss: 1.7402e-04 - val_loss: 2.2043e-04\n",
      "Epoch 1310/2000\n",
      "3622/3622 [==============================] - 1s 240us/step - loss: 1.5465e-04 - val_loss: 2.2543e-04\n",
      "Epoch 1311/2000\n",
      "3622/3622 [==============================] - 1s 230us/step - loss: 1.5065e-04 - val_loss: 2.5380e-04\n",
      "Epoch 1312/2000\n",
      "3622/3622 [==============================] - 1s 232us/step - loss: 1.4862e-04 - val_loss: 2.3052e-04\n",
      "Epoch 1313/2000\n",
      "3622/3622 [==============================] - 1s 230us/step - loss: 1.5134e-04 - val_loss: 2.1838e-04\n",
      "Epoch 1314/2000\n",
      "3622/3622 [==============================] - 1s 231us/step - loss: 1.4718e-04 - val_loss: 2.3795e-04\n",
      "Epoch 1315/2000\n",
      "3622/3622 [==============================] - 1s 230us/step - loss: 1.4844e-04 - val_loss: 2.2266e-04\n",
      "Epoch 1316/2000\n",
      "3622/3622 [==============================] - 1s 229us/step - loss: 1.4817e-04 - val_loss: 2.2420e-04\n",
      "Epoch 1317/2000\n",
      "3622/3622 [==============================] - 1s 229us/step - loss: 1.4582e-04 - val_loss: 2.4343e-04\n",
      "Epoch 1318/2000\n",
      "3622/3622 [==============================] - 1s 229us/step - loss: 1.4834e-04 - val_loss: 2.2712e-04\n",
      "Epoch 1319/2000\n",
      "3622/3622 [==============================] - 1s 229us/step - loss: 1.4326e-04 - val_loss: 2.2585e-04\n",
      "Epoch 1320/2000\n",
      "3622/3622 [==============================] - 1s 230us/step - loss: 1.4309e-04 - val_loss: 2.2370e-04\n",
      "Epoch 1321/2000\n",
      "3622/3622 [==============================] - 1s 230us/step - loss: 1.4215e-04 - val_loss: 2.3046e-04\n",
      "Epoch 1322/2000\n",
      "3622/3622 [==============================] - 1s 229us/step - loss: 1.4320e-04 - val_loss: 2.1965e-04\n",
      "Epoch 1323/2000\n",
      "3622/3622 [==============================] - 1s 229us/step - loss: 1.4250e-04 - val_loss: 3.9366e-04\n",
      "Epoch 1324/2000\n",
      "3622/3622 [==============================] - 1s 229us/step - loss: 1.9478e-04 - val_loss: 2.2206e-04\n",
      "Epoch 1325/2000\n",
      "3622/3622 [==============================] - 1s 229us/step - loss: 1.6392e-04 - val_loss: 2.2129e-04\n",
      "Epoch 1326/2000\n",
      "3622/3622 [==============================] - 1s 228us/step - loss: 1.5534e-04 - val_loss: 2.4818e-04\n",
      "Epoch 1327/2000\n",
      "3622/3622 [==============================] - 1s 229us/step - loss: 1.6560e-04 - val_loss: 2.8459e-04\n",
      "Epoch 1328/2000\n",
      "3622/3622 [==============================] - 1s 227us/step - loss: 1.6389e-04 - val_loss: 2.2242e-04\n",
      "Epoch 1329/2000\n",
      "3622/3622 [==============================] - 1s 234us/step - loss: 1.5083e-04 - val_loss: 2.2996e-04\n",
      "Epoch 1330/2000\n",
      "3622/3622 [==============================] - 1s 229us/step - loss: 1.4531e-04 - val_loss: 2.1949e-04\n",
      "Epoch 1331/2000\n",
      "3622/3622 [==============================] - 1s 229us/step - loss: 1.4479e-04 - val_loss: 2.5750e-04\n",
      "Epoch 1332/2000\n",
      "3622/3622 [==============================] - 1s 229us/step - loss: 1.5875e-04 - val_loss: 2.4271e-04\n",
      "Epoch 1333/2000\n",
      "3622/3622 [==============================] - 1s 230us/step - loss: 1.6019e-04 - val_loss: 3.0279e-04\n",
      "Epoch 1334/2000\n",
      "3622/3622 [==============================] - 1s 229us/step - loss: 1.6247e-04 - val_loss: 2.6472e-04\n",
      "Epoch 1335/2000\n",
      "3622/3622 [==============================] - 1s 228us/step - loss: 1.7995e-04 - val_loss: 2.6351e-04\n",
      "Epoch 1336/2000\n",
      "3622/3622 [==============================] - 1s 229us/step - loss: 1.8155e-04 - val_loss: 2.5628e-04\n",
      "Epoch 1337/2000\n",
      "3622/3622 [==============================] - 1s 230us/step - loss: 1.6095e-04 - val_loss: 2.2162e-04\n",
      "Epoch 1338/2000\n",
      "3622/3622 [==============================] - 1s 230us/step - loss: 1.4530e-04 - val_loss: 2.3866e-04\n",
      "Epoch 1339/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3622/3622 [==============================] - 1s 229us/step - loss: 1.6174e-04 - val_loss: 2.2276e-04\n",
      "Epoch 1340/2000\n",
      "3622/3622 [==============================] - 1s 228us/step - loss: 1.5570e-04 - val_loss: 2.6845e-04\n",
      "Epoch 1341/2000\n",
      "3622/3622 [==============================] - 1s 226us/step - loss: 1.5144e-04 - val_loss: 2.1811e-04\n",
      "Epoch 1342/2000\n",
      "3622/3622 [==============================] - 1s 228us/step - loss: 1.4730e-04 - val_loss: 2.1985e-04\n",
      "Epoch 1343/2000\n",
      "3622/3622 [==============================] - 1s 228us/step - loss: 1.4521e-04 - val_loss: 2.3483e-04\n",
      "Epoch 1344/2000\n",
      "3622/3622 [==============================] - 1s 230us/step - loss: 1.5018e-04 - val_loss: 2.3622e-04\n",
      "Epoch 1345/2000\n",
      "3622/3622 [==============================] - 1s 239us/step - loss: 1.5783e-04 - val_loss: 2.4863e-04\n",
      "Epoch 1346/2000\n",
      "3622/3622 [==============================] - 1s 232us/step - loss: 1.5146e-04 - val_loss: 2.2068e-04\n",
      "Epoch 1347/2000\n",
      "3622/3622 [==============================] - 1s 229us/step - loss: 1.4274e-04 - val_loss: 2.1961e-04\n",
      "Epoch 1348/2000\n",
      "3622/3622 [==============================] - 1s 234us/step - loss: 1.4169e-04 - val_loss: 2.2318e-04\n",
      "Epoch 1349/2000\n",
      "3622/3622 [==============================] - 1s 229us/step - loss: 1.4531e-04 - val_loss: 2.2166e-04\n",
      "Epoch 1350/2000\n",
      "3622/3622 [==============================] - 1s 229us/step - loss: 1.4157e-04 - val_loss: 2.3294e-04\n",
      "Epoch 1351/2000\n",
      "3622/3622 [==============================] - 1s 230us/step - loss: 1.4971e-04 - val_loss: 2.2108e-04\n",
      "Epoch 1352/2000\n",
      "3622/3622 [==============================] - 1s 230us/step - loss: 1.4544e-04 - val_loss: 2.5187e-04\n",
      "Epoch 1353/2000\n",
      "3622/3622 [==============================] - 1s 231us/step - loss: 1.4937e-04 - val_loss: 2.3130e-04\n",
      "Epoch 1354/2000\n",
      "3622/3622 [==============================] - 1s 229us/step - loss: 1.4823e-04 - val_loss: 2.2203e-04\n",
      "Epoch 1355/2000\n",
      "3622/3622 [==============================] - 1s 231us/step - loss: 1.4272e-04 - val_loss: 2.2121e-04\n",
      "Epoch 1356/2000\n",
      "3622/3622 [==============================] - 1s 230us/step - loss: 1.4656e-04 - val_loss: 2.2339e-04\n",
      "Epoch 1357/2000\n",
      "3622/3622 [==============================] - 1s 231us/step - loss: 1.4774e-04 - val_loss: 2.2204e-04\n",
      "Epoch 1358/2000\n",
      "3622/3622 [==============================] - 1s 230us/step - loss: 1.4568e-04 - val_loss: 2.2342e-04\n",
      "Epoch 1359/2000\n",
      "3622/3622 [==============================] - 1s 232us/step - loss: 1.4315e-04 - val_loss: 2.2800e-04\n",
      "Epoch 1360/2000\n",
      "3622/3622 [==============================] - 1s 229us/step - loss: 1.4748e-04 - val_loss: 2.6039e-04\n",
      "Epoch 1361/2000\n",
      "3622/3622 [==============================] - 1s 230us/step - loss: 1.4599e-04 - val_loss: 3.0106e-04\n",
      "Epoch 1362/2000\n",
      "3622/3622 [==============================] - 1s 228us/step - loss: 1.5350e-04 - val_loss: 2.2369e-04\n",
      "Epoch 1363/2000\n",
      "3622/3622 [==============================] - 1s 231us/step - loss: 1.4514e-04 - val_loss: 2.2318e-04\n",
      "Epoch 1364/2000\n",
      "3622/3622 [==============================] - 1s 233us/step - loss: 1.5043e-04 - val_loss: 2.3273e-04\n",
      "Epoch 1365/2000\n",
      "3622/3622 [==============================] - 1s 228us/step - loss: 1.5151e-04 - val_loss: 2.6637e-04\n",
      "Epoch 1366/2000\n",
      "3622/3622 [==============================] - 1s 229us/step - loss: 1.4588e-04 - val_loss: 2.2069e-04\n",
      "Epoch 1367/2000\n",
      "3622/3622 [==============================] - 1s 234us/step - loss: 1.4904e-04 - val_loss: 2.2072e-04\n",
      "Epoch 1368/2000\n",
      "3622/3622 [==============================] - 1s 230us/step - loss: 1.4830e-04 - val_loss: 2.2190e-04\n",
      "Epoch 1369/2000\n",
      "3622/3622 [==============================] - 1s 228us/step - loss: 1.4497e-04 - val_loss: 2.5475e-04\n",
      "Epoch 1370/2000\n",
      "3622/3622 [==============================] - 1s 229us/step - loss: 1.5215e-04 - val_loss: 2.3644e-04\n",
      "Epoch 1371/2000\n",
      "3622/3622 [==============================] - 1s 229us/step - loss: 1.4255e-04 - val_loss: 2.2756e-04\n",
      "Epoch 1372/2000\n",
      "3622/3622 [==============================] - 1s 229us/step - loss: 1.4106e-04 - val_loss: 2.5673e-04\n",
      "Epoch 1373/2000\n",
      "3622/3622 [==============================] - 1s 229us/step - loss: 1.6597e-04 - val_loss: 2.2009e-04\n",
      "Epoch 1374/2000\n",
      "3622/3622 [==============================] - 1s 229us/step - loss: 1.5359e-04 - val_loss: 2.6696e-04\n",
      "Epoch 1375/2000\n",
      "3622/3622 [==============================] - 1s 231us/step - loss: 1.5208e-04 - val_loss: 2.6090e-04\n",
      "Epoch 1376/2000\n",
      "3622/3622 [==============================] - 1s 230us/step - loss: 1.5259e-04 - val_loss: 2.2856e-04\n",
      "Epoch 1377/2000\n",
      "3622/3622 [==============================] - 1s 228us/step - loss: 1.5774e-04 - val_loss: 2.4944e-04\n",
      "Epoch 1378/2000\n",
      "3622/3622 [==============================] - 1s 229us/step - loss: 1.5368e-04 - val_loss: 2.3811e-04\n",
      "Epoch 1379/2000\n",
      "3622/3622 [==============================] - 1s 229us/step - loss: 1.5154e-04 - val_loss: 2.2651e-04\n",
      "Epoch 1380/2000\n",
      "3622/3622 [==============================] - 1s 229us/step - loss: 1.5046e-04 - val_loss: 2.6603e-04\n",
      "Epoch 1381/2000\n",
      "3622/3622 [==============================] - 1s 234us/step - loss: 1.5409e-04 - val_loss: 2.2240e-04\n",
      "Epoch 1382/2000\n",
      "3622/3622 [==============================] - 1s 232us/step - loss: 1.4222e-04 - val_loss: 2.1954e-04\n",
      "Epoch 1383/2000\n",
      "3622/3622 [==============================] - 1s 229us/step - loss: 1.4160e-04 - val_loss: 2.6071e-04\n",
      "Epoch 1384/2000\n",
      "3622/3622 [==============================] - 1s 230us/step - loss: 1.5389e-04 - val_loss: 2.2141e-04\n",
      "Epoch 1385/2000\n",
      "3622/3622 [==============================] - 1s 230us/step - loss: 1.4058e-04 - val_loss: 2.2036e-04\n",
      "Epoch 1386/2000\n",
      "3622/3622 [==============================] - 1s 234us/step - loss: 1.4369e-04 - val_loss: 2.5959e-04\n",
      "Epoch 1387/2000\n",
      "3622/3622 [==============================] - 1s 232us/step - loss: 1.5777e-04 - val_loss: 2.2050e-04\n",
      "Epoch 1388/2000\n",
      "3622/3622 [==============================] - 1s 231us/step - loss: 1.5578e-04 - val_loss: 2.1674e-04\n",
      "Epoch 1389/2000\n",
      "3622/3622 [==============================] - 1s 228us/step - loss: 1.5209e-04 - val_loss: 2.2253e-04\n",
      "Epoch 1390/2000\n",
      "3622/3622 [==============================] - 1s 229us/step - loss: 1.4232e-04 - val_loss: 2.3401e-04\n",
      "Epoch 1391/2000\n",
      "3622/3622 [==============================] - 1s 229us/step - loss: 1.4660e-04 - val_loss: 2.3787e-04\n",
      "Epoch 1392/2000\n",
      "3622/3622 [==============================] - 1s 229us/step - loss: 1.7154e-04 - val_loss: 3.1622e-04\n",
      "Epoch 1393/2000\n",
      "3622/3622 [==============================] - 1s 230us/step - loss: 1.9596e-04 - val_loss: 3.6738e-04\n",
      "Epoch 1394/2000\n",
      "3622/3622 [==============================] - 1s 228us/step - loss: 1.9254e-04 - val_loss: 2.3915e-04\n",
      "Epoch 1395/2000\n",
      "3622/3622 [==============================] - 1s 230us/step - loss: 1.4616e-04 - val_loss: 2.4113e-04\n",
      "Epoch 1396/2000\n",
      "3622/3622 [==============================] - 1s 228us/step - loss: 1.4487e-04 - val_loss: 2.1865e-04\n",
      "Epoch 1397/2000\n",
      "3622/3622 [==============================] - 1s 229us/step - loss: 1.4647e-04 - val_loss: 2.8934e-04\n",
      "Epoch 1398/2000\n",
      "3622/3622 [==============================] - 1s 229us/step - loss: 1.5871e-04 - val_loss: 2.5858e-04\n",
      "Epoch 1399/2000\n",
      "3622/3622 [==============================] - 1s 228us/step - loss: 1.4492e-04 - val_loss: 2.1970e-04\n",
      "Epoch 1400/2000\n",
      "3622/3622 [==============================] - 1s 229us/step - loss: 1.4163e-04 - val_loss: 2.2680e-04\n",
      "Epoch 1401/2000\n",
      "3622/3622 [==============================] - 1s 228us/step - loss: 1.4635e-04 - val_loss: 2.2227e-04\n",
      "Epoch 1402/2000\n",
      "3622/3622 [==============================] - 1s 232us/step - loss: 1.4494e-04 - val_loss: 3.1906e-04\n",
      "Epoch 1403/2000\n",
      "3622/3622 [==============================] - 1s 229us/step - loss: 1.6776e-04 - val_loss: 2.4928e-04\n",
      "Epoch 1404/2000\n",
      "3622/3622 [==============================] - 1s 229us/step - loss: 1.5127e-04 - val_loss: 2.2555e-04\n",
      "Epoch 1405/2000\n",
      "3622/3622 [==============================] - 1s 234us/step - loss: 1.4304e-04 - val_loss: 2.9984e-04\n",
      "Epoch 1406/2000\n",
      "3622/3622 [==============================] - 1s 232us/step - loss: 1.5482e-04 - val_loss: 2.3543e-04\n",
      "Epoch 1407/2000\n",
      "3622/3622 [==============================] - 1s 230us/step - loss: 1.5991e-04 - val_loss: 2.2378e-04\n",
      "Epoch 1408/2000\n",
      "3622/3622 [==============================] - 1s 230us/step - loss: 1.5354e-04 - val_loss: 2.5937e-04\n",
      "Epoch 1409/2000\n",
      "3622/3622 [==============================] - 1s 229us/step - loss: 1.4975e-04 - val_loss: 2.6132e-04\n",
      "Epoch 1410/2000\n",
      "3622/3622 [==============================] - 1s 231us/step - loss: 1.6273e-04 - val_loss: 2.9228e-04\n",
      "Epoch 1411/2000\n",
      "3622/3622 [==============================] - 1s 229us/step - loss: 1.7185e-04 - val_loss: 3.3792e-04\n",
      "Epoch 1412/2000\n",
      "3622/3622 [==============================] - 1s 230us/step - loss: 1.8186e-04 - val_loss: 4.0660e-04\n",
      "Epoch 1413/2000\n",
      "3622/3622 [==============================] - 1s 230us/step - loss: 1.7830e-04 - val_loss: 2.2242e-04\n",
      "Epoch 1414/2000\n",
      "3622/3622 [==============================] - 1s 231us/step - loss: 1.5121e-04 - val_loss: 2.2636e-04\n",
      "Epoch 1415/2000\n",
      "3622/3622 [==============================] - 1s 228us/step - loss: 1.3847e-04 - val_loss: 2.2647e-04\n",
      "Epoch 1416/2000\n",
      "3622/3622 [==============================] - 1s 230us/step - loss: 1.4220e-04 - val_loss: 2.4092e-04\n",
      "Epoch 1417/2000\n",
      "3622/3622 [==============================] - 1s 234us/step - loss: 1.5337e-04 - val_loss: 3.4411e-04\n",
      "Epoch 1418/2000\n",
      "3622/3622 [==============================] - 1s 229us/step - loss: 1.9999e-04 - val_loss: 3.3565e-04\n",
      "Epoch 1419/2000\n",
      "3622/3622 [==============================] - 1s 229us/step - loss: 1.7155e-04 - val_loss: 2.2973e-04\n",
      "Epoch 1420/2000\n",
      "3622/3622 [==============================] - 1s 229us/step - loss: 1.4527e-04 - val_loss: 2.4185e-04\n",
      "Epoch 1421/2000\n",
      "3622/3622 [==============================] - 1s 230us/step - loss: 1.5085e-04 - val_loss: 2.8606e-04\n",
      "Epoch 1422/2000\n",
      "3622/3622 [==============================] - 1s 229us/step - loss: 1.7888e-04 - val_loss: 2.5665e-04\n",
      "Epoch 1423/2000\n",
      "3622/3622 [==============================] - 1s 231us/step - loss: 1.5216e-04 - val_loss: 3.3721e-04\n",
      "Epoch 1424/2000\n",
      "3622/3622 [==============================] - 1s 230us/step - loss: 1.6522e-04 - val_loss: 2.4236e-04\n",
      "Epoch 1425/2000\n",
      "3622/3622 [==============================] - 1s 232us/step - loss: 1.7175e-04 - val_loss: 2.8556e-04\n",
      "Epoch 1426/2000\n",
      "3622/3622 [==============================] - 1s 229us/step - loss: 1.6655e-04 - val_loss: 2.2718e-04\n",
      "Epoch 1427/2000\n",
      "3622/3622 [==============================] - 1s 229us/step - loss: 1.4359e-04 - val_loss: 2.7181e-04\n",
      "Epoch 1428/2000\n",
      "3622/3622 [==============================] - 1s 229us/step - loss: 1.5393e-04 - val_loss: 2.3808e-04\n",
      "Epoch 1429/2000\n",
      "3622/3622 [==============================] - 1s 227us/step - loss: 1.5448e-04 - val_loss: 2.2322e-04\n",
      "Epoch 1430/2000\n",
      "3622/3622 [==============================] - 1s 229us/step - loss: 1.4185e-04 - val_loss: 2.4806e-04\n",
      "Epoch 1431/2000\n",
      "3622/3622 [==============================] - 1s 229us/step - loss: 1.4667e-04 - val_loss: 2.2804e-04\n",
      "Epoch 1432/2000\n",
      "3622/3622 [==============================] - 1s 229us/step - loss: 1.3924e-04 - val_loss: 2.2675e-04\n",
      "Epoch 1433/2000\n",
      "3622/3622 [==============================] - 1s 229us/step - loss: 1.4008e-04 - val_loss: 2.4236e-04\n",
      "Epoch 1434/2000\n",
      "3622/3622 [==============================] - 1s 233us/step - loss: 1.4496e-04 - val_loss: 2.6657e-04\n",
      "Epoch 1435/2000\n",
      "3622/3622 [==============================] - 1s 228us/step - loss: 1.5527e-04 - val_loss: 2.4738e-04\n",
      "Epoch 1436/2000\n",
      "3622/3622 [==============================] - 1s 230us/step - loss: 1.6180e-04 - val_loss: 2.6852e-04\n",
      "Epoch 1437/2000\n",
      "3622/3622 [==============================] - 1s 228us/step - loss: 1.4835e-04 - val_loss: 2.5090e-04\n",
      "Epoch 1438/2000\n",
      "3622/3622 [==============================] - 1s 229us/step - loss: 1.4784e-04 - val_loss: 2.3277e-04\n",
      "Epoch 1439/2000\n",
      "3622/3622 [==============================] - 1s 228us/step - loss: 1.5233e-04 - val_loss: 2.3967e-04\n",
      "Epoch 1440/2000\n",
      "3622/3622 [==============================] - 1s 230us/step - loss: 1.4360e-04 - val_loss: 2.8245e-04\n",
      "Epoch 1441/2000\n",
      "3622/3622 [==============================] - 1s 229us/step - loss: 1.5431e-04 - val_loss: 2.2691e-04\n",
      "Epoch 1442/2000\n",
      "3622/3622 [==============================] - 1s 227us/step - loss: 1.4255e-04 - val_loss: 2.9257e-04\n",
      "Epoch 1443/2000\n",
      "3622/3622 [==============================] - 1s 232us/step - loss: 1.6995e-04 - val_loss: 2.2354e-04\n",
      "Epoch 1444/2000\n",
      "3622/3622 [==============================] - 1s 233us/step - loss: 1.4401e-04 - val_loss: 2.7661e-04\n",
      "Epoch 1445/2000\n",
      "3622/3622 [==============================] - 1s 229us/step - loss: 1.4619e-04 - val_loss: 2.2128e-04\n",
      "Epoch 1446/2000\n",
      "3622/3622 [==============================] - 1s 230us/step - loss: 1.4178e-04 - val_loss: 2.1983e-04\n",
      "Epoch 1447/2000\n",
      "3622/3622 [==============================] - 1s 229us/step - loss: 1.4563e-04 - val_loss: 2.2336e-04\n",
      "Epoch 1448/2000\n",
      "3622/3622 [==============================] - 1s 231us/step - loss: 1.5130e-04 - val_loss: 2.3073e-04\n",
      "Epoch 1449/2000\n",
      "3622/3622 [==============================] - 1s 231us/step - loss: 1.4589e-04 - val_loss: 2.2156e-04\n",
      "Epoch 1450/2000\n",
      "3622/3622 [==============================] - 1s 229us/step - loss: 1.4980e-04 - val_loss: 2.2528e-04\n",
      "Epoch 1451/2000\n",
      "3622/3622 [==============================] - 1s 229us/step - loss: 1.5102e-04 - val_loss: 2.1951e-04\n",
      "Epoch 1452/2000\n",
      "3622/3622 [==============================] - 1s 229us/step - loss: 1.5314e-04 - val_loss: 3.2128e-04\n",
      "Epoch 1453/2000\n",
      "3622/3622 [==============================] - 1s 238us/step - loss: 2.0780e-04 - val_loss: 3.0205e-04\n",
      "Epoch 1454/2000\n",
      "3622/3622 [==============================] - 1s 229us/step - loss: 1.7929e-04 - val_loss: 4.4376e-04\n",
      "Epoch 1455/2000\n",
      "3622/3622 [==============================] - 1s 229us/step - loss: 2.0036e-04 - val_loss: 2.7598e-04\n",
      "Epoch 1456/2000\n",
      "3622/3622 [==============================] - 1s 230us/step - loss: 1.7862e-04 - val_loss: 2.5234e-04\n",
      "Epoch 1457/2000\n",
      "3622/3622 [==============================] - 1s 228us/step - loss: 1.6206e-04 - val_loss: 2.4873e-04\n",
      "Epoch 1458/2000\n",
      "3622/3622 [==============================] - 1s 228us/step - loss: 1.5335e-04 - val_loss: 2.2372e-04\n",
      "Epoch 1459/2000\n",
      "3622/3622 [==============================] - 1s 229us/step - loss: 1.4229e-04 - val_loss: 2.3717e-04\n",
      "Epoch 1460/2000\n",
      "3622/3622 [==============================] - 1s 228us/step - loss: 1.4228e-04 - val_loss: 2.2585e-04\n",
      "Epoch 1461/2000\n",
      "3622/3622 [==============================] - 1s 230us/step - loss: 1.3789e-04 - val_loss: 2.5771e-04\n",
      "Epoch 1462/2000\n",
      "3622/3622 [==============================] - 1s 227us/step - loss: 1.5059e-04 - val_loss: 2.3199e-04\n",
      "Epoch 1463/2000\n",
      "3622/3622 [==============================] - 1s 234us/step - loss: 1.3523e-04 - val_loss: 2.1689e-04\n",
      "Epoch 1464/2000\n",
      "3622/3622 [==============================] - 1s 229us/step - loss: 1.3537e-04 - val_loss: 2.2059e-04\n",
      "Epoch 1465/2000\n",
      "3622/3622 [==============================] - 1s 229us/step - loss: 1.4151e-04 - val_loss: 2.2396e-04\n",
      "Epoch 1466/2000\n",
      "3622/3622 [==============================] - 1s 227us/step - loss: 1.4953e-04 - val_loss: 2.1942e-04\n",
      "Epoch 1467/2000\n",
      "3622/3622 [==============================] - 1s 230us/step - loss: 1.4152e-04 - val_loss: 2.2923e-04\n",
      "Epoch 1468/2000\n",
      "3622/3622 [==============================] - 1s 229us/step - loss: 1.4016e-04 - val_loss: 2.6527e-04\n",
      "Epoch 1469/2000\n",
      "3622/3622 [==============================] - 1s 230us/step - loss: 1.5031e-04 - val_loss: 2.2938e-04\n",
      "Epoch 1470/2000\n",
      "3622/3622 [==============================] - 1s 228us/step - loss: 1.4839e-04 - val_loss: 2.9027e-04\n",
      "Epoch 1471/2000\n",
      "3622/3622 [==============================] - 1s 229us/step - loss: 1.7250e-04 - val_loss: 2.2936e-04\n",
      "Epoch 1472/2000\n",
      "3622/3622 [==============================] - 1s 229us/step - loss: 1.5317e-04 - val_loss: 2.7091e-04\n",
      "Epoch 1473/2000\n",
      "3622/3622 [==============================] - 1s 229us/step - loss: 1.4406e-04 - val_loss: 2.3265e-04\n",
      "Epoch 1474/2000\n",
      "3622/3622 [==============================] - 1s 229us/step - loss: 1.4750e-04 - val_loss: 2.2652e-04\n",
      "Epoch 1475/2000\n",
      "3622/3622 [==============================] - 1s 228us/step - loss: 1.4348e-04 - val_loss: 2.1693e-04\n",
      "Epoch 1476/2000\n",
      "3622/3622 [==============================] - 1s 228us/step - loss: 1.3733e-04 - val_loss: 2.1875e-04\n",
      "Epoch 1477/2000\n",
      "3622/3622 [==============================] - 1s 229us/step - loss: 1.3756e-04 - val_loss: 2.2604e-04\n",
      "Epoch 1478/2000\n",
      "3622/3622 [==============================] - 1s 229us/step - loss: 1.3460e-04 - val_loss: 2.2299e-04\n",
      "Epoch 1479/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3622/3622 [==============================] - 1s 228us/step - loss: 1.3658e-04 - val_loss: 2.2096e-04\n",
      "Epoch 1480/2000\n",
      "3622/3622 [==============================] - 1s 230us/step - loss: 1.3676e-04 - val_loss: 2.1867e-04\n",
      "Epoch 1481/2000\n",
      "3622/3622 [==============================] - 1s 229us/step - loss: 1.3516e-04 - val_loss: 2.2470e-04\n",
      "Epoch 1482/2000\n",
      "3622/3622 [==============================] - 1s 234us/step - loss: 1.3342e-04 - val_loss: 2.2005e-04\n",
      "Epoch 1483/2000\n",
      "3622/3622 [==============================] - 1s 230us/step - loss: 1.3941e-04 - val_loss: 2.2984e-04\n",
      "Epoch 1484/2000\n",
      "3622/3622 [==============================] - 1s 229us/step - loss: 1.3592e-04 - val_loss: 2.2964e-04\n",
      "Epoch 1485/2000\n",
      "3622/3622 [==============================] - 1s 228us/step - loss: 1.4073e-04 - val_loss: 3.2702e-04\n",
      "Epoch 1486/2000\n",
      "3622/3622 [==============================] - 1s 230us/step - loss: 1.6235e-04 - val_loss: 3.2608e-04\n",
      "Epoch 1487/2000\n",
      "3622/3622 [==============================] - 1s 229us/step - loss: 1.7911e-04 - val_loss: 2.4567e-04\n",
      "Epoch 1488/2000\n",
      "3622/3622 [==============================] - 1s 229us/step - loss: 1.4884e-04 - val_loss: 2.4937e-04\n",
      "Epoch 1489/2000\n",
      "3622/3622 [==============================] - 1s 235us/step - loss: 1.4801e-04 - val_loss: 2.3772e-04\n",
      "Epoch 1490/2000\n",
      "3622/3622 [==============================] - 1s 228us/step - loss: 1.5025e-04 - val_loss: 2.7301e-04\n",
      "Epoch 1491/2000\n",
      "3622/3622 [==============================] - 1s 228us/step - loss: 1.4797e-04 - val_loss: 4.0274e-04\n",
      "Epoch 1492/2000\n",
      "3622/3622 [==============================] - 1s 230us/step - loss: 1.8393e-04 - val_loss: 2.6156e-04\n",
      "Epoch 1493/2000\n",
      "3622/3622 [==============================] - 1s 229us/step - loss: 1.5887e-04 - val_loss: 2.1928e-04\n",
      "Epoch 1494/2000\n",
      "3622/3622 [==============================] - 1s 228us/step - loss: 1.4523e-04 - val_loss: 2.7154e-04\n",
      "Epoch 1495/2000\n",
      "3622/3622 [==============================] - 1s 230us/step - loss: 1.4306e-04 - val_loss: 2.5855e-04\n",
      "Epoch 1496/2000\n",
      "3622/3622 [==============================] - 1s 230us/step - loss: 1.4409e-04 - val_loss: 2.6504e-04\n",
      "Epoch 1497/2000\n",
      "3622/3622 [==============================] - 1s 230us/step - loss: 1.6066e-04 - val_loss: 2.3495e-04\n",
      "Epoch 1498/2000\n",
      "3622/3622 [==============================] - 1s 229us/step - loss: 1.4653e-04 - val_loss: 3.1078e-04\n",
      "Epoch 1499/2000\n",
      "3622/3622 [==============================] - 1s 231us/step - loss: 1.5064e-04 - val_loss: 2.2057e-04\n",
      "Epoch 1500/2000\n",
      "3622/3622 [==============================] - 1s 228us/step - loss: 1.4408e-04 - val_loss: 2.3050e-04\n",
      "Epoch 1501/2000\n",
      "3622/3622 [==============================] - 1s 234us/step - loss: 1.4072e-04 - val_loss: 2.1671e-04\n",
      "Epoch 1502/2000\n",
      "3622/3622 [==============================] - 1s 229us/step - loss: 1.3212e-04 - val_loss: 2.2344e-04\n",
      "Epoch 1503/2000\n",
      "3622/3622 [==============================] - 1s 228us/step - loss: 1.3314e-04 - val_loss: 2.2739e-04\n",
      "Epoch 1504/2000\n",
      "3622/3622 [==============================] - 1s 229us/step - loss: 1.4089e-04 - val_loss: 2.3527e-04\n",
      "Epoch 1505/2000\n",
      "3622/3622 [==============================] - 1s 229us/step - loss: 1.4090e-04 - val_loss: 2.2552e-04\n",
      "Epoch 1506/2000\n",
      "3622/3622 [==============================] - 1s 230us/step - loss: 1.3160e-04 - val_loss: 2.2804e-04\n",
      "Epoch 1507/2000\n",
      "3622/3622 [==============================] - 1s 229us/step - loss: 1.4526e-04 - val_loss: 2.9824e-04\n",
      "Epoch 1508/2000\n",
      "3622/3622 [==============================] - 1s 229us/step - loss: 1.7820e-04 - val_loss: 2.3671e-04\n",
      "Epoch 1509/2000\n",
      "3622/3622 [==============================] - 1s 231us/step - loss: 1.4813e-04 - val_loss: 2.3621e-04\n",
      "Epoch 1510/2000\n",
      "3622/3622 [==============================] - 1s 231us/step - loss: 1.4021e-04 - val_loss: 2.2581e-04\n",
      "Epoch 1511/2000\n",
      "3622/3622 [==============================] - 1s 229us/step - loss: 1.3596e-04 - val_loss: 2.4329e-04\n",
      "Epoch 1512/2000\n",
      "3622/3622 [==============================] - 1s 229us/step - loss: 1.3822e-04 - val_loss: 2.3795e-04\n",
      "Epoch 1513/2000\n",
      "3622/3622 [==============================] - 1s 228us/step - loss: 1.3633e-04 - val_loss: 2.6829e-04\n",
      "Epoch 1514/2000\n",
      "3622/3622 [==============================] - 1s 226us/step - loss: 1.4989e-04 - val_loss: 2.2001e-04\n",
      "Epoch 1515/2000\n",
      "3622/3622 [==============================] - 1s 227us/step - loss: 1.3693e-04 - val_loss: 2.2659e-04\n",
      "Epoch 1516/2000\n",
      "3622/3622 [==============================] - 1s 229us/step - loss: 1.3323e-04 - val_loss: 2.4845e-04\n",
      "Epoch 1517/2000\n",
      "3622/3622 [==============================] - 1s 230us/step - loss: 1.3747e-04 - val_loss: 2.2280e-04\n",
      "Epoch 1518/2000\n",
      "3622/3622 [==============================] - 1s 229us/step - loss: 1.3633e-04 - val_loss: 2.3774e-04\n",
      "Epoch 1519/2000\n",
      "3622/3622 [==============================] - 1s 229us/step - loss: 1.3664e-04 - val_loss: 2.6269e-04\n",
      "Epoch 1520/2000\n",
      "3622/3622 [==============================] - 1s 232us/step - loss: 1.3587e-04 - val_loss: 2.1902e-04\n",
      "Epoch 1521/2000\n",
      "3622/3622 [==============================] - 1s 230us/step - loss: 1.3814e-04 - val_loss: 2.8387e-04\n",
      "Epoch 1522/2000\n",
      "3622/3622 [==============================] - 1s 229us/step - loss: 1.5956e-04 - val_loss: 2.8575e-04\n",
      "Epoch 1523/2000\n",
      "3622/3622 [==============================] - 1s 229us/step - loss: 1.5746e-04 - val_loss: 2.1876e-04\n",
      "Epoch 1524/2000\n",
      "3622/3622 [==============================] - 1s 230us/step - loss: 1.4109e-04 - val_loss: 2.4028e-04\n",
      "Epoch 1525/2000\n",
      "3622/3622 [==============================] - 1s 234us/step - loss: 1.4085e-04 - val_loss: 3.2636e-04\n",
      "Epoch 1526/2000\n",
      "3622/3622 [==============================] - 1s 228us/step - loss: 1.5279e-04 - val_loss: 2.6636e-04\n",
      "Epoch 1527/2000\n",
      "3622/3622 [==============================] - 1s 229us/step - loss: 1.3976e-04 - val_loss: 2.2018e-04\n",
      "Epoch 1528/2000\n",
      "3622/3622 [==============================] - 1s 230us/step - loss: 1.3576e-04 - val_loss: 2.2479e-04\n",
      "Epoch 1529/2000\n",
      "3622/3622 [==============================] - 1s 229us/step - loss: 1.3655e-04 - val_loss: 2.3344e-04\n",
      "Epoch 1530/2000\n",
      "3622/3622 [==============================] - 1s 230us/step - loss: 1.4602e-04 - val_loss: 2.2337e-04\n",
      "Epoch 1531/2000\n",
      "3622/3622 [==============================] - 1s 229us/step - loss: 1.3494e-04 - val_loss: 2.2492e-04\n",
      "Epoch 1532/2000\n",
      "3622/3622 [==============================] - 1s 229us/step - loss: 1.3575e-04 - val_loss: 2.6830e-04\n",
      "Epoch 1533/2000\n",
      "3622/3622 [==============================] - 1s 228us/step - loss: 1.3800e-04 - val_loss: 2.3444e-04\n",
      "Epoch 1534/2000\n",
      "3622/3622 [==============================] - 1s 229us/step - loss: 1.4321e-04 - val_loss: 2.2043e-04\n",
      "Epoch 1535/2000\n",
      "3622/3622 [==============================] - 1s 231us/step - loss: 1.3683e-04 - val_loss: 2.7659e-04\n",
      "Epoch 1536/2000\n",
      "3622/3622 [==============================] - 1s 229us/step - loss: 1.3850e-04 - val_loss: 2.3464e-04\n",
      "Epoch 1537/2000\n",
      "3622/3622 [==============================] - 1s 230us/step - loss: 1.5985e-04 - val_loss: 3.4081e-04\n",
      "Epoch 1538/2000\n",
      "3622/3622 [==============================] - 1s 230us/step - loss: 1.6091e-04 - val_loss: 2.8364e-04\n",
      "Epoch 1539/2000\n",
      "3622/3622 [==============================] - 1s 232us/step - loss: 1.4519e-04 - val_loss: 2.2788e-04\n",
      "Epoch 1540/2000\n",
      "3622/3622 [==============================] - 1s 231us/step - loss: 1.4227e-04 - val_loss: 2.9092e-04\n",
      "Epoch 1541/2000\n",
      "3622/3622 [==============================] - 1s 229us/step - loss: 1.5671e-04 - val_loss: 2.2505e-04\n",
      "Epoch 1542/2000\n",
      "3622/3622 [==============================] - 1s 230us/step - loss: 1.3434e-04 - val_loss: 2.3942e-04\n",
      "Epoch 1543/2000\n",
      "3622/3622 [==============================] - 1s 229us/step - loss: 1.4442e-04 - val_loss: 2.3714e-04\n",
      "Epoch 1544/2000\n",
      "3622/3622 [==============================] - 1s 229us/step - loss: 1.3514e-04 - val_loss: 2.4098e-04\n",
      "Epoch 1545/2000\n",
      "3622/3622 [==============================] - 1s 229us/step - loss: 1.3900e-04 - val_loss: 2.2524e-04\n",
      "Epoch 1546/2000\n",
      "3622/3622 [==============================] - 1s 229us/step - loss: 1.4651e-04 - val_loss: 2.3182e-04\n",
      "Epoch 1547/2000\n",
      "3622/3622 [==============================] - 1s 229us/step - loss: 1.2925e-04 - val_loss: 2.3507e-04\n",
      "Epoch 1548/2000\n",
      "3622/3622 [==============================] - 1s 229us/step - loss: 1.3960e-04 - val_loss: 2.5816e-04\n",
      "Epoch 1549/2000\n",
      "3622/3622 [==============================] - 1s 230us/step - loss: 1.4486e-04 - val_loss: 2.5317e-04\n",
      "Epoch 1550/2000\n",
      "3622/3622 [==============================] - 1s 229us/step - loss: 1.5518e-04 - val_loss: 2.2113e-04\n",
      "Epoch 1551/2000\n",
      "3622/3622 [==============================] - 1s 227us/step - loss: 1.4306e-04 - val_loss: 3.2686e-04\n",
      "Epoch 1552/2000\n",
      "3622/3622 [==============================] - 1s 230us/step - loss: 1.8296e-04 - val_loss: 2.2014e-04\n",
      "Epoch 1553/2000\n",
      "3622/3622 [==============================] - 1s 229us/step - loss: 1.4982e-04 - val_loss: 2.9960e-04\n",
      "Epoch 1554/2000\n",
      "3622/3622 [==============================] - 1s 230us/step - loss: 1.5611e-04 - val_loss: 2.4597e-04\n",
      "Epoch 1555/2000\n",
      "3622/3622 [==============================] - 1s 228us/step - loss: 1.5393e-04 - val_loss: 2.2812e-04\n",
      "Epoch 1556/2000\n",
      "3622/3622 [==============================] - 1s 229us/step - loss: 1.3405e-04 - val_loss: 2.2185e-04\n",
      "Epoch 1557/2000\n",
      "3622/3622 [==============================] - 1s 230us/step - loss: 1.3457e-04 - val_loss: 3.4485e-04\n",
      "Epoch 1558/2000\n",
      "3622/3622 [==============================] - 1s 232us/step - loss: 1.5413e-04 - val_loss: 2.3987e-04\n",
      "Epoch 1559/2000\n",
      "3622/3622 [==============================] - 1s 234us/step - loss: 1.4252e-04 - val_loss: 3.1918e-04\n",
      "Epoch 1560/2000\n",
      "3622/3622 [==============================] - 1s 230us/step - loss: 1.7006e-04 - val_loss: 2.3101e-04\n",
      "Epoch 1561/2000\n",
      "3622/3622 [==============================] - 1s 237us/step - loss: 1.7060e-04 - val_loss: 3.1838e-04\n",
      "Epoch 1562/2000\n",
      "3622/3622 [==============================] - 1s 230us/step - loss: 1.3996e-04 - val_loss: 2.2831e-04\n",
      "Epoch 1563/2000\n",
      "3622/3622 [==============================] - 1s 228us/step - loss: 1.5478e-04 - val_loss: 2.8156e-04\n",
      "Epoch 1564/2000\n",
      "3622/3622 [==============================] - 1s 231us/step - loss: 1.4813e-04 - val_loss: 2.1886e-04\n",
      "Epoch 1565/2000\n",
      "3622/3622 [==============================] - 1s 228us/step - loss: 1.4486e-04 - val_loss: 3.3413e-04\n",
      "Epoch 1566/2000\n",
      "3622/3622 [==============================] - 1s 230us/step - loss: 1.4612e-04 - val_loss: 2.5443e-04\n",
      "Epoch 1567/2000\n",
      "3622/3622 [==============================] - 1s 229us/step - loss: 1.4224e-04 - val_loss: 2.3636e-04\n",
      "Epoch 1568/2000\n",
      "3622/3622 [==============================] - 1s 229us/step - loss: 1.3211e-04 - val_loss: 2.2580e-04\n",
      "Epoch 1569/2000\n",
      "3622/3622 [==============================] - 1s 229us/step - loss: 1.2698e-04 - val_loss: 2.2416e-04\n",
      "Epoch 1570/2000\n",
      "3622/3622 [==============================] - 1s 230us/step - loss: 1.2496e-04 - val_loss: 2.2423e-04\n",
      "Epoch 1571/2000\n",
      "3622/3622 [==============================] - 1s 229us/step - loss: 1.2527e-04 - val_loss: 2.5175e-04\n",
      "Epoch 1572/2000\n",
      "3622/3622 [==============================] - 1s 228us/step - loss: 1.4276e-04 - val_loss: 2.2809e-04\n",
      "Epoch 1573/2000\n",
      "3622/3622 [==============================] - 1s 230us/step - loss: 1.5317e-04 - val_loss: 4.5584e-04\n",
      "Epoch 1574/2000\n",
      "3622/3622 [==============================] - 1s 228us/step - loss: 1.6531e-04 - val_loss: 2.7861e-04\n",
      "Epoch 1575/2000\n",
      "3622/3622 [==============================] - 1s 230us/step - loss: 1.7237e-04 - val_loss: 2.5373e-04\n",
      "Epoch 1576/2000\n",
      "3622/3622 [==============================] - 1s 230us/step - loss: 1.6630e-04 - val_loss: 2.2364e-04\n",
      "Epoch 1577/2000\n",
      "3622/3622 [==============================] - 1s 232us/step - loss: 1.4247e-04 - val_loss: 2.4389e-04\n",
      "Epoch 1578/2000\n",
      "3622/3622 [==============================] - 1s 234us/step - loss: 1.3502e-04 - val_loss: 2.5322e-04\n",
      "Epoch 1579/2000\n",
      "3622/3622 [==============================] - 1s 230us/step - loss: 1.3996e-04 - val_loss: 2.3193e-04\n",
      "Epoch 1580/2000\n",
      "3622/3622 [==============================] - 1s 231us/step - loss: 1.2813e-04 - val_loss: 2.3172e-04\n",
      "Epoch 1581/2000\n",
      "3622/3622 [==============================] - 1s 228us/step - loss: 1.3241e-04 - val_loss: 2.8574e-04\n",
      "Epoch 1582/2000\n",
      "3622/3622 [==============================] - 1s 229us/step - loss: 1.5081e-04 - val_loss: 2.3930e-04\n",
      "Epoch 1583/2000\n",
      "3622/3622 [==============================] - 1s 229us/step - loss: 1.4547e-04 - val_loss: 2.6083e-04\n",
      "Epoch 1584/2000\n",
      "3622/3622 [==============================] - 1s 229us/step - loss: 1.3297e-04 - val_loss: 2.3541e-04\n",
      "Epoch 1585/2000\n",
      "3622/3622 [==============================] - 1s 230us/step - loss: 1.3383e-04 - val_loss: 2.5412e-04\n",
      "Epoch 1586/2000\n",
      "3622/3622 [==============================] - 1s 229us/step - loss: 1.3999e-04 - val_loss: 2.3339e-04\n",
      "Epoch 1587/2000\n",
      "3622/3622 [==============================] - 1s 229us/step - loss: 1.2881e-04 - val_loss: 2.5377e-04\n",
      "Epoch 1588/2000\n",
      "3622/3622 [==============================] - 1s 230us/step - loss: 1.2666e-04 - val_loss: 2.2606e-04\n",
      "Epoch 1589/2000\n",
      "3622/3622 [==============================] - 1s 229us/step - loss: 1.3002e-04 - val_loss: 2.6764e-04\n",
      "Epoch 1590/2000\n",
      "3622/3622 [==============================] - 1s 230us/step - loss: 1.3051e-04 - val_loss: 2.3132e-04\n",
      "Epoch 1591/2000\n",
      "3622/3622 [==============================] - 1s 229us/step - loss: 1.2650e-04 - val_loss: 3.4571e-04\n",
      "Epoch 1592/2000\n",
      "3622/3622 [==============================] - 1s 230us/step - loss: 1.6428e-04 - val_loss: 2.5726e-04\n",
      "Epoch 1593/2000\n",
      "3622/3622 [==============================] - 1s 229us/step - loss: 1.4950e-04 - val_loss: 3.4554e-04\n",
      "Epoch 1594/2000\n",
      "3622/3622 [==============================] - 1s 230us/step - loss: 1.6350e-04 - val_loss: 2.2620e-04\n",
      "Epoch 1595/2000\n",
      "3622/3622 [==============================] - 1s 229us/step - loss: 1.4711e-04 - val_loss: 2.6221e-04\n",
      "Epoch 1596/2000\n",
      "3622/3622 [==============================] - 1s 229us/step - loss: 1.4605e-04 - val_loss: 2.2005e-04\n",
      "Epoch 1597/2000\n",
      "3622/3622 [==============================] - 1s 242us/step - loss: 1.2815e-04 - val_loss: 2.2996e-04\n",
      "Epoch 1598/2000\n",
      "3622/3622 [==============================] - 1s 229us/step - loss: 1.2814e-04 - val_loss: 2.4533e-04\n",
      "Epoch 1599/2000\n",
      "3622/3622 [==============================] - 1s 228us/step - loss: 1.2645e-04 - val_loss: 2.3036e-04\n",
      "Epoch 1600/2000\n",
      "3622/3622 [==============================] - 1s 228us/step - loss: 1.2441e-04 - val_loss: 2.2850e-04\n",
      "Epoch 1601/2000\n",
      "3622/3622 [==============================] - 1s 230us/step - loss: 1.2655e-04 - val_loss: 2.4946e-04\n",
      "Epoch 1602/2000\n",
      "3622/3622 [==============================] - 1s 231us/step - loss: 1.3450e-04 - val_loss: 2.4729e-04\n",
      "Epoch 1603/2000\n",
      "3622/3622 [==============================] - 1s 229us/step - loss: 1.2971e-04 - val_loss: 2.2718e-04\n",
      "Epoch 1604/2000\n",
      "3622/3622 [==============================] - 1s 229us/step - loss: 1.2813e-04 - val_loss: 4.5631e-04\n",
      "Epoch 1605/2000\n",
      "3622/3622 [==============================] - 1s 229us/step - loss: 1.7525e-04 - val_loss: 2.3876e-04\n",
      "Epoch 1606/2000\n",
      "3622/3622 [==============================] - 1s 229us/step - loss: 1.3943e-04 - val_loss: 2.5465e-04\n",
      "Epoch 1607/2000\n",
      "3622/3622 [==============================] - 1s 231us/step - loss: 1.4455e-04 - val_loss: 2.5175e-04\n",
      "Epoch 1608/2000\n",
      "3622/3622 [==============================] - 1s 231us/step - loss: 1.3696e-04 - val_loss: 2.4267e-04\n",
      "Epoch 1609/2000\n",
      "3622/3622 [==============================] - 1s 231us/step - loss: 1.2422e-04 - val_loss: 2.2911e-04\n",
      "Epoch 1610/2000\n",
      "3622/3622 [==============================] - 1s 230us/step - loss: 1.4160e-04 - val_loss: 2.3363e-04\n",
      "Epoch 1611/2000\n",
      "3622/3622 [==============================] - 1s 229us/step - loss: 1.4544e-04 - val_loss: 2.4840e-04\n",
      "Epoch 1612/2000\n",
      "3622/3622 [==============================] - 1s 235us/step - loss: 1.3607e-04 - val_loss: 3.2898e-04\n",
      "Epoch 1613/2000\n",
      "3622/3622 [==============================] - 1s 231us/step - loss: 1.4688e-04 - val_loss: 2.8715e-04\n",
      "Epoch 1614/2000\n",
      "3622/3622 [==============================] - 1s 227us/step - loss: 1.5769e-04 - val_loss: 2.2389e-04\n",
      "Epoch 1615/2000\n",
      "3622/3622 [==============================] - 1s 229us/step - loss: 1.3954e-04 - val_loss: 2.9129e-04\n",
      "Epoch 1616/2000\n",
      "3622/3622 [==============================] - 1s 234us/step - loss: 1.4885e-04 - val_loss: 2.7420e-04\n",
      "Epoch 1617/2000\n",
      "3622/3622 [==============================] - 1s 229us/step - loss: 1.3776e-04 - val_loss: 2.4303e-04\n",
      "Epoch 1618/2000\n",
      "3622/3622 [==============================] - 1s 228us/step - loss: 1.2884e-04 - val_loss: 2.3066e-04\n",
      "Epoch 1619/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3622/3622 [==============================] - 1s 230us/step - loss: 1.2166e-04 - val_loss: 2.7640e-04\n",
      "Epoch 1620/2000\n",
      "3622/3622 [==============================] - 1s 229us/step - loss: 1.2505e-04 - val_loss: 2.3298e-04\n",
      "Epoch 1621/2000\n",
      "3622/3622 [==============================] - 1s 231us/step - loss: 1.2298e-04 - val_loss: 2.3202e-04\n",
      "Epoch 1622/2000\n",
      "3622/3622 [==============================] - 1s 230us/step - loss: 1.2212e-04 - val_loss: 2.3309e-04\n",
      "Epoch 1623/2000\n",
      "3622/3622 [==============================] - 1s 228us/step - loss: 1.2802e-04 - val_loss: 2.4492e-04\n",
      "Epoch 1624/2000\n",
      "3622/3622 [==============================] - 1s 230us/step - loss: 1.2950e-04 - val_loss: 2.2801e-04\n",
      "Epoch 1625/2000\n",
      "3622/3622 [==============================] - 1s 229us/step - loss: 1.3335e-04 - val_loss: 2.3306e-04\n",
      "Epoch 1626/2000\n",
      "3622/3622 [==============================] - 1s 229us/step - loss: 1.3413e-04 - val_loss: 2.3379e-04\n",
      "Epoch 1627/2000\n",
      "3622/3622 [==============================] - 1s 231us/step - loss: 1.3349e-04 - val_loss: 2.3610e-04\n",
      "Epoch 1628/2000\n",
      "3622/3622 [==============================] - 1s 227us/step - loss: 1.2183e-04 - val_loss: 2.2750e-04\n",
      "Epoch 1629/2000\n",
      "3622/3622 [==============================] - 1s 230us/step - loss: 1.3168e-04 - val_loss: 2.3253e-04\n",
      "Epoch 1630/2000\n",
      "3622/3622 [==============================] - 1s 232us/step - loss: 1.2828e-04 - val_loss: 2.3380e-04\n",
      "Epoch 1631/2000\n",
      "3622/3622 [==============================] - 1s 229us/step - loss: 1.2546e-04 - val_loss: 2.2799e-04\n",
      "Epoch 1632/2000\n",
      "3622/3622 [==============================] - 1s 230us/step - loss: 1.2822e-04 - val_loss: 2.4049e-04\n",
      "Epoch 1633/2000\n",
      "3622/3622 [==============================] - 1s 233us/step - loss: 1.2527e-04 - val_loss: 2.7052e-04\n",
      "Epoch 1634/2000\n",
      "3622/3622 [==============================] - 1s 228us/step - loss: 1.4685e-04 - val_loss: 2.9281e-04\n",
      "Epoch 1635/2000\n",
      "3622/3622 [==============================] - 1s 233us/step - loss: 1.5968e-04 - val_loss: 2.2663e-04\n",
      "Epoch 1636/2000\n",
      "3622/3622 [==============================] - 1s 229us/step - loss: 1.5490e-04 - val_loss: 2.9850e-04\n",
      "Epoch 1637/2000\n",
      "3622/3622 [==============================] - 1s 229us/step - loss: 1.3996e-04 - val_loss: 2.5138e-04\n",
      "Epoch 1638/2000\n",
      "3622/3622 [==============================] - 1s 227us/step - loss: 1.2902e-04 - val_loss: 2.8855e-04\n",
      "Epoch 1639/2000\n",
      "3622/3622 [==============================] - 1s 228us/step - loss: 1.5301e-04 - val_loss: 2.3152e-04\n",
      "Epoch 1640/2000\n",
      "3622/3622 [==============================] - 1s 230us/step - loss: 1.3242e-04 - val_loss: 2.9762e-04\n",
      "Epoch 1641/2000\n",
      "3622/3622 [==============================] - 1s 230us/step - loss: 1.4580e-04 - val_loss: 2.8828e-04\n",
      "Epoch 1642/2000\n",
      "3622/3622 [==============================] - 1s 229us/step - loss: 1.5083e-04 - val_loss: 2.2176e-04\n",
      "Epoch 1643/2000\n",
      "3622/3622 [==============================] - 1s 228us/step - loss: 1.4480e-04 - val_loss: 2.6420e-04\n",
      "Epoch 1644/2000\n",
      "3622/3622 [==============================] - 1s 229us/step - loss: 1.2614e-04 - val_loss: 2.4980e-04\n",
      "Epoch 1645/2000\n",
      "3622/3622 [==============================] - 1s 231us/step - loss: 1.2412e-04 - val_loss: 2.3570e-04\n",
      "Epoch 1646/2000\n",
      "3622/3622 [==============================] - 1s 229us/step - loss: 1.2062e-04 - val_loss: 2.3528e-04\n",
      "Epoch 1647/2000\n",
      "3622/3622 [==============================] - 1s 228us/step - loss: 1.2827e-04 - val_loss: 2.4097e-04\n",
      "Epoch 1648/2000\n",
      "3622/3622 [==============================] - 1s 230us/step - loss: 1.2649e-04 - val_loss: 2.3322e-04\n",
      "Epoch 1649/2000\n",
      "3622/3622 [==============================] - 1s 229us/step - loss: 1.3099e-04 - val_loss: 2.2586e-04\n",
      "Epoch 1650/2000\n",
      "3622/3622 [==============================] - 1s 229us/step - loss: 1.3160e-04 - val_loss: 2.5344e-04\n",
      "Epoch 1651/2000\n",
      "3622/3622 [==============================] - 1s 229us/step - loss: 1.2240e-04 - val_loss: 2.2271e-04\n",
      "Epoch 1652/2000\n",
      "3622/3622 [==============================] - 1s 229us/step - loss: 1.1774e-04 - val_loss: 2.6661e-04\n",
      "Epoch 1653/2000\n",
      "3622/3622 [==============================] - 1s 229us/step - loss: 1.2195e-04 - val_loss: 2.2479e-04\n",
      "Epoch 1654/2000\n",
      "3622/3622 [==============================] - 1s 231us/step - loss: 1.2033e-04 - val_loss: 2.3794e-04\n",
      "Epoch 1655/2000\n",
      "3622/3622 [==============================] - 1s 232us/step - loss: 1.2097e-04 - val_loss: 2.2588e-04\n",
      "Epoch 1656/2000\n",
      "3622/3622 [==============================] - 1s 229us/step - loss: 1.2639e-04 - val_loss: 2.6548e-04\n",
      "Epoch 1657/2000\n",
      "3622/3622 [==============================] - 1s 228us/step - loss: 1.2934e-04 - val_loss: 2.3970e-04\n",
      "Epoch 1658/2000\n",
      "3622/3622 [==============================] - 1s 229us/step - loss: 1.2705e-04 - val_loss: 2.3155e-04\n",
      "Epoch 1659/2000\n",
      "3622/3622 [==============================] - 1s 228us/step - loss: 1.2097e-04 - val_loss: 2.5589e-04\n",
      "Epoch 1660/2000\n",
      "3622/3622 [==============================] - 1s 228us/step - loss: 1.3217e-04 - val_loss: 2.5999e-04\n",
      "Epoch 1661/2000\n",
      "3622/3622 [==============================] - 1s 228us/step - loss: 1.2279e-04 - val_loss: 2.2671e-04\n",
      "Epoch 1662/2000\n",
      "3622/3622 [==============================] - 1s 229us/step - loss: 1.1820e-04 - val_loss: 2.2721e-04\n",
      "Epoch 1663/2000\n",
      "3622/3622 [==============================] - 1s 228us/step - loss: 1.3404e-04 - val_loss: 3.5056e-04\n",
      "Epoch 1664/2000\n",
      "3622/3622 [==============================] - 1s 229us/step - loss: 1.3954e-04 - val_loss: 2.5669e-04\n",
      "Epoch 1665/2000\n",
      "3622/3622 [==============================] - 1s 230us/step - loss: 1.2840e-04 - val_loss: 2.5565e-04\n",
      "Epoch 1666/2000\n",
      "3622/3622 [==============================] - 1s 229us/step - loss: 1.2735e-04 - val_loss: 2.4627e-04\n",
      "Epoch 1667/2000\n",
      "3622/3622 [==============================] - 1s 230us/step - loss: 1.2691e-04 - val_loss: 3.1350e-04\n",
      "Epoch 1668/2000\n",
      "3622/3622 [==============================] - 1s 233us/step - loss: 1.4668e-04 - val_loss: 2.2968e-04\n",
      "Epoch 1669/2000\n",
      "3622/3622 [==============================] - 1s 234us/step - loss: 1.2392e-04 - val_loss: 3.6503e-04\n",
      "Epoch 1670/2000\n",
      "3622/3622 [==============================] - 1s 230us/step - loss: 1.5683e-04 - val_loss: 2.2566e-04\n",
      "Epoch 1671/2000\n",
      "3622/3622 [==============================] - 1s 228us/step - loss: 1.4335e-04 - val_loss: 2.9324e-04\n",
      "Epoch 1672/2000\n",
      "3622/3622 [==============================] - 1s 229us/step - loss: 1.4160e-04 - val_loss: 2.2153e-04\n",
      "Epoch 1673/2000\n",
      "3622/3622 [==============================] - 1s 232us/step - loss: 1.2317e-04 - val_loss: 2.6612e-04\n",
      "Epoch 1674/2000\n",
      "3622/3622 [==============================] - 1s 230us/step - loss: 1.1941e-04 - val_loss: 2.3834e-04\n",
      "Epoch 1675/2000\n",
      "3622/3622 [==============================] - 1s 230us/step - loss: 1.2195e-04 - val_loss: 2.4056e-04\n",
      "Epoch 1676/2000\n",
      "3622/3622 [==============================] - 1s 229us/step - loss: 1.1869e-04 - val_loss: 2.5917e-04\n",
      "Epoch 1677/2000\n",
      "3622/3622 [==============================] - 1s 229us/step - loss: 1.2382e-04 - val_loss: 2.6580e-04\n",
      "Epoch 1678/2000\n",
      "3622/3622 [==============================] - 1s 229us/step - loss: 1.2389e-04 - val_loss: 2.4357e-04\n",
      "Epoch 1679/2000\n",
      "3622/3622 [==============================] - 1s 229us/step - loss: 1.2477e-04 - val_loss: 2.6126e-04\n",
      "Epoch 1680/2000\n",
      "3622/3622 [==============================] - 1s 230us/step - loss: 1.2550e-04 - val_loss: 3.3720e-04\n",
      "Epoch 1681/2000\n",
      "3622/3622 [==============================] - 1s 230us/step - loss: 1.4198e-04 - val_loss: 2.7053e-04\n",
      "Epoch 1682/2000\n",
      "3622/3622 [==============================] - 1s 229us/step - loss: 1.4934e-04 - val_loss: 2.3295e-04\n",
      "Epoch 1683/2000\n",
      "3622/3622 [==============================] - 1s 229us/step - loss: 1.3410e-04 - val_loss: 2.3898e-04\n",
      "Epoch 1684/2000\n",
      "3622/3622 [==============================] - 1s 228us/step - loss: 1.1822e-04 - val_loss: 2.2774e-04\n",
      "Epoch 1685/2000\n",
      "3622/3622 [==============================] - 1s 229us/step - loss: 1.1890e-04 - val_loss: 2.9106e-04\n",
      "Epoch 1686/2000\n",
      "3622/3622 [==============================] - 1s 230us/step - loss: 1.2281e-04 - val_loss: 2.3033e-04\n",
      "Epoch 1687/2000\n",
      "3622/3622 [==============================] - 1s 231us/step - loss: 1.2698e-04 - val_loss: 2.8312e-04\n",
      "Epoch 1688/2000\n",
      "3622/3622 [==============================] - 1s 230us/step - loss: 1.2771e-04 - val_loss: 2.3624e-04\n",
      "Epoch 1689/2000\n",
      "3622/3622 [==============================] - 1s 227us/step - loss: 1.2924e-04 - val_loss: 2.9072e-04\n",
      "Epoch 1690/2000\n",
      "3622/3622 [==============================] - 1s 229us/step - loss: 1.3076e-04 - val_loss: 3.1819e-04\n",
      "Epoch 1691/2000\n",
      "3622/3622 [==============================] - 1s 229us/step - loss: 1.3798e-04 - val_loss: 2.6491e-04\n",
      "Epoch 1692/2000\n",
      "3622/3622 [==============================] - 1s 228us/step - loss: 1.3267e-04 - val_loss: 2.4479e-04\n",
      "Epoch 1693/2000\n",
      "3622/3622 [==============================] - 1s 234us/step - loss: 1.2180e-04 - val_loss: 2.4076e-04\n",
      "Epoch 1694/2000\n",
      "3622/3622 [==============================] - 1s 229us/step - loss: 1.1793e-04 - val_loss: 2.4126e-04\n",
      "Epoch 1695/2000\n",
      "3622/3622 [==============================] - 1s 229us/step - loss: 1.1913e-04 - val_loss: 2.4353e-04\n",
      "Epoch 1696/2000\n",
      "3622/3622 [==============================] - 1s 230us/step - loss: 1.1483e-04 - val_loss: 2.4634e-04\n",
      "Epoch 1697/2000\n",
      "3622/3622 [==============================] - 1s 229us/step - loss: 1.1978e-04 - val_loss: 2.4150e-04\n",
      "Epoch 1698/2000\n",
      "3622/3622 [==============================] - 1s 229us/step - loss: 1.2528e-04 - val_loss: 3.1027e-04\n",
      "Epoch 1699/2000\n",
      "3622/3622 [==============================] - 1s 230us/step - loss: 1.3709e-04 - val_loss: 2.6073e-04\n",
      "Epoch 1700/2000\n",
      "3622/3622 [==============================] - 1s 228us/step - loss: 1.3768e-04 - val_loss: 2.3996e-04\n",
      "Epoch 1701/2000\n",
      "3622/3622 [==============================] - 1s 228us/step - loss: 1.2363e-04 - val_loss: 2.4406e-04\n",
      "Epoch 1702/2000\n",
      "3622/3622 [==============================] - 1s 231us/step - loss: 1.1778e-04 - val_loss: 2.3637e-04\n",
      "Epoch 1703/2000\n",
      "3622/3622 [==============================] - 1s 229us/step - loss: 1.2798e-04 - val_loss: 2.3555e-04\n",
      "Epoch 1704/2000\n",
      "3622/3622 [==============================] - 1s 229us/step - loss: 1.2770e-04 - val_loss: 2.6809e-04\n",
      "Epoch 1705/2000\n",
      "3622/3622 [==============================] - 1s 235us/step - loss: 1.3283e-04 - val_loss: 2.2180e-04\n",
      "Epoch 1706/2000\n",
      "3622/3622 [==============================] - 1s 230us/step - loss: 1.3247e-04 - val_loss: 2.3132e-04\n",
      "Epoch 1707/2000\n",
      "3622/3622 [==============================] - 1s 230us/step - loss: 1.2845e-04 - val_loss: 2.7512e-04\n",
      "Epoch 1708/2000\n",
      "3622/3622 [==============================] - 1s 229us/step - loss: 1.4163e-04 - val_loss: 2.2662e-04\n",
      "Epoch 1709/2000\n",
      "3622/3622 [==============================] - 1s 230us/step - loss: 1.4070e-04 - val_loss: 3.0459e-04\n",
      "Epoch 1710/2000\n",
      "3622/3622 [==============================] - 1s 230us/step - loss: 1.4166e-04 - val_loss: 2.6144e-04\n",
      "Epoch 1711/2000\n",
      "3622/3622 [==============================] - 1s 226us/step - loss: 1.2410e-04 - val_loss: 2.2682e-04\n",
      "Epoch 1712/2000\n",
      "3622/3622 [==============================] - 1s 233us/step - loss: 1.1589e-04 - val_loss: 2.6566e-04\n",
      "Epoch 1713/2000\n",
      "3622/3622 [==============================] - 1s 229us/step - loss: 1.2100e-04 - val_loss: 2.3929e-04\n",
      "Epoch 1714/2000\n",
      "3622/3622 [==============================] - 1s 228us/step - loss: 1.2420e-04 - val_loss: 2.2438e-04\n",
      "Epoch 1715/2000\n",
      "3622/3622 [==============================] - 1s 230us/step - loss: 1.1719e-04 - val_loss: 2.3924e-04\n",
      "Epoch 1716/2000\n",
      "3622/3622 [==============================] - 1s 235us/step - loss: 1.1805e-04 - val_loss: 2.4086e-04\n",
      "Epoch 1717/2000\n",
      "3622/3622 [==============================] - 1s 228us/step - loss: 1.2200e-04 - val_loss: 2.5373e-04\n",
      "Epoch 1718/2000\n",
      "3622/3622 [==============================] - 1s 229us/step - loss: 1.2179e-04 - val_loss: 2.7909e-04\n",
      "Epoch 1719/2000\n",
      "3622/3622 [==============================] - 1s 230us/step - loss: 1.3600e-04 - val_loss: 2.3437e-04\n",
      "Epoch 1720/2000\n",
      "3622/3622 [==============================] - 1s 230us/step - loss: 1.1980e-04 - val_loss: 2.4657e-04\n",
      "Epoch 1721/2000\n",
      "3622/3622 [==============================] - 1s 231us/step - loss: 1.1344e-04 - val_loss: 2.5147e-04\n",
      "Epoch 1722/2000\n",
      "3622/3622 [==============================] - 1s 229us/step - loss: 1.1730e-04 - val_loss: 2.4329e-04\n",
      "Epoch 1723/2000\n",
      "3622/3622 [==============================] - 1s 230us/step - loss: 1.1792e-04 - val_loss: 2.8568e-04\n",
      "Epoch 1724/2000\n",
      "3622/3622 [==============================] - 1s 230us/step - loss: 1.3297e-04 - val_loss: 2.4978e-04\n",
      "Epoch 1725/2000\n",
      "3622/3622 [==============================] - 1s 230us/step - loss: 1.1271e-04 - val_loss: 2.4828e-04\n",
      "Epoch 1726/2000\n",
      "3622/3622 [==============================] - 1s 228us/step - loss: 1.1453e-04 - val_loss: 2.2742e-04\n",
      "Epoch 1727/2000\n",
      "3622/3622 [==============================] - 1s 230us/step - loss: 1.1521e-04 - val_loss: 2.4280e-04\n",
      "Epoch 1728/2000\n",
      "3622/3622 [==============================] - 1s 227us/step - loss: 1.1226e-04 - val_loss: 2.2948e-04\n",
      "Epoch 1729/2000\n",
      "3622/3622 [==============================] - 1s 229us/step - loss: 1.1303e-04 - val_loss: 2.3870e-04\n",
      "Epoch 1730/2000\n",
      "3622/3622 [==============================] - 1s 229us/step - loss: 1.2047e-04 - val_loss: 2.3217e-04\n",
      "Epoch 1731/2000\n",
      "3622/3622 [==============================] - 1s 235us/step - loss: 1.1301e-04 - val_loss: 2.8029e-04\n",
      "Epoch 1732/2000\n",
      "3622/3622 [==============================] - 1s 229us/step - loss: 1.3310e-04 - val_loss: 2.9758e-04\n",
      "Epoch 1733/2000\n",
      "3622/3622 [==============================] - 1s 231us/step - loss: 1.4341e-04 - val_loss: 2.4068e-04\n",
      "Epoch 1734/2000\n",
      "3622/3622 [==============================] - 1s 230us/step - loss: 1.2643e-04 - val_loss: 3.6295e-04\n",
      "Epoch 1735/2000\n",
      "3622/3622 [==============================] - 1s 229us/step - loss: 1.3840e-04 - val_loss: 3.3061e-04\n",
      "Epoch 1736/2000\n",
      "3622/3622 [==============================] - 1s 230us/step - loss: 1.4545e-04 - val_loss: 2.7789e-04\n",
      "Epoch 1737/2000\n",
      "3622/3622 [==============================] - 1s 232us/step - loss: 1.2746e-04 - val_loss: 2.4780e-04\n",
      "Epoch 1738/2000\n",
      "3622/3622 [==============================] - 1s 229us/step - loss: 1.2349e-04 - val_loss: 2.4967e-04\n",
      "Epoch 1739/2000\n",
      "3622/3622 [==============================] - 1s 228us/step - loss: 1.2052e-04 - val_loss: 2.5679e-04\n",
      "Epoch 1740/2000\n",
      "3622/3622 [==============================] - 1s 232us/step - loss: 1.2385e-04 - val_loss: 3.3943e-04\n",
      "Epoch 1741/2000\n",
      "3622/3622 [==============================] - 1s 233us/step - loss: 1.2815e-04 - val_loss: 2.5892e-04\n",
      "Epoch 1742/2000\n",
      "3622/3622 [==============================] - 1s 229us/step - loss: 1.2159e-04 - val_loss: 2.7800e-04\n",
      "Epoch 1743/2000\n",
      "3622/3622 [==============================] - 1s 228us/step - loss: 1.1187e-04 - val_loss: 2.7848e-04\n",
      "Epoch 1744/2000\n",
      "3622/3622 [==============================] - 1s 229us/step - loss: 1.1301e-04 - val_loss: 2.4737e-04\n",
      "Epoch 1745/2000\n",
      "3622/3622 [==============================] - 1s 230us/step - loss: 1.2229e-04 - val_loss: 2.7389e-04\n",
      "Epoch 1746/2000\n",
      "3622/3622 [==============================] - 1s 229us/step - loss: 1.1436e-04 - val_loss: 2.6830e-04\n",
      "Epoch 1747/2000\n",
      "3622/3622 [==============================] - 1s 230us/step - loss: 1.1774e-04 - val_loss: 2.7645e-04\n",
      "Epoch 1748/2000\n",
      "3622/3622 [==============================] - 1s 229us/step - loss: 1.1631e-04 - val_loss: 2.4739e-04\n",
      "Epoch 1749/2000\n",
      "3622/3622 [==============================] - 1s 228us/step - loss: 1.1815e-04 - val_loss: 2.3820e-04\n",
      "Epoch 1750/2000\n",
      "3622/3622 [==============================] - 1s 234us/step - loss: 1.1430e-04 - val_loss: 2.3931e-04\n",
      "Epoch 1751/2000\n",
      "3622/3622 [==============================] - 1s 228us/step - loss: 1.1308e-04 - val_loss: 2.4852e-04\n",
      "Epoch 1752/2000\n",
      "3622/3622 [==============================] - 1s 230us/step - loss: 1.1136e-04 - val_loss: 2.5986e-04\n",
      "Epoch 1753/2000\n",
      "3622/3622 [==============================] - 1s 230us/step - loss: 1.1132e-04 - val_loss: 2.4478e-04\n",
      "Epoch 1754/2000\n",
      "3622/3622 [==============================] - 1s 229us/step - loss: 1.1315e-04 - val_loss: 2.9849e-04\n",
      "Epoch 1755/2000\n",
      "3622/3622 [==============================] - 1s 229us/step - loss: 1.3780e-04 - val_loss: 3.0547e-04\n",
      "Epoch 1756/2000\n",
      "3622/3622 [==============================] - 1s 228us/step - loss: 1.3120e-04 - val_loss: 3.6570e-04\n",
      "Epoch 1757/2000\n",
      "3622/3622 [==============================] - 1s 229us/step - loss: 1.6091e-04 - val_loss: 2.4147e-04\n",
      "Epoch 1758/2000\n",
      "3622/3622 [==============================] - 1s 230us/step - loss: 1.5229e-04 - val_loss: 3.1961e-04\n",
      "Epoch 1759/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3622/3622 [==============================] - 1s 230us/step - loss: 1.4299e-04 - val_loss: 2.6118e-04\n",
      "Epoch 1760/2000\n",
      "3622/3622 [==============================] - 1s 231us/step - loss: 1.4099e-04 - val_loss: 2.4647e-04\n",
      "Epoch 1761/2000\n",
      "3622/3622 [==============================] - 1s 227us/step - loss: 1.2078e-04 - val_loss: 2.4026e-04\n",
      "Epoch 1762/2000\n",
      "3622/3622 [==============================] - 1s 229us/step - loss: 1.0998e-04 - val_loss: 2.4524e-04\n",
      "Epoch 1763/2000\n",
      "3622/3622 [==============================] - 1s 229us/step - loss: 1.1535e-04 - val_loss: 2.9069e-04\n",
      "Epoch 1764/2000\n",
      "3622/3622 [==============================] - 1s 229us/step - loss: 1.2540e-04 - val_loss: 2.6385e-04\n",
      "Epoch 1765/2000\n",
      "3622/3622 [==============================] - 1s 228us/step - loss: 1.4112e-04 - val_loss: 2.3380e-04\n",
      "Epoch 1766/2000\n",
      "3622/3622 [==============================] - 1s 230us/step - loss: 1.2275e-04 - val_loss: 3.2562e-04\n",
      "Epoch 1767/2000\n",
      "3622/3622 [==============================] - 1s 229us/step - loss: 1.1531e-04 - val_loss: 2.9318e-04\n",
      "Epoch 1768/2000\n",
      "3622/3622 [==============================] - 1s 231us/step - loss: 1.3695e-04 - val_loss: 2.9774e-04\n",
      "Epoch 1769/2000\n",
      "3622/3622 [==============================] - 1s 233us/step - loss: 1.6120e-04 - val_loss: 2.6312e-04\n",
      "Epoch 1770/2000\n",
      "3622/3622 [==============================] - 1s 233us/step - loss: 1.3861e-04 - val_loss: 2.4358e-04\n",
      "Epoch 1771/2000\n",
      "3622/3622 [==============================] - 1s 229us/step - loss: 1.2241e-04 - val_loss: 2.3133e-04\n",
      "Epoch 1772/2000\n",
      "3622/3622 [==============================] - 1s 229us/step - loss: 1.1505e-04 - val_loss: 2.3368e-04\n",
      "Epoch 1773/2000\n",
      "3622/3622 [==============================] - 1s 228us/step - loss: 1.2643e-04 - val_loss: 2.5563e-04\n",
      "Epoch 1774/2000\n",
      "3622/3622 [==============================] - 1s 229us/step - loss: 1.2726e-04 - val_loss: 2.2193e-04\n",
      "Epoch 1775/2000\n",
      "3622/3622 [==============================] - 1s 228us/step - loss: 1.2630e-04 - val_loss: 3.0685e-04\n",
      "Epoch 1776/2000\n",
      "3622/3622 [==============================] - 1s 234us/step - loss: 1.3037e-04 - val_loss: 2.7323e-04\n",
      "Epoch 1777/2000\n",
      "3622/3622 [==============================] - 1s 232us/step - loss: 1.2650e-04 - val_loss: 2.5418e-04\n",
      "Epoch 1778/2000\n",
      "3622/3622 [==============================] - 1s 229us/step - loss: 1.2465e-04 - val_loss: 3.9692e-04\n",
      "Epoch 1779/2000\n",
      "3622/3622 [==============================] - 1s 230us/step - loss: 1.7339e-04 - val_loss: 2.4088e-04\n",
      "Epoch 1780/2000\n",
      "3622/3622 [==============================] - 1s 230us/step - loss: 1.4029e-04 - val_loss: 2.8239e-04\n",
      "Epoch 1781/2000\n",
      "3622/3622 [==============================] - 1s 231us/step - loss: 1.1891e-04 - val_loss: 2.3507e-04\n",
      "Epoch 1782/2000\n",
      "3622/3622 [==============================] - 1s 229us/step - loss: 1.1289e-04 - val_loss: 2.5909e-04\n",
      "Epoch 1783/2000\n",
      "3622/3622 [==============================] - 1s 230us/step - loss: 1.1827e-04 - val_loss: 2.3322e-04\n",
      "Epoch 1784/2000\n",
      "3622/3622 [==============================] - 1s 229us/step - loss: 1.1236e-04 - val_loss: 2.7940e-04\n",
      "Epoch 1785/2000\n",
      "3622/3622 [==============================] - 1s 230us/step - loss: 1.1413e-04 - val_loss: 2.3651e-04\n",
      "Epoch 1786/2000\n",
      "3622/3622 [==============================] - 1s 232us/step - loss: 1.1638e-04 - val_loss: 2.5401e-04\n",
      "Epoch 1787/2000\n",
      "3622/3622 [==============================] - 1s 229us/step - loss: 1.1523e-04 - val_loss: 3.0106e-04\n",
      "Epoch 1788/2000\n",
      "3622/3622 [==============================] - 1s 230us/step - loss: 1.2544e-04 - val_loss: 2.3658e-04\n",
      "Epoch 1789/2000\n",
      "3622/3622 [==============================] - 1s 232us/step - loss: 1.2080e-04 - val_loss: 4.0352e-04\n",
      "Epoch 1790/2000\n",
      "3622/3622 [==============================] - 1s 230us/step - loss: 1.5039e-04 - val_loss: 2.5638e-04\n",
      "Epoch 1791/2000\n",
      "3622/3622 [==============================] - 1s 231us/step - loss: 1.3770e-04 - val_loss: 4.0185e-04\n",
      "Epoch 1792/2000\n",
      "3622/3622 [==============================] - 1s 235us/step - loss: 1.5749e-04 - val_loss: 2.4500e-04\n",
      "Epoch 1793/2000\n",
      "3622/3622 [==============================] - 1s 231us/step - loss: 1.2838e-04 - val_loss: 2.3484e-04\n",
      "Epoch 1794/2000\n",
      "3622/3622 [==============================] - 1s 234us/step - loss: 1.1593e-04 - val_loss: 2.5386e-04\n",
      "Epoch 1795/2000\n",
      "3622/3622 [==============================] - 1s 235us/step - loss: 1.1291e-04 - val_loss: 2.3969e-04\n",
      "Epoch 1796/2000\n",
      "3622/3622 [==============================] - 1s 229us/step - loss: 1.1531e-04 - val_loss: 2.4273e-04\n",
      "Epoch 1797/2000\n",
      "3622/3622 [==============================] - 1s 233us/step - loss: 1.2100e-04 - val_loss: 2.5855e-04\n",
      "Epoch 1798/2000\n",
      "3622/3622 [==============================] - 1s 235us/step - loss: 1.2504e-04 - val_loss: 2.3400e-04\n",
      "Epoch 1799/2000\n",
      "3622/3622 [==============================] - 1s 234us/step - loss: 1.1461e-04 - val_loss: 2.6187e-04\n",
      "Epoch 1800/2000\n",
      "3622/3622 [==============================] - 1s 232us/step - loss: 1.2654e-04 - val_loss: 2.3379e-04\n",
      "Epoch 1801/2000\n",
      "3622/3622 [==============================] - 1s 233us/step - loss: 1.1990e-04 - val_loss: 2.4925e-04\n",
      "Epoch 1802/2000\n",
      "3622/3622 [==============================] - 1s 234us/step - loss: 1.2057e-04 - val_loss: 2.3702e-04\n",
      "Epoch 1803/2000\n",
      "3622/3622 [==============================] - 1s 232us/step - loss: 1.3025e-04 - val_loss: 2.5439e-04\n",
      "Epoch 1804/2000\n",
      "3622/3622 [==============================] - 1s 232us/step - loss: 1.2112e-04 - val_loss: 2.4390e-04\n",
      "Epoch 1805/2000\n",
      "3622/3622 [==============================] - 1s 234us/step - loss: 1.1652e-04 - val_loss: 3.0621e-04\n",
      "Epoch 1806/2000\n",
      "3622/3622 [==============================] - 1s 234us/step - loss: 1.2276e-04 - val_loss: 2.5470e-04\n",
      "Epoch 1807/2000\n",
      "3622/3622 [==============================] - 1s 238us/step - loss: 1.1334e-04 - val_loss: 2.3401e-04\n",
      "Epoch 1808/2000\n",
      "3622/3622 [==============================] - 1s 237us/step - loss: 1.0527e-04 - val_loss: 2.4410e-04\n",
      "Epoch 1809/2000\n",
      "3622/3622 [==============================] - 1s 234us/step - loss: 1.0689e-04 - val_loss: 2.3626e-04\n",
      "Epoch 1810/2000\n",
      "3622/3622 [==============================] - 1s 233us/step - loss: 1.1040e-04 - val_loss: 2.4833e-04\n",
      "Epoch 1811/2000\n",
      "3622/3622 [==============================] - 1s 233us/step - loss: 1.1231e-04 - val_loss: 2.5604e-04\n",
      "Epoch 1812/2000\n",
      "3622/3622 [==============================] - 1s 241us/step - loss: 1.0788e-04 - val_loss: 2.3187e-04\n",
      "Epoch 1813/2000\n",
      "3622/3622 [==============================] - 1s 237us/step - loss: 1.0887e-04 - val_loss: 2.6400e-04\n",
      "Epoch 1814/2000\n",
      "3622/3622 [==============================] - 1s 235us/step - loss: 1.1636e-04 - val_loss: 2.6426e-04\n",
      "Epoch 1815/2000\n",
      "3622/3622 [==============================] - 1s 234us/step - loss: 1.1603e-04 - val_loss: 2.4464e-04\n",
      "Epoch 1816/2000\n",
      "3622/3622 [==============================] - 1s 232us/step - loss: 1.0736e-04 - val_loss: 3.2992e-04\n",
      "Epoch 1817/2000\n",
      "3622/3622 [==============================] - 1s 237us/step - loss: 1.0619e-04 - val_loss: 2.5483e-04\n",
      "Epoch 1818/2000\n",
      "3622/3622 [==============================] - 1s 238us/step - loss: 1.0820e-04 - val_loss: 3.0394e-04\n",
      "Epoch 1819/2000\n",
      "3622/3622 [==============================] - 1s 231us/step - loss: 1.2431e-04 - val_loss: 2.3923e-04\n",
      "Epoch 1820/2000\n",
      "3622/3622 [==============================] - 1s 237us/step - loss: 1.1295e-04 - val_loss: 2.5684e-04\n",
      "Epoch 1821/2000\n",
      "3622/3622 [==============================] - 1s 236us/step - loss: 1.0471e-04 - val_loss: 2.7798e-04\n",
      "Epoch 1822/2000\n",
      "3622/3622 [==============================] - 1s 228us/step - loss: 1.0210e-04 - val_loss: 2.7295e-04\n",
      "Epoch 1823/2000\n",
      "3622/3622 [==============================] - 1s 232us/step - loss: 1.1667e-04 - val_loss: 3.1136e-04\n",
      "Epoch 1824/2000\n",
      "3622/3622 [==============================] - 1s 228us/step - loss: 1.2092e-04 - val_loss: 2.6799e-04\n",
      "Epoch 1825/2000\n",
      "3622/3622 [==============================] - 1s 231us/step - loss: 1.3146e-04 - val_loss: 2.3077e-04\n",
      "Epoch 1826/2000\n",
      "3622/3622 [==============================] - 1s 234us/step - loss: 1.0875e-04 - val_loss: 3.2820e-04\n",
      "Epoch 1827/2000\n",
      "3622/3622 [==============================] - 1s 232us/step - loss: 1.4098e-04 - val_loss: 2.3101e-04\n",
      "Epoch 1828/2000\n",
      "3622/3622 [==============================] - 1s 229us/step - loss: 1.3425e-04 - val_loss: 2.4470e-04\n",
      "Epoch 1829/2000\n",
      "3622/3622 [==============================] - 1s 230us/step - loss: 1.1913e-04 - val_loss: 2.5950e-04\n",
      "Epoch 1830/2000\n",
      "3622/3622 [==============================] - 1s 230us/step - loss: 1.1919e-04 - val_loss: 3.3677e-04\n",
      "Epoch 1831/2000\n",
      "3622/3622 [==============================] - 1s 229us/step - loss: 1.1739e-04 - val_loss: 3.4983e-04\n",
      "Epoch 1832/2000\n",
      "3622/3622 [==============================] - 1s 232us/step - loss: 1.3323e-04 - val_loss: 2.4195e-04\n",
      "Epoch 1833/2000\n",
      "3622/3622 [==============================] - 1s 229us/step - loss: 1.1888e-04 - val_loss: 2.5721e-04\n",
      "Epoch 1834/2000\n",
      "3622/3622 [==============================] - 1s 228us/step - loss: 1.1751e-04 - val_loss: 2.9616e-04\n",
      "Epoch 1835/2000\n",
      "3622/3622 [==============================] - 1s 230us/step - loss: 1.2668e-04 - val_loss: 2.3273e-04\n",
      "Epoch 1836/2000\n",
      "3622/3622 [==============================] - 1s 228us/step - loss: 1.1377e-04 - val_loss: 2.7146e-04\n",
      "Epoch 1837/2000\n",
      "3622/3622 [==============================] - 1s 229us/step - loss: 1.3455e-04 - val_loss: 2.4164e-04\n",
      "Epoch 1838/2000\n",
      "3622/3622 [==============================] - 1s 228us/step - loss: 1.3960e-04 - val_loss: 2.5002e-04\n",
      "Epoch 1839/2000\n",
      "3622/3622 [==============================] - 1s 230us/step - loss: 1.1380e-04 - val_loss: 2.5692e-04\n",
      "Epoch 1840/2000\n",
      "3622/3622 [==============================] - 1s 229us/step - loss: 1.1556e-04 - val_loss: 2.8208e-04\n",
      "Epoch 1841/2000\n",
      "3622/3622 [==============================] - 1s 231us/step - loss: 1.1480e-04 - val_loss: 2.6113e-04\n",
      "Epoch 1842/2000\n",
      "3622/3622 [==============================] - 1s 230us/step - loss: 1.0960e-04 - val_loss: 4.0338e-04\n",
      "Epoch 1843/2000\n",
      "3622/3622 [==============================] - 1s 230us/step - loss: 1.3096e-04 - val_loss: 3.0140e-04\n",
      "Epoch 1844/2000\n",
      "3622/3622 [==============================] - 1s 229us/step - loss: 1.4155e-04 - val_loss: 2.9607e-04\n",
      "Epoch 1845/2000\n",
      "3622/3622 [==============================] - 1s 234us/step - loss: 1.2877e-04 - val_loss: 2.8098e-04\n",
      "Epoch 1846/2000\n",
      "3622/3622 [==============================] - 1s 232us/step - loss: 1.1632e-04 - val_loss: 2.9101e-04\n",
      "Epoch 1847/2000\n",
      "3622/3622 [==============================] - 1s 233us/step - loss: 1.0567e-04 - val_loss: 2.7752e-04\n",
      "Epoch 1848/2000\n",
      "3622/3622 [==============================] - 1s 234us/step - loss: 1.2072e-04 - val_loss: 2.7263e-04\n",
      "Epoch 1849/2000\n",
      "3622/3622 [==============================] - 1s 232us/step - loss: 1.1876e-04 - val_loss: 3.4979e-04\n",
      "Epoch 1850/2000\n",
      "3622/3622 [==============================] - 1s 229us/step - loss: 1.3227e-04 - val_loss: 2.4685e-04\n",
      "Epoch 1851/2000\n",
      "3622/3622 [==============================] - 1s 231us/step - loss: 1.1687e-04 - val_loss: 3.1950e-04\n",
      "Epoch 1852/2000\n",
      "3622/3622 [==============================] - 1s 230us/step - loss: 1.3830e-04 - val_loss: 2.7370e-04\n",
      "Epoch 1853/2000\n",
      "3622/3622 [==============================] - 1s 230us/step - loss: 1.1716e-04 - val_loss: 2.4690e-04\n",
      "Epoch 1854/2000\n",
      "3622/3622 [==============================] - 1s 229us/step - loss: 1.3495e-04 - val_loss: 3.3103e-04\n",
      "Epoch 1855/2000\n",
      "3622/3622 [==============================] - 1s 230us/step - loss: 1.4344e-04 - val_loss: 2.6761e-04\n",
      "Epoch 1856/2000\n",
      "3622/3622 [==============================] - 1s 230us/step - loss: 1.0844e-04 - val_loss: 2.6098e-04\n",
      "Epoch 1857/2000\n",
      "3622/3622 [==============================] - 1s 229us/step - loss: 1.0927e-04 - val_loss: 2.8037e-04\n",
      "Epoch 1858/2000\n",
      "3622/3622 [==============================] - 1s 230us/step - loss: 1.0944e-04 - val_loss: 2.6353e-04\n",
      "Epoch 1859/2000\n",
      "3622/3622 [==============================] - 1s 229us/step - loss: 1.1666e-04 - val_loss: 2.6570e-04\n",
      "Epoch 1860/2000\n",
      "3622/3622 [==============================] - 1s 229us/step - loss: 1.1006e-04 - val_loss: 3.5543e-04\n",
      "Epoch 1861/2000\n",
      "3622/3622 [==============================] - 1s 229us/step - loss: 1.2878e-04 - val_loss: 2.5520e-04\n",
      "Epoch 1862/2000\n",
      "3622/3622 [==============================] - 1s 229us/step - loss: 1.1313e-04 - val_loss: 2.5974e-04\n",
      "Epoch 1863/2000\n",
      "3622/3622 [==============================] - 1s 229us/step - loss: 1.1174e-04 - val_loss: 2.9214e-04\n",
      "Epoch 1864/2000\n",
      "3622/3622 [==============================] - 1s 236us/step - loss: 1.0778e-04 - val_loss: 3.2202e-04\n",
      "Epoch 1865/2000\n",
      "3622/3622 [==============================] - 1s 232us/step - loss: 1.2258e-04 - val_loss: 3.2506e-04\n",
      "Epoch 1866/2000\n",
      "3622/3622 [==============================] - 1s 230us/step - loss: 1.2890e-04 - val_loss: 2.5649e-04\n",
      "Epoch 1867/2000\n",
      "3622/3622 [==============================] - 1s 229us/step - loss: 1.3123e-04 - val_loss: 3.0307e-04\n",
      "Epoch 1868/2000\n",
      "3622/3622 [==============================] - 1s 229us/step - loss: 1.2504e-04 - val_loss: 2.9348e-04\n",
      "Epoch 1869/2000\n",
      "3622/3622 [==============================] - 1s 229us/step - loss: 1.2193e-04 - val_loss: 2.5211e-04\n",
      "Epoch 1870/2000\n",
      "3622/3622 [==============================] - 1s 231us/step - loss: 1.0883e-04 - val_loss: 2.8033e-04\n",
      "Epoch 1871/2000\n",
      "3622/3622 [==============================] - 1s 230us/step - loss: 1.0475e-04 - val_loss: 2.3735e-04\n",
      "Epoch 1872/2000\n",
      "3622/3622 [==============================] - 1s 228us/step - loss: 1.1061e-04 - val_loss: 2.7502e-04\n",
      "Epoch 1873/2000\n",
      "3622/3622 [==============================] - 1s 229us/step - loss: 1.0156e-04 - val_loss: 2.6425e-04\n",
      "Epoch 1874/2000\n",
      "3622/3622 [==============================] - 1s 227us/step - loss: 1.0101e-04 - val_loss: 2.7344e-04\n",
      "Epoch 1875/2000\n",
      "3622/3622 [==============================] - 1s 229us/step - loss: 1.1576e-04 - val_loss: 2.6562e-04\n",
      "Epoch 1876/2000\n",
      "3622/3622 [==============================] - 1s 229us/step - loss: 1.0258e-04 - val_loss: 2.6388e-04\n",
      "Epoch 1877/2000\n",
      "3622/3622 [==============================] - 1s 229us/step - loss: 1.0752e-04 - val_loss: 2.5594e-04\n",
      "Epoch 1878/2000\n",
      "3622/3622 [==============================] - 1s 230us/step - loss: 1.0742e-04 - val_loss: 3.2215e-04\n",
      "Epoch 1879/2000\n",
      "3622/3622 [==============================] - 1s 229us/step - loss: 1.1009e-04 - val_loss: 2.7886e-04\n",
      "Epoch 1880/2000\n",
      "3622/3622 [==============================] - 1s 229us/step - loss: 1.0853e-04 - val_loss: 2.7595e-04\n",
      "Epoch 1881/2000\n",
      "3622/3622 [==============================] - 1s 230us/step - loss: 9.9261e-05 - val_loss: 2.5264e-04\n",
      "Epoch 1882/2000\n",
      "3622/3622 [==============================] - 1s 230us/step - loss: 9.9570e-05 - val_loss: 2.9343e-04\n",
      "Epoch 1883/2000\n",
      "3622/3622 [==============================] - 1s 236us/step - loss: 1.0449e-04 - val_loss: 2.4015e-04\n",
      "Epoch 1884/2000\n",
      "3622/3622 [==============================] - 1s 234us/step - loss: 1.1146e-04 - val_loss: 2.5129e-04\n",
      "Epoch 1885/2000\n",
      "3622/3622 [==============================] - 1s 228us/step - loss: 1.0249e-04 - val_loss: 2.4335e-04\n",
      "Epoch 1886/2000\n",
      "3622/3622 [==============================] - 1s 228us/step - loss: 1.0620e-04 - val_loss: 2.6893e-04\n",
      "Epoch 1887/2000\n",
      "3622/3622 [==============================] - 1s 229us/step - loss: 1.0084e-04 - val_loss: 2.6445e-04\n",
      "Epoch 1888/2000\n",
      "3622/3622 [==============================] - 1s 230us/step - loss: 9.8266e-05 - val_loss: 2.7884e-04\n",
      "Epoch 1889/2000\n",
      "3622/3622 [==============================] - 1s 228us/step - loss: 1.0007e-04 - val_loss: 2.8375e-04\n",
      "Epoch 1890/2000\n",
      "3622/3622 [==============================] - 1s 227us/step - loss: 9.6377e-05 - val_loss: 3.5003e-04\n",
      "Epoch 1891/2000\n",
      "3622/3622 [==============================] - 1s 230us/step - loss: 1.1113e-04 - val_loss: 2.8644e-04\n",
      "Epoch 1892/2000\n",
      "3622/3622 [==============================] - 1s 229us/step - loss: 1.0721e-04 - val_loss: 2.5698e-04\n",
      "Epoch 1893/2000\n",
      "3622/3622 [==============================] - 1s 231us/step - loss: 1.0116e-04 - val_loss: 2.6773e-04\n",
      "Epoch 1894/2000\n",
      "3622/3622 [==============================] - 1s 231us/step - loss: 9.5148e-05 - val_loss: 2.6097e-04\n",
      "Epoch 1895/2000\n",
      "3622/3622 [==============================] - 1s 229us/step - loss: 1.0018e-04 - val_loss: 3.3682e-04\n",
      "Epoch 1896/2000\n",
      "3622/3622 [==============================] - 1s 230us/step - loss: 9.5490e-05 - val_loss: 2.8559e-04\n",
      "Epoch 1897/2000\n",
      "3622/3622 [==============================] - 1s 228us/step - loss: 1.0070e-04 - val_loss: 3.1069e-04\n",
      "Epoch 1898/2000\n",
      "3622/3622 [==============================] - 1s 229us/step - loss: 1.1256e-04 - val_loss: 2.9578e-04\n",
      "Epoch 1899/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3622/3622 [==============================] - 1s 229us/step - loss: 1.1533e-04 - val_loss: 2.5617e-04\n",
      "Epoch 1900/2000\n",
      "3622/3622 [==============================] - 1s 231us/step - loss: 1.0605e-04 - val_loss: 3.6396e-04\n",
      "Epoch 1901/2000\n",
      "3622/3622 [==============================] - 1s 228us/step - loss: 9.9150e-05 - val_loss: 2.9558e-04\n",
      "Epoch 1902/2000\n",
      "3622/3622 [==============================] - 1s 232us/step - loss: 1.0728e-04 - val_loss: 3.6489e-04\n",
      "Epoch 1903/2000\n",
      "3622/3622 [==============================] - 1s 234us/step - loss: 1.3255e-04 - val_loss: 2.5269e-04\n",
      "Epoch 1904/2000\n",
      "3622/3622 [==============================] - 1s 229us/step - loss: 1.4545e-04 - val_loss: 3.9457e-04\n",
      "Epoch 1905/2000\n",
      "3622/3622 [==============================] - 1s 229us/step - loss: 1.1956e-04 - val_loss: 3.1534e-04\n",
      "Epoch 1906/2000\n",
      "3622/3622 [==============================] - 1s 229us/step - loss: 1.2670e-04 - val_loss: 3.0703e-04\n",
      "Epoch 1907/2000\n",
      "3622/3622 [==============================] - 1s 230us/step - loss: 1.1458e-04 - val_loss: 2.6026e-04\n",
      "Epoch 1908/2000\n",
      "3622/3622 [==============================] - 1s 229us/step - loss: 1.0529e-04 - val_loss: 2.7612e-04\n",
      "Epoch 1909/2000\n",
      "3622/3622 [==============================] - 1s 230us/step - loss: 9.6501e-05 - val_loss: 2.5695e-04\n",
      "Epoch 1910/2000\n",
      "3622/3622 [==============================] - 1s 229us/step - loss: 9.6630e-05 - val_loss: 2.7663e-04\n",
      "Epoch 1911/2000\n",
      "3622/3622 [==============================] - 1s 230us/step - loss: 9.2467e-05 - val_loss: 2.6643e-04\n",
      "Epoch 1912/2000\n",
      "3622/3622 [==============================] - 1s 229us/step - loss: 1.0135e-04 - val_loss: 3.6394e-04\n",
      "Epoch 1913/2000\n",
      "3622/3622 [==============================] - 1s 231us/step - loss: 1.0685e-04 - val_loss: 2.5367e-04\n",
      "Epoch 1914/2000\n",
      "3622/3622 [==============================] - 1s 229us/step - loss: 1.0473e-04 - val_loss: 2.6511e-04\n",
      "Epoch 1915/2000\n",
      "3622/3622 [==============================] - 1s 229us/step - loss: 1.0791e-04 - val_loss: 4.1353e-04\n",
      "Epoch 1916/2000\n",
      "3622/3622 [==============================] - 1s 230us/step - loss: 1.3199e-04 - val_loss: 3.4202e-04\n",
      "Epoch 1917/2000\n",
      "3622/3622 [==============================] - 1s 230us/step - loss: 1.0769e-04 - val_loss: 2.4931e-04\n",
      "Epoch 1918/2000\n",
      "3622/3622 [==============================] - 1s 229us/step - loss: 1.0468e-04 - val_loss: 2.8285e-04\n",
      "Epoch 1919/2000\n",
      "3622/3622 [==============================] - 1s 235us/step - loss: 1.0168e-04 - val_loss: 3.1049e-04\n",
      "Epoch 1920/2000\n",
      "3622/3622 [==============================] - 1s 230us/step - loss: 1.0617e-04 - val_loss: 2.5698e-04\n",
      "Epoch 1921/2000\n",
      "3622/3622 [==============================] - 1s 230us/step - loss: 1.0154e-04 - val_loss: 3.6832e-04\n",
      "Epoch 1922/2000\n",
      "3622/3622 [==============================] - 1s 235us/step - loss: 1.3252e-04 - val_loss: 2.6935e-04\n",
      "Epoch 1923/2000\n",
      "3622/3622 [==============================] - 1s 230us/step - loss: 1.2646e-04 - val_loss: 3.2231e-04\n",
      "Epoch 1924/2000\n",
      "3622/3622 [==============================] - 1s 228us/step - loss: 1.2654e-04 - val_loss: 3.5021e-04\n",
      "Epoch 1925/2000\n",
      "3622/3622 [==============================] - 1s 229us/step - loss: 1.1888e-04 - val_loss: 2.4988e-04\n",
      "Epoch 1926/2000\n",
      "3622/3622 [==============================] - 1s 228us/step - loss: 1.0160e-04 - val_loss: 2.4158e-04\n",
      "Epoch 1927/2000\n",
      "3622/3622 [==============================] - 1s 228us/step - loss: 9.9176e-05 - val_loss: 2.6964e-04\n",
      "Epoch 1928/2000\n",
      "3622/3622 [==============================] - 1s 230us/step - loss: 9.7537e-05 - val_loss: 3.6986e-04\n",
      "Epoch 1929/2000\n",
      "3622/3622 [==============================] - 1s 228us/step - loss: 1.2083e-04 - val_loss: 3.4015e-04\n",
      "Epoch 1930/2000\n",
      "3622/3622 [==============================] - 1s 230us/step - loss: 1.0780e-04 - val_loss: 3.1086e-04\n",
      "Epoch 1931/2000\n",
      "3622/3622 [==============================] - 1s 230us/step - loss: 1.0295e-04 - val_loss: 2.7598e-04\n",
      "Epoch 1932/2000\n",
      "3622/3622 [==============================] - 1s 230us/step - loss: 1.1583e-04 - val_loss: 3.5943e-04\n",
      "Epoch 1933/2000\n",
      "3622/3622 [==============================] - 1s 230us/step - loss: 1.1801e-04 - val_loss: 2.8579e-04\n",
      "Epoch 1934/2000\n",
      "3622/3622 [==============================] - 1s 231us/step - loss: 1.1434e-04 - val_loss: 2.7719e-04\n",
      "Epoch 1935/2000\n",
      "3622/3622 [==============================] - 1s 229us/step - loss: 1.0297e-04 - val_loss: 2.5483e-04\n",
      "Epoch 1936/2000\n",
      "3622/3622 [==============================] - 1s 229us/step - loss: 9.8327e-05 - val_loss: 3.0872e-04\n",
      "Epoch 1937/2000\n",
      "3622/3622 [==============================] - 1s 229us/step - loss: 9.8558e-05 - val_loss: 2.5999e-04\n",
      "Epoch 1938/2000\n",
      "3622/3622 [==============================] - 1s 230us/step - loss: 1.1481e-04 - val_loss: 3.8285e-04\n",
      "Epoch 1939/2000\n",
      "3622/3622 [==============================] - 1s 231us/step - loss: 1.2021e-04 - val_loss: 2.5487e-04\n",
      "Epoch 1940/2000\n",
      "3622/3622 [==============================] - 1s 230us/step - loss: 1.0452e-04 - val_loss: 4.2344e-04\n",
      "Epoch 1941/2000\n",
      "3622/3622 [==============================] - 1s 233us/step - loss: 1.2153e-04 - val_loss: 2.8701e-04\n",
      "Epoch 1942/2000\n",
      "3622/3622 [==============================] - 1s 230us/step - loss: 1.1986e-04 - val_loss: 2.9567e-04\n",
      "Epoch 1943/2000\n",
      "3622/3622 [==============================] - 1s 229us/step - loss: 1.1665e-04 - val_loss: 3.5708e-04\n",
      "Epoch 1944/2000\n",
      "3622/3622 [==============================] - 1s 228us/step - loss: 1.3503e-04 - val_loss: 4.2974e-04\n",
      "Epoch 1945/2000\n",
      "3622/3622 [==============================] - 1s 230us/step - loss: 1.1281e-04 - val_loss: 3.0631e-04\n",
      "Epoch 1946/2000\n",
      "3622/3622 [==============================] - 1s 229us/step - loss: 1.1215e-04 - val_loss: 2.5505e-04\n",
      "Epoch 1947/2000\n",
      "3622/3622 [==============================] - 1s 228us/step - loss: 1.0196e-04 - val_loss: 2.7732e-04\n",
      "Epoch 1948/2000\n",
      "3622/3622 [==============================] - 1s 231us/step - loss: 1.0292e-04 - val_loss: 3.3461e-04\n",
      "Epoch 1949/2000\n",
      "3622/3622 [==============================] - 1s 229us/step - loss: 1.2262e-04 - val_loss: 2.4791e-04\n",
      "Epoch 1950/2000\n",
      "3622/3622 [==============================] - 1s 229us/step - loss: 1.3524e-04 - val_loss: 2.5219e-04\n",
      "Epoch 1951/2000\n",
      "3622/3622 [==============================] - 1s 229us/step - loss: 1.1726e-04 - val_loss: 2.5219e-04\n",
      "Epoch 1952/2000\n",
      "3622/3622 [==============================] - 1s 230us/step - loss: 1.1228e-04 - val_loss: 2.3858e-04\n",
      "Epoch 1953/2000\n",
      "3622/3622 [==============================] - 1s 228us/step - loss: 1.0730e-04 - val_loss: 3.1217e-04\n",
      "Epoch 1954/2000\n",
      "3622/3622 [==============================] - 1s 229us/step - loss: 1.0974e-04 - val_loss: 2.7628e-04\n",
      "Epoch 1955/2000\n",
      "3622/3622 [==============================] - 1s 237us/step - loss: 9.8725e-05 - val_loss: 2.6114e-04\n",
      "Epoch 1956/2000\n",
      "3622/3622 [==============================] - 1s 231us/step - loss: 9.8791e-05 - val_loss: 2.7228e-04\n",
      "Epoch 1957/2000\n",
      "3622/3622 [==============================] - 1s 231us/step - loss: 1.0114e-04 - val_loss: 3.1637e-04\n",
      "Epoch 1958/2000\n",
      "3622/3622 [==============================] - 1s 229us/step - loss: 1.2369e-04 - val_loss: 2.7749e-04\n",
      "Epoch 1959/2000\n",
      "3622/3622 [==============================] - 1s 228us/step - loss: 1.1287e-04 - val_loss: 2.5106e-04\n",
      "Epoch 1960/2000\n",
      "3622/3622 [==============================] - 1s 236us/step - loss: 9.3459e-05 - val_loss: 2.5364e-04\n",
      "Epoch 1961/2000\n",
      "3622/3622 [==============================] - 1s 229us/step - loss: 9.7070e-05 - val_loss: 3.0340e-04\n",
      "Epoch 1962/2000\n",
      "3622/3622 [==============================] - 1s 229us/step - loss: 1.0514e-04 - val_loss: 2.5152e-04\n",
      "Epoch 1963/2000\n",
      "3622/3622 [==============================] - 1s 230us/step - loss: 1.0539e-04 - val_loss: 3.1912e-04\n",
      "Epoch 1964/2000\n",
      "3622/3622 [==============================] - 1s 229us/step - loss: 9.5262e-05 - val_loss: 2.9456e-04\n",
      "Epoch 1965/2000\n",
      "3622/3622 [==============================] - 1s 229us/step - loss: 9.0831e-05 - val_loss: 2.7652e-04\n",
      "Epoch 1966/2000\n",
      "3622/3622 [==============================] - 1s 229us/step - loss: 8.9186e-05 - val_loss: 2.7979e-04\n",
      "Epoch 1967/2000\n",
      "3622/3622 [==============================] - 1s 229us/step - loss: 9.2351e-05 - val_loss: 2.9695e-04\n",
      "Epoch 1968/2000\n",
      "3622/3622 [==============================] - 1s 230us/step - loss: 8.6838e-05 - val_loss: 2.9255e-04\n",
      "Epoch 1969/2000\n",
      "3622/3622 [==============================] - 1s 229us/step - loss: 9.2651e-05 - val_loss: 2.4967e-04\n",
      "Epoch 1970/2000\n",
      "3622/3622 [==============================] - 1s 229us/step - loss: 1.0815e-04 - val_loss: 3.4969e-04\n",
      "Epoch 1971/2000\n",
      "3622/3622 [==============================] - 1s 230us/step - loss: 1.4183e-04 - val_loss: 2.8718e-04\n",
      "Epoch 1972/2000\n",
      "3622/3622 [==============================] - 1s 237us/step - loss: 1.3123e-04 - val_loss: 3.1360e-04\n",
      "Epoch 1973/2000\n",
      "3622/3622 [==============================] - 1s 236us/step - loss: 1.1782e-04 - val_loss: 2.9348e-04\n",
      "Epoch 1974/2000\n",
      "3622/3622 [==============================] - 1s 230us/step - loss: 1.2455e-04 - val_loss: 3.3682e-04\n",
      "Epoch 1975/2000\n",
      "3622/3622 [==============================] - 1s 230us/step - loss: 1.1242e-04 - val_loss: 2.8611e-04\n",
      "Epoch 1976/2000\n",
      "3622/3622 [==============================] - 1s 228us/step - loss: 9.9927e-05 - val_loss: 2.7116e-04\n",
      "Epoch 1977/2000\n",
      "3622/3622 [==============================] - 1s 229us/step - loss: 9.6656e-05 - val_loss: 2.6395e-04\n",
      "Epoch 1978/2000\n",
      "3622/3622 [==============================] - 1s 229us/step - loss: 9.1372e-05 - val_loss: 3.1382e-04\n",
      "Epoch 1979/2000\n",
      "3622/3622 [==============================] - 1s 236us/step - loss: 9.3264e-05 - val_loss: 2.6363e-04\n",
      "Epoch 1980/2000\n",
      "3622/3622 [==============================] - 1s 231us/step - loss: 9.4105e-05 - val_loss: 2.8760e-04\n",
      "Epoch 1981/2000\n",
      "3622/3622 [==============================] - 1s 228us/step - loss: 9.3080e-05 - val_loss: 2.8897e-04\n",
      "Epoch 1982/2000\n",
      "3622/3622 [==============================] - 1s 231us/step - loss: 8.8562e-05 - val_loss: 3.8243e-04\n",
      "Epoch 1983/2000\n",
      "3622/3622 [==============================] - 1s 227us/step - loss: 8.8762e-05 - val_loss: 2.5753e-04\n",
      "Epoch 1984/2000\n",
      "3622/3622 [==============================] - 1s 230us/step - loss: 9.3141e-05 - val_loss: 4.7560e-04\n",
      "Epoch 1985/2000\n",
      "3622/3622 [==============================] - 1s 229us/step - loss: 1.0579e-04 - val_loss: 2.6336e-04\n",
      "Epoch 1986/2000\n",
      "3622/3622 [==============================] - 1s 229us/step - loss: 9.6254e-05 - val_loss: 3.1183e-04\n",
      "Epoch 1987/2000\n",
      "3622/3622 [==============================] - 1s 229us/step - loss: 1.1681e-04 - val_loss: 3.5915e-04\n",
      "Epoch 1988/2000\n",
      "3622/3622 [==============================] - 1s 230us/step - loss: 1.3386e-04 - val_loss: 2.7818e-04\n",
      "Epoch 1989/2000\n",
      "3622/3622 [==============================] - 1s 229us/step - loss: 1.0170e-04 - val_loss: 2.6444e-04\n",
      "Epoch 1990/2000\n",
      "3622/3622 [==============================] - 1s 227us/step - loss: 1.1145e-04 - val_loss: 2.5903e-04\n",
      "Epoch 1991/2000\n",
      "3622/3622 [==============================] - 1s 237us/step - loss: 9.5749e-05 - val_loss: 2.5471e-04\n",
      "Epoch 1992/2000\n",
      "3622/3622 [==============================] - 1s 229us/step - loss: 1.0746e-04 - val_loss: 3.2710e-04\n",
      "Epoch 1993/2000\n",
      "3622/3622 [==============================] - 1s 230us/step - loss: 1.1891e-04 - val_loss: 4.3137e-04\n",
      "Epoch 1994/2000\n",
      "3622/3622 [==============================] - 1s 229us/step - loss: 1.4314e-04 - val_loss: 2.7230e-04\n",
      "Epoch 1995/2000\n",
      "3622/3622 [==============================] - 1s 230us/step - loss: 1.2094e-04 - val_loss: 3.0133e-04\n",
      "Epoch 1996/2000\n",
      "3622/3622 [==============================] - 1s 228us/step - loss: 1.0353e-04 - val_loss: 2.5722e-04\n",
      "Epoch 1997/2000\n",
      "3622/3622 [==============================] - 1s 227us/step - loss: 9.9746e-05 - val_loss: 2.5246e-04\n",
      "Epoch 1998/2000\n",
      "3622/3622 [==============================] - 1s 232us/step - loss: 9.2173e-05 - val_loss: 2.6822e-04\n",
      "Epoch 1999/2000\n",
      "3622/3622 [==============================] - 1s 231us/step - loss: 8.7626e-05 - val_loss: 2.6164e-04\n",
      "Epoch 2000/2000\n",
      "3622/3622 [==============================] - 1s 227us/step - loss: 1.0457e-04 - val_loss: 3.1452e-04\n"
     ]
    }
   ],
   "source": [
    "final_model = build_lstm(**params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'lstmsize': 126,\n",
       " 'density': 154,\n",
       " 'optimizer': 'adam',\n",
       " 'full_density': True,\n",
       " 'activation': 'relu',\n",
       " 'shuffle': True,\n",
       " 'twice': False,\n",
       " 'callbacks': [<keras.callbacks.callbacks.ModelCheckpoint at 0x1b2aeb24dc8>]}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_model[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_182\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_270 (LSTM)              (None, 126)               66528     \n",
      "_________________________________________________________________\n",
      "dense_915 (Dense)            (None, 154)               19558     \n",
      "_________________________________________________________________\n",
      "dense_916 (Dense)            (None, 77)                11935     \n",
      "_________________________________________________________________\n",
      "dense_917 (Dense)            (None, 38)                2964      \n",
      "_________________________________________________________________\n",
      "dense_918 (Dense)            (None, 19)                741       \n",
      "_________________________________________________________________\n",
      "dense_919 (Dense)            (None, 1)                 20        \n",
      "=================================================================\n",
      "Total params: 101,746\n",
      "Trainable params: 101,746\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "final_model[0].summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_model[0].load_weights('./checkpoints/IBM_2days/IBM.(best).hdf5')\n",
    "y_predicted = final_model[0].predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_y_test = y_normaliser.inverse_transform(y_predicted)\n",
    "true_y_test = y_normaliser.inverse_transform(Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE is 7.85\n",
      "Medium error is 1.86\n"
     ]
    }
   ],
   "source": [
    "loss = np.mean((true_y_test - predicted_y_test)**2)\n",
    "medium_error = np.mean(abs(true_y_test - predicted_y_test))\n",
    "print(f'MSE is {loss:.2f}\\nMedium error is {medium_error:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "up_down_test = []\n",
    "up_down_predicted = []\n",
    "\n",
    "for y,x in zip(Y_test,X_test):\n",
    "    if y[0] > x[-1][0]: up_down_test.append(1)\n",
    "    elif y[0] < x[-1][0]: up_down_test.append(-1)\n",
    "    else: up_down_test.append(0)\n",
    "        \n",
    "for y,x in zip(y_predicted,X_test):\n",
    "    if y[0] > x[-1][0]: up_down_predicted.append(1)\n",
    "    elif y[0] < x[-1][0]: up_down_predicted.append(-1)\n",
    "    else: up_down_predicted.append(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trend accuracy is: 65.44%\n"
     ]
    }
   ],
   "source": [
    "print(f'Trend accuracy is: {(sum(1 for r,p in zip(up_down_test,up_down_predicted) if r == p)/len(up_down_test))*100:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for upward trend is: 62.72%\n",
      "Accuracy for downward trend is: 68.34%\n"
     ]
    }
   ],
   "source": [
    "print(f'Accuracy for upward trend is: {(sum(1 for r,p in zip(up_down_test,up_down_predicted) if r == 1 and p == 1)/sum(1 for v in up_down_test if v == 1))*100:.2f}%')\n",
    "print(f'Accuracy for downward trend is: {(sum(1 for r,p in zip(up_down_test,up_down_predicted) if r == -1 and p == -1)/sum(1 for v in up_down_test if v == -1))*100:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Predictions over the last 92 days in test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 3200x2400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from matplotlib.pyplot import figure\n",
    "figure(num=None, figsize=(40, 30), dpi=80, facecolor='w', edgecolor='k')\n",
    "\n",
    "plt.plot(range(92), true_y_test[-92:])\n",
    "plt.plot(range(92), predicted_y_test[-92:])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Last 8 days + prediction of last 4 days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 3200x2400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "figure(num=None, figsize=(40, 30), dpi=80, facecolor='w', edgecolor='k')\n",
    "\n",
    "plt.plot(range(8), true_y_test[-8:])\n",
    "plt.plot(range(8), np.append(true_y_test[-8:-4], predicted_y_test[-4:]))\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
